[
  "Topic 9\nQuestion #1\nFor this question, refer to the TerramEarth case study. To be compliant with European GDPR regulation, TerramEarth is required\nto delete data generated from its \nEuropean customers after a period of 36 months when it contains personal data. In the new architecture, this data will be\nstored in both Cloud Storage and \nBigQuery. What should you do? \nA. \nCreate a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use\ngsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.\nB. \nCreate a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use\ngsutil to create a SetStorageClass to NONE action when with an Age condition of 36 months.\nC. \nCreate a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For\nCloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.\nMost Voted\nD. \nCreate a BigQuery time-partitioned table for the European data, and set the partition expiration \nperiod to 36 months. For\nCloud Storage, use gsutil to create a SetStorageClass to NONE action with an Age condition of 36 months.\nCorrect Answer:\n \nC",
  "Topic 9\nQuestion #2\nFor this question, refer to the TerramEarth case study. TerramEarth has decided to store data files in Cloud Storage. You need\nto configure Cloud Storage lifecycle rule to store 1 year of data and minimize file storage cost. \nWhich two actions should you take? \nA. \nCreate a Cloud Storage lifecycle rule with Age: \nג\n€30\nג\n ,€Storage Class: \nג\n€Standard\nג\n ,€and Action: \nג\n€Set to Coldline\nג\n,€\nand create a second GCS life-cycle rule with Age: \nג\n€365\nג\n ,€Storage Class: \nג\n€Coldline\nג\n ,€and Action: \nג\n€Delete\nג\n .€\nMost Voted\nB. \nCreate a Cloud Storage lifecycle rule with Age: \nג\n€30\nג\n ,€Storage Class: \nג\n€Coldline\nג\n ,€and Action: \nג\n€Set to Nearline\nג\n,€\nand create a second GCS life-cycle rule with Age: \nג\n€91\nג\n ,€Storage Class: \nג\n€Coldline\nג\n ,€and Action: \nג\n€Set to Nearline\nג\n.€\nC. \nCreate a Cloud Storage lifecycle rule with Age: \nג\n€90\nג\n ,€Storage Class: \nג\n€Standard\nג\n ,€and Action: \nג\n€Set to Nearline\nג\n,€\nand create a second GCS life-cycle rule with Age: \nג\n€91\nג\n ,€Storage Class: \nג\n€Nearline\nג\n ,€and Action: \nג\n€Set to Coldline\nג\n.€\nD. \nCreate a Cloud Storage lifecycle rule with Age: \nג\n€30\nג\n ,€Storage Class: \nג\n€Standard\nג\n ,€and Action: \nג\n€Set to Coldline\nג\n,€\nand create a second GCS life-cycle rule with Age: \nג\n€365\nג\n ,€Storage Class: \nג\n€Nearline\nג\n ,€and Action: \nג\n€Delete\nג\n.€\nCorrect Answer:\n \nA",
  "Topic 9\nQuestion #3\nFor this question, refer to the TerramEarth case study. You need to implement a reliable, scalable GCP solution for the data\nwarehouse for your company, \nTerramEarth. \nConsidering the TerramEarth business and technical requirements, what should you do? \nA. \nReplace the existing data warehouse with BigQuery. Use table partitioning. \nMost Voted\nB. \nReplace the existing data warehouse with a Compute Engine instance with 96 CPUs.\nC. \nReplace the existing data warehouse with BigQuery. Use federated data sources.\nD. \nReplace the existing data warehouse with a Compute Engine instance with 96 CPUs. Add an additional Compute Engine\npreemptible instance with 32 CPUs.\nCorrect Answer:\n \nA",
  "Topic 9\nQuestion #4\nFor this question, refer to the TerramEarth case study. A new architecture that writes all incoming data to BigQuery has been\nintroduced. You notice that the data is dirty, and want to ensure data quality on an automated daily basis while managing cost.\nWhat should you do? \nA. \nSet up a streaming Cloud Dataflow job, receiving data by the ingestion process. Clean the data in a Cloud Dataflow\npipeline.\nB. \nCreate a Cloud Function that reads data from BigQuery and cleans it. Trigger the Cloud Function from a Compute Engine\ninstance.\nC. \nCreate a SQL statement on the data in BigQuery, and save it as a view. Run the view daily, and save the result to a new\ntable.\nD. \nUse Cloud Dataprep and configure the BigQuery tables as the source. Schedule a daily job to clean the data. \nMost Voted\nCorrect Answer:\n \nD",
  "Topic 9\nQuestion #5\nFor this question, refer to the TerramEarth case study. Considering the technical requirements, how should you reduce the\nunplanned vehicle downtime in GCP? \nA. \nUse BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud\nPub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting. \nMost Voted\nB. \nUse BigQuery as the data warehouse. Connect all vehicles to the network and upload gzip files to a Multi-Regional Cloud\nStorage bucket using gcloud. Use Google Data Studio for analysis and reporting.\nC. \nUse Cloud Dataproc Hive as the data warehouse. Upload gzip files to a Multi-Regional Cloud Storage bucket. Upload this\ndata into BigQuery using gcloud. Use Google Data Studio for analysis and reporting.\nD. \nUse Cloud Dataproc Hive as the data warehouse. Directly stream data into partitioned Hive tables. Use Pig scripts to\nanalyze data.\nCorrect Answer:\n \nA",
  "Topic 9\nQuestion #6\nFor this question, refer to the TerramEarth case study. You are asked to design a new architecture for the ingestion of the data\nof the 200,000 vehicles that are connected to a cellular network. You want to follow Google-recommended practices. \nConsidering the technical requirements, which components should you use for the ingestion of the data? \nA. \nGoogle Kubernetes Engine with an SSL Ingress\nB. \nCloud IoT Core with public/private key pairs \nMost Voted\nC. \nCompute Engine with project-wide SSH keys\nD. \nCompute Engine with specific SSH keys\nCorrect Answer:\n \nB"
]