[
  "Topic 8\nQuestion #1\nTerramEarth's CTO wants to use the raw data from connected vehicles to help identify approximately when a vehicle in the\nfield will have a catastrophic failure. \nYou want to allow analysts to centrally query the vehicle data. \nWhich architecture should you recommend? \nA. \n \nB. \n  \nC. \n \nD. \n  \nCorrect Answer:\n \nA",
  "Topic 8\nQuestion #2\nThe TerramEarth development team wants to create an API to meet the company's business requirements. You want the\ndevelopment team to focus their development effort on business value versus creating a custom framework. \nWhich method should they use? \nA. \nUse Google App Engine with Google Cloud Endpoints. Focus on an API for dealers and partners \nMost Voted\nB. \nUse Google App Engine with a JAX-RS Jersey Java-based framework. Focus on an API for the public\nC. \nUse Google App Engine with the Swagger (Open API Specification) framework. Focus on an API for the public\nD. \nUse Google Container Engine with a Django Python container. Focus on an API for the public\nE. \nUse Google Container Engine with a Tomcat container with the Swagger (Open API Specification) framework. Focus on an\nAPI for dealers and partners\nCorrect Answer:\n \nA",
  "Topic 8\nQuestion #3\nYour development team has created a structured API to retrieve vehicle data. They want to allow third parties to develop tools\nfor dealerships that use this vehicle event data. You want to support delegated authorization against this data. \nWhat should you do? \nA. \nBuild or leverage an OAuth-compatible access control system \nMost Voted\nB. \nBuild SAML 2.0 SSO compatibility into your authentication system\nC. \nRestrict data access based on the source IP address of the partner systems\nD. \nCreate secondary credentials for each dealer that can be given to the trusted third party\nCorrect Answer:\n \nA",
  "Topic 8\nQuestion #4\nTerramEarth plans to connect all 20 million vehicles in the field to the cloud. This increases the volume to 20 million 600 byte\nrecords a second for 40 TB an hour. \nHow should you design the data ingestion? \nA. \nVehicles write data directly to GCS\nB. \nVehicles write data directly to Google Cloud Pub/Sub \nMost Voted\nC. \nVehicles stream data directly to Google BigQuery\nD. \nVehicles continue to write data using the existing system (FTP)\nCorrect Answer:\n \nB",
  "Topic 8\nQuestion #5\nYou analyzed TerramEarth's business requirement to reduce downtime, and found that they can achieve a majority of time\nsaving by reducing customer's wait time for parts. You decided to focus on reduction of the 3 weeks aggregate reporting time. \nWhich modifications to the company's processes should you recommend? \nA. \nMigrate from CSV to binary format, migrate from FTP to SFTP transport, and develop machine learning analysis of\nmetrics\nB. \nMigrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of\nmetrics\nC. \nIncrease fleet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning\nanalysis of metrics \nMost Voted\nD. \nMigrate from FTP to SFTP transport, develop machine learning analysis of metrics, and increase dealer local inventory\nby a fixed factor\nCorrect Answer:\n \nC",
  "Topic 8\nQuestion #6\nWhich of TerramEarth's legacy enterprise processes will experience significant change as a result of increased Google Cloud\nPlatform adoption? \nA. \nOpex/capex allocation, LAN changes, capacity planning\nB. \nCapacity planning, TCO calculations, opex/capex allocation \nMost Voted\nC. \nCapacity planning, utilization measurement, data center expansion\nD. \nData Center expansion, TCO calculations, utilization measurement\nCorrect Answer:\n \nB",
  "Topic 8\nQuestion #7\nTo speed up data retrieval, more vehicles will be upgraded to cellular connections and be able to transmit data to the ETL\nprocess. The current FTP process is error-prone and restarts the data transfer from the start of the file when connections fail,\nwhich happens often. You want to improve the reliability of the solution and minimize data transfer time on the cellular\nconnections. \nWhat should you do? \nA. \nUse one Google Container Engine cluster of FTP servers. Save the data to a Multi-Regional bucket. Run the ETL process\nusing data in the bucket\nB. \nUse multiple Google Container Engine clusters running FTP servers located in different regions. Save the data to Multi-\nRegional buckets in US, EU, and Asia. Run the ETL process using the data in the bucket\nC. \nDirectly transfer the files to different Google Cloud Multi-Regional Storage bucket locations in US, EU, and Asia using\nGoogle APIs over HTTP(S). Run the ETL process using the data in the bucket\nD. \nDirectly transfer the files to a different Google Cloud Regional Storage bucket location in US, EU, and Asia using Google\nAPIs over HTTP(S). Run the ETL process to retrieve the data from each Regional bucket \nMost Voted\nCorrect Answer:\n \nD",
  "Topic 8\nQuestion #8\nTerramEarth's 20 million vehicles are scattered around the world. Based on the vehicle's location, its telemetry data is stored\nin a Google Cloud Storage (GCS) regional bucket (US, Europe, or Asia). The CTO has asked you to run a report on the raw\ntelemetry data to determine why vehicles are breaking down after 100 K miles. You want to run this job on all the data. \nWhat is the most cost-effective way to run this job? \nA. \nMove all the data into 1 zone, then launch a Cloud Dataproc cluster to run the job\nB. \nMove all the data into 1 region, then launch a Google Cloud Dataproc cluster to run the job\nC. \nLaunch a cluster in each region to preprocess and compress the raw data, then move the data into a multi-region bucket\nand use a Dataproc cluster to finish the job\nD. \nLaunch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and\nuse a Cloud Dataproc cluster to finish the job \nMost Voted\nCorrect Answer:\n \nD",
  "Topic 8\nQuestion #9\nTerramEarth has equipped all connected trucks with servers and sensors to collect telemetry data. Next year they want to use\nthe data to train machine learning models. They want to store this data in the cloud while reducing costs. \nWhat should they do? \nA. \nHave the vehicle's computer compress the data in hourly snapshots, and store it in a Google Cloud Storage (GCS)\nNearline bucket\nB. \nPush the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Google\nBigQuery\nC. \nPush the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Cloud Bigtable\nD. \nHave the vehicle's computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket \nMost Voted\nCorrect Answer:\n \nD",
  "Topic 8\nQuestion #10\nYour agricultural division is experimenting with fully autonomous vehicles. You want your architecture to promote strong\nsecurity during vehicle operation. \nWhich two architectures should you consider? (Choose two.) \nA. \nTreat every micro service call between modules on the vehicle as untrusted. \nMost Voted\nB. \nRequire IPv6 for connectivity to ensure a secure address space.\nC. \nUse a trusted platform module (TPM) and verify firmware and binaries on boot. \nMost Voted\nD. \nUse a functional programming language to isolate code execution cycles.\nE. \nUse multiple connectivity subsystems for redundancy.\nF. \nEnclose the vehicle's drive electronics in a Faraday cage to isolate chips.\nCorrect Answer:\n \nAC",
  "Topic 8\nQuestion #11\nOperational parameters such as oil pressure are adjustable on each of TerramEarth's vehicles to increase their efficiency,\ndepending on their environmental conditions. Your primary goal is to increase the operating efficiency of all 20 million cellular\nand unconnected vehicles in the field. \nHow can you accomplish this goal? \nA. \nHave you engineers inspect the data for patterns, and then create an algorithm with rules that make operational\nadjustments automatically\nB. \nCapture all operating data, train machine learning models that identify ideal operations, and run locally to make\noperational adjustments automatically \nMost Voted\nC. \nImplement a Google Cloud Dataflow streaming job with a sliding window, and use Google Cloud Messaging (GCM) to\nmake operational adjustments automatically\nD. \nCapture all operating data, train machine learning models that identify ideal operations, and host in Google Cloud\nMachine Learning (ML) Platform to make operational adjustments automatically\nCorrect Answer:\n \nB"
]