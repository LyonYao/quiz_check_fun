 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #1
Your company has decided to make a major revision of their API in order to create better experiences for their developers.
They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the
new API. They want to keep the same SSL and DNS records in place to serve both APIs. 
What should they do? 
A. 
Configure a new load balancer for the new version of the API
B. 
Reconfigure old clients to use a new endpoint for the new API
C. 
Have the old API forward traffic to the new API based on the path
D. 
Use separate backend pools for each API path behind the load balancer 
Most Voted
Correct Answer:
 
D 
Comments
shandy
shandy
 
Highly Voted
 
3 months, 2 weeks ago
D is the answer because HTTP(S) load balancer can direct traffic reaching a single IP to different backends based on the
incoming URL. A is not correct because configuring a new load balancer would require a new or different SSL and DNS records
which conflicts with the requirements to keep the same SSL and DNS records. B is not correct because it goes against the
requirements. The company wants to keep the old API available while new customers and testers try the new API. C is not correct
because it is not a requirement to decommission the implementation behind the old API. Moreover, it introduces unnecessary risk
in case bugs or incompatibilities are discovered in the new API.
upvoted 
105 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
D is right
upvoted 
3 
times
AWS56
AWS56
 
Highly Voted
 
5 years, 1 month ago
agreed, The answer is D
upvoted 
20 
times
XiaobinJiang
XiaobinJiang
 
Most Recent
 
1 week ago
Selected Answer: 
D
Community vote distribution
D (100%)Selected Answer: 
D
D is right!
upvoted 
1 
times
FI22
FI22
 
3 weeks, 4 days ago
Selected Answer: 
D
D is good for keep the same records.
upvoted 
1 
times
Ghorbel
Ghorbel
 
1 month ago
Selected Answer: 
D
Using separate backend pools for each API version behind a single load balancer fulfills all requirements:
Single SSL and DNS Endpoint:
Both API versions share the same SSL certificate and DNS records, so users and developers don’t need to reconfigure their
endpoints.
The load balancer handles routing transparently.
Separation of APIs:
Different backend pools allow for clear segregation between the old and new API versions.
Each API can be deployed, updated, and scaled independently.
Traffic Management:
The load balancer can route traffic based on request paths (e.g., /v1 for the old API and /v2 for the new API).
This allows smooth coexistence of the APIs while developers gradually adopt the new version.
upvoted 
1 
times
ddatta
ddatta
 
1 month, 2 weeks ago
Selected Answer: 
D
You have to split the traffic
upvoted 
1 
times
devenderpraksh
devenderpraksh
 
1 month, 2 weeks ago
Answer: D. Use separate backend pools for each API path behind the load balancer
This is the correct option because:
It maintains a single entry point with existing SSL/DNS
Allows path-based routing to direct traffic appropriately
Requires no client reconfiguration
Provides clean separation between versions
Follows common API versioning patterns
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
The best answer here is D. Use separate backend pools for each API path behind the load balancer.
Here's why:
* Maintaining the same DNS and SSL records: This requirement implies that both API versions need to be accessible through the
same domain and endpoint.
* Separate backend pools: By using separate backend pools for each API version, the load balancer can direct traffic to the
appropriate servers based on the request path. This allows for seamless co-existence of both versions without any conflict.
* Flexibility and Control: This approach provides flexibility in managing traffic, allowing you to gradually shift users to the new API or
run A/B testing.
upvoted 
1 
times
nhatpham
nhatpham
 
3 months, 2 weeks ago
You have a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in
response to the load on your application.The instances have a shutdown script that removes REDIS database entries associated
with the instance.You see that many database entries have not been removed, and 
you suspect that the shutdown script is the
problem.You need to ensure that the commands in the shutdown script are run reliably 
every time an instance is shut down. You
create a Cloud Function to remove the database entries.What should you do next?
A.A.
Modify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue.
B.
Modify the shutdown script to wait for 30 seconds before triggering the Cloud Function.
C.
Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud logging
D.
Do not use the Cloud Function. Modify the shutdown script 
to restart if it has not completed in 30 seconds. 
what is your answer?
upvoted 
2 
times
pupi08
pupi08
 
2 years, 9 months ago
c it's correct
upvoted 
1 
times
nhatpham
nhatpham
 
3 months, 2 weeks ago
Your company and one of its partners each have a Google cloud project in separate organizations.Your complny's project(prj-a)
runs in virtual Private Cloud (vpc-a). The partner's project (prj-b) runs in vpc-b. There are two instances running on vpc-a and one
instance running on vpc-b 
Subnets defined in both VPCs are not overlapping.You need to ensure that all instances communicate with each other via internal
IPs 
minimizing latency and maximizing throughput.What should you do 
A.Set up a network peering between vpc-a and vpc-b
B.Set up a VPN between vpc-a and vpc-b using cloud VPN 
C.Configure IAP TCP forwarding on the instance in vpc-b, and then launch the following gcloud command from one of the
instances 
vpc-a gcloud:
gcloud compute start-iap-tunnel INSTANCE_NAME_VPC_
--local-host-port=localhost:22
D.
1.Create an additional instance in vpc-a. 
2.Create an additional instance in vpc-b. 
3.Install OpenVPN in newly created instances 
4.Configure a VPN tunnel between vpc-a and vpc-b and b with the help of OpenVPN.
what is your answer?
upvoted 
1 
times
RGTest
RGTest
 
2 years, 7 months ago
A. VPC peering
upvoted 
2 
times
nhatpham
nhatpham
 
3 months, 2 weeks ago
Your company has an application running on App engine that allows users to upload music files and share therm with other people
.You want to allow users to upload files directly into Cloud storage from their browser session.The payload should not be passed
through the 
backend. What should you do?
A.
Set a CORS configuration in the target Cloud storage bucket where the base URL of the App Engine application is an allowed
origin.Use the Cloud Storage signed URL feature to generate a POST URL.
B.
Set a CORS configuration in the target cloud storage bucket where the base URL of the App Engine application is an allowed
origin. Assign the Cloud Storage WRITER role to users who upload files.
C.
Use the Cloud Storage Signed URL feature to generate a 
POST URL.Use App Engine default credentials to sign requests against
Cloud Storage.
D.
Assign the Cloud Storage WRITER role to users who upload files; use App Engine default credentials to sign requests against
Cloud Storage.
what is your answer
？
upvoted 
1 
times
davidbilla
davidbilla
 
2 years, 7 months ago
Must be A
upvoted 
1 
timesMQQNB
MQQNB
 
2 years, 4 months ago
C.
Use the Cloud Storage Signed URL feature to generate a POST URL
upvoted 
1 
times
snome
snome
 
2 years, 1 month ago
A, The Cross Origin Resource Sharing (CORS) spec was developed by the World Wide Web Consortium (W3C) to remove the
limit of Same Origin Policy. Cloud Storage supports this specification by allowing you to configure your buckets to support
CORS. 
https://cloud.google.com/storage/docs/cross-origin
Then for authentication I'll go with signed URL feature, it gives more security
upvoted 
1 
times
nhatpham
nhatpham
 
3 months, 2 weeks ago
You are deploying an application to Google Cloud. The of a system.The application in Google Cloud must communicate to private
network with applications in a non-Google cloud environment 
The expected average throughput is 200 kbps.The business require 
·as close to 100% system availability as possible
·cost optimization
You need to design the connectivity between the business requirements. What should you provision? 
A.
An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.
B.
A single Cloud VPN gateway connected to an on-premises VPN gateway.
C.Two Classic Cloud VPN gateways connected to two on-premises VPN gateways. Configure each classic cloud VPN gateway to
have two tunnels,each connected to different on-premises VPN gateways.
D.
Two HA Cloud VPN gateways connected to two on-premises VPN gateways. Configure each HA Cloud VPN gateway to have two
tunnels,each connected to different on-premises VPN gateways.
what is your answer?
upvoted 
1 
times
Skr6266
Skr6266
 
2 years, 9 months ago
Option D seems to be the right one..
cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#to_peer_vpn_gateways
upvoted 
2 
times
MQQNB
MQQNB
 
2 years, 4 months ago
I think it should be A. refer to the documentation, to have HA 99.99%, it needs at least 2 tunnels. And one HA VPN with one peer
VPN could have 2 tunnels. In Google Cloud, the REDUNDANCY_TYPE for this configuration takes the value
SINGLE_IP_INTERNALLY_REDUNDANT.
D. not cost optimized
B. didn't mention 2 tunnels
C. not simple
upvoted 
1 
times
maxdanny
maxdanny
 
3 months, 2 weeks ago
Selected Answer: 
D
D because This approach allows you to keep the same SSL and DNS records while directing traffic based on the API path. The
load balancer can be configured to route requests to different backend pools depending on whether the request is for the old or
new API version. This ensures that both versions are accessible under the same domain, providing a seamless transition for both
old and new users.
upvoted 
3 
times
RickMorais
RickMorais
 
6 months ago
Selected Answer: 
D
D is right
upvoted 
1 
times
SaurabhL
SaurabhL
 
7 months, 4 weeks ago
Selected Answer: 
DSelected Answer: 
D
D is the correct
upvoted 
1 
times
juanlopezcervero
juanlopezcervero
 
8 months, 2 weeks ago
D is the answer
upvoted 
1 
times
eloyus
eloyus
 
11 months, 2 weeks ago
Selected Answer: 
D
D is ok
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #2
Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24hrs a day. Your
business analysts have experience only with using a SQL interface. 
How should you store the data to optimize it for ease of analysis? 
A. 
Load data into Google BigQuery 
Most Voted
B. 
Insert data into Google Cloud SQL
C. 
Put flat files into Google Cloud Storage
D. 
Stream data into Google Cloud Datastore
Correct Answer:
 
A 
Comments
Eroc
Eroc
 
Highly Voted
 
3 months, 2 weeks ago
This question could go either way for A or B. But Big Query was designed with this in mind, according to numerous Google
presentation and videos. Cloud Datastore is a NoSQL database (https://cloud.google.com/datastore/docs/concepts/overview)
Cloud Storage does not have an SQL interface. The previous two sentences eliminate options C and D. So I'd pick "A".
upvoted 
35 
times
tartar
tartar
 
4 years, 5 months ago
A is ok
upvoted 
16 
times
0xE8D4A51000
0xE8D4A51000
 
2 years, 2 months ago
IMHO, it should be A only. The reason is that they want to perform analysis on the data and BigQuery excels in that over Cloud
SQL. You can run SQL queries in both but I BigQuery has better analytical tools. It can do ad-hoc analysis like Cloud SQL using
Cloud Standard SQL and it can do geo-spatial and ML analysis via its Cloud Standard SQL interface.
upvoted 
1 
times
0xE8D4A51000
0xE8D4A51000
 
2 years, 2 months ago
Also the question does not say whether the data is relational or not. So we cannot assume it is only relational. Therefore, for
maximum flexibility BQ is the correct option also. Note that Cloud SQL storage capacity is now at 64TB
Community vote distribution
A (97%)
B
(3%)maximum flexibility BQ is the correct option also. Note that Cloud SQL storage capacity is now at 64TB
upvoted 
5 
times
b9be167
b9be167
 
1 week, 1 day ago
Also cloudsql cannot handle multi peta byte data whereas biq query can. please correct me if i'm wrong.
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
Cloud SQL does not scale to that magnitude also Cloud SQL is not meant for OLAP
Answer is BigQuery
upvoted 
4 
times
kinghin
kinghin
 
2 years, 11 months ago
B is not correct because Cloud SQL storage limit doesn't fit the requirement.
upvoted 
13 
times
clouddude
clouddude
 
Highly Voted
 
4 years, 7 months ago
I'll go with A because BQ (and BT) are usually meant for analytics.
B isn't correct because Cloud SQL does not scale to that volume.
C isn't correct because Cloud Storage does not provide a standard SQL mechanism.
D could be right but it sounds off because of the analytics requirement.
upvoted 
14 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 10 hours ago
Selected Answer: 
A
A is the answer.
upvoted 
1 
times
XiaobinJiang
XiaobinJiang
 
1 week ago
Selected Answer: 
A
I'll go A as BQ has compatible usage of SQL
upvoted 
1 
times
devenderpraksh
devenderpraksh
 
1 month, 2 weeks ago
Answer: A. Load data into Google BigQuery
This is the correct choice because it's the only option that meets all requirements:
Can handle multi-petabyte scale
Provides the required SQL interface for analysts
Ensures 24/7 availability
Optimized for analytical queries
Serverless and automatically scales
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
A
The best answer here is A. Load data into Google BigQuery.
Here's why:
1. Designed for large datasets: BigQuery is a serverless, highly scalable, and cost-effective multicloud data warehouse designed
specifically for analyzing massive datasets. Petabyte-scale data is exactly what it excels at.
2. SQL interface: Your analysts are already familiar with SQL, and BigQuery uses standard SQL, making the transition easy and
minimizing the learning curve.
3. High availability: BigQuery offers high availability with built-in redundancy and replication.
4. Performance: BigQuery is optimized for analytical queries and can handle complex queries across massive datasets very
efficiently.
upvoted 
1 
times
dev_evening
dev_evening
 
3 months, 1 week agodev_evening
dev_evening
 
3 months, 1 week ago
Selected Answer: 
A
A, BigQuery is more suitable for analysis. While Cloud SQL can work, it's more generic
upvoted 
1 
times
amxexam
amxexam
 
3 months, 2 weeks ago
Let's go with option elimination
A. Load data into Google BigQuery
>>Big Query = Analytic + SQL (Ease of using SQL) Storage hence the solution
B. Insert data into Google Cloud SQL
>> Yes you can SQL query with your own applicaiton console compared to BigQuery SQL console, and 24 hrs avalablity but you
won't have 1-2 sec response on petabytes of data, as you can do in GCP BigQuery partitioned and clustered tables.
C. Put flat files into Google Cloud Storage
>>The requirement is for analytics and SQL querying of data. You can store it in the flat file but will need to use GCP BigQuery to
do that
D. Stream data into Google Cloud Datastore
>> Only dealing with storage problems does not address analytics and SQL querying
upvoted 
2 
times
amxexam
amxexam
 
3 years, 3 months ago
Hence Option A
upvoted 
2 
times
i_am_robot
i_am_robot
 
3 months, 2 weeks ago
A. Load data into Google BigQuery
BigQuery is a fully managed, cloud-native data warehousing solution that makes it easy to analyze large and complex datasets. It
is optimized for analyzing large amounts of data quickly, and can handle petabyte-scale datasets with ease. It also has a SQL-like
interface that is familiar to business analysts, making it easy for them to query and analyze the data. Additionally, BigQuery is
highly scalable and can handle high query concurrency, making it a good choice for storing data that must be available 24/7.
Option B, inserting data into Google Cloud SQL, is not a good choice for a multi-petabyte dataset because Cloud SQL is not
designed to handle such large volumes of data. Option C, putting flat files into Cloud Storage, is also not a good choice because it
is not optimized for querying and analyzing data. Option D, streaming data into Cloud Datastore, is not a good choice because
Cloud Datastore is a NoSQL database and does not have a SQL-like interface.
upvoted 
2 
times
omermahgoub
omermahgoub
 
3 months, 2 weeks ago
A. Load data into Google BigQuery
To optimize the storage of the multi-petabyte data set for ease of analysis by business analysts who have experience only with
using a SQL interface, you should load the data into Google BigQuery. BigQuery is a fully-managed, cloud-native data warehouse
that allows you to perform fast SQL queries on large amounts of data. By loading the data into BigQuery, you can provide your
business analysts with a familiar SQL interface for querying the data, making it easier for them to analyze the data set.
Other options, such as inserting data into Google Cloud SQL, putting flat files into Google Cloud Storage, or streaming data into
Google Cloud Datastore, may not provide the necessary SQL interface or query performance for efficient analysis of the data set.
upvoted 
2 
times
juanlopezcervero
juanlopezcervero
 
8 months, 2 weeks ago
A is correct
upvoted 
1 
times
sanjeevisubhash
sanjeevisubhash
 
10 months, 1 week ago
A is ok
upvoted 
1 
times
lisabisa
lisabisa
 
10 months, 3 weeks ago
A BigQuery formula is similar to SQL.
B Google Cloud SQL cannot handle multiple petabyte data.
D Google Cloud Datastore is NoSQL.
upvoted 
1 
times
cidom35694
cidom35694
 
11 months, 1 week agocidom35694
cidom35694
 
11 months, 1 week ago
Selected Answer: 
A
A is right answer!
Get Up-to-date: https://www.pinterest.com/pin/937522847419094382
upvoted 
1 
times
hzaoui
hzaoui
 
12 months ago
Selected Answer: 
A
BigQuery
upvoted 
2 
times
yas_cloud
yas_cloud
 
1 year ago
B doesn’t fit the bill as cloud SQL is good for data up to 30 TB. I would go with option A.
upvoted 
2 
times
sam422
sam422
 
1 year ago
I got with A
BigQuery is a serverless, highly scalable data warehouse designed for analytics:
High-performance querying: BigQuery allows large datasets to be queried quickly and efficiently, making it ideal for business
analysts who need to analyze data frequently.
SQL compatibility: BigQuery uses a standard SQL interface, allowing business analysts to leverage their existing SQL skills
without needing to learn new tools or languages.
24/7 availability: BigQuery offers 99.95% availability, ensuring that your data is accessible to your business analysts whenever they
need it.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #3
The operations manager asks you for a list of recommended practices that she should consider when migrating a J2EE
application to the cloud. 
Which three practices should you recommend? (Choose three.) 
A. 
Port the application code to run on Google App Engine
B. 
Integrate Cloud Dataflow into the application to capture real-time metrics
C. 
Instrument the application with a monitoring tool like Stackdriver Debugger 
Most Voted
D. 
Select an automation framework to reliably provision the cloud infrastructure 
Most Voted
E. 
Deploy a continuous integration tool with automated testing in a staging environment 
Most Voted
F. 
Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable
Correct Answer:
 
CDE 
Comments
NapoleonBorntoparty
NapoleonBorntoparty
 
Highly Voted
 
3 months, 2 weeks ago
This is talking about the APPLICATION not the infrastructure, therefore I believe we should focus on the APP-side of things:
1. port the app to app engine for content delivery
2. add monitoring for troubleshooting
3. use a CI/CD workflow for continuous delivery w/testing for a stable application
so, for me: A, C and E should be the answers
upvoted 
63 
times
segkhachat
segkhachat
 
1 year, 9 months ago
the person who asking you recommendation is operation manager, it can be related to infrastructure
upvoted 
5 
times
amxexam
amxexam
 
Highly Voted
 
3 years, 4 months ago
Let's go with option elimination
A. Port the application code to run on Google App Engine
>> PaaS serverless managed service, so all my infra provisioning is taken care by GCP.
Community vote distribution
CDE (47%)
ADE (30%)
ACE (22%)
Other
(1%)>> PaaS serverless managed service, so all my infra provisioning is taken care by GCP.
B. Integrate Cloud Dataflow into the application to capture real-time metrics
>> Good to have 
C. Instrument the application with a monitoring tool like Stackdriver Debugger
>> Is a must for debugging issues and monitoring application logs this is now GCP Cloud monitoring and logging.
D. Select an automation framework to reliably provision the cloud infrastructure
>> App Engine is a PaaS so the infrastructure is taken care of by App Engine, I would select this if I have not selected A, hence will
eliminate this option for now
E. Deploy a continuous integration tool with automated testing in a staging environment
>> Good to have 
F. Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable
>> There is no requirement for DB enhancement hence will elimination this option
A and C are must-have 
B and E are Good to have, but E has more importance than Big
Hence will go with ACE
upvoted 
13 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 10 hours ago
Selected Answer: 
CDE
I will go for CDE.
upvoted 
1 
times
Ishu_awsguy
Ishu_awsguy
 
1 week, 2 days ago
Selected Answer: 
ACD
I would say A C D . 
E has no relevance to the question , creating a CI CD automation testing pipeline that too in staging , yes is a good thing but it is
not relavant I feel. 
Similary doing Data analysis is also irrelevant . 
hence B & E go out for the same reason .
upvoted 
1 
times
canaya
canaya
 
3 weeks, 3 days ago
Selected Answer: 
ADE
I voted ADE.
A - App Engine could be used for J2EE application
B - Dataflow is not used for real-time metrics
C - Debugger is not used for monitoring
D - Good practice
E - Good practice
F - Depends on the situation but I don't think Datastore or BigTable is a good choice for MySQL migration 
I Passed my exam from
ITExamsLab 
https://cloud.google.com/stackdriver/docs/deprecations/debugger-deprecation
upvoted 
2 
times
motimoti
motimoti
 
1 month ago
Selected Answer: 
ADE
I think C is wrong now because it was already deprecated.
https://cloud.google.com/stackdriver/docs/deprecations/debugger-deprecation
upvoted 
1 
times
KV_2001
KV_2001
 
1 month, 2 weeks ago
Would recommending Cloud debugger be the right option? As it is no longer available
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
CDE
Answer is : 
C, D, E.
C. Instrument the application with a monitoring tool like Stackdriver Debugger: 
Visibility is key in the cloud. Tools like Stackdriver
Debugger (now called Cloud Debugger) allow you to inspect the state of your application in real-time without stopping or slowing it
down. D. Select an automation framework to reliably provision the cloud infrastructure: 
Manual configuration is error-prone and doesn't
scale. Infrastructure-as-code tools like Terraform or Deployment Manager let you define your infrastructure in code, making it
repeatable, version-controlled, and easier to manage. 
E. Deploy a continuous integration tool with automated testing in a staging environment: 
A robust CI/CD pipeline is essential for
rapid and reliable deployments. Automated testing in a staging environment that mirrors production helps catch issues early,
ensuring a smoother transition and reducing the risk of production outages.
upvoted 
2 
times
Prabhuanandan
Prabhuanandan
 
1 week, 4 days ago
agreed. other options are less critical in this specific context to the operation manager.
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
agree CDE
upvoted 
1 
times
ehgm
ehgm
 
3 months, 2 weeks ago
I chose ACE, but ADE make sense.
A. Port the application code to run on Google App Engine.
Ok. It's a good practice use managed services when possible, we shouldn't worry about infrastructure.
B. Integrate Cloud Dataflow into the application to capture real-time metrics.
No Ok. It's just a J2EE application, the question says nothin about a batch or stream pipeline or real-time in insight.
C. Instrument the application with a monitoring tool like Stackdriver Debugger.
No Ok. App Engine already have natively logging and monitoring, we only have to enable debugger to fix some problem.
D. Select an automation framework to reliably provision the cloud infrastructure.
Ok. It's a good practice use IaC (infrastructure as code).
E. Deploy a continuous integration tool with automated testing in a staging environment.
Ok. It's a good practice use CI/CD and tests.
F. Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable.
No Ok. The question says nothin about Database.
upvoted 
3 
times
OrangeTiger
OrangeTiger
 
3 months, 2 weeks ago
Absolutely different
：
B No need to use DataFlow 
F No need to use NOSQL.We should use CloudSQL.
Absolutely Correct:A
、
E
A First Step.
I'm at a loss:C,D,E
C It is microservices app best practice.App Engine is microservices app.
AndIt is also written on this page.(Configuring your App with app.yaml)
D This is Correct, but App Engine does it automatically.
E Automatically test is a Java best practice.
upvoted 
2 
times
Gini
Gini
 
3 months, 2 weeks ago
Selected Answer: 
ADE
I voted ADE.
A - App Engine could be used for J2EE application
B - Dataflow is not used for real-time metrics
C - Debugger is not used for monitoring
D - Good practice
E - Good practice
F - Depends on the situation but I don't think Datastore or BigTable is a good choice for MySQL migration
upvoted 
4 
times
H_S
H_S
 
3 months, 2 weeks ago
Selected Answer: 
ADEI voted ADE.
A - App Engine could be used for J2EE application
B - Dataflow is not used for real-time metrics
C - Debugger is not used for monitoring
D - Good practice
E - Good practice
F - Depends on the situation but I don't think Datastore or BigTable is a good choice for MySQL migration
upvoted 
3 
times
SureshbabuK
SureshbabuK
 
3 months, 2 weeks ago
Selected Answer: 
ACE
I think it is ACE
A. Port the application code to run on Google App Engine - Correct - Best Practice to migrate J2EE app to APP engine.
B. Integrate Cloud Dataflow into the application to capture real-time metrics - Incorrect - Cloud Data flow in not relevant in
migrating existing J2EE app to cloud
C. Instrument the application with a monitoring tool like Stackdriver Debugger - Correct - Only because it is relevant in this use
case of migrating J2EE app to GAE have to 3 best answers. otherwise App engine is already enabled with Debugger by default,
Nothing to do extra.
D. Select an automation framework to reliably provision the cloud infrastructure - Incorrect - As APP enigne is managed service no
requirement to automate provision of infrastructure.
E. Deploy a continuous integration tool with automated testing in a staging environment - Correct - This is a best practice for using
a cloud native CI/CD
upvoted 
1 
times
nosense
nosense
 
2 years, 1 month ago
C is not correct as Stackdriver Debugger is not the monitoring tool
upvoted 
1 
times
omermahgoub
omermahgoub
 
3 months, 2 weeks ago
D: Automation frameworks can help you reliably provision the necessary cloud infrastructure for your application, ensuring that the
migration process is smooth and consistent.
E: Continuous integration tools can help you automate the testing process, ensuring that your application is properly tested before
it is deployed to the cloud. A staging environment can provide a separate testing environment that is isolated from the production
environment, allowing you to test your application before it goes live.
C: Monitoring tools like Stackdriver Debugger can help you identify and troubleshoot issues with your application after it is migrated
to the cloud. This can help ensure that your application is running smoothly and efficiently in the cloud.
upvoted 
5 
times
omermahgoub
omermahgoub
 
2 years ago
Other practices, such as porting the application code to run on Google App Engine, integrating Cloud Dataflow into the application
to capture real-time metrics, or migrating from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable,
may not be necessary for all J2EE applications and may depend on the specific requirements and goals of the migration.
upvoted 
1 
times
sam422
sam422
 
3 months, 2 weeks ago
CDE
The three recommended practices you should recommend to the operations manager are:
C. Instrument the application with a monitoring tool like Stackdriver Debugger: Monitoring the application's performance and health
is crucial for identifying and resolving issues quickly. Stackdriver Debugger provides detailed insights into the application's
behavior, helping diagnose performance bottlenecks and debug errors.
D. Select an automation framework to reliably provision the cloud infrastructure: Automating infrastructure provisioning reduces
manual effort and ensures consistent configuration across environments. This can be achieved using tools like Terraform or
Ansible.
E. Deploy a continuous integration tool with automated testing in a staging environment: Continuous integration and continuous
delivery (CI/CD) pipelines automate the build, test, and deployment process, ensuring code changes are delivered reliably and with
minimal downtime. Automated testing in a staging environment helps identify and fix regressions before they impact production.
upvoted 
2 
times
maxdanny
maxdanny
 
3 months, 2 weeks agomaxdanny
maxdanny
 
3 months, 2 weeks ago
Selected Answer: 
CDE
CDE
C: Monitoring tools like Stackdriver (now Google Cloud Operations) help track the application's performance
D: Automation frameworks (such as Terraform or Google Deployment Manager) for your environment is reproducible and can be
scaled or modified with minimal manual intervention.
E : is crucial to ensure that the application functions correctly after migration and during subsequent updates
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #4
A news feed web service has the following code running on Google App Engine. During peak load, users report that they can
see news articles they already viewed. 
What is the most likely cause of this problem? 
 
A. 
The session variable is local to just a single instance 
Most Voted
B. 
The session variable is being overwritten in Cloud Datastore
C. 
The URL of the API needs to be modified to prevent caching
D. 
The HTTP Expires header needs to be set to -1 stop caching
Correct Answer:
 
A 
Community vote distribution
A (100%)Comments
jackdbd
jackdbd
 
Highly Voted
 
3 months, 2 weeks ago
It's A. AppEngine spins up new containers automatically according to the load. During peak traffic, HTTP requests originated by the
same user could be served by different containers. Given that the variable `sessions` is recreated for each container, it might store
different data.
The problem here is that this Flask app is stateful. The `sessions` variable is the state of this app. And stateful variables in
AppEngine / Cloud Run / Cloud Functions are problematic.
A solution would be to store the session in some database (e.g. Firestore, Memorystore) and retrieve it from there. This way the
app would fetch the session from a single place and would be stateless.
upvoted 
117 
times
omermahgoub
omermahgoub
 
2 years ago
Very well stated, jack. I just wanted to point, GAE is a webserver platform anyway, so making application stateless or stateful is
up to the developer and has nothing to do with GAE. The issue is about session consistency. GAE spin new container if there's a
need, and based on the code, the session is stored locally, this means, there's no consistency between container, and there's no
grantee that the same container might serve the same user. Thank you Jack, very good explanation
upvoted 
14 
times
JoeShmoe
JoeShmoe
 
Highly Voted
 
5 years, 1 month ago
A is correct
upvoted 
29 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
A
Answer is A. The session variable is local to just a single instance.
Here's why:
* App Engine Instances: Google App Engine automatically scales your application by creating multiple instances to handle
incoming traffic. Each instance runs independently and has its own memory space.
* Session Variable: In the provided code, the `sessions` variable is a simple Python dictionary stored in the memory of each
instance. This means each instance has its own copy of the `sessions` data.
* The Problem: When a user logs in, their viewed articles are stored in the `sessions` variable of the instance that handled their
request. If subsequent requests from the same user are routed to a different instance, that instance won't have any record of the
previously viewed articles, causing them to be displayed again.
upvoted 
3 
times
desertlotus1211
desertlotus1211
 
1 month, 1 week ago
how would you change the code?
upvoted 
1 
times
omermahgoub
omermahgoub
 
3 months, 2 weeks ago
A
The most likely cause of the issue described in the code is that the session variable is local to just a single instance. In this code,
the session variable is defined as a local dictionary within the Flask application. This means that it is not shared across different
instances of the application and will not be persisted between requests. As a result, when the application is running on multiple
instances, each instance will have its own local copy of the session variable, and users may see news articles that they have
already viewed on other instances.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
To fix this issue, you could consider using a persistent storage solution, such as Cloud Datastore or Cloud SQL, to store the
session data in a way that is shared across all instances of the application. This would allow you to maintain a consistent view of
the session data for each user across all instances of the application.
Other potential causes for this issue, such as modifying the URL of the API to prevent caching or setting the HTTP Expires
header to -1 to stop caching, are not related to the issue described in the code and would not likely address the problem.upvoted 
2 
times
Badri9898
Badri9898
 
3 months, 2 weeks ago
The most likely cause of the reported issue is that the session variable is local to just a single instance.
In the code provided, the sessions variable is a dictionary that stores the viewed news articles for each user. However, this
variable is only stored in memory on the instance that handles the request, and it is not shared between instances. Therefore,
when a new request is handled by a different instance, it will not have access to the same session data, and the user may see
previously viewed news articles.
To solve this problem, a shared session management system should be used that can be accessed by all instances. Google App
Engine provides a few options for session management, such as using Memcache or Cloud Datastore to store the session data.
By using a shared session management system, all instances can access the same session data, and users will not see
previously viewed news articles.
upvoted 
8 
times
kurili
kurili
 
3 months, 2 weeks ago
A.The sessions dictionary is used to store user-specific data, such as which news articles have been viewed. This dictionary is
created as an in-memory variable within the Flask app.
In a cloud environment, like Google App Engine, the application may be running on multiple instances, especially during peak
loads. Since the sessions dictionary is stored in memory, it is local to each instance.
This means that if a user is routed to a different instance (due to load balancing), their session data will not be available on that
new instance, causing the application to serve news articles they’ve already seen.
upvoted 
1 
times
simonab23
simonab23
 
1 year, 11 months ago
A is the right answer
upvoted 
1 
times
jay9114
jay9114
 
2 years ago
Where in the code does it show that the session variable is local to just a single instance?
upvoted 
3 
times
Amrit123_
Amrit123_
 
2 years ago
A is correct
upvoted 
1 
times
angelumesh
angelumesh
 
2 years, 1 month ago
Selected Answer: 
A
stateful variable should be in firestore (redis).
upvoted 
1 
times
Racinely
Racinely
 
2 years, 1 month ago
I agree with ackdbd
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A is the correct answer app becoming stateful and it should not be sin case of app engine, cloud run and functions
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
A
A. The session variable is local to just a single instance
The others are not relevant
upvoted 
1 
times
sgofficial
sgofficial
 
2 years, 5 months ago
Thank you that was nice explanation
upvoted 
2 
timesjay9114
jay9114
 
2 years, 5 months ago
Where was this presented in the GCP Architecture training & labs?
upvoted 
2 
times
backhand
backhand
 
2 years, 5 months ago
vote A
- rule out C,D not thing to do with problem
- rule out B, Q is not mention datastore
upvoted 
2 
times
nicoueron
nicoueron
 
2 years, 6 months ago
Selected Answer: 
A
A of course, it's just a code pb here
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #5
An application development team believes their current logging tool will not meet their needs for their new cloud-based
product. They want a better tool to capture errors and help them analyze their historical log data. You want to help them find a
solution that meets their needs. 
What should you do? 
A. 
Direct them to download and install the Google StackDriver logging agent
B. 
Send them a list of online resources about logging best practices
C. 
Help them define their requirements and assess viable logging tools 
Most Voted
D. 
Help them upgrade their current tool to take advantage of any new features
Correct Answer:
 
C 
Comments
dummyemailforexam
dummyemailforexam
 
Highly Voted
 
4 years, 8 months ago
A. This is GCP exam. They will always promote their services. Not a third party solution.
upvoted 
114 
times
Ziegler
Ziegler
 
4 years, 7 months ago
Remember that agent is only required for non cloud based resources. The question is saying their cloud based... 
feel C meets
this need
upvoted 
11 
times
kkhurana
kkhurana
 
2 years, 11 months ago
logging agent is required 
for compute engine too.
upvoted 
4 
times
try_jai
try_jai
 
3 years, 6 months ago
It's given as 'cloud based resource' but didn't mention if it is 'GCP'. It could be any cloud provider. So Stackdriver might be the
answer.
upvoted 
4 
times
Community vote distribution
C (51%)
A (49%)Meyucho
Meyucho
 
8 months, 4 weeks ago
Is an Architect exam... the team THINKS that the tool WILL NOT MEET requirements... you should help to undesntend what they
need... Is C... it not says that you will offert third party solution
upvoted 
5 
times
willrof
willrof
 
4 years ago
Totally Agree. offering Stackdriver Logging is what they want from a GCA. answer is A.
upvoted 
6 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Agreed
upvoted 
1 
times
MeasService
MeasService
 
Highly Voted
 
5 years, 2 months ago
C should be the correct answer here
upvoted 
58 
times
techalik
techalik
 
3 months, 2 weeks ago
Review the logging requirements and use existing logging utility. is not right.
You know the requirements as you have found the current logging tool will not meet the needs for the new cloud-based product.
Install Google Cloud logging agent on all VMs. is the right answer.
The Logging agent streams logs from common third-party applications and system software to Logging. It is a best practice to
run the Logging agent on all your VM instances. The agent runs under both Linux and Windows. The Logging agent is compatible
with both GCP Compute Engine instances as well as AWS EC2 instances. Google, through its partners, provides logging
services for on-premise and hybrid cloud platforms consistently and predictably.
Ref: https://cloud.google.com/logging/docs/agent
upvoted 
4 
times
okixavi
okixavi
 
4 years ago
Why do you think they are using VM on GCE?
upvoted 
1 
times
penelop
penelop
 
3 years, 3 months ago
This is wrong. This certification is meant to prepare you to be a cloud architect (focusing on GCP). This does not mean that you
are going to recommend everything that has google on its name. You need to understand client requirements first.
upvoted 
5 
times
aceton999
aceton999
 
2 years, 11 months ago
Actually it does. A lot of questions guide you to the, in best case, fully managed GCP services. They should rename the
certificate to "Professional Google Cloud Sales Person".
upvoted 
5 
times
tartar
tartar
 
4 years, 5 months ago
C is ok
upvoted 
10 
times
tartar
tartar
 
4 years, 4 months ago
I would love to choose B, but need to keep my job..
upvoted 
15 
times
Vika
Vika
 
3 years, 9 months ago
What all viable logging you would suggest in this scenario! Cloud operations suite has everything.. cloud logging helps with
many thing..In my mind this question is not meant for being a perfectionist but what would mostly work while option C is an
approach that we will take questions ask about tool. Hence selecting A make sense to me. Thoughts!approach that we will take questions ask about tool. Hence selecting A make sense to me. Thoughts!
upvoted 
2 
times
lynx256
lynx256
 
3 years, 9 months ago
I'm surprised, @MeasService.
I guess you had created the question an sugested ans. A. 
Then you wrote "C should be the correct answer here".
Do you change your mind ?
upvoted 
7 
times
aatt1122
aatt1122
 
2 years ago
The Stackderiver agent currently known as Ops agent is the primary agent for collecting telemetry from your Compute Engine
instances. It needs to be installed on GCP services such as GCE instances in order to collect logs from those instances and
send them to cloud logging and monitoring. https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/installation.
upvoted 
1 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 10 hours ago
Selected Answer: 
C
I will go for C because you first need to understand their requirements to choose a correct tool to fulfill all theirs needs.
upvoted 
1 
times
moshamcloud
moshamcloud
 
6 days, 16 hours ago
Selected Answer: 
A
A. This is GCP exam. They will always promote their services
upvoted 
1 
times
ramjisriram
ramjisriram
 
6 days, 16 hours ago
Selected Answer: 
A
You are writing an Architect exam, not a Business requirement (Product Manager) exam or a Software Consultant or Security
Consultant to upgrade their tool. So as a GCP architect, only A is correct, rest all are not applicable.
upvoted 
1 
times
RushiSS
RushiSS
 
3 weeks, 1 day ago
Selected Answer: 
C
Ops agent can be deployed only with GCE instances, and not 
with VMs deployed in AWS/ Azure or On-premise.
Question talks about "new cloud-based product", without revealing the cloud it uses. So C makes sense.
For GCP based workloads, Cloud logging will make more sense.
upvoted 
1 
times
Riyaz_Ahmed_Gcp
Riyaz_Ahmed_Gcp
 
1 month ago
Selected Answer: 
C
Option C is the best approach. Helping the team define their requirements and assess viable logging tools ensures they find a
solution that truly meets their needs. This involves understanding their specific requirements, such as:
The types of errors they need to capture
The volume of log data they expect
Integration with their existing systems
Ease of use and setup
Cost considerations
Once these requirements are clear, you can evaluate different logging tools like ELK Stack, Splunk, or Datadog to see which one
aligns best with their needs. This tailored approach is more likely to result in a successful implementation compared to simply
upgrading their current tool or directing them to a specific product without understanding their needs.
upvoted 
1 
times
alihabib
alihabib
 
1 month ago
Selected Answer: 
A
A should be the answer, assuming the cloud based product is hosted in Google Cloud. The architect has to recommend a GCP
Solution, not an external one which create Network & Security overheads
upvoted 
2 
times
ddatta
ddatta
 
1 month, 2 weeks agoddatta
ddatta
 
1 month, 2 weeks ago
Selected Answer: 
C
C is Best possible answer.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
C
Answer is C. 
Here's why:
Understanding Needs: 
Jumping straight to a solution (like Stackdriver) might not be the best fit. It's crucial to first understand the
team's specific logging requirements for their cloud-based product. This includes:
1. What kinds of events need to be logged (errors, warnings, information, debug)?
2. How much logging data do they anticipate generating?
3. How long do they need to store logs?
4. What kind of analysis do they want to perform on their log data?
5. Does the logging tool need to integrate with other systems (monitoring, alerting, etc.)?
6. Are there any cost constraints?
upvoted 
1 
times
VedaSW
VedaSW
 
3 months, 2 weeks ago
The key to the answer is this: "You want to help them find a solution that meets their needs."
You only start to find solution after you know the needs.
Hence, this implied that you have already done the "Help them define their requirements and assess viable logging tools" (This
rules out C)
B is a lazy way do things and did not address the needs. (You are suppose to do the work, not ask them to do it themselves)
D - "Help them upgrade their current tool to take advantage of any new features", well, this is a solution, but it is costly and it is not
a business functions/features that worth the resources (you would go for COTS) 
So, left with A....
upvoted 
2 
times
A21325412
A21325412
 
3 months, 2 weeks ago
I've seen a lot of folks chosen C with a good argument of "requirements before solution". While I do agree with that argument. My
choice would still be A.
Reason being: 
(c.f: https://cloud.google.com/products/operations?hl=en)
Note: Stackdriver was the former name, it's now Google Cloud's Operation Suite, and it works both "inside and outside" of Google
Cloud.
Therefore, if this new "cloud-based product" is on AWS or some other cloud provider, it would still work. 
Finally I saw a comment where someone said Google "will always promote their services". This is not true, especially for the
Architect Exam.
upvoted 
2 
times
santoshchauhan
santoshchauhan
 
3 months, 2 weeks ago
Selected Answer: 
C
C. Help them define their requirements and assess viable logging tools.
This approach is strategic because it starts with understanding the specific needs of the development team, which can vary widely
depending on the nature of the cloud-based product they are working on. Here’s why this is the most suitable option:
Requirement Analysis: By defining the exact requirements, you ensure that the selected logging tool will have the necessary
features to capture errors and analyze historical log data effectively.
Tool Assessment: Once the requirements are clear, you can perform an informed assessment of available logging tools that meet
these criteria, which could include Google StackDriver, other cloud-native tools, or third-party solutions.
Long-term Solution: Choosing the right tool based on precise requirements is likely to offer a long-term solution rather than a
temporary fix.
upvoted 
1 
timesupvoted 
1 
times
RickMorais
RickMorais
 
6 months, 2 weeks ago
Selected Answer: 
C
C. Help them define their requirements and assess viable logging tools
Here's why the other options are less suitable:
A. Downloading Stackdriver Logging Agent: While Stackdriver Logging Agent is a component of Cloud Logging, it's just the agent
for sending logs. They need a broader solution that addresses their needs and evaluates alternatives.
B. Logging Best Practices: Best practices are helpful, but they don't directly address finding the right tool for their specific needs.
D. Upgrading Current Tool: Upgrading might be an option, but it doesn't consider if a better tool exists that could be a better fit.
By helping them define their requirements (error capturing, historical data analysis) and assess various logging tools (including
Cloud Logging with Stackdriver), you provide a more comprehensive solution.
upvoted 
1 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
Selected Answer: 
A
All who says C, better not go to GCP. 
It is A, stackdriver agent is GCP Agent. Are you studying for GCP or what?
upvoted 
3 
times
a2le
a2le
 
7 months ago
Selected Answer: 
C
Maybe, at the end of a requirement analysis, the solution might be A, but a tool hardly represents a solution to a business concern.
upvoted 
1 
times
apclb
apclb
 
7 months, 2 weeks ago
Selected Answer: 
C
It's C. Option A makes assumptions, that is not what an architect does. DOn't get baited by the that it's a GCP exam and therefore
needs to be a GCP product. But most importantly, an agent is for deploying on compute instances. The solution might not even
use VMs so A is 100% the wrong answer based on the info we have.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #6
You need to reduce the number of unplanned rollbacks of erroneous production deployments in your company's web hosting
platform. Improvement to the QA/ 
Test processes accomplished an 80% reduction. 
Which additional two approaches can you take to further reduce the rollbacks? (Choose two.) 
A. 
Introduce a green-blue deployment model 
Most Voted
B. 
Replace the QA environment with canary releases
C. 
Fragment the monolithic platform into microservices 
Most Voted
D. 
Reduce the platform's dependency on relational database systems
E. 
Replace the platform's relational database systems with a NoSQL database
Correct Answer:
 
AC 
Comments
JustJack21
JustJack21
 
Highly Voted
 
3 years, 4 months ago
D) and E) are pointless in this context.
C) is certainly a good practice.
Now between A) and B)
A) Blue green deployment is an application release model that gradually transfers user traffic from a previous version of an app or
microservice to a nearly identical new release—both of which are running in production. 
c) In software, a canary process is usually the first instance that receives live production traffic about a new configuration update,
either a binary or configuration rollout. The new release only goes to the canary at first. The fact that the canary handles real user
traffic is key: if it breaks, real users get affected, so canarying should be the first step in your deployment process, as opposed to
the last step in testing in production. "
While both green-blue and canary releases are useful, B) suggests "replacing QA" with canary releases - which is not good. QA
got the issue down by 80%. Hence A) and C)
upvoted 
66 
times
jdpinto
jdpinto
 
Highly Voted
 
3 years, 7 months ago
A & C for me
upvoted 
34 
times
MikeMike7
MikeMike7
 
Most Recent
 
4 weeks, 1 day ago
Community vote distribution
AC (84%)
Other (16%)MikeMike7
MikeMike7
 
Most Recent
 
4 weeks, 1 day ago
Selected Answer: 
BC
it is blue green not green blue
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
AC
Answer is A & C.
A. Introduce a green-blue deployment model: This is a great way to reduce risk and downtime during deployments. You have two
identical production environments ("green" and "blue"). You deploy the new version to the "blue" environment while "green" remains
live. After testing and verification in "blue," you switch traffic to "blue," making it the new live environment. If issues arise, you can
quickly switch back to "green."
C. Fragment the monolithic platform into microservices: This is a more involved architectural change, but it can significantly
improve deployment safety and flexibility.
B. is not correct because while canary releases are valuable, they are a testing strategy, not a replacement for a dedicated QA
environment. You still need a controlled environment for thorough testing before releasing to production.
upvoted 
2 
times
LEIChan
LEIChan
 
6 months, 2 weeks ago
B & C see should be the correct answer. 
There is no green-blue deployment but rather a blue green.
upvoted 
2 
times
MikeMike7
MikeMike7
 
4 weeks, 1 day ago
Agree, also Canary is a safe good option
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 3 weeks ago
C is also dumb even if it is a good answer but the question never specifies whether it is a monolithic platform or not. I hate tests
because of this kind of incomplete context.
upvoted 
2 
times
a2le
a2le
 
7 months ago
Selected Answer: 
AC
B. Replace the QA environment with canary releases
canary release is not a replacement for a QA environment.
D. Reduce the platform's dependency on relational database systems
E. Replace the platform's relational database systems with a NoSQL database
a relational database system is not, as it is, an obstacle to improving the deployment success of the application.
Then, in my opinion, AC is the correct answer.
upvoted 
1 
times
Mela_89
Mela_89
 
10 months, 2 weeks ago
Selected Answer: 
AC
A & C is the correct answer
upvoted 
2 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
A and C, the description given by JustJack21 is all you need.
upvoted 
1 
times
hzaoui
hzaoui
 
11 months, 4 weeks ago
Selected Answer: 
AC
A and C
upvoted 
2 
times
cfigueiredo
cfigueiredo
 
1 year ago
D & F for meD & F for me
upvoted 
1 
times
spuyol
spuyol
 
1 year, 1 month ago
A is the only answer.
C is a general improvement but does not guarantee the reduction of rollbacks due to quality failures if programming errors remain.
B, canary only makes sense in PRO. The statement is ambiguous. In any case, if what we want is to reduce the current situation,
it does not seem convenient to remove what now helps.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
BC
B. Replace the QA environment with canary releases
C. Fragment the monolithic platform into microservices
As splitting monolithinc application in Microservices means that code and dependencies are bundled together and DEV, TEST, QA
and PROD will have same docker image. Replacing QA environment with Canary will ensure testing the final code with sub-set of
users before Go-Live. There is no RollBack and no downtime. Even if testing with sub-set users fails that previous PROD
deployment will continue to serve traffic. However in case of Blue-Green deployment, you will have Current PROD code and new
prod code, In case new code fails post deployment, it has to be rolled-back to working code.
and the ask is to reduce or eliminate Rollback.
upvoted 
3 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
B. Replace the QA environment with canary releases
C. Fragment the monolithic platform into microservices
As splitting monolithinc application in Microservices means that code and dependencies are bundled together and DEV, TEST, QA
and PROD will have same docker image. Also canary release will ensure testing the final code with sub-set of users before Go-
Live. Which will reduce your rollbacks. (in Blue-Green deployment, we are actually making ourself ready for rollback in case things
go wrong).
upvoted 
1 
times
yilexar
yilexar
 
1 year, 3 months ago
Use Blue-Green to reduce rollback. Check this blog (https://circleci.com/blog/canary-vs-blue-green-
downtime/#:~:text=In%20blue%2Dgreen%20deployment%20you,first%2C%20before%20finishing%20the%20others.): Using your
load balancers to direct traffic keeps your blue environment running seamlessly for production users while you test and deploy to
your green environment. When your deployment and testing are successful, you can switch your load balancer to target your
green environment with no perceptible change for your users.
When testing in Green environment, you don't perform rollback if test failed in Green.
upvoted 
1 
times
ChinaSailor
ChinaSailor
 
1 year, 3 months ago
A to validate your deployment and C to ensure that errors do not cascade across the process
upvoted 
2 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
A&C is correct!
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #7
To reduce costs, the Director of Engineering has required all developers to move their development infrastructure resources
from on-premises virtual machines 
(VMs) to Google Cloud Platform. These resources go through multiple start/stop events during the day and require state to
persist. You have been asked to design the process of running a development environment in Google Cloud while providing
cost visibility to the finance department. 
Which two steps should you take? (Choose two.) 
A. 
Use the - -no-auto-delete flag on all persistent disks and stop the VM 
Most Voted
B. 
Use the - -auto-delete flag on all persistent disks and terminate the VM
C. 
Apply VM CPU utilization label and include it in the BigQuery billing export
D. 
Use Google BigQuery billing export and labels to associate cost to groups 
Most Voted
E. 
Store all state into local SSD, snapshot the persistent disks, and terminate the VM
F. 
Store all state in Google Cloud Storage, snapshot the persistent disks, and terminate the VM
Correct Answer:
 
AD 
Comments
[Removed]
[Removed]
 
Highly Voted
 
3 months, 2 weeks ago
I spent all morning researching this question. I just popped over and took the GCP Practice exam on Google's website and guess
what... this question was on it word for word, but it had slightly different answers, but not by much here is what I learned. 
The
correct answer is 100% A / D and here is why. 
On the sample question, the "F" option is gone. 
"A" is there but slightly reworked, it
now says: "Use persistent disks to store the state. Start and stop the VM as needed" which makes much more sense. 
The
practice exam says A and D are correct. Given the wording of this question, if A and B, where there then both would be correct
because of the word "persistent" and not because of the flag. The "no-auto-delete" makes A slightly safer than B, but it is the
"persistent disk" that makes them right, not the flag. 
Hope that helps! F is not right because that is a complex way of solving the
issue that by choosing Persistent Disk solves it up front. HTH
upvoted 
74 
times
[Removed]
[Removed]
 
2 years, 4 months ago
(A) is not sense because the flag is to preserve disk when the istances was deleted, when the istances was stopped the data on
persistend disk are not deleted. So good to 
know that the response was reworked
Community vote distribution
AD (66%)
DF (23%)
Other (11%)persistend disk are not deleted. So good to 
know that the response was reworked
(B) is wrong because only on AWS you can 
terminate istances. On GCP the "terminate" action do not exist .
upvoted 
4 
times
rishab86
rishab86
 
Highly Voted
 
3 years, 7 months ago
A and D looks correct as per https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete#--auto-delete
;
https://cloud.google.com/billing/docs/how-to/export-data-bigquery
upvoted 
22 
times
RKS_2021
RKS_2021
 
3 years, 5 months ago
-no-auto-delete flag does not have effect on the state of the application. I believe D and F are correct ANS,
https://cloud.google.com/compute/docs/instances/stop-start-instance
upvoted 
3 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
AD
Answer is : A& D
A. Use the --no-auto-delete flag on all persistent disks and stop the VM:
1. Cost Savings: When you stop a VM, you only pay for the persistent disks attached to it. The --no-auto-delete flag ensures that
the disks remain available even when the VM is stopped, preserving the developers' work and avoiding the cost of recreating the
environment from scratch each time.
2. State Persistence: This approach ensures that the development environment's state is saved on the persistent disk, allowing
developers to resume their work seamlessly when they restart the VM.
D. Use Google BigQuery billing export and labels to associate cost to groups:
1. Cost Visibility: BigQuery billing export allows you to analyze your Google Cloud costs in detail. By applying labels to your
resources (e.g., "environment: development", "team: frontend"), you can categorize and track costs associated with different
development groups.
upvoted 
2 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Choose A and D
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 3 weeks ago
Another confusing question because I took the "these machines go through multiple stop/starts during the day" as a part of the
migration, not as a part of daily functionality, so none of the answers other than D made much sense to me. People need to word
the questions better on tests and give more than enough context or people like me are going to get confused, second guess our
answers, and fail.
upvoted 
1 
times
afsarkhan
afsarkhan
 
7 months, 4 weeks ago
Selected Answer: 
AD
F is too complex solution to solve this problem.
E local SSD does not persist on termination of vm so this is also a wrong option
A, B suggest persistent disk but I think A makes better sense.
So my answer is A and D
upvoted 
1 
times
44eacc1
44eacc1
 
8 months, 2 weeks ago
E wrong:
Scenarios where Compute Engine does not persist Local SSD data
Data on Local SSD disks does not persist through the following events:
If you shut down the guest operating system and force the VM to stop.
upvoted 
1 
times
shikha344
shikha344
 
9 months, 2 weeks ago
Hi all, i am trying to view questions from 135.But i cannot access the page as it is asking for contributer access.Is it same forHi all, i am trying to view questions from 135.But i cannot access the page as it is asking for contributer access.Is it same for
everyone?
upvoted 
1 
times
d0094d6
d0094d6
 
11 months ago
Selected Answer: 
A
From the GCP Practice exam... A and D
upvoted 
2 
times
Teckexam
Teckexam
 
11 months, 2 weeks ago
Based on documentation A is correct https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete
Also the documentation clearly states that this flag will be help retain the state when VM is started/stopped.
https://cloud.google.com/blog/products/storage-data-transfer/save-money-by-stopping-and-starting-compute-engine-instances-on-
schedule
For cost visibility option D is correct.
upvoted 
1 
times
hzaoui
hzaoui
 
11 months, 4 weeks ago
Selected Answer: 
DE
D. Use Google BigQuery billing export and labels to associate cost to groups: This offers granular cost visibility across various
development teams or projects through BigQuery data analysis. Combining labels with billing export allows you to associate
resource consumption with specific groups, enabling chargeback mechanisms and fostering cost accountability.
E. Store all state into local SSD, snapshot the persistent disks, and terminate the VM: This option minimizes ongoing cost by
utilizing low-cost local SSD for active state during runtime and terminating the VM when development isn't ongoing. Snapshots
offer quick restoration back to the latest state without incurring persistent disk charges during downtime.
upvoted 
1 
times
kshlgpt
kshlgpt
 
1 year ago
DF. This is the question in google practice test.
upvoted 
1 
times
cfigueiredo
cfigueiredo
 
1 year ago
Selected Answer: 
DF
D & F for me
upvoted 
1 
times
Prakzz
Prakzz
 
1 year, 2 months ago
How can D ever be right coz it's Bigquery Billing Export and question is about VM billing
upvoted 
1 
times
ArtistS
ArtistS
 
1 year, 1 month ago
This means you can export the billing to the BQ and then do analysis.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
AD
A is correct, Use of persistent disk mean the data is preserved even after restart. -np-auto-delete on persistent disk means
pesistent disk won't be deleted when VM is deleted.
D is correct, becuase second part of question asks for billing report to finanace department and Label and BQ helps in cost
analysis.
upvoted 
4 
times
lucaluca1982
lucaluca1982
 
1 year, 5 months ago
Selected Answer: 
BD
B and D. A is wrong because, 
(--no-auto-delete) would lead to extra storage costs for the disks even when VMs are not running.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
same question, official option:same question, official option:
A. Use persistent disks to store the state. Start and stop the VM as needed.
B. Use the "gcloud --auto-delete" flag on all persistent disks before stopping the VM.
C. Apply VM CPU utilization label and include it in the BigQuery billing export.
D. Use BigQuery billing export and labels to relate cost to groups.
E. Store all state in a Local SSD, snapshot the persistent disks, and terminate the VM.
This question will not be tested, no need to read
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #8
Your company wants to track whether someone is present in a meeting room reserved for a scheduled meeting. There are 1000
meeting rooms across 5 offices on 3 continents. Each room is equipped with a motion sensor that reports its status every
second. The data from the motion detector includes only a sensor ID and several different discrete items of information.
Analysts will use this data, together with information about account owners and office locations. 
Which database type should you use? 
A. 
Flat file
B. 
NoSQL 
Most Voted
C. 
Relational
D. 
Blobstore
Correct Answer:
 
B 
Comments
clouddude
clouddude
 
Highly Voted
 
4 years, 7 months ago
I'll go with B.
This is time series data. 
We also have no idea what kinds of data are being captured so it doesn't appear structurd.
A does not seem reasonable because a flat file is not easy to query and analyze.
B seems reasonable because this accommodates unstructured data.
C seems unreasonable because we have no idea on the structure of the data.
D seems unreasonable beacause there is no such Google database type.
upvoted 
34 
times
ddatta
ddatta
 
Most Recent
 
1 month, 1 week ago
Selected Answer: 
B
We don't know the data type. Only nosql make sense.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
B
Community vote distribution
B (94%)
C (6%)Answer is B.
1. High Volume and Velocity of Data: You have 1000 rooms reporting data every second, resulting in a massive amount of data
with high velocity. NoSQL databases are designed to handle this kind of volume and speed efficiently.
2. Simple Data Structure: The data from the motion sensor is relatively simple (sensor ID and discrete information). NoSQL
databases are well-suited for storing and processing this type of data without the need for complex schemas.
3. Flexible Schema: NoSQL databases offer schema flexibility, allowing you to easily adapt to changes in the data structure if
needed. This is important as your tracking requirements might evolve over time.
4.Scalability: NoSQL databases are highly scalable, making it easy to accommodate future growth in the number of meeting
rooms or data volume.
upvoted 
1 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
Selected Answer: 
B
With frequencies of data (per second), the best case would be using pub/sub and NoSQL. Relational DB/BlobStore/FlatFile are
not good for Near realtime data.
upvoted 
2 
times
hzaoui
hzaoui
 
11 months, 4 weeks ago
Selected Answer: 
C
C. Relational database.
Here's why:
Scalability: A relational database can handle the data volume from 1000 sensors reporting every second effectively.
Structure: It provides a well-defined schema for organizing data like sensor ID, timestamp, motion status, account owner, and
office location, making it easily queryable and understandable for analysts.
Relationships: It allows establishing relationships between tables, such as linking sensor data to specific meeting rooms and their
corresponding owners and locations. This facilitates analyses involving multiple data sources.
Flexibility: Relational databases offer flexibility for expanding data collection beyond motion sensors in the future to include other
sensor types or meeting room details.
upvoted 
1 
times
_kartik_raj
_kartik_raj
 
1 year, 2 months ago
B, It is
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
B
Unstructured realtime aata
upvoted 
1 
times
ChinaSailor
ChinaSailor
 
1 year, 3 months ago
Selected Answer: 
B
b [ You need seperate fields and keys -- you do not need to relate them
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
NOSQL DB's are meant for these kind of workloads
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
The requirement to join the data to other data sets implies RDBMS.
BigQuery can handle 1GB/s when streaming inserts, I doubt these 1000 sensors will send that much data.
Bigtable seems over the top and not able to fulfil all the requirements.
upvoted 
2 
times
Deb2293
Deb2293
 
1 year, 10 months ago
Selected Answer: 
B
This will be time-series data. The best DB would be a Big Table (also sensorID can be used in the row key for faster retrieval ofThis will be time-series data. The best DB would be a Big Table (also sensorID can be used in the row key for faster retrieval of
data)
upvoted 
3 
times
AShrujit
AShrujit
 
2 years ago
B for me
upvoted 
1 
times
Jaldhi24
Jaldhi24
 
2 years ago
Selected Answer: 
B
B is right
upvoted 
1 
times
angelumesh
angelumesh
 
2 years, 1 month ago
Selected Answer: 
B
B (No SQL should be the right answer)
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
surprised by the options given, this is a great use case of Bigtable so NoSQL
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
B is right
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
B. NoSQL - unstructured data
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #9
You set up an autoscaling instance group to serve web traffic for an upcoming launch. After configuring the instance group as
a backend service to an HTTP(S) load balancer, you notice that virtual machine (VM) instances are being terminated and re-
launched every minute. The instances do not have a public IP address. 
You have verified the appropriate web response is coming from each instance using the curl command. You want to ensure the
backend is configured correctly. 
What should you do? 
A. 
Ensure that a firewall rules exists to allow source traffic on HTTP/HTTPS to reach the load balancer.
B. 
Assign a public IP to each instance and configure a firewall rule to allow the load balancer to reach the instance public
IP.
C. 
Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.
Most Voted
D. 
Create a tag on each instance with the name of the load balancer. Configure a firewall rule with the name of the load
balancer as the source and the instance tag as the destination.
Correct Answer:
 
C 
Comments
Eroc
Eroc
 
Highly Voted
 
3 months, 2 weeks ago
"A" and "B" wouldn't turn the VMs on or off, it would jsut prevent traffic. "C" would turn them off if the health check is configured to
terminate the VM is it fails. "D" is the start of a pseudo health check without any logic, so it also isn't an answer because it is like
"A" and "B". Correct Answer: "C"
upvoted 
35 
times
tartar
tartar
 
4 years, 5 months ago
C is ok
upvoted 
14 
times
nitinz
nitinz
 
3 years, 10 months ago
C because terminated and relaunch.... something wrong with HC.
Community vote distribution
C (100%)C because terminated and relaunch.... something wrong with HC.
upvoted 
6 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
agreed and C is right
upvoted 
2 
times
TheCloudBoy77
TheCloudBoy77
 
Highly Voted
 
3 years, 1 month ago
A. Ensure that a firewall rules exists to allow source traffic on HTTP/HTTPS to reach the load balancer. >> not correct, load
balancer is not the issue here.
B. Assign a public IP to each instance and configure a firewall rule to allow the load balancer to reach the instance public IP. >>
defeats the purpose of getting load balancers , not correct
C. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.>> Correct. if
using different port then appropriate FW rule need to be setup to ensure LB can reach backend instances for healthcheck. if
healthcheck traffic is blcked, instances will be marked unhealthy and will be restarted.
D. Create a tag on each instance with the name of the load balancer. Configure a firewall rule with the name of the load balancer
as the source and the instance tag as the destination.>> tagging is not useful here as the instance is not the source of traffic, just
the port need to be opened on FW.
upvoted 
7 
times
Houssemonline
Houssemonline
 
Most Recent
 
1 day, 16 hours ago
Selected Answer: 
C
C is correct because health check failures lead to a VM being marked unhealthy and can result in termination if the health check
continues to fail. Because you have already verified that the instances are functioning properly, the next step would be to determine
why the health check is continuously failing.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
C
1. Health Checks are Essential: Load balancers rely on health checks to determine if instances in the backend pool are healthy
and able to serve traffic. If the health checks fail, the load balancer assumes the instance is unhealthy and terminates it, leading to
the constant cycling you're observing.
2. Firewall Rules for Health Checks: Even though your instances don't have public IPs, the load balancer needs to communicate
with them through internal IPs for health checks. Firewall rules must be configured to allow this communication.
3. How Health Checks Work: The load balancer sends requests (e.g., HTTP, HTTPS, TCP) to the instances on a specific port and
expects a certain response. The firewall needs to allow these requests to reach the instances and the responses to return to the
load balancer.
upvoted 
1 
times
subramanyam46
subramanyam46
 
10 months ago
c is right
upvoted 
1 
times
hzaoui
hzaoui
 
11 months, 4 weeks ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
yas_cloud
yas_cloud
 
1 year ago
Most likely the problem of instances terminating is with the threshold settings on the health check. It’s thinking too sooner that
some VMs can be terminated due to less load.
upvoted 
1 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
One thing that i couldn't understand is - How VMs getting terminated and relaunched for not setting health-checks in the load
balancer ? how will that affect VM's uptime ?
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
C
If the healthcheck is not successful, it will keep on re-creating the instances in MIG.
upvoted 
1 
timesupvoted 
1 
times
angelumesh
angelumesh
 
2 years, 1 month ago
Selected Answer: 
C
C (LB Health check should be taken care of)
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
C
C. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.
upvoted 
3 
times
YAS007
YAS007
 
2 years, 8 months ago
answer C:
https://cloud.google.com/load-balancing/docs/health-check-concepts#ip-ranges
upvoted 
2 
times
AWS56
AWS56
 
2 years, 11 months ago
Selected Answer: 
C
C is corect.
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
3 years ago
C is corect.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
3 years ago
' (VM) instances are being terminated and re-launched every minute. '
Isn't it because the health check is failing.
A & D Maybe aleady there.curl command passed.
B What are you doing. Absolutely no.
upvoted 
2 
times
haroldbenites
haroldbenites
 
3 years, 1 month ago
Go for C.
This questions is in sample quesitons of Google
https://docs.google.com/forms/d/e/1FAIpQLSdvf8Xq6m0kvyIoysdr8WZYCG32WHENStftiHTSdtW4ad2-0w/viewform
upvoted 
4 
times
vincy2202
vincy2202
 
3 years, 1 month ago
C is the correct answer.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #10
You write a Python script to connect to Google BigQuery from a Google Compute Engine virtual machine. The script is printing
errors that it cannot connect to 
BigQuery. 
What should you do to fix the script? 
A. 
Install the latest BigQuery API client library for Python
B. 
Run your script on a new virtual machine with the BigQuery access scope enabled
C. 
Create a new service account with BigQuery access and execute your script with that user 
Most Voted
D. 
Install the bq component for gcloud with the command gcloud components install bq.
Correct Answer:
 
C 
Comments
kalschi
kalschi
 
Highly Voted
 
3 months, 2 weeks ago
A - If client library was not installed, the python scripts won't run - since the question states the script reports "cannot connect" -
the client library must have been installed. so it's B or C.
B - https://cloud.google.com/bigquery/docs/authorization an access scope is how your client application retrieve access_token
with access permission in OAuth when you want to access services via API call - in this case, it is possible that the python script
use an API call instead of library, if this is true, then access scope is required. client library requires no access scope (as it does
not go through OAuth)
C - service account is Google Cloud's best practice
So prefer C.
upvoted 
99 
times
rishab86
rishab86
 
3 years, 2 months ago
Access scopes are the legacy method of specifying permissions for your instance. read from >
https://cloud.google.com/compute/docs/access/service-accounts . So , I would go with C
upvoted 
11 
times
Vika
Vika
 
3 years, 10 months ago
Community vote distribution
C (82%)
Other (18%)agreed to comment here . C seems like a good option
upvoted 
4 
times
MQQNB
MQQNB
 
2 years, 4 months ago
agree
access scope is enabled by default
https://cloud.google.com/bigquery/docs/authorization#authenticate_with_oauth_20
If you use the BigQuery client libraries, you do not need this information, as this is done for you automatically.
upvoted 
2 
times
Musk
Musk
 
4 years, 6 months ago
Might be an old version
upvoted 
4 
times
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
Why not B? It looks better for me.
upvoted 
13 
times
techalik
techalik
 
4 years, 1 month ago
Configure the Python API to use a service account with relevant BigQuery access enabled. is the right answer.
It is likely that this service account this script is running under does not have the permissions to connect to BigQuery and that
could be causing issues. You can prevent these by using a service account that has the necessary roles to access BigQuery.
Ref: https://cloud.google.com/bigquery/docs/reference/libraries#cloud-console
A service account is a special kind of account used by an application or a virtual machine (VM) instance, not a person.
Ref: https://cloud.google.com/iam/docs/service-accounts
upvoted 
6 
times
tartar
tartar
 
4 years, 5 months ago
C is ok
upvoted 
11 
times
tartar
tartar
 
4 years, 4 months ago
Sorry, B is ok. You can create service account, add user to service account, and grant the user role as Service Account User.
You still need to enable BigQuery scope to make the Python script running the instance to access BigQuery.
upvoted 
15 
times
cloudguy1
cloudguy1
 
4 years, 4 months ago
Stop confusing people, B) doesn't make any sense. Why would you use or create a whole new VM just because of a
permission issue? If anything, just stop the instance and edit the scope of the default Compute Service Account and grant it the
role through IAM. C) is the most appropriate answer since you can only set scopes of the default Compute Service Account, if
you're using any other, there's no scope option - its access is dictated strictly by IAM in such scenario. So C) is the answer:
Stop the VM, change the Service Account with the appropriate permissions and done. B) would still need to have permission
the set through IAM & Admin, the scope isn't enough with the default Compute Service Account.
upvoted 
36 
times
certificatores
certificatores
 
4 years, 1 month ago
cloud guy1, relax. tartar is the hero for google cloud and if you read his answer, he explains the service account user's role
granting on this one as that is the best practice
upvoted 
4 
times
nitinz
nitinz
 
3 years, 10 months ago
C, no brainer. You need SA for using API period. Thats where your start your troubleshooting.
upvoted 
6 
times
[Removed]
[Removed]
 
2 years agoCreate a new service account with BigQuery access and execute your script with that user: If you want to run the script on an
existing virtual machine, you can create a new service account with the necessary permissions to access BigQuery and then
execute the script using that service account. This will allow the script to connect to BigQuery and access the data it needs.
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
I stand corrected, B you need to have scope. It is union of Scope + Service Account. If scope is not there, you are screwed
anyways.
upvoted 
2 
times
gcloud007
gcloud007
 
Most Recent
 
1 week, 1 day ago
Selected Answer: 
B
Actually the correct answer is B, reason is , you need to define the scope first, service accounts comes later.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
C
A and C are correct, but we eliminated A because they mentioned "cannot connect" which means the script can run which means
the client library was already installed, so final answer is only "C"
"C" was chosen because in order to access BigQuery, the script needs to authenticate and be authorized. The recommended
way to do this for applications running on Compute Engine is to use a service account. Create a service account with the
appropriate permissions (e.g., "BigQuery Data Editor") to access your BigQuery data. When running the script, make sure it uses
the service account credentials to authenticate. This can be done by setting the GOOGLE_APPLICATION_CREDENTIALS
environment variable to the path of the service account key file.
upvoted 
1 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Choose C
upvoted 
1 
times
kingfighers
kingfighers
 
6 months, 4 weeks ago
I suppose all of them are correct, but we should choose the least effort, B is correct..
upvoted 
1 
times
kingfighers
kingfighers
 
6 months, 4 weeks ago
run script on a new vm, not create a new vm..
upvoted 
1 
times
a2le
a2le
 
6 months, 4 weeks ago
Selected Answer: 
C
Tricky question.
However, as you can read in gcloud compute instances create documentation:
--scopes=[SCOPE,…]
If not provided, the instance will be assigned the default scopes, described below. However, if neither --scopes nor --no-scopes
are specified and the project has no default service account, then the instance will be created with no scopes. Note that the level of
access that a service account has is determined by a combination of access scopes and IAM roles so you must configure both
access scopes and IAM roles for the service account to work properly.
So, probably, B is the right one, as for the "new vm", I guess that this is because you don't want to stop the current one before
having the working one ready...
upvoted 
2 
times
Robert0
Robert0
 
7 months, 2 weeks ago
Selected Answer: 
C
C - service account is Google Cloud's best practice
upvoted 
1 
times
researched_answer_boi
researched_answer_boi
 
8 months, 2 weeks ago
Selected Answer: 
CYou don't need to create a new VM to have different access scopes:
https://cloud.google.com/compute/docs/access/service-accounts#accesscopesiam
This weakens answer B.
When a user-managed service account is attached to the instance, the access scope defaults to cloud-platform:
https://cloud.google.com/compute/docs/access/service-accounts#scopes_best_practice
See Step 6 in: https://cloud.google.com/compute/docs/instances/change-service-account#changeserviceaccountandscopes
These facts leave C as the valid answer.
upvoted 
2 
times
santoshchauhan
santoshchauhan
 
10 months ago
Selected Answer: 
C
C. Create a new service account with BigQuery access and execute your script with that user.
Service accounts are used for server-to-server interactions, such as those between a virtual machine and BigQuery. You would
need to create a service account that has the necessary permissions to access BigQuery, then download the service account key
in JSON format. Once you have the key, you can set an environment variable (GOOGLE_APPLICATION_CREDENTIALS) to the
path of the JSON key file before running your script, which will authenticate your requests to BigQuery.
upvoted 
3 
times
Powerboy
Powerboy
 
9 months, 3 weeks ago
better than creating and downloading a service account key would be to impersonate the service account
upvoted 
1 
times
tosinogunfile
tosinogunfile
 
11 months ago
The answer is C
https://cloud.google.com/bigquery/docs/authentication
For most services, you must attach the service account when you create the resource that will run your code; you cannot add or
replace the service account later. Compute Engine is an exception—it lets you attach a service account to a VM instance at any
time.
upvoted 
1 
times
hzaoui
hzaoui
 
11 months, 4 weeks ago
Selected Answer: 
C
Connecting to BigQuery from a script requires proper authorization. Service accounts provide a secure way to grant access
without sharing user credentials.
upvoted 
1 
times
public_figure
public_figure
 
1 year ago
It should be B,
Script cannot be run by user and user cannot be assigned with Service account, SA can be assigned to a VM
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
C
Best practice is that SA with least privilege from a CE should access BQ.
upvoted 
1 
times
spatters
spatters
 
1 year, 1 month ago
B is silly because there's no need to create a new VM just to change the access scope. 
You can edit the existing VM's access
scope, although you do have to stop it first.
upvoted 
1 
times
yilexar
yilexar
 
1 year, 3 months ago
Closest is C. https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#gcloud
The confusion part is that it should never use the word user to represent service account
upvoted 
1 
times
duzapo
duzapo
 
1 year, 3 months ago
Selected Answer: 
C
Recomended best practiceupvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #11
Your customer is moving an existing corporate application to Google Cloud Platform from an on-premises data center. The
business owners require minimal user disruption. There are strict security team requirements for storing passwords. 
What authentication strategy should they use? 
A. 
Use G Suite Password Sync to replicate passwords into Google
B. 
Federate authentication via SAML 2.0 to the existing Identity Provider 
Most Voted
C. 
Provision users in Google using the Google Cloud Directory Sync tool
D. 
Ask users to set their Google password to match their corporate password
Correct Answer:
 
B 
Comments
gcp_aws
gcp_aws
 
Highly Voted
 
4 years, 8 months ago
The correct answer is B. 
GCDS tool only copies the usernames, not the passwords. And more over strict security requirements for the passwords. Not
allowed to copy them onto Google, I think. 
Federation technique help resolve this issue. Please correct me if I am wrong.
upvoted 
79 
times
brss39
brss39
 
1 year, 2 months ago
B is the answer. Why ?
GCDS syncs passwords - Ok but which passwords? Clients need to provide a new password for accessing Google Cloud after
GCDS sync.
Google recognizes the user because GCDS populated the user list. The user is
redirected to a standard Google sign-in screen where they enter their standard username and Google Cloud-specific password. 
The issue here is the two sets of passwords. Even if a user manually sets them both to the same value, they aren’t managed in a
single place. If you need to update your password, you’d have to do that in AD and then again in Google Cloud Identity. In some
cases, this approach can allow for better separation between your on-premises environment and Google Cloud, but it’s also one
more password to manage for your users.
upvoted 
17 
times
 
7 months, 2 weeks ago
Community vote distribution
B (78%)
C (22%)Robert0
Robert0
 
7 months, 2 weeks ago
This should be the top comment. It explains in detail the proccess
upvoted 
3 
times
Neferith
Neferith
 
2 years, 4 months ago
Passwords are also synchronized:
https://support.google.com/a/answer/6120130?hl=en&ref_topic=2679497
upvoted 
8 
times
ExamTopicsFan
ExamTopicsFan
 
3 years, 6 months ago
GCDS synchronises password as well and that is the reason why B is the correct answer. Only in B the password doesn't get
copied to GCP.
upvoted 
11 
times
zr79
zr79
 
2 years, 2 months ago
C is the answer
upvoted 
3 
times
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
"A" will syncronise passwords between on pre-mise and the GCP, this duplicates the existing strategy plus Google's "built-in"
encryption of
all the data. "B" does not support the moving to GCP. "C" The directory sync tool copies the filesystem settings between servers,
UNIX filesystems
have permission settings built in and passwords to log into the permission groups, syncing these would set GCP up the same way
their on-premises
is, plus Google's "built-in" encryption. "D" disrupts the users, so this is not correct. The debate should be between "A" and "C", "C"
includes
"A" according to (https://cloud.google.com/solutions/migrating-consumer-accounts-to-cloud-identity-or-g-suite-best-practices-
federation) so
choose "C"
upvoted 
22 
times
Gobblegobble
Gobblegobble
 
4 years, 6 months ago
B is supported read https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on
upvoted 
4 
times
tsys
tsys
 
3 years, 10 months ago
There is no mention SSO is needed.
upvoted 
3 
times
tartar
tartar
 
4 years, 5 months ago
B is ok.
upvoted 
5 
times
tartar
tartar
 
4 years, 4 months ago
miss typed.. C is ok
upvoted 
11 
times
nitinz
nitinz
 
3 years, 10 months ago
B, you dont want to store password as per security guidelines provided in question.
upvoted 
3 
times
cetanx
cetanx
 
4 years, 6 months ago
GCDS syncs user accounts and some other LDAP attributes but not the passwords, with hybrid connectivity to GCP, SAML (or
federation) is the preferred method.
Answer should be "B"
https://cloud.google.com/solutions/patterns-for-authenticating-corporate-users-in-a-hybrid-environmenthttps://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-synchronizing-user-
accounts#deciding_what_to_provision
upvoted 
16 
times
squishy_fishy
squishy_fishy
 
2 years, 11 months ago
This is the best answer so far.
upvoted 
1 
times
SamirJ
SamirJ
 
4 years, 3 months ago
GCDS does sync passwords. Please refer - https://support.google.com/a/answer/6120130. Since the question says client
wants to move to GCP , C should be the answer.
upvoted 
5 
times
MilkyMist
MilkyMist
 
5 days, 21 hours ago
In Active Directory, passwords are stored as write-only. They can't be read through any interface, such as LDAP. Therefore,
conventional synchronization methods (for example, Google Cloud Directory Sync) can't access them. The only way to read
passwords is to capture them when they’re set or changed.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
The article implies that ADFS is best but suggests you also need the GCDS. 
This makes sense, you need the users in Google
to allocate permissions but you don't want to copy the passwords across hence ADFS.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
B
1. Minimal User Disruption: 
Federated authentication allows users to use their existing corporate credentials to access the
application in Google Cloud. This eliminates the need for them to create and remember new passwords, minimizing disruption and
improving user experience.
2. Strict Security Requirements: 
SAML 2.0 is a widely used, secure standard for authentication and authorization. It allows the
existing identity provider (IdP) to handle password management and security policies, ensuring compliance with the security
team's requirements.
3. Centralized Identity Management: 
Federation keeps identity management centralized within the existing corporate infrastructure.
This simplifies user management and reduces the overhead of managing identities in multiple places.
upvoted 
2 
times
selected
selected
 
2 months, 2 weeks ago
Selected Answer: 
B
cross-domain SSO can be achieved by SAML
upvoted 
1 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 4 weeks ago
Selected Answer: 
B
I think B is correct
upvoted 
1 
times
maxdanny
maxdanny
 
4 months ago
Selected Answer: 
B
Minimal user disruption: By federating authentication via SAML 2.0, users can continue using their existing corporate credentials
without having to manage or remember new passwords.
Security requirements: SAML 2.0 federation allows your organization to maintain control over user authentication and password
management within the existing Identity Provider (IdP). Passwords do not need to be stored in Google’s systems, which aligns
with strict security requirements.
upvoted 
1 
times
Manishjb006
Manishjb006
 
4 months, 3 weeks ago
B is right one . Because C 
While Google Cloud Directory Syc (GCDS) helps sync users between an on-premises directory and
Google, it does not address the password management aspect. Users may still face disruptions as this method might not handleexisting passwords securely.
upvoted 
1 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Choose B
upvoted 
1 
times
kingfighers
kingfighers
 
6 months, 4 weeks ago
the most convenient way is B, but the principle of this kind of exam is use cloud provider's native tools, so the C is correct.. this
principle is also used on aws
upvoted 
1 
times
santoshchauhan
santoshchauhan
 
10 months ago
Selected Answer: 
B
B. Federate authentication via SAML 2.0 to the existing Identity Provider.
Here's why:
Security: SAML 2.0 allows for secure single sign-on (SSO) without storing passwords on Google's side. It ensures that
authentication happens against the corporate Identity Provider (IdP), which maintains control over the user credentials.
Minimal Disruption: Users can continue to use their existing corporate credentials to access the application on GCP without having
to remember a new set of credentials or go through a password change process.
Compliance: It satisfies the security team's requirements for password storage by ensuring that passwords remain within the
corporate boundary.
Integration: SAML is widely supported and can be integrated with many IdPs, allowing for a seamless transition to cloud-based
resources while leveraging existing identity management infrastructure.
upvoted 
6 
times
lisabisa
lisabisa
 
10 months, 2 weeks ago
The correct answer is C.
Google Cloud Directory Sync will provide federated authentications.
B is wrong because SAML is used for Single sign-on. It also doesn't mention how the cloud can be authenticated to the existing
Identity Provider. SAML by itself is not enough to do the job.
upvoted 
2 
times
xxoox
xxoox
 
10 months, 2 weeks ago
Selected Answer: 
B
Federating authentication aligns with strict security team requirements for password storage, as it avoids the need to store or sync
passwords outside the corporate environment.
upvoted 
2 
times
hzaoui
hzaoui
 
11 months, 4 weeks ago
Selected Answer: 
B
Minimal User Disruption:
Users continue using their existing corporate credentials for both on-premises and GCP applications, avoiding password resets or
new account creations.
Security Team Requirements:
GCP doesn't store or manage corporate passwords; authentication relies on the existing Identity Provider (IdP), meeting strict
password storage requirements.
upvoted 
1 
times
02fc23a
02fc23a
 
1 year, 1 month ago
Selected Answer: 
B
B is a preferred solution nowadays, that's why:
https://cloud.google.com/architecture/framework/security/identity-access#use_a_single_identity_provider
upvoted 
2 
times
nideesh
nideesh
 
1 year, 1 month agonideesh
nideesh
 
1 year, 1 month ago
Selected Answer: 
C
GCDS is better as it is a corporate application. The requirements for storing password can be met by GCP. As GCP has many
security features
For SAML, the corporate needs to have Identity provider service such as the one provided by Google, Facebook
upvoted 
1 
times
nideesh
nideesh
 
1 year, 1 month ago
Also the application needs to be modified to use identity provider service, if they are going by choice B
upvoted 
1 
times
asciimo
asciimo
 
1 year, 1 month ago
Selected Answer: 
B
main reason for B are strict storage requirements.
upvoted 
1 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
B is the correct answer
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #12
Your company has successfully migrated to the cloud and wants to analyze their data stream to optimize operations. They do
not have any existing code for this analysis, so they are exploring all their options. These options include a mix of batch and
stream processing, as they are running some hourly jobs and live- processing some data as it comes in. 
Which technology should they use for this? 
A. 
Google Cloud Dataproc
B. 
Google Cloud Dataflow 
Most Voted
C. 
Google Container Engine with Bigtable
D. 
Google Compute Engine with Google BigQuery
Correct Answer:
 
B 
Comments
Eroc
Eroc
 
Highly Voted
 
3 months, 2 weeks ago
All four options can accomplish what the question asks, in regards to batching and streaming processes. "A" is for Apache Spark
and Hadoop, a juggernaut in speed of data processing. "B" is Google's best attempt at TIBCO, Ab Initio, and other processing
technology, built explicity for visualizing batch operations and streams without through various labeled circuit boards. "C" and "D"
are used within "A" and "B" and would require more work and higher risk. I'd guess Google wants you to select "B"
upvoted 
36 
times
2g
2g
 
Highly Voted
 
4 years, 11 months ago
answer: B
upvoted 
6 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
B
1. Unified Batch and Stream Processing: Dataflow is a fully managed service designed for both batch and stream data
processing. This makes it ideal for your company's needs, as they require both hourly batch jobs and live stream processing.
2. No Existing Code: Dataflow provides a unified programming model and SDKs (Java, Python) for building data pipelines, which is
beneficial since your company doesn't have existing code and needs to develop new solutions.
3. Serverless and Scalable: Dataflow is serverless, meaning you don't need to manage infrastructure. It automatically scales
resources based on the workload, ensuring efficient processing of both batch and stream data.
Community vote distribution
B (100%)resources based on the workload, ensuring efficient processing of both batch and stream data.
4. Cost-Effective: Dataflow's autoscaling and pay-per-use model optimize costs by only utilizing resources when needed.
upvoted 
4 
times
Singapore123
Singapore123
 
3 months, 1 week ago
Selected Answer: 
B
B. Google Cloud Dataflow
Explanation:
Unified Processing:
Google Cloud Dataflow is designed to handle both batch and stream processing in a unified manner. This means you can process
data as it arrives (stream processing) and also perform scheduled batch jobs efficiently.
Serverless and Scalable:
Dataflow is serverless, which means you don’t have to worry about managing the underlying infrastructure. It automatically scales
to handle varying workloads, making it ideal for optimizing operations based on live data streams and scheduled jobs.
Integration with Other Google Cloud Services:
Dataflow integrates well with other Google Cloud services, such as Google Cloud Storage, BigQuery, and Pub/Sub. This makes it
easier to build a comprehensive data pipeline that can analyze data streams effectively.
Flexible SDKs:
Dataflow supports popular programming languages like Java and Python, allowing your team to write custom processing logic as
needed.
upvoted 
3 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Choose B
upvoted 
1 
times
hzaoui
hzaoui
 
11 months, 4 weeks ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
devakram
devakram
 
1 year ago
chatGPT answers:
B. Google Cloud Dataflow
Google Cloud Dataflow is a fully managed service for stream and batch data processing. It is built on Apache Beam and provides
a unified programming model, making it an ideal choice for scenarios where both batch and stream data processing are required.
Dataflow simplifies the complexities of data parallel processing, allowing for easy development and maintenance of data
processing pipelines. It integrates well with other Google Cloud services, like BigQuery for analytics and Cloud Storage for storing
data, providing a comprehensive solution for real-time and batch data processing needs.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
The word analysis throws me off. 
Wonder if the question is just written incorrectly here? 
I'd say Dataflow is a key tool to enable the
processing of the data to be able to do the analysis but feels like the final analysis should be in a database.
upvoted 
5 
times
alekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
B
B is the answer
upvoted 
2 
times
Deb2293
Deb2293
 
1 year, 10 months ago
Selected Answer: 
B
A is a managed Hadoop and Spark service. C and D are mostly for petabyte kinds of data. So remains B (suitable for ETL jobs)
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
To analyze a data stream and optimize operations, your company could consider using Google Cloud Dataflow, which is a fully-To analyze a data stream and optimize operations, your company could consider using Google Cloud Dataflow, which is a fully-
managed, cloud-native data processing service that can handle both batch and stream processing.
Google Cloud Dataflow is designed to handle large volumes of data and can scale up or down automatically to meet the needs of
the workload. It provides a number of pre-built connectors and integrations that make it easy to ingest data from a variety of
sources, and it offers a range of processing options, including batch processing and stream processing, that can be used to
analyze the data in real-time.
Option A: Google Cloud Dataproc, option C: Google Container Engine with Bigtable, and option D: Google Compute Engine with
Google BigQuery, while potentially useful for certain types of data processing, would not necessarily be well-suited to handle both
batch and stream processing in the way that Google Cloud Dataflow can
upvoted 
3 
times
thamaster
thamaster
 
2 years ago
answer is D for me the question is which tool for analyse data. Dataflow does not analyse data
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
ok for B
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
B
B is the right answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
B is correct
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
B. Google Cloud Dataflow
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
correct is B use data flow for stream and batch process
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #13
Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30
seconds to load for some of their users. 
This behavior was not reported before the update. 
What strategy should you take? 
A. 
Work with your ISP to diagnose the problem
B. 
Open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application
C. 
Roll back to an earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the problem in a
development/test/staging environment 
Most Voted
D. 
Roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use
Stackdriver Trace and Logging to diagnose the problem
Correct Answer:
 
C 
Comments
TosO
TosO
 
Highly Voted
 
5 years, 1 month ago
C is the answer
upvoted 
27 
times
MyPractice
MyPractice
 
Highly Voted
 
3 months, 2 weeks ago
Key word: This behavior was not reported before the update
A - Not Correct as it was working before with same ISP
B - New code update caused an issue- why to open support ticket
C - I agree with C
D - This requires downtime and live prod affected too
upvoted 
17 
times
MyPractice
MyPractice
 
5 years ago
"then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment" they are NOT
asking us to setup Dev/Text/Stage.. meaning the environment already exist and we have to use it
upvoted 
1 
times
Community vote distribution
C (96%)
D
(4%)upvoted 
1 
times
hafid
hafid
 
4 years, 6 months ago
"then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment" this is not asking
for set environment either, it just says to diagnose problem in other environment so C it is
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
C
1. Prioritize User Experience: Rolling back to a stable version quickly minimizes user impact and restores the application to a
functional state. This should be the immediate first step.
2. Controlled Environment: Diagnosing the issue in a development/test/staging environment allows you to investigate without
affecting real users. You can reproduce the problem, gather data, and test potential solutions safely.
3. Powerful Diagnostic Tools: Stackdriver Trace helps you pinpoint performance bottlenecks by tracing requests across your
application. Stackdriver Logging provides detailed logs to understand application behavior and identify errors.
upvoted 
3 
times
hzaoui
hzaoui
 
11 months, 4 weeks ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
C
Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to
load for some of their users.
This behavior was not reported before the update.
What strategy should you take?
Here the application (our code) is updated and only some users are facing lantecy (Cloud Trace) issue.
The issue is not with ISP (A), Not an issue with Google (B).
Rollback must be done as mitigation, but testing should be done in Non-Prod environments (C), not on prod environment (D).
Hence C is correct answer.
upvoted 
1 
times
jrisl1991
jrisl1991
 
1 year, 3 months ago
Selected Answer: 
C
I'm going for C. While D may be "better" in case this is an issue that only occurs in production, I think that keeping the disruption at
minimum would be the best practice, which D would not really do. Plus, if the problem is load related, having this released at a
quieter period may not surface the problem either.
upvoted 
2 
times
frankryuu
frankryuu
 
1 year, 6 months ago
Selected Answer: 
C
Although it sounds like the right answer to do network tracing in stg again, this may be a network pass-through related issue and it
is felt that the problem may not be reproduced if not checked in a prod environment.
upvoted 
2 
times
frankryuu
frankryuu
 
1 year, 6 months ago
Although it sounds like the right answer to do network tracing in stg again, this may be a network pass-through related issue and it
is felt that the problem may not be reproduced if not checked in a prod environment.
upvoted 
1 
times
FigVam
FigVam
 
1 year, 8 months ago
Selected Answer: 
C
should be C
upvoted 
2 
times
alekonko
alekonko
 
1 year, 9 months agoalekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
C
C is the answer
upvoted 
2 
times
JC0926
JC0926
 
1 year, 9 months ago
Option C is also a valid strategy in this scenario. Rolling back to an earlier known good release initially and using Stackdriver Trace
and Logging to diagnose the problem in a development/test/staging environment can help diagnose the issue without impacting
production users.
However, the reason why option D may be a better approach is that it allows for investigation during a quieter period, which can
reduce the impact of any issues that may occur during the investigation. Rolling back to a known good release and then pushing
the release again at a quieter period can help to ensure that users are not impacted during the investigation.
upvoted 
3 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
ok for C
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is perfect to troubleshoot latency issues with app
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
C
C. Roll back to an earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the problem in a
development/test/staging environment
A and B are not relevant
D - no IT manager will ever allow re-deployment of erroneous code in production, even in a quiet period...!
upvoted 
3 
times
Kiroo
Kiroo
 
1 year, 8 months ago
I agree why not D, but in the past I faced issues only reproducible in prd, at that situation D was a possibility but usually yep C is
for sure
upvoted 
2 
times
holerina
holerina
 
2 years, 3 months ago
correct answer is C use the standard practise
upvoted 
1 
times
Amit_arch
Amit_arch
 
2 years, 3 months ago
Selected Answer: 
D
How come everyone is agreeing to C!! In option C after rollback, the investigation will happen only on the earlier good release.
Whereas in option D, all the troubleshooting will happen on current/problematic build. Option D should be the right option as it
resolves the issue in short term and provides room for further investigation without downtime.
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
you want to minimize the business loose, best option is to rollback and use stack-driver to diagnose the issue
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
2 years, 3 months agoOption C is investigating the bad build in test. 
The problem with option D is it is user impacting. 
Always best to attempt to find the
problem in a test environment first. 
D could end-up being an option of last resort if all attempts to diagnose in test fail but I doubt
any business person would be happy with D as it impacts service.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #14
A production database virtual machine on Google Compute Engine has an ext4-formatted persistent disk for data files. The
database is about to run out of storage space. 
How can you remediate the problem with the least amount of downtime? 
A. 
In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.
Most Voted
B. 
Shut down the virtual machine, use the Cloud Platform Console to increase the persistent disk size, then restart the
virtual machine
C. 
In the Cloud Platform Console, increase the size of the persistent disk and verify the new space is ready to use with the
fdisk command in Linux
D. 
In the Cloud Platform Console, create a new persistent disk attached to the virtual machine, format and mount it, and
configure the database service to move the files to the new disk
E. 
In the Cloud Platform Console, create a snapshot of the persistent disk restore the snapshot to a new larger disk,
unmount the old disk, mount the new disk and restart the database service
Correct Answer:
 
A 
Comments
TosO
TosO
 
Highly Voted
 
5 years, 1 month ago
A is the correct answer because the question says "with minimum downtime"
upvoted 
28 
times
passnow
passnow
 
Highly Voted
 
5 years ago
least amount of downtime? is the sugar word. You miss that you miss all. Everything there is correct but I believe its only A that fits
that requirement
upvoted 
14 
times
raj117
raj117
 
3 years, 9 months ago
but in option A, nowhere it is mentioned to shut down the VM.
Community vote distribution
A (89%)
Other (11%)upvoted 
1 
times
monkeym
monkeym
 
3 years, 5 months ago
No need to reboot.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
A
1. Online Resizing: Google Cloud Platform allows you to increase the size of a persistent disk while it's attached to a running VM.
This means you don't need to shut down the database server.
2. resize2fs: This Linux command extends the file system to utilize the newly added space on the disk. It can be run while the file
system is mounted, minimizing downtime.
upvoted 
2 
times
SerGCP
SerGCP
 
2 months ago
Selected Answer: 
C
A) can not work becouse you must extend partition and after than you can extend filesystem 
C) ok becouse you can use fdisk in linux to extend partition and after than you can resize ext4 filesystem
upvoted 
1 
times
Nora9
Nora9
 
1 year, 1 month ago
Selected Answer: 
A
A is the right answer.You should resize the disk, take a snapshot, then resize the filesystem and partitions (eg.) ext4, xfs etc.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
A
Unlike Azure, in google you can dynamically resize the persistent disk while VM is running. This narrows down the option to A or C.
Since the question says "ext4-formatted persistent disk", we need to choose correct command (resize2fs or fdisk ) for Linux for
resizing ext4 file format disk. To resize an ext4 file system in Linux, you can use the resize2fs command. FDISK to manipulate
partition tables in Linux.
upvoted 
8 
times
blackhawk86
blackhawk86
 
1 year, 4 months ago
Selected Answer: 
A
According to the right url, A is the right answer. https://cloud.google.com/compute/docs/disks/resize-persistent-disk
upvoted 
4 
times
blackhawk86
blackhawk86
 
1 year, 4 months ago
The right URL for the oficial document is, https://cloud.google.com/compute/docs/disks/resize-persistent-disk
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
A is no brainer
upvoted 
1 
times
jalberto
jalberto
 
1 year, 4 months ago
Selected Answer: 
E
E is the correct because is true you need minimum downtime but in Production a backup is a must.
upvoted 
2 
times
jalberto
jalberto
 
1 year, 4 months ago
E because you are in Production, and you need a backup
upvoted 
1 
times
alekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
AA is correct, resize disk don't required reboot or downtime
https://cloud.google.com/compute/docs/disks/resize-persistent-disk
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
A: Increasing the size of the persistent disk in the Cloud Platform Console and using the resize2fs command in Linux.
Increasing the size of the persistent disk can be done without requiring the virtual machine to be shut down, and the resize2fs
command can be used to resize the ext4 filesystem on the disk to take advantage of the additional space. This will allow you to
add more storage space to the virtual machine without disrupting the database service.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A https://cloud.google.com/compute/docs/disks/resize-persistent-disk?_ga=2.233866652.-3622898.1631303718
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
A
A. In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
yes, A is right
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
A resize the disk standard command
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #15
Your application needs to process credit card transactions. You want the smallest scope of Payment Card Industry (PCI)
compliance without compromising the ability to analyze transactional data and trends relating to which payment methods are
used. 
How should you design your architecture? 
A. 
Create a tokenizer service and store only tokenized data 
Most Voted
B. 
Create separate projects that only process credit card data
C. 
Create separate subnetworks and isolate the components that process credit card data
D. 
Streamline the audit discovery phase by labeling all of the virtual machines (VMs) that process PCI data
E. 
Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor
Correct Answer:
 
A 
Comments
AD2AD4
AD2AD4
 
Highly Voted
 
4 years, 7 months ago
Final Decision to go with Option A. I have done PCI DSS Audit for my project and thats the best suited case. 100% sure to use
tokenised data instead of actual card number
upvoted 
47 
times
Musk
Musk
 
4 years, 6 months ago
But with A you cannot extract statistics. That is the second r4equirement.
upvoted 
4 
times
Arimaverick
Arimaverick
 
3 years, 12 months ago
Analyzing Transaction does not require Credit Card number I guess. Only amount of transaction or balance what is needed. We
also perform something similar with transactional data with tokenized PII information. So CC can be tokenized. So answer
should be A.
upvoted 
6 
times
Musk
Musk
 
4 years, 5 months ago
Community vote distribution
A (100%)Thinking about that better, I think you can because you are only tokenizing the sensitive data, not the transaction type.
upvoted 
2 
times
RitwickKumar
RitwickKumar
 
2 years, 4 months ago
You can as the generated token for a given credit card would be same(generally but there are approaches which can give you
different token for the same sensitive data input). Only thing that you won't know is the actual card number which is not required
for the trend analysis. 
When the trend analysis involves referential integrity then tokenization process becomes challenging but still once data is
tokenized correctly you should be able to perform any kind of the analysis.
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
I agree. A is the best option
upvoted 
3 
times
omermahgoub
omermahgoub
 
Highly Voted
 
2 years ago
To minimize the scope of Payment Card Industry (PCI) compliance while still allowing for the analysis of transactional data and
trends related to payment methods, you should consider using a tokenizer service and storing only tokenized data, as described in
option A.
Tokenization is a process of replacing sensitive data, such as credit card numbers, with unique, randomly-generated tokens that
cannot be used for fraudulent purposes. By using a tokenizer service and storing only tokenized data, you can reduce the scope of
PCI compliance to only the tokenization service, rather than the entire application. This can help minimize the amount of sensitive
data that needs to be protected and reduce the overall compliance burden.
upvoted 
33 
times
oxfordcommaa
oxfordcommaa
 
1 year, 11 months ago
man, this is an amazing answer. props
upvoted 
5 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
thanks to chagpt, are you serious saying that?
upvoted 
1 
times
Saxena_Vibhor
Saxena_Vibhor
 
1 year ago
Nicely explained, thanks.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
A
1. Reduced PCI Scope: Tokenization replaces sensitive credit card data with non-sensitive tokens. This significantly reduces the
scope of PCI DSS compliance, as you no longer store actual cardholder data. Only the tokenization service needs to be PCI
compliant.
2. Data Analysis: You can still analyze transactional data and trends using the tokenized data. The tokens can be linked back to the
original cardholder data if needed for specific analysis or reporting purposes, but this would be done in a controlled and secure
environment.
upvoted 
2 
times
hzaoui
hzaoui
 
11 months, 3 weeks ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
devinss
devinss
 
1 year, 4 months ago
Not sure why 100% agree on A. To limit PCI DSS scope, the data handling should be done in a separate project with very limited
access. Only in this project should tokenization be done and made available for analytics. The first requirement however, is
isolating the payment and tokenization code in a separate project. Answer should be B.
upvoted 
2 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months agoheretolearnazure
heretolearnazure
 
1 year, 4 months ago
Tokenizing is the best way to protect PCI information.
upvoted 
1 
times
nocrush
nocrush
 
1 year, 5 months ago
Selected Answer: 
A
A is my best option
upvoted 
1 
times
KjChen
KjChen
 
2 years, 1 month ago
Selected Answer: 
A
https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss
upvoted 
4 
times
andreavale
andreavale
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
A
A. Create a tokenizer service and store only tokenized data
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
2 years, 3 months ago
B appears the most thorough but the question asks to comply with the smallest scope, network segmentation is not a must.
Tokenization is simpler. 
C is similar to B, more than required. 
D & E do not address the problem.
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
correct answer is A use tokenize
upvoted 
2 
times
abirroy
abirroy
 
2 years, 3 months ago
Selected Answer: 
A
Correct answer A
upvoted 
1 
times
Nirca
Nirca
 
2 years, 8 months ago
Selected Answer: 
A
The mandatory stage in PCI is having a encryption/ description system. Data must not be stored as is with PAN. 
So A IS A MUST.
The rest are nice to have.
upvoted 
1 
times
ryzior
ryzior
 
2 years, 9 months ago
Selected Answer: 
A
I think it should be A and C 
- the paper states clearly, a proper network segmentation is still required to disparate the vault and
token servers from the rest of the flat network.
upvoted 
1 
times
sjmsummer
sjmsummer
 
2 years, 11 months ago
I chose A. But why C is not good?
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
A is the correct answer
https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-
dss#a_service_for_handling_sensitive_information
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #16
You have been asked to select the storage system for the click-data of your company's large portfolio of websites. This data is
streamed in from a custom website analytics package at a typical rate of 6,000 clicks per minute. With bursts of up to 8,500
clicks per second. It must have been stored for future analysis by your data science and user experience teams. 
Which storage infrastructure should you choose? 
A. 
Google Cloud SQL
B. 
Google Cloud Bigtable 
Most Voted
C. 
Google Cloud Storage
D. 
Google Cloud Datastore
Correct Answer:
 
B 
Comments
victory108
victory108
 
Highly Voted
 
3 years, 6 months ago
B. Google Cloud Bigtable
upvoted 
12 
times
jeff001
jeff001
 
Highly Voted
 
3 years, 8 months ago
B, Bigtable due to the IoT like requirements
upvoted 
10 
times
fff2e69
fff2e69
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
B
Google Cloud Bigtable is well-suited for handling high-throughput, low-latency workloads like clickstream data. It is optimized for
analytics on time-series and event data at scale, and it supports high write rates, with the capacity to handle thousands of writes
per second. This makes it ideal for storing large volumes of clickstream data with bursts, ensuring data is available for analysis by
data science and user experience teams.
upvoted 
3 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
B
Community vote distribution
B (88%)
C (12%)1. High throughput for clickstream data: Bigtable is a NoSQL database designed for high write throughput, making it ideal for
handling the continuous stream of click data with bursts up to 8,500 clicks per second.
2. Scalability: Bigtable is highly scalable, allowing you to handle increasing data volumes as your website portfolio grows.
3. Low latency: Bigtable provides low latency data access, which is important for real-time analysis and reporting on clickstream
data.
4. Integration with BigQuery: Bigtable integrates well with BigQuery, enabling your data science team to perform complex analysis
and generate insights from the clickstream data.
upvoted 
4 
times
upliftinghut
upliftinghut
 
6 months, 2 weeks ago
Selected Answer: 
C
Big table if it needs real time but here the need for analysis is not urgent => Google cloud storage.
upvoted 
2 
times
pakilodi
pakilodi
 
1 year, 1 month ago
Selected Answer: 
B
The right answer is BigTable for a such large volume of data.
upvoted 
1 
times
Nora9
Nora9
 
1 year, 1 month ago
Selected Answer: 
B
B. Google cloud Bigtable is most suitable as it is for analysis and streamed data.
upvoted 
1 
times
MartinFish
MartinFish
 
1 year, 4 months ago
Selected Answer: 
B
Note: DataStore is now rename as FireStore.
upvoted 
1 
times
hiromi
hiromi
 
1 year, 9 months ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
alekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
B
Bigtable is a high-perf NoSQL db service that handle large volumes of structured data with low latency
upvoted 
3 
times
zerg0
zerg0
 
1 year, 11 months ago
Selected Answer: 
B
The Google Cloud Bigtable 
goes together with the BigQuery. The question itself gives away a bit.
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
For storing click-data that is streamed in at a rate of 6,000 clicks per minute, with bursts of up to 8,500 clicks per second, and that
needs to be stored for future analysis by your data science and user experience teams, you should consider using a scalable,
high-performance, and low-latency NoSQL database such as Google Cloud Bigtable, option B.
Google Cloud Bigtable is a fully managed, high-performance NoSQL database service that is designed to handle large volumes of
structured data with low latency. It is well-suited for storing high-velocity data streams and can scale to handle millions of reads
and writes per second.
Option A: Google Cloud SQL, option C: Google Cloud Storage, and option D: Google Cloud Datastore, would not be suitable for
this use case, as they are not designed to handle high-velocity data streams at this scale.
upvoted 
4 
times
i_am_robot
i_am_robot
 
2 years ago
B. Google Cloud BigtableB. Google Cloud Bigtable
Google Cloud Bigtable is a scalable, high-performance NoSQL database that is well-suited for storing large amounts of data with
low latency. It is designed for high-throughput workloads such as streaming data, and is able to handle bursts of up to millions of
reads and writes per second.
Given the high volume of click data that needs to be stored and the requirement for low latency, Google Cloud Bigtable would be a
good choice for storing the data. It is able to handle the high rate of incoming data and provide fast access to the data for analysis
by the data science and user experience teams.
Google Cloud SQL is a fully-managed relational database service, and may not be the best choice for storing high-volume
streaming data. Google Cloud Storage is an object storage service, and may not provide the necessary performance for storing
and querying large amounts of data in real-time. Google Cloud Datastore is a NoSQL document database, and while it may be
suitable for storing large amounts of data, it may not provide the necessary performance for handling high volumes of streaming
data.
upvoted 
2 
times
Melampos
Melampos
 
2 years ago
Selected Answer: 
B
https://cloud.google.com/bigtable#section-9
upvoted 
1 
times
Bry_040706
Bry_040706
 
2 years, 1 month ago
B. Google Cloud Bigtable
upvoted 
1 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
B
Cloud Bigtable has all the features to fulfill the requirements mentioned in the question
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
D is right answer
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #17
You are creating a solution to remove backup files older than 90 days from your backup Cloud Storage bucket. You want to
optimize ongoing Cloud Storage spend. 
What should you do? 
A. 
Write a lifecycle management rule in XML and push it to the bucket with gsutil
B. 
Write a lifecycle management rule in JSON and push it to the bucket with gsutil 
Most Voted
C. 
Schedule a cron script using gsutil ls 
ג
"€lr gs://backups/** to find and remove items older than 90 days
D. 
Schedule a cron script using gsutil ls 
ג
"€l gs://backups/** to find and remove items older than 90 days and schedule it
with cron
Correct Answer:
 
B 
Comments
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
All four are correct answers. Google has built in cron job schduling with Cloud Schedule, so that would place "D" behind "C" in
Google's perspective. Google also has it's own lifecycle management command line prompt gcloud lifecycle so "A" or "B" could be
used. JSON is slightly faster than XML because of the "{" verse "<c>" distinguisher, with a Trie tree used for alphanumeric parsing.
So between "A" and "B", choose "B". Between "B" and "A", "B" is slightly more efficient from the GCP operator perspective. So
choose "B".
upvoted 
36 
times
ghitesh
ghitesh
 
4 years, 11 months ago
gsutil command takes only json as input for lifecycle management. In case of API, both XML and json can be used.
https://cloud.google.com/storage/docs/gsutil/commands/lifecycle
https://cloud.google.com/storage/docs/xml-api/put-bucket-lifecycle
https://cloud.google.com/storage/docs/json_api/v1/buckets/update
upvoted 
31 
times
tartar
tartar
 
4 years, 5 months ago
B is ok
upvoted 
8 
times
 
3 years, 10 months ago
Community vote distribution
B (100%)nitinz
nitinz
 
3 years, 10 months ago
B is correct. Policy = JSON format. No matter if its AWS or GCP.
upvoted 
10 
times
clouddude
clouddude
 
Highly Voted
 
4 years, 7 months ago
I'll go with B.
A is not reasonable because life cycle policies are not written in XML.
B is reasonable and is cloud native.
C requires a cron script which needs something to run the script and is a non-cloud native approach.
D requires a cron script which needs something to run the script and is a non-cloud native approach.
upvoted 
15 
times
Mitthugcpguru
Mitthugcpguru
 
Most Recent
 
2 weeks, 4 days ago
Selected Answer: 
B
Gsutil take only json as input so B answer
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
B
1. Lifecycle Management: Google Cloud Storage offers built-in lifecycle management rules specifically designed for automated
data retention and deletion. This is the most efficient and cost-effective way to manage your backup files.
2. JSON Format: Lifecycle rules are defined in JSON format.
3. gsutil: The gsutil command-line tool is used to interact with Cloud Storage, including setting lifecycle configuration.
upvoted 
1 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
choose B
upvoted 
1 
times
squishy_fishy
squishy_fishy
 
1 year, 2 months ago
The correct available answer is B. But in real life, we use Terraform tfvars file.
upvoted 
1 
times
eka_nostra
eka_nostra
 
1 year, 5 months ago
Selected Answer: 
B
Cloud Storage has lifecycle management rules and could be applied with gsutil and gcloud storage buckets. It is common to use
JSON for transferring data.
upvoted 
2 
times
alekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
B
B, gsutil can set policy using json file
https://cloud.google.com/storage/docs/gsutil/commands/lifecycle#examples
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
To remove backup files older than 90 days from a Cloud Storage bucket and optimize ongoing Cloud Storage spend, you should
consider writing a lifecycle management rule in JSON and pushing it to the bucket with gsutil, as described in option B.
Lifecycle management rules allow you to automatically delete objects from a Cloud Storage bucket based on age or other criteria,
such as the object's storage class. By writing a rule in JSON and pushing it to the bucket with gsutil, you can specify that objects
older than 90 days should be deleted, ensuring that the bucket only contains current backup files and minimizing Cloud Storage
spend.
Option A, C and D would not be suitable for this use case, as they do not allow you to specify lifecycle management rules that
delete objects based on age.
upvoted 
3 
times
i_am_robot
i_am_robot
 
2 years agoi_am_robot
i_am_robot
 
2 years ago
B. Write a lifecycle management rule in JSON and push it to the bucket with gsutil
To remove backup files older than 90 days from a Cloud Storage bucket, you can use the lifecycle management feature in Cloud
Storage. This feature allows you to specify rules to automatically delete objects based on their age.
upvoted 
1 
times
Melampos
Melampos
 
2 years ago
Selected Answer: 
B
https://cloud.google.com/storage/docs/gsutil/commands/lifecycle
upvoted 
1 
times
Bry_040706
Bry_040706
 
2 years, 1 month ago
B. Life cycle management using JSON.
upvoted 
1 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
B
B with JSON option is correct
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
B
B is the right answer
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
life cycle management is the answer written in JSON format. JSON is easier to write and read compared to XML which you can
not use in commands
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Lifecycle management with JSON is right .. I will go with B
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #18
Your company is forecasting a sharp increase in the number and size of Apache Spark and Hadoop jobs being run on your local
datacenter. You want to utilize the cloud to help you scale this upcoming demand with the least amount of operations work and
code change. 
Which product should you use? 
A. 
Google Cloud Dataflow
B. 
Google Cloud Dataproc 
Most Voted
C. 
Google Compute Engine
D. 
Google Kubernetes Engine
Correct Answer:
 
B 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
"B. Google Cloud Dataproc" is the answer
upvoted 
19 
times
VinayakBudapanahalli
VinayakBudapanahalli
 
Highly Voted
 
3 years, 11 months ago
Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing,
querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save
money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on
your jobs and your data.
https://cloud.google.com/dataproc/docs/concepts/overview#:~:text=Dataproc%20is%20a%20managed%20Spark,%2C%20strea
ming%2C%20and%20machine%20learning.&text=With%20less%20time%20and%20money,your%20jobs%20and%20your%20da
ta.
upvoted 
13 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Agreed
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Community vote distribution
B (100%)Ekramy_Elnaggar
Ekramy_Elnaggar
 
 
1 month, 3 weeks ago
Selected Answer: 
B
1. Managed Hadoop and Spark: Dataproc is specifically designed for running and managing Apache Spark and Hadoop clusters,
which directly addresses your company's needs.
2. Scalability: Dataproc allows you to easily scale your clusters to handle the increasing number and size of jobs. You can add or
remove nodes as needed to accommodate the workload.
3. Minimal Operations Work: Dataproc automates cluster creation, configuration, and management, minimizing the operational
overhead. This is crucial since you want to reduce operations work.
4. Code Compatibility: Dataproc is compatible with existing Spark and Hadoop code, so you can migrate your jobs with minimal or
no code changes.
upvoted 
2 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 4 weeks ago
Selected Answer: 
B
B is correct because Dataproc is uded for Apache Hadoop and Spark
upvoted 
1 
times
eka_nostra
eka_nostra
 
1 year, 5 months ago
Selected Answer: 
B
Dataflow for data stream and batch.
Dataproc for data process with Apache Spark and Hadoop.
Compute Engine for VM.
Kubernetes Engine for Kubernetes Cluster with Compute Engine under the hood.
upvoted 
3 
times
alekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
B
B, Dataproc is Hadoop/Spark managed 
service in GCP
upvoted 
2 
times
examch
examch
 
2 years ago
Selected Answer: 
B
Dataproc is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+
open source tools and frameworks. Use Dataproc for data lake modernization, ETL, and secure data science, at scale, integrated
with Google Cloud, at a fraction of the cost.
https://cloud.google.com/dataproc
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
To scale the number and size of Apache Spark and Hadoop jobs being run on a local datacenter with the least amount of
operations work and code change, you should consider using Google Cloud Dataproc, option B. Google Cloud Dataproc is a fully-
managed service that makes it easy to run Apache Spark and Hadoop workloads in the cloud. It is designed to simplify the
process of setting up and managing clusters for data processing, and allows you to scale quickly and easily as demand increases.
With Cloud Dataproc, you can create and delete clusters in just a few minutes, and you can use the familiar Apache Spark and
Hadoop APIs and tools to process data. This means that you can utilize the cloud to scale your workloads with minimal changes to
your code and operations work.
Option A: Google Cloud Dataflow, option C: Google Compute Engine, and option D: Google Kubernetes Engine, would not be
suitable for this use case, as they do not provide the same level of support for running Apache Spark and Hadoop workloads as
Cloud Dataproc.
upvoted 
2 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
B
B. Dataproc is managed Apache Spark and Hadoop in GCP
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
Dataproc for Hadoop and spark ecosystem
upvoted 
1 
timesupvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
B. Google Cloud Dataproc
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
B data proc for hadoop and spark
upvoted 
1 
times
Dhiraj03
Dhiraj03
 
2 years, 6 months ago
Keyword - Apache Spark and Hadoop jobs - Go with Dataproc
upvoted 
1 
times
Superr
Superr
 
2 years, 7 months ago
Selected Answer: 
B
dataproc
upvoted 
1 
times
Nirca
Nirca
 
2 years, 8 months ago
Selected Answer: 
B
Google Cloud Dataproc == managed Spark and Hadoop service
upvoted 
2 
times
pakochiu
pakochiu
 
2 years, 8 months ago
Selected Answer: 
B
B - Dataproc Lift&Shift of Apache Spark and Hadoop jobs
upvoted 
1 
times
llanerox
llanerox
 
2 years, 11 months ago
B is ok.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #19
The database administration team has asked you to help them improve the performance of their new database server running
on Google Compute Engine. The database is for importing and normalizing their performance statistics and is built with MySQL
running on Debian Linux. They have an n1-standard-8 virtual machine with 80 GB of SSD persistent disk. 
What should they change to get better performance from this system? 
A. 
Increase the virtual machine's memory to 64 GB
B. 
Create a new virtual machine running PostgreSQL
C. 
Dynamically resize the SSD persistent disk to 500 GB 
Most Voted
D. 
Migrate their performance metrics warehouse to BigQuery
E. 
Modify all of their batch jobs to use bulk inserts into the database
Correct Answer:
 
C 
Comments
shandy
shandy
 
Highly Voted
 
5 years, 1 month ago
Answer is C because persistent disk performance is based on the total persistent disk capacity attached to an instance and the
number of vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in
turn improve the performance of MySQL.
upvoted 
69 
times
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
Assuming that the database is approaching its hardware limits... both options A and C would improve performance, A would
increase number of CPUs and memory, but C would increase memory by more. If it a software problem, it is likly it is a hashing
problem (the search and sort algorithms are not specific enough to search within the database). This problem would not be fixed
just by migrating to PostgreSQL or BigQuery but modifying the inserts would help the situation because it would entail
specifications of data lookups. However, it wouldn't help with search performance just inserts and it doesn't help in normalization.
So B, D, and E are eliminated. Since statistics is based on sets, the larger the number of sets the better the predictions. This
means that the largest amount of memory would not only increase computer performance but also knowledge enhancements. So
C beats A.
upvoted 
35 
times
nitinz
nitinz
 
3 years, 10 months ago
Community vote distribution
C (72%)
A (12%)
Other (16%)C. 
universal truth - OLTP D/B performance is depended on IOPs. SSD is the best solution for higher IOPs. In GCP bigger the
disk size higher the IOPs.
upvoted 
8 
times
trainor
trainor
 
4 years ago
Also, if you increased the memory size, it would not be a n1-standard-8 anymore. You should eventually change machine type,
not simply increase memory.
upvoted 
6 
times
tartar
tartar
 
4 years, 5 months ago
C is ok.
upvoted 
8 
times
haroldbenites
haroldbenites
 
3 years, 1 month ago
When you increase the memory yo need to shutdown the machine, but when you increase the disk, it is not necessary. Answer
is B.
upvoted 
2 
times
Ric350
Ric350
 
4 months, 2 weeks ago
Repectfully, this isn't accurate. 
On Google Compute Engine, you can often increase the memory of a running virtual machine
without needing to shut it down. This is known as live migration or memory hot-add.
upvoted 
1 
times
Mission94
Mission94
 
6 months, 3 weeks ago
Since its using SQL, so there will be a Maintenance Window, so this change can be implemented during the downtime( also
there is no mention that the system should be always avaliable)
upvoted 
1 
times
Dclaiborne41
Dclaiborne41
 
2 years, 7 months ago
there isn't "without downtime"
upvoted 
1 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 8 hours ago
Selected Answer: 
C
I will go for C
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
C
OLTP D/B performance is depended on IOPs. SSD is the best solution for higher IOPs. In GCP bigger the disk size higher the
IOPs.
upvoted 
3 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Choose C
upvoted 
1 
times
ukivanlamlpi
ukivanlamlpi
 
5 months, 4 weeks ago
Selected Answer: 
A
increase size will not increase performance, it either increase RAM or serverless. 
A or D. 
if no cost concern will pick D
upvoted 
2 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
Selected Answer: 
C
I was looking for CloudSQL in options, since it is not there, C is best
upvoted 
1 
times
hzaoui
hzaoui
 
11 months, 3 weeks agohzaoui
hzaoui
 
11 months, 3 weeks ago
Selected Answer: 
E
The fact that the database is used for importing and normalizing performance statistics suggests frequent data insertions.
Optimizing this process through bulk inserts directly addresses a likely performance bottleneck.
upvoted 
1 
times
JohnDohertyDoe
JohnDohertyDoe
 
12 months ago
Selected Answer: 
A
The answer according to Google is A. This question is part of the Google's sample questions for the certification.
upvoted 
1 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
Yes, it is, and says it is C.
upvoted 
1 
times
JPA210
JPA210
 
1 year, 2 months ago
I see most of the people here replying C, but I do not think that the size of the disk we bring much gains in performance. D, yes,
seems to me that will bring much improvements in performance, management, operations and cost. So B, 
Migrate their
performance metrics warehouse to BigQuery
upvoted 
3 
times
Palan
Palan
 
1 year, 4 months ago
I would go with option E because Bulk Insert improves performance drastically unless it is been implemented already.
upvoted 
1 
times
eka_nostra
eka_nostra
 
1 year, 5 months ago
Selected Answer: 
C
Increasing disk size will also increase its performance.
https://cloud.google.com/compute/docs/disks/performance#optimize_disk_performance
upvoted 
4 
times
JC0926
JC0926
 
1 year, 8 months ago
Selected Answer: 
C
C. Dynamically resize the SSD persistent disk to 500 GB
By increasing the size of the SSD persistent disk, the database server can achieve better performance. A larger SSD persistent
disk provides higher IOPS (input/output operations per second) and throughput, allowing for faster read and write operations. This
can help improve the performance of the MySQL database server running on the Google Compute Engine instance.
upvoted 
3 
times
mifrah
mifrah
 
1 year, 9 months ago
On another website I found the question with the hint "you are not allowed to reboot the VM before next maintenance window". That
makes it more clear --> C.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
E
E. Modify all of their batch jobs to use bulk inserts into the database: This can be a very effective solution for improving
performance. Bulk inserts can greatly reduce the number of round-trips to the database, which can help to minimize latency and
improve overall throughput.
Therefore, option E is the best choice for improving performance in this scenario.
upvoted 
4 
times
Jackalski
Jackalski
 
2 years ago
Selected Answer: 
D
in option C - even increasing disc can gain performance - that will take few months to face new limits. mySQL is not desiged for
OLAP/analytics - but OLTP.OLAP/analytics - but OLTP.
so 
I vote on D
upvoted 
3 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
C
Correct answer is C. Increased disk capacity improved I/O and direct impacts the performance
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #20
You want to optimize the performance of an accurate, real-time, weather-charting application. The data comes from 50,000
sensors sending 10 readings a second, in the format of a timestamp and sensor reading. 
Where should you store the data? 
A. 
Google BigQuery
B. 
Google Cloud SQL
C. 
Google Cloud Bigtable 
Most Voted
D. 
Google Cloud Storage
Correct Answer:
 
C 
Comments
victory108
victory108
 
Highly Voted
 
3 years, 6 months ago
C. Google Cloud Bigtable
upvoted 
12 
times
khadar
khadar
 
2 years, 3 months ago
I too got this question in 10-09-22 exam with similar option and result is pass
upvoted 
4 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
C
1. High Write Throughput: Bigtable excels at handling high-volume write operations, which is crucial for your application receiving
data from 50,000 sensors sending 10 readings per second.
2. Low Latency: Bigtable offers very low latency for read operations, essential for real-time charting and data visualization.
3. Time-Series Data: Bigtable is well-suited for storing and querying time-series data, like your weather sensor readings with
timestamps.
4. Scalability: Bigtable can handle massive amounts of data and scale seamlessly as your application grows.
upvoted 
3 
times
lisabisa
lisabisa
 
10 months, 2 weeks ago
Community vote distribution
C (100%)Bigtable - NoSQL, high-throughput, low-latency, making it suitable for storing time-series data from sensors
upvoted 
2 
times
alekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
C
BigTable is NoSQL for IoT
upvoted 
2 
times
sivaamum
sivaamum
 
1 year, 10 months ago
C is correct
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
To optimize the performance of an accurate, real-time, weather-charting application that receives data from 50,000 sensors
sending 10 readings per second, it would be most appropriate to store the data in a distributed, horizontally scalable, NoSQL
database such as Google Cloud Bigtable
Other options, such as Google BigQuery, Google Cloud SQL, and Google Cloud Storage, may not be as well-suited for handling
high volumes of real-time data and may not provide the same level of performance and scalability as Google Cloud Bigtable.
upvoted 
3 
times
Bry_040706
Bry_040706
 
2 years, 1 month ago
C. Bigtable, IoT data.
upvoted 
1 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
C
C Bigtable
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
C bigtable right answer
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
real-time, IoT, time series and huge writes are some of the keywords to look after for Bigtable
upvoted 
4 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
C
C. Google Cloud Bigtable
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
C big table for IOT data
upvoted 
1 
times
abirroy
abirroy
 
2 years, 4 months ago
Selected Answer: 
C
Google Cloud Bigtable
upvoted 
1 
times
Dhiraj03
Dhiraj03
 
2 years, 6 months ago
Keyword - Timestamp - Big table
upvoted 
2 
times
szanio
szanio
 
2 years, 7 months agoGo for c
upvoted 
1 
times
Nirca
Nirca
 
2 years, 8 months ago
Selected Answer: 
C
C. Google Cloud Bigtable is the Best Practice option
upvoted 
1 
times
belly265
belly265
 
2 years, 10 months ago
Ans c - when ever thier is input from IOT devices across and time series data which is huge go for big table in gcp
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Big Table is right choice, hence C is correct
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #21
Your company's user-feedback portal comprises a standard LAMP stack replicated across two zones. It is deployed in the us-
central1 region and uses autoscaled managed instance groups on all layers, except the database. Currently, only a small group
of select customers have access to the portal. The portal meets a 
99,99% availability SLA under these conditions. However next quarter, your company will be making the portal available to all
users, including unauthenticated users. You need to develop a resiliency testing strategy to ensure the system maintains the
SLA once they introduce additional user load. 
What should you do? 
A. 
Capture existing users input, and replay captured user load until autoscale is triggered on all layers. At the same time,
terminate all resources in one of the zones
B. 
Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and
introduce 
ג
€chaos
ג
 €to the system by terminating random resources on both zones 
Most Voted
C. 
Expose the new system to a larger group of users, and increase group size each day until autoscale logic is triggered on
all layers. At the same time, terminate random resources on both zones
D. 
Capture existing users input, and replay captured user load until resource utilization crosses 80%. Also, derive estimated
number of users based on existing user's usage of the app, and deploy enough resources to handle 200% of expected load
Correct Answer:
 
B 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
resilience test is not about load, is about terminate resources and service not affected. Think it's B. The best for resilience in to
introduce chaos in the infraestructure
upvoted 
91 
times
rockstar9622
rockstar9622
 
4 years, 11 months ago
I agree with @jcmoranp, B) is correct for more info - https://cloud.google.com/solutions/scalable-and-resilient-
apps#test_your_resilience
upvoted 
20 
times
 
3 years, 1 month ago
Community vote distribution
B (81%)
Other (19%)AWSPro24
AWSPro24
 
3 years, 1 month ago
Isn't A superior in one way. 
It will demonstrate that the app is regionally redundant by demonstrating it can survive the loss of an
entire zone. 
B only demonstrates the app is zonally redundant and can lose a random instance here and there within individual
zones which is not that resilient. 
Thoughts?
upvoted 
8 
times
0xE8D4A51000
0xE8D4A51000
 
2 years, 2 months ago
No. It is only terminating the service in ONE zone. B caters for terminating the service in both zones randomly. You want to be
able to test resiliency when either zone has an outage.
upvoted 
7 
times
OSNG
OSNG
 
Highly Voted
 
4 years, 1 month ago
Will go with A. Reason: 
1. SLA in question is about the Availability (The portal meets a
99,99% availability SLA under these conditions.) therefore maintaining SLA means Availability.
2. Its a user-feedback portal and type of user input is going to be similar or same (A is capturing the user input and replaying it).
Why not B: 
The infrastructure is using MIG (Instances created using templates) most likely to be used with Health Check and killing random
VMs cannot test the availability (neither affect the availability as health check will immediately kill the effected Instances and create
the other one.)
Why not D:
SLA is about Availability not reliability or scaling. (As all of it does work hand to hand but still major focus should be on availability.)
--- IF AGREE PLEASE UP VOTE TO MAKE IT CLEAR FOR THE OTHERS --- Thank you.
upvoted 
62 
times
RitwickKumar
RitwickKumar
 
2 years, 4 months ago
Only problem with A is that it says "replay captured user load". We are not testing for the incoming unpredictable load due to the
inclusion of unauthenticated users and something that we haven't captured earlier.
Option B covers breadth and depth for the desired SLA.
upvoted 
10 
times
jay9114
jay9114
 
2 years, 4 months ago
What does "replay captured user load" mean?
upvoted 
3 
times
bolu
bolu
 
4 years ago
valuable input in terms of 'availability'. did you select this answer in exam too?
upvoted 
1 
times
amxexam
amxexam
 
3 years, 3 months ago
We are talking about resilience testing where as SLA is an argument of the system.
upvoted 
1 
times
amxexam
amxexam
 
3 years, 3 months ago
And resilience means the capacity to recover from failure.
upvoted 
1 
times
AWSPro24
AWSPro24
 
3 years, 1 month ago
A ensures the app can withstand the loss of a whole Zone which I think is important as well.
upvoted 
1 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 8 hours ago
Selected Answer: 
B
I will go for B
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks agoEkramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
B
1. Synthetic Load Generation: Creating synthetic user input allows you to simulate a wide range of user behavior and load patterns,
including spikes and sustained high traffic. This helps you test the system's ability to scale and handle unexpected loads.
2. Autoscaling Validation: By replaying the synthetic load, you can verify that the autoscaling logic is working correctly across all
layers of the LAMP stack. This ensures that the system can dynamically adjust resources to meet demand.
3. Chaos Engineering: Introducing chaos by terminating random resources simulates real-world failures and helps you test the
system's resilience to unexpected disruptions. This is crucial for maintaining the 99.99% availability SLA.
4. Controlled Environment: This approach allows you to conduct testing in a controlled environment without impacting real users.
You can gradually increase the load and introduce chaos in a measured way to identify weaknesses and improve resilience.
upvoted 
2 
times
potorange
potorange
 
4 months, 1 week ago
Selected Answer: 
A
A: requirements states "all layers" and "resiliency testing"
upvoted 
1 
times
Robert0
Robert0
 
7 months, 1 week ago
Selected Answer: 
A
I would go with A.
This solution test autoscale policy of each layer (not only one as option B refers).
Also, it propose a regional shutdown. This is a very good test commonly requested if your application is geo-redundant. In crontast,
option B propose random termination of resources, not a bad practice but a little bit vague that can be implemented terrible wrong
(for example you do not kill the interesting services or you kill the same service in both regions, thus generating a blackout)
upvoted 
1 
times
lisabisa
lisabisa
 
10 months, 2 weeks ago
Selected Answer: 
B
We need to do 1. load testing and 2. reliability test ( failover redundency )
B does both
A only tests one zone
C impacts real user experience
D 200% not necessary
upvoted 
4 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
A
You need to develop a resiliency testing strategy to ensure the system maintains the SLA once they introduce additional user load.
Need to maintain SLA of 99.9% means multiple zones, resilience means fault tolerance. Teminating all resources in one zone is
also creating a chaos.
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
B is correct
upvoted 
1 
times
VaraSrinvas
VaraSrinvas
 
1 year, 6 months ago
Selected Answer: 
D
Option D is the best resiliency testing strategy in this scenario as it ensures that the system is tested with actual user data, takes
into account the expected increase in user load, and ensures that the system is adequately scaled to handle the anticipated load.
upvoted 
2 
times
didek1986
didek1986
 
1 year, 4 months ago
Do not agree. B is 100% correct
upvoted 
1 
times
jrisl1991
jrisl1991
 
1 year, 3 months ago
But this would assume that the user load will not change; plus, the current application is visible only to a small group of select
customers - this is the current production setup. The deployment should be prepared for all existing users plus unauthenticatedcustomers - this is the current production setup. The deployment should be prepared for all existing users plus unauthenticated
users, and the load increase is unknown, so testing for 200% of "expected load" is very ambiguous.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 8 months ago
Selected Answer: 
B
B. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce
ג
€chaos
ג
 €to the system by terminating random resources on both zones.
By creating synthetic random user input and replaying the load, you can simulate the expected increased user traffic and trigger
the autoscale logic on different layers of the application. Introducing chaos to the system by terminating random resources in both
zones helps test the resiliency and redundancy of the system under stress. This strategy will help ensure that the system can
maintain the 99.99% availability SLA when subjected to additional user load.
upvoted 
5 
times
telp
telp
 
1 year, 9 months ago
Selected Answer: 
B
chaos == test resilience for google
upvoted 
2 
times
Deb2293
Deb2293
 
1 year, 10 months ago
Selected Answer: 
B
This is chaos engineering used by Netflix. https://netflixtechblog.com/tagged/chaos-engineering
upvoted 
2 
times
roaming_panda
roaming_panda
 
1 year, 11 months ago
Selected Answer: 
B
chaos == checking resilience
upvoted 
4 
times
holerina
holerina
 
2 years ago
right thought ,
upvoted 
1 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
B
B is correct; Using synthetic/random input is recommended. Chaos Engineering/Symian Army from Netflix is one of the proven
mechanism to test the resilience of the application.
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #22
One of the developers on your team deployed their application in Google Container Engine with the Dockerfile below. They
report that their application deployments are taking too long. 
 
You want to optimize this Dockerfile for faster deployment times without adversely affecting the app's functionality. 
Which two actions should you take? (Choose two.) 
A. 
Remove Python after running pip
B. 
Remove dependencies from requirements.txt
C. 
Use a slimmed-down base image like Alpine Linux 
Most Voted
D. 
Use larger machine types for your Google Container Engine node pools
E. 
Copy the source after he package dependencies (Python and pip) are installed 
Most Voted
Correct Answer:
 
CE 
Comments
aviratna
aviratna
 
Highly Voted
 
3 years, 6 months ago
C & E:
C: Smaller the base image with minimum dependency faster the container will start
E: 
Docker image build uses caching. Docker Instructions sequence matter because 
application’s dependencies change less frequently than the Python code which will help to reuse the cached layer of dependency
and only add new layer for code change for Python Source code.
upvoted 
59 
times
vincy2202
vincy2202
 
Highly Voted
 
3 years, 1 month ago
C & E are the correct answers. 
Kindly refer - https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/
upvoted 
12 
times
Community vote distribution
CE (100%)upvoted 
12 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
CE
C. Use a slimmed-down base image like Alpine Linux: 
The ubuntu:16.04 image is a full-fledged operating system, which means it's
larger and takes longer to download and build. Alpine Linux is a minimal distribution designed for containers, resulting in
significantly smaller images and faster deployments.
E. Copy the source after the package dependencies (Python and pip) are installed: 
Docker builds images in layers. Each RUN,
COPY, and ADD instruction creates a new layer. By copying the source code after installing dependencies, you can take
advantage of Docker's caching mechanism. If your source code changes, only the layers related to the source code need to be
rebuilt, not the layers related to dependencies.
upvoted 
3 
times
alessandroGPC
alessandroGPC
 
8 months, 2 weeks ago
Hi everyone, I have a doubt. The question talks about "deployment" not "build". CE are more correct, in my opinion, to accelerate
the build phase (and application management) rather than the simple deployment (with docker swarm, simple docker, kubernetes
etc etc)
upvoted 
1 
times
nickcin77
nickcin77
 
1 year ago
C&E
C - Use small images.
E - "Try to make expensive steps appear near the beginning of the Dockerfile. Steps that change often should appear near the end
of the Dockerfile" 
https://docs.docker.com/build/cache/
upvoted 
3 
times
jrisl1991
jrisl1991
 
1 year, 3 months ago
Selected Answer: 
CE
C is an obvious choice, as an optimized image will be much better for a container than the one in the file. For E, I found an
explanation here about COPY that helped me confirm that's the other solution - https://test-
dockerrr.readthedocs.io/en/latest/userguide/eng-image/dockerfile_best-practices/#:~:text=required%20files%20change.-
,For%20example%3A,-COPY%20requirements.txt.
upvoted 
1 
times
poplot321
poplot321
 
1 year, 6 months ago
I know C & E makes the most sense, but the Dockerfile is has a bug. If you don't copy requirements.txt from source, you have
nothing to pip install. The line after FROM should be: 
COPY requirements.txt requirements.txt
upvoted 
4 
times
Badri9898
Badri9898
 
1 year, 9 months ago
The two actions that should be taken to optimize the Dockerfile for faster deployment times without adversely affecting the app's
functionality are:
B. Remove dependencies from requirements.txt: The requirements.txt file should only contain necessary dependencies to reduce
the number of packages to be installed.
C. Use a slimmed-down base image like Alpine Linux: The Alpine Linux image is smaller than Ubuntu and has a smaller attack
surface, which reduces the container's build time and image size.
Therefore, options A, D, and E are not correct as they do not directly address the issue of slow deployment times caused by a
bloated Dockerfile.
upvoted 
1 
times
jrisl1991
jrisl1991
 
1 year, 3 months ago
But how do you know that requirements.txt has other dependencies? We don't have the file to confirm if there are unnecessary
dependencies. Assuming that we only have the necessary ones, removing dependencies would break the deployment.
upvoted 
1 
times
alekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
CEC: use smaller image decrease pull time
E: 
optimize build time using previous cache layer image. generate new layer only for a different app code and requirements
A: 
can't remove python
B: 
the developer choose right deps
D: changing istance type don't directly reduce deploy time
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
C & E
Using a slimmed-down base image like Alpine Linux can help reduce the size of your Docker image, which can lead to faster
deployment times. Alpine Linux is a lightweight Linux distribution that is often used as a base image for Docker images because of
its small size.
Additionally, copying the source code after installing the package dependencies can help reduce the image build time because the
dependencies will only need to be installed once, rather than every time the source code is changed. This can lead to faster
deployment times because the image build process will be faster.
upvoted 
7 
times
omermahgoub
omermahgoub
 
2 years ago
It is not recommended to remove dependencies from the requirements.txt file or remove Python after running pip, as this could
adversely affect the functionality of the application. Similarly, using larger machine types for your Google Container Engine node
pools may not directly affect the deployment times of your application, as the deployment times are primarily dependent on the
size and complexity of the Docker image being deployed.
upvoted 
1 
times
sfsdeniso
sfsdeniso
 
2 years, 2 months ago
B & E
base image is already cached - so no improvement in build time
B is about removing unnecessary dependencies and not about all of them
i remember saw this question on google's site - BE are correct
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
CE and is perfect
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
CE
C. Use a slimmed-down base image like Alpine Linux
E. Copy the source after he package dependencies (Python and pip) are installed
upvoted 
1 
times
backhand
backhand
 
2 years, 5 months ago
vote C, E
https://cloud.google.com/architecture/best-practices-for-building-containers
upvoted 
1 
times
MathMedrado
MathMedrado
 
2 years, 6 months ago
As far as I know, it is necessary to copy the requirements.txt first in order to run pip, it is not possible to run pip without first copying
the requirements.txt. if they copy requirements.txt first then E would work.
upvoted 
2 
times
amxexam
amxexam
 
2 years, 8 months ago
Selected Answer: 
CE
By means of elimination A 
B, dont make sence.D . is optimizing the mackie not script. Hence we are left with C& E
upvoted 
3 
times
potorange
potorange
 
2 years, 8 months ago
Selected Answer: 
CEC.Alpine is a ligweight Linux distro, with smaller image E. Pushing often changing files down the Dockerfile helps reducing image
layers variations. Both help faster image pull operations
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #23
Your solution is producing performance bugs in production that you did not see in staging and test environments. You want to
adjust your test and deployment procedures to avoid this problem in the future. 
What should you do? 
A. 
Deploy fewer changes to production
B. 
Deploy smaller changes to production
C. 
Increase the load on your test and staging environments 
Most Voted
D. 
Deploy changes to a small subset of users before rolling out to production
Correct Answer:
 
C 
Comments
ghitesh
ghitesh
 
Highly Voted
 
4 years, 11 months ago
Question Statement: You want to adjust your test and deployment procedures to avoid this problem in the future
So based on this, I think the option "C" is correct, since it is the only one talking about doing changes in the test environment.
upvoted 
83 
times
Sephethus
Sephethus
 
6 months, 3 weeks ago
There is no indication given anywhere that the load is the problem or that the bugs are a result of load and not some other issue
encountered when using a specific feature.
upvoted 
2 
times
alihabib
alihabib
 
1 month ago
"Performance" bug is a result of Load
upvoted 
1 
times
VedaSW
VedaSW
 
4 years, 3 months ago
C. Increase the load on your test and staging environments.
As you have pointed out in "Question Statement", I do not see C covering "deployment procedures". Test and Staging
Community vote distribution
C (62%)
D (35%)
B
(4%)As you have pointed out in "Question Statement", I do not see C covering "deployment procedures". Test and Staging
environment is more on testing, but not about deployment procedure to production.
So, the only option that cover test and deployment is D. (Yes, kind of unacceptable to have the users to do "testing", but we make
it "ok" by calling it "canary deployment")
upvoted 
20 
times
francescogugliottagm
francescogugliottagm
 
1 year, 2 months ago
With canary deployment we expose the new version to a small portion of users. With this approach maybe we don't see
performance bugs in the canary release, since we don't have the 100% of traffic on the canary. But when we migrate the 100%
of traffic to the new release (previous canary) we can see performance bugs.
upvoted 
9 
times
Urban_Life
Urban_Life
 
3 years ago
The answer is D
upvoted 
9 
times
RegisFTM
RegisFTM
 
3 years ago
"Your solution is producing performance bugs in production..." - I don't see how "D" would help to detect performance bugs. 
- "C" looks more adequate.
upvoted 
19 
times
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
A wouldn't prevent the bugs, it would just avoid them. B would help with root-cause analysis because it'd be a smaller change to
review. C would test the performance of the system at its peak processing rates, so this assumes the bugs in production only
occur because of usage. D would allow you to test the new code against smaller user sets to see if it occurs then, and if it still
does you know it is not because of more user responses. So it's a tossup between C and D, D would be the cheaper/quicker
answer so I'd choose D first then C if it's because of usage.
upvoted 
38 
times
nitinz
nitinz
 
3 years, 10 months ago
D, canary rollout
upvoted 
7 
times
michael_m
michael_m
 
2 years, 4 months ago
It has nothing to do with the "performance bugs"
upvoted 
1 
times
michael_m
michael_m
 
2 years, 4 months ago
According to the question, [Your solution is producing "performance" bugs in production], so I think it is about the load. Plus
canary test will not reproduce the bugs related to high load, I 
vote for C
upvoted 
2 
times
Sreekey
Sreekey
 
4 years, 5 months ago
The question is about the performance of the existing Code that they did not detect in Test environments . This is not about new
API release . In order to test the performance they should increase the load in test environment and hence answer C.
upvoted 
18 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is the best
upvoted 
2 
times
deep316
deep316
 
Most Recent
 
3 weeks, 2 days ago
Selected Answer: 
C
Bugs are related to performance. So you would need to perform performance testing thoroughly.
upvoted 
1 
times
Daniaw
Daniaw
 
3 weeks, 3 days ago
Selected Answer: 
DC could be correct but this can help identify performance bottlenecks, but it might not fully replicate the complexity and
unpredictability of real-world production traffic.
So D is the most correct one.
upvoted 
1 
times
alihabib
alihabib
 
1 month ago
Selected Answer: 
C
"Performance" bug, not a "Development" Bug, so it more aligns with C, to increase the load to determine performance related
bugs
upvoted 
1 
times
drinkwater
drinkwater
 
1 month, 1 week ago
I think the right answer here is D. It applies a best best practice of the release managment like canary deplyments. 
Why is not C; adding more load in staging and testing environments can help identify some performance issues, it is often
impossible to replicate the exact conditions of a production environment.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
C
Guys, you need to focus on the KEYWORDS in any question, it will help you to determine the best answer. The keyword for this
question is "Performance", it is very clear that the load test on stage was not planned correctly (i.e/ lower than it should), so the
performance bugs didn't appear, but when it comes to production with much bigger load the issues appear.
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
C & D both works but D make sense
upvoted 
1 
times
dfizban
dfizban
 
2 months, 3 weeks ago
Selected Answer: 
D
It's D
upvoted 
1 
times
maxdanny
maxdanny
 
4 months ago
Selected Answer: 
C
C because The performance issues in production might not have been seen in staging or test environments because the load
(number of users, transactions, data volume, etc.) in those environments is not representative of the load in production. By
increasing the load on your test and staging environments to match or exceed production levels, you can better simulate real-world
conditions and catch performance issues before deployment
upvoted 
2 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
I will choose D. 
C: The question does not say the error caused by the load.
upvoted 
1 
times
Haigk
Haigk
 
6 months, 1 week ago
Selected Answer: 
D
Without overthinking the wording, canary (and similar) deployment methodologies are often recommended in Google
documentation, whereas increasing load in dev environments aren't. (My $0.02...)
upvoted 
3 
times
a2le
a2le
 
7 months ago
Selected Answer: 
C
Me too! I can't see how a "performance bug" might be mitigated via a canary deployment.
However, I see that C doesn't cover the "deployment" part of the question, then I deduce that the question is ambiguously
formulated.
upvoted 
1 
times
Robert0
Robert0
 
7 months, 1 week agoRobert0
Robert0
 
7 months, 1 week ago
Selected Answer: 
C
Although all answers can be good practices, I think only option C address the problem described.
upvoted 
1 
times
santoshchauhan
santoshchauhan
 
9 months, 3 weeks ago
Selected Answer: 
D
D. Deploy changes to a small subset of users before rolling out to production.
This approach, known as canary releasing or canary deployment, involves rolling out changes to a small group of users before
deploying them to the entire user base. It is a very effective way to catch performance issues that might not have been apparent
during testing.
C. Increase the load on your test and staging environments: This is definitely a good practice, as it can help simulate production-
like conditions more closely. However, it may still not capture all real-world scenarios and user behaviors that can lead to
performance issues.
upvoted 
1 
times
Patrick2708
Patrick2708
 
1 year ago
C looks good to me
upvoted 
2 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
C
Canary deployment is perfect to test new feature but to do stress testing, I do development for 25 years, when we want to resolve
performance and scalability issues we do stress and load testing in pre prod environment, something you can't do by exposing the
new feature to subset of users.
upvoted 
6 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #24
A small number of API requests to your microservices-based application take a very long time. You know that each request to
the API can traverse many services. 
You want to know which service takes the longest in those cases. 
What should you do? 
A. 
Set timeouts on your application so that you can fail requests faster
B. 
Send custom metrics for each of your requests to Stackdriver Monitoring
C. 
Use Stackdriver Monitoring to look for insights that show when your API latencies are high
D. 
Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice
Most Voted
Correct Answer:
 
D 
Comments
euclid
euclid
 
Highly Voted
 
5 years ago
D is correct !
upvoted 
23 
times
tartar
tartar
 
4 years, 5 months ago
D is ok
upvoted 
9 
times
nitinz
nitinz
 
3 years, 10 months ago
D, trace is just for latency testing.
upvoted 
4 
times
omermahgoub
omermahgoub
 
Highly Voted
 
2 years ago
D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice
Stackdriver Trace is a distributed tracing system that allows you to understand the relationships between requests and the various
microservices that they touch as they pass through your application. By instrumenting your application with Stackdriver Trace, you
Community vote distribution
D (95%)
B (5%)microservices that they touch as they pass through your application. By instrumenting your application with Stackdriver Trace, you
can get a detailed breakdown of the latencies at each microservice, which can help you identify which service is taking the longest
in those cases where a small number of API requests take a very long time.
Setting timeouts on your application or sending custom metrics to Stackdriver Monitoring may not provide the level of detail that
you need to identify the specific service that is causing the latency issues. Looking for insights in Stackdriver Monitoring may also
not provide the necessary level of detail, as it may not show the individual latencies at each microservice.
upvoted 
10 
times
Ishu_awsguy
Ishu_awsguy
 
Most Recent
 
6 days, 20 hours ago
Selected Answer: 
D
The service is named Cloud trace now
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
D
1. Distributed Tracing: Stackdriver Trace (now called Cloud Trace) is specifically designed to analyze the performance of requests
as they travel through multiple services in a microservices architecture. It helps you pinpoint exactly where latency is occurring.
2. Detailed Breakdown: Cloud Trace will provide a detailed breakdown of how long each microservice takes to process the
request, allowing you to identify the bottleneck quickly.
3. Visualization: Cloud Trace provides visualizations like flame graphs and waterfall diagrams that make it easy to see the flow of
the request and identify slow services.
upvoted 
3 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Choose D
upvoted 
1 
times
Robert0
Robert0
 
7 months, 1 week ago
Selected Answer: 
D
This question is not updated. It will be refered as Cloud Trace as part of Google Cloud Operation suite
upvoted 
4 
times
Robert0
Robert0
 
7 months, 1 week ago
This question is not updated. Stackdriver is now called "Google Cloud operations"
upvoted 
2 
times
eka_nostra
eka_nostra
 
1 year, 5 months ago
Selected Answer: 
D
Trace is a signal that can be used to follow the program's data and flow, including the duration of the program's components.
upvoted 
2 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
D should be correct, the headline in GC trace documentation says it all: "Cloud Trace is a distributed tracing system for Google
Cloud that collects latency data from applications and displays it in near real-time in the Google Cloud Console."
upvoted 
2 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
even got confused with the C ... after reading the conversation and reference doc D make sense to me.
upvoted 
1 
times
alekonko
alekonko
 
1 year, 9 months ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
MestreCholas
MestreCholas
 
1 year, 10 months ago
Why not C?
upvoted 
1 
timesupvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
D
Stacdrive Trace would trace the APIs and helps to identify the bottleneck
upvoted 
1 
times
kchandank
kchandank
 
2 years, 1 month ago
Selected Answer: 
D
D is correct trace would report to the latency
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
D is correct trace would report to the latency
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
D
D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
this is the best option to find more details
upvoted 
1 
times
abirroy
abirroy
 
2 years, 4 months ago
Selected Answer: 
D
Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #25
During a high traffic portion of the day, one of your relational databases crashes, but the replica is never promoted to a master.
You want to avoid this in the future. 
What should you do? 
A. 
Use a different database
B. 
Choose larger instances for your database
C. 
Create snapshots of your database more regularly
D. 
Implement routinely scheduled failovers of your databases 
Most Voted
Correct Answer:
 
D 
Comments
Narigdo
Narigdo
 
Highly Voted
 
5 years, 1 month ago
Answer is D
upvoted 
97 
times
Jos
Jos
 
5 years ago
Yep, +1 for D
upvoted 
20 
times
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
@chiar, I agree the question i s not clear. In GCP larger instances have larger number of CPUs, Memory and come with their own
private network. So increases the instance size would help prevent the need for failover during high traffic times. However,
routinely scheduled failovers would allow the team to test the failover when it is not requried. This would make sure it is working
when it is required.
upvoted 
45 
times
Shariq
Shariq
 
5 years, 1 month ago
exactly how do you know the optimal size. it will be a guess. answer should be D
upvoted 
9 
times
Community vote distribution
D (67%)
B (33%)alihabib
alihabib
 
Most Recent
 
1 month ago
Selected Answer: 
D
Only a routine check, would indicate a best practice. Larger Instance doesn't guarantee a failover would be prevented
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
D
1. Proactive Testing: Regularly scheduled failovers proactively test your database's ability to recover and ensure the replica can
successfully take over as the master. This helps identify and address any issues in the failover process before a real crisis
occurs.
2. Confidence in Failover: By routinely testing failover, you gain confidence that your system can recover automatically in case of a
primary database failure, minimizing downtime and data loss.
3. Improved Recovery Time: Regular failovers help optimize the recovery process, reducing the time it takes to switch to the
replica.
upvoted 
2 
times
beagle_Masato
beagle_Masato
 
2 months ago
Selected Answer: 
D
Answer is D
upvoted 
1 
times
james2033
james2033
 
7 months, 1 week ago
Selected Answer: 
D
- **A. Use a different database**: Simply switching to a different database does not inherently solve the problem of failover and
promotion mechanisms. The issue is more about the setup and management of failover strategies rather than the specific
database technology used.
- **B. Choose larger instances for your database**: While using larger instances might improve performance and potentially
reduce the risk of a crash due to resource constraints, it does not address the failover mechanism. The key problem is the replica
not being promoted, which larger instances alone won't fix.
- **C. Create snapshots of your database more regularly**: Regular snapshots are useful for backups and recovery but do not
help with automatic failover and high availability. Snapshots do not ensure that a replica will be promoted to a master if the primary
fails.
upvoted 
2 
times
huuthanhdlv
huuthanhdlv
 
7 months, 2 weeks ago
I think the answer is B, as the most important thing is customer experience. We can NOT expect database fails as a normal event
which have direct customer and business impacts (if current data base fail because of load then replica database may fail as
well).
Of course we need to setup the failover process to work, but the more important task will be to increase database load first, thus I
choose B.
upvoted 
1 
times
Jen3
Jen3
 
10 months ago
I think the answer is D because we can not assume the crash was due to load.
upvoted 
1 
times
Jen3
Jen3
 
10 months ago
Further the issue isn't that a crash occurred, the issue is the contingency that was in place didn't kick in. Hence D as my choice.
upvoted 
2 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
Selected Answer: 
D
D is correct with given choice, because what if larger instance size is also failed. How ever question ignores logging and
networking completely, the best way is to use logging why the DB is crashed, failover is just a remedy to solve the issues at
current, not solving the problem itself.
upvoted 
2 
timesupvoted 
2 
times
discuss24
discuss24
 
1 year ago
It is important to identify key words, the issue is replica not being promoted when primary instance fails. Regular testing would
identify issue
upvoted 
2 
times
AWS_Sam
AWS_Sam
 
1 year ago
The correct answer is D
The question is asking how to avoid the situation of not failing over. The answer is to test the failover procedure. The question does
*NOT*ask about what happens in the future and whether the secondary node is large enough.
upvoted 
1 
times
pakilodi
pakilodi
 
1 year, 1 month ago
Selected Answer: 
D
I would say D. Beacuse also B is correct, but you have a replica here, that is never 
promoted. So we need failover strategy.
upvoted 
1 
times
owenshinobi
owenshinobi
 
1 year, 1 month ago
Selected Answer: 
B
My Answer is B
Why not D ?
I think
- Each day we cannot know. What times of the day are peak usage times? Implementing routinely scheduled failovers won't solve
the problem.
- if Implement routinely scheduled failovers of your databases but 
replica database server is a same spec with main database
server , 
replica database server will crashes by high traffic portion same a main database server
upvoted 
2 
times
Nora9
Nora9
 
1 year, 1 month ago
Selected Answer: 
D
Answer is D. Implement routinely scheduled failovers of your databases
This option is most aligned with addressing the issue. Routine failovers can help ensure that the failover process is working
correctly and that the system is resilient to crashes. It can be part of a disaster recovery plan, where you routinely test the failover
to the replica to ensure that it can handle being promoted to a master if needed. For the above reasons, i believe D is correct.
upvoted 
1 
times
piiizu
piiizu
 
1 year, 3 months ago
Why not B? 
Using a large instance during low traffic hours means incurring more cost than benefit except the instance is elastic. Therefore
using a larger instance is not cost effective and the answer is D.
upvoted 
1 
times
Palan
Palan
 
1 year, 4 months ago
Answer must be D i.e. Implement routinely scheduled failovers of your database because the main issue was the replica couldnt
get created. So, its wise to ensure that failovers are checked on routine basis.
upvoted 
1 
times
Samley
Samley
 
1 year, 4 months ago
Answer is D
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #26
Your organization requires that metrics from all applications be retained for 5 years for future analysis in possible legal
proceedings. 
Which approach should you use? 
A. 
Grant the security team access to the logs in each Project
B. 
Configure Stackdriver Monitoring for all Projects, and export to BigQuery
C. 
Configure Stackdriver Monitoring for all Projects with the default retention policies
D. 
Configure Stackdriver Monitoring for all Projects, and export to Google Cloud Storage 
Most Voted
Correct Answer:
 
D 
Comments
JoeShmoe
JoeShmoe
 
Highly Voted
 
5 years, 1 month ago
D is correct and best practice for long term log storage
upvoted 
149 
times
AndreaMa
AndreaMa
 
6 months, 1 week ago
same for me. The best approach for the long time log is to export it from monitoring to Cloud Storage Archival type
upvoted 
1 
times
AndreUanKenobi
AndreUanKenobi
 
3 years, 9 months ago
+1. For archival purposes, Customer should use Cloud Storage. BigQuery is a datawarehouse, and could eventually import data
from Cloud Storage if necessary.
upvoted 
22 
times
anjuagrawal
anjuagrawal
 
2 years, 12 months ago
+1 Due to long term storage, cloud storage is better answer than BigQuery
upvoted 
14 
times
MeasService
MeasService
 
Highly Voted
 
5 years ago
Community vote distribution
D (74%)
B (26%)A and C can be quickly ruled out because none of them is solution for the requirements "retained for 5 years"
Between B and D, the different is where to store, BigQuery or Cloud Storage. Since the main concern is extended storing period,
D (Correct Answer) is better choice, and the "retained for 5 years for future analysis" further qualifies it, for example, using Coldline
storage class.
With regards of BigQuery, while it is also a low-cost storage, but the main purpose is for analysis. Also, logs stored in Cloud
Storage is easy to transport to BigQuery or do query directly against the files saved in Cloud Storage if and whenever needed.
upvoted 
72 
times
Shyeom
Shyeom
 
5 years ago
point : organization requires that metrics from all applications be retained for 5 years
upvoted 
2 
times
Shyeom
Shyeom
 
5 years ago
I mean answer : D
upvoted 
4 
times
Cloudy_Apple_Juice
Cloudy_Apple_Juice
 
4 years, 3 months ago
If you have 2 viable solutions (B&D), then always chose the one that is cost optimised - I chose D
upvoted 
5 
times
jvale
jvale
 
2 years, 9 months ago
Bigquery long term storage cost: $0.020 per GB
Cloud Storage archive cost: $0,0012 per GB
Only if metrics need less than 10 GB (free service part on Bigquery) then the correct solution will be B... But all metrics for all
applications during more than 5 years... I think never will be the case :D
upvoted 
11 
times
Vika
Vika
 
3 years, 10 months ago
second that! like the way u explained..
upvoted 
1 
times
trainor
trainor
 
4 years ago
The question is about metrics, not logs. I'd go for B.
See https://cloud.google.com/solutions/stackdriver-monitoring-metric-export
upvoted 
16 
times
bnlcnd
bnlcnd
 
3 years, 11 months ago
This is a good example. thanks.
But, we can easily change that implementation to dump the metrics to buckets to save lots of money. And, when talking about
legal purpose, 1 hour interval may not be enough. You may have to keep more frequent metrics. So, only cold line or archive
work for that purpose.
upvoted 
5 
times
alihabib
alihabib
 
Most Recent
 
1 month ago
Selected Answer: 
D
Since the data is not actively queried, and cited as "possible" use qualifies it to be archived. Which means Cloud Storage
upvoted 
1 
times
drinkwater
drinkwater
 
1 month, 1 week ago
B is the right answer 
why is not D, because while Google Cloud Storage can handle long-term storage, it is less efficient than BigQuery for analysis.
Retrieving and querying metrics from Cloud Storage would require additional tools or steps, making it less suitable for the
described use case.
upvoted 
1 
times
alpay
alpay
 
1 month, 2 weeks ago
B in my opinion.B in my opinion.
Take a look: https://cloud.google.com/architecture/monitoring-metric-export#store_metrics 
BQ is also long-term storage with option to reduce cost of older data.
https://cloud.google.com/bigquery/docs/best-practices-storage
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
B
1. Long-term Retention: BigQuery is a data warehouse designed for long-term storage and analysis of large datasets. It's the ideal
place to store metrics for 5 years to meet your organization's legal requirements.
2. Cost-Effective: BigQuery's storage pricing is very competitive, especially for long-term data retention.
3. Analysis and Reporting: BigQuery provides powerful tools for analyzing and querying data, making it easy to extract insights and
generate reports from the stored metrics.
4. Integration: Stackdriver Monitoring (now Cloud Monitoring) can be easily configured to export metrics to BigQuery.
D is not correct as while Cloud Storage can store data for long periods, it's not optimized for querying and analyzing data like
BigQuery.
upvoted 
1 
times
VedaSW
VedaSW
 
4 months ago
I go for B, as the question is about 5 years worth of data "for future analysis in possible legal proceedings", and the "future" can be
next day, based on when the legal proceeding happen.
It is not about long term log storage.
Even the argument of "future" means 100 years later, the Cold Storage Archival still does not fulfill the "analysis" portion of the
requirements.
You will need to move the data from Cold Storage to BigQuery for the analysis.
So the ideal answer should be combination of D and B, but we do not have such option, hence the answer can meet all
requirements is B.
upvoted 
1 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
D is answer. Monitoring has only 24 months retention.
upvoted 
1 
times
joecloud12
joecloud12
 
5 months ago
Selected Answer: 
D
for storing the logs you need cloud storage.
upvoted 
1 
times
monus
monus
 
5 months, 1 week ago
Selected Answer: 
B
B should be correct. How can Cloud Storage analyze the data?
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
5 months, 1 week ago
The statement is not about the actual analysis of the data, but 'where' to store the data for future analysis. 
Who know when that
will be??? So GCS is the best answer. When need be, it can be import into BQ
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
5 months, 1 week ago
Also it said retained for 5 years for future analysis... you don't store in BQ
upvoted 
1 
times
a2le
a2le
 
7 months ago
Selected Answer: 
D
I mean, "for possible future legal proceedings", I think that immutable storage that grants data integrity is the best option here,
what's more, it's also the cheapest one...
upvoted 
1 
times
james2033
james2033
 
7 months, 1 week ago
Selected Answer: 
DGoogle Cloud Storage for 5 years storing legally.
upvoted 
1 
times
arrase
arrase
 
8 months, 3 weeks ago
Selected Answer: 
D
best practice for long term log storage
upvoted 
1 
times
tosinogunfile
tosinogunfile
 
11 months ago
Selected Answer: 
B
I think B is the right answer because transferring the data could be a basis for discrediting the data for legal use. Since BIg Query
can store the data and retain it with all the metadata intact, I will go for it.
upvoted 
1 
times
arielrahamim
arielrahamim
 
11 months, 1 week ago
Selected Answer: 
D
"organization requires that metrics from all applications be retained for 5 years" means cloud storage
upvoted 
1 
times
Teckexam
Teckexam
 
11 months, 2 weeks ago
Selected Answer: 
D
Cloud storage is appropriate for long term storage
upvoted 
1 
times
yas_cloud
yas_cloud
 
11 months, 3 weeks ago
This should be D. The tool lists option B as correct option which doesn't sound right. Bigquery export is fine for analysis but not for
long term storage for 5 yrs.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #27
Your company has decided to build a backup replica of their on-premises user authentication PostgreSQL database on Google
Cloud Platform. The database is 4 
TB, and large updates are frequent. Replication requires private address space communication. 
Which networking approach should you use? 
A. 
Google Cloud Dedicated Interconnect 
Most Voted
B. 
Google Cloud VPN connected to the data center network
C. 
A NAT and TLS translation gateway installed on-premises
D. 
A Google Compute Engine instance with a VPN server installed connected to the data center network
Correct Answer:
 
A 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
A is the one
upvoted 
28 
times
tartar
tartar
 
4 years, 5 months ago
A is ok
upvoted 
8 
times
nitinz
nitinz
 
3 years, 10 months ago
A, direct connect is private. VPN not enough for 4 TB with huge frequent changes.
upvoted 
3 
times
amxexam
amxexam
 
Highly Voted
 
3 years, 4 months ago
Let's go with option elimination
A. Google Cloud Dedicated Interconnect
>> Secured, fast connection, hence the choice. This will allow private connection from GCP to the data centre with a fast
connection. Cost is not mentioned in the requirement to eliminate this option.
B. Google Cloud VPN connected to the data centre network
Community vote distribution
A (63%)
B (37%)B. Google Cloud VPN connected to the data centre network
>> We have to think about data flowing on the internet and the requirement talks about private connect. Also not sure how well you
connect VPN with Data Center until you use the hybrid option. https://cloud.google.com/network-
connectivity/docs/vpn/concepts/overview hence eliminate
C. A NAT and TLS translation gateway installed on-premises
>>This is a VM option to reach outside won't for this requirement hence eliminate
D. A Google Compute Engine instance with a VPN server installed connected to the data centre network
>>This is a slow option hence eliminate
Hence A
upvoted 
17 
times
ramjisriram
ramjisriram
 
Most Recent
 
6 days, 14 hours ago
Selected Answer: 
A
Key clue is "Replication requires private address space communication" . Only Google Cloud Dedicated Interconnect has private
address. VPN is public, encrypted and slower !!! Another clue is "Large updates are frequent" means you need faster Dedicated
interconnect, as VPN will be slower.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
A
1. High Bandwidth and Reliability: Dedicated Interconnect provides a direct physical connection between your on-premises
network and Google Cloud, offering high bandwidth and low latency. This is essential for replicating a 4TB database with frequent
large updates.
2. Private Address Space: Dedicated Interconnect allows you to extend your private IP address space to Google Cloud, ensuring
secure and private communication for database replication.
3. Security: Dedicated Interconnect offers a more secure connection compared to VPN, as traffic doesn't traverse the public
internet.
Note: Question didn't mention anything about costs, so guys please stop overthinking and focus on the question key words.
upvoted 
4 
times
Robert0
Robert0
 
7 months, 1 week ago
Selected Answer: 
B
Option B
Interconnect is incredibly expensive and the usecase do not justify it.
Properly configured, a VPN provides similar features. If the question included an inusally high SLA, I will go with Interconnect. If not,
VPN is a great option.
upvoted 
4 
times
tocsa
tocsa
 
7 months ago
A simple VPN may not provide enough bandwidth for replication if the DB is busy. We know that the auth DB is 4TB, I'd say this
must be a quite big company, possibly they can offer an interconnect? But it surely is expensive
upvoted 
1 
times
sidiosidi
sidiosidi
 
8 months, 1 week ago
Selected Answer: 
B
I'll go for VPN.
First, the database is only for authentication and updates will be on this part, small portion of data needs to be replicated between
on-premise and cloud. so no need for high bandwidth. the first migration will needs bandwidth but not toom much (5T) can be
migrated using VPN.
VPN permits to use private networking and it's secure.
VPN not expensive as direct connect.
As architect you should also evaluate the cost over the requirement, at the end you need convice business with solution. Paying
5K will kick you out the project for such small requirement.
upvoted 
3 
times
Robert0
Robert0
 
7 months, 1 week ago
I think this fella hit the righ nail. Interconnect is incredibly expensive and the usecase do not justify it.
Properly configured, a VPN provides similar features. If the question included an inusally high SLA, I will go with Interconnect. If
not, VPN is a great option.
upvoted 
1 
timesupvoted 
1 
times
Jen3
Jen3
 
10 months ago
If you tried to sell me on Interconnect when all I needed was a VPN (meets bandwidth req, private address space, encryption of
traffic possible), I would reach out to AWS for a quote...
upvoted 
1 
times
lisabisa
lisabisa
 
1 year, 3 months ago
GoogleVPN throughput is 3Gbps. It supports private IP connection and cheaper than DIrect Connection.
Direct connect supports 8 * 10Gbps or 2*100Gbps. 
But too expensive for this
upvoted 
3 
times
eka_nostra
eka_nostra
 
1 year, 5 months ago
Selected Answer: 
A
Connect to private space with high-speed bandwidth will go to A.
upvoted 
2 
times
mrhege
mrhege
 
1 year, 7 months ago
B: Dedicated Interconnect would be a major overkill here and a quite expensive one as well. Requirements mention private
_address space_, not private connection. Data over VPN is just as secure. Also there is no mention that a Google PoP would be
available.
https://cloud.google.com/network-connectivity/docs/how-to/choose-product
upvoted 
1 
times
mohideenks
mohideenks
 
2 years, 1 month ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A is great but expensive for just a database DR but what can we do about that
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
VPN is not private, it is public but encrypted. Also, VPN is not suitable for large updates that happen frequently
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
without any second thought A is right
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
A
A. Google Cloud Dedicated Interconnect - large updates and better security, however may not be the most cost effective choice
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 7 months ago
Selected Answer: 
A
A is the one
upvoted 
1 
times
Nirca
Nirca
 
2 years, 8 months ago
Selected Answer: 
A
Direct connect.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #28
Auditors visit your teams every 12 months and ask to review all the Google Cloud Identity and Access Management (Cloud IAM)
policy changes in the previous 12 months. You want to streamline and expedite the analysis and audit process. 
What should you do? 
A. 
Create custom Google Stackdriver alerts and send them to the auditor
B. 
Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor 
Most Voted
C. 
Use cloud functions to transfer log entries to Google Cloud SQL and use ACLs and views to limit an auditor's view
D. 
Enable Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegate access to the bucket
Correct Answer:
 
B 
Comments
ghitesh
ghitesh
 
Highly Voted
 
4 years, 11 months ago
B. https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors
upvoted 
98 
times
rockstar9622
rockstar9622
 
4 years, 11 months ago
b) seems correct
upvoted 
3 
times
anton_royce
anton_royce
 
4 years, 9 months ago
I agree. Answer B
upvoted 
5 
times
MikeB19
MikeB19
 
3 years, 4 months ago
The article references either gcs or bq. 
I think this q is referring to gcs
upvoted 
1 
times
TheCloudBoy77
TheCloudBoy77
 
3 years, 1 month ago
B makes more sense after reading it. thx
Community vote distribution
B (68%)
D (32%)upvoted 
4 
times
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
Think B is better. Export to Bigquery and restrict access to queries with ACLs to auditors
upvoted 
37 
times
trainor
trainor
 
4 years ago
I think D is better. B implies too much data manipulation to make it suitable for an audit.
upvoted 
4 
times
tartar
tartar
 
4 years, 5 months ago
D is ok.
upvoted 
7 
times
tartar
tartar
 
4 years, 4 months ago
Sorry, changed my view. B is the recommended practice
upvoted 
14 
times
alii
alii
 
3 years, 11 months ago
don't change your view, D was right :)
upvoted 
4 
times
RKS_2021
RKS_2021
 
3 years, 5 months ago
B is correct
upvoted 
1 
times
nitinz
nitinz
 
3 years, 10 months ago
D, rest all options are no good.
upvoted 
3 
times
AmitAr
AmitAr
 
2 years, 7 months ago
Please check the keywords in question -- "streamline and expedite" -- Bigquery is suitable not storage bucket. so it should be
(B)
upvoted 
3 
times
passnow
passnow
 
5 years ago
I thought same as well. I would go with B
upvoted 
5 
times
ramjisriram
ramjisriram
 
Most Recent
 
6 days, 13 hours ago
Selected Answer: 
B
Key clue from the question is "You want to streamline and expedite the analysis". How can you expect streamline and analysis
capabilities from Cloud Storage? Rather BigQuery has the analytics and streamline capabilities inbuilt in the tool. User access can
be controlled in both, so the correct answer with key clues is to Enable Logging export to Google BigQuery (this will store all the
logs streamlined) and use ACLs (these are filters, so according to the audior) and views (these are read only, there is no
obfuscation or tokenization or sensistive data to be handled from the logs !!!!) to scope the data shared with the auditor.
upvoted 
2 
times
Ishu_awsguy
Ishu_awsguy
 
6 days, 19 hours ago
Selected Answer: 
B
The power word here is ,
Streamline analysis and audit , hence BQ is the answer.
If it would have been only audit , then D would have been beteer
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
B1. Comprehensive Audit Trail: Cloud Logging automatically captures audit logs for all Cloud IAM activity. Exporting these logs to
BigQuery provides a centralized and comprehensive audit trail for analysis.
2. Powerful Analysis: BigQuery's analytical capabilities allow auditors to efficiently query and analyze IAM policy changes over the
12-month period. They can filter, aggregate, and generate reports to identify any anomalies or security concerns.
3. Granular Access Control: BigQuery's Access Control Lists (ACLs) and views enable you to precisely control which data the
auditors can access. This ensures that they only see the information relevant to their audit without exposing sensitive data.
Note: While exporting logs to Cloud Storage is possible, it's less efficient for analysis compared to BigQuery.
upvoted 
3 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Answer B
upvoted 
1 
times
maxdanny
maxdanny
 
4 months ago
Selected Answer: 
B
Option B is the best approach. Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with
the auditor. This method provides robust querying capabilities, ensures that historical IAM policy changes can be analyzed
effectively, and allows you to control access securely.
upvoted 
1 
times
joecloud12
joecloud12
 
5 months ago
Selected Answer: 
B
b is correct because it is easier to implement compared to D
upvoted 
1 
times
H_S
H_S
 
5 months, 3 weeks ago
Selected Answer: 
D
READ THIS, ACL is not available in BIG QUERY , thereforeD. Enable Google Cloud Storage (GCS) log export to audit logs into a
GCS bucket and delegate access to the bucket
upvoted 
4 
times
Jen3
Jen3
 
10 months ago
ACLs would provide year-round access to the data which is more privileges than necessary. Logs will need to be retained for a full
year because hypothetically, January logs could be looked at in December. Cloud Storage offers signed URLs, and less expensive
storage options.
upvoted 
1 
times
lisabisa
lisabisa
 
10 months, 2 weeks ago
Both B and D are ok.
Using cloud storage requires additional setup for auditors, pulling data to BQ.
Using BQ would satisfy "streamline and expedite the analysis and audit process"
upvoted 
1 
times
Teckexam
Teckexam
 
11 months, 2 weeks ago
Selected Answer: 
B
Based on google documentation B is the correct answer.
https://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors
Dashboard is available in BigQuery to review historic logs and in case anamoly is found elevated access is provided. Access is
revoked after audit activities are done.
upvoted 
4 
times
kip21
kip21
 
11 months, 3 weeks ago
D - Correct
B - his option requires additional work to set up the ACLs and views to limit an auditor's view of the data. This could be time-
consuming and complex to implement. Furthermore, BigQuery may not be the ideal tool for auditors who are only interested in
reviewing Cloud IAM policy changes.
upvoted 
2 
timesupvoted 
2 
times
CloudDom
CloudDom
 
1 year, 1 month ago
Selected Answer: 
B
That‘s the only logical one also Bard is confirming this one
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
D
I will not go with B, as the requirement is once for 12 months. Push the data in Coldline for 12 months and retrieve it during audit is
enough. Save costs.
upvoted 
3 
times
thewalker
thewalker
 
1 year, 1 month ago
Coldline / Archive
upvoted 
2 
times
hogtrough
hogtrough
 
12 months ago
Streamline and expedite analysis is the goal. Costs are never brought up.
upvoted 
2 
times
krisek
krisek
 
1 year, 2 months ago
Selected Answer: 
B
Reading from Cloud Storage raw audit logs (without filtering applied) is everything but streamlined. Imagine the auditor fetching all
audit logs, then write some script to analyze them...
upvoted 
2 
times
Prakzz
Prakzz
 
1 year, 2 months ago
Selected Answer: 
D
B talks about ACL in BigQuery and ACL is not associated with BigQuery but with GCS.
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #29
You are designing a large distributed application with 30 microservices. Each of your distributed microservices needs to
connect to a database back-end. You want to store the credentials securely. 
Where should you store the credentials? 
A. 
In the source code
B. 
In an environment variable
C. 
In a secret management system 
Most Voted
D. 
In a config file that has restricted access through ACLs
Correct Answer:
 
C 
Comments
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
Google Secret Management was designed explicitly for this purpose.
upvoted 
33 
times
tartar
tartar
 
4 years, 5 months ago
C is ok
upvoted 
7 
times
nitinz
nitinz
 
3 years, 10 months ago
C, microservices = GKE = Kubernetes = secrets.
upvoted 
6 
times
shandy
shandy
 
Highly Voted
 
5 years, 1 month ago
C is the answer, since key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage
permissions to those keys.
A is incorrect because storing credentials in source code and source control is discoverable, in plain text, by anyone with access
to the source code. This also introduces the requirement to update code and do a deployment each time the credentials are
rotated. B is not correct because consistently populating environment variables would require the credentials to be available, in
plain text, when the session is started. D is incorrect because instead of managing access to the config file and updating manually
Community vote distribution
C (100%)plain text, when the session is started. D is incorrect because instead of managing access to the config file and updating manually
as keys are rotated, it would be better to leverage a key management system. Additionally, there is increased risk if the config file
contains the credentials in plain text.
upvoted 
13 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
C
1. Centralized and Secure Storage: Secret management systems like HashiCorp Vault, AWS Secrets Manager, or Google Cloud
Secret Manager provide a centralized and secure location to store sensitive credentials. This ensures that database credentials
are not scattered across multiple microservices or configuration files.
2. Access Control: Secret management systems offer fine-grained access control, allowing you to restrict access to secrets
based on roles and permissions. This ensures that only authorized microservices and users can access the database
credentials.
3. Rotation and Auditing: These systems often provide features for automatic secret rotation and auditing, which helps improve
security and compliance.
4. Integration: Secret management systems can integrate with your deployment pipelines and orchestration tools, making it easier
to manage secrets throughout the application lifecycle.
upvoted 
2 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
Selected Answer: 
C
Other options are not best practices.
upvoted 
1 
times
Teckexam
Teckexam
 
11 months, 2 weeks ago
Selected Answer: 
C
Need to use key management system for this usecase since other options are not secure.
upvoted 
1 
times
hiromi
hiromi
 
1 year, 9 months ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
C. In a secret management system
It is important to store the credentials for your database back-end securely in order to protect them from unauthorized access.
One way to do this is by using a secret management system, such as Google Cloud's Secret Manager. Secret Manager is a
secure and convenient storage system for API keys, passwords, and other sensitive data that is designed to protect against
unauthorized access. By storing the credentials in Secret Manager, you can ensure that they are kept secure and can be easily
accessed by your microservices as needed.
Storing the credentials in the source code, an environment variable, or a config file that has restricted access through ACLs may
not provide the same level of security as a dedicated secret management system. It is important to ensure that your credentials
are stored in a secure and controlled manner to protect against unauthorized access.
upvoted 
4 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
C
C is correct; If credential then always use secret manager.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
secret manager is the answer
upvoted 
1 
timesAzureDP900
AzureDP900
 
2 years, 2 months ago
C is right
upvoted 
1 
times
Kubernetes
Kubernetes
 
2 years, 3 months ago
answer is C
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 10 months ago
Selected Answer: 
C
Use Google Secret Manager
upvoted 
1 
times
rogerlovato
rogerlovato
 
2 years, 11 months ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years, 1 month ago
Go for C
upvoted 
1 
times
vincy2202
vincy2202
 
3 years, 1 month ago
C is the right answer
upvoted 
1 
times
unnikrisb
unnikrisb
 
3 years, 2 months ago
Google Practice exam question with option C : 
In a key management system
C is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage
permissions to those keys.
https://cloud.google.com/kms/
For this question, refer to the Mountkirk Games case study.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #30
A lead engineer wrote a custom tool that deploys virtual machines in the legacy data center. He wants to migrate the custom
tool to the new cloud environment. 
You want to advocate for the adoption of Google Cloud Deployment Manager. 
What are two business risks of migrating to Cloud Deployment Manager? (Choose two.) 
A. 
Cloud Deployment Manager uses Python
B. 
Cloud Deployment Manager APIs could be deprecated in the future
C. 
Cloud Deployment Manager is unfamiliar to the company's engineers 
Most Voted
D. 
Cloud Deployment Manager requires a Google APIs service account to run
E. 
Cloud Deployment Manager can be used to permanently delete cloud resources
F. 
Cloud Deployment Manager only supports automation of Google Cloud resources 
Most Voted
Correct Answer:
 
CF 
Comments
victory108
victory108
 
Highly Voted
 
3 years, 6 months ago
E. Cloud Deployment Manager can be used to permanently delete cloud resources
F. Cloud Deployment Manager only supports automation of Google Cloud resources
upvoted 
80 
times
Gregwaw
Gregwaw
 
1 year, 3 months ago
F is not a risk, it is a limitation of solution. Risk is something that is not known for sure and is manageable (risk can be mitigated,
avoided). You cannot manage the limitation of solution. You can use it with this limitation or not and you know it in advance.
upvoted 
5 
times
Terryhsieh
Terryhsieh
 
1 year ago
Advocating to adopt Google Cloud Deployment Manager will become a risk if the lead engineer or other business need ask for
use other cloud platform.
upvoted 
3 
times
Community vote distribution
CF (45%)
CE (18%)
EF (18%)
Other (19%)poseidon24
poseidon24
 
3 years, 5 months ago
Yup, E + F. In GCP documentation it states as a warning note that deletion made through Deployment Manager scripts cannot be
undone, if devs are not well trained a human errors can impact Business
upvoted 
13 
times
AK2020
AK2020
 
Highly Voted
 
3 years, 6 months ago
C and F- make sense to me
upvoted 
42 
times
ssepiro
ssepiro
 
3 years, 1 month ago
I think this is right. the key of the question is "business risks".
upvoted 
3 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
yes, C and F right
upvoted 
3 
times
[Removed]
[Removed]
 
1 year, 9 months ago
C - Makes sense, because company engineer may take longer to develop, so more cost and more 'time-to-market'
Reg F:
Can I pls ask how does business care whether you are Google Cloud Resources or legacy data center tools, as long as it servs
business requirement? 
So I'm leaning towards E, as the engineers are still in the process of learning CDM and may accidently delete VMs bringing
down the entire application.
upvoted 
5 
times
[Removed]
[Removed]
 
1 year, 9 months ago
Forgot to mention, once determined as "risks", the mitigation actions below can be followed:
C: Train the existing resources, Hire an experienced personnel
E: Peer Reviews, QA, thorough testing etc.
upvoted 
2 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 7 hours ago
Selected Answer: 
CF
I will go for CF
upvoted 
1 
times
kino_1994
kino_1994
 
6 days, 1 hour ago
Selected Answer: 
BF
My answer is BF and aligns with similar arguments as those presented by @Choopaower
B: This option can be deprecated, as the custom tool has been used for a long time.
F: Cloud Deployment Manager only supports automation for Google Cloud resources, not custom resources as often happens in
On-Premise environments.
C is not a business risk. It is a technical risk. Their engineers may not know it yet, but they can learn.
upvoted 
1 
times
ramjisriram
ramjisriram
 
6 days, 13 hours ago
Selected Answer: 
BC
B and C are the business risks, rest of the options are technical limitations. 
B. Cloud Deployment Manager APIs could be deprecated in the future 
C. Cloud Deployment Manager is unfamiliar to the company's engineers
The other options might present technical challenges or considerations but are not necessarily business risks. For example:
A is more about the programming language used.A is more about the programming language used.
D is about the service account requirement.
E is a capability that, if misused, could cause issues but is not inherently a business risk.
F limits scope but isn't a direct business risk.
upvoted 
1 
times
drinkwater
drinkwater
 
1 month, 1 week ago
the right answers are C & E
C: because the engineers are dealing with legacy technologies 
E: Cloud Deployment Manager has the ability to delete resources
upvoted 
2 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
CF
I guess everyone agree on C, the debate is between B and F.
Before I start, I need to stress on the word "Business" not "Technical" risks, we need to take this in our minds.
B: Cloud Deployment Manager APIs could be deprecated in the future 
>> this is not a risk at all as the APIs cannot be changed
without an announcement and also there is a backward compatibility to prevent such issues, this is something that is tackled a
decade ago in all cloud providers.
F: Cloud Deployment Manager only supports automation of Google Cloud >> Indeed this is a business risk, as the question didn't
mention explicitly that the customer is using only GCP, they left it vague by saying "new cloud environment", so we have to
assume that it is multi-cloud strategy including On-premise BTW ( Hybrid-Cloud as well ), so we need a tool that can handle
deployments on all of those targets.
Based on all of that, I recommend ( C & F )
upvoted 
4 
times
beagle_Masato
beagle_Masato
 
2 months ago
Selected Answer: 
CF
C and F right
upvoted 
1 
times
Shasha1
Shasha1
 
2 months, 3 weeks ago
BF are correct
upvoted 
1 
times
Leo212003
Leo212003
 
3 months, 1 week ago
Selected Answer: 
EF
E and F
upvoted 
1 
times
maxdanny
maxdanny
 
4 months ago
Selected Answer: 
BC
B. Cloud Deployment Manager APIs could be deprecated in the future: There's always a risk that APIs and tools can be
deprecated or replaced with new versions. This could impact the long-term stability of your deployment process if the APIs used
by Cloud Deployment Manager are deprecated and require migration to new APIs.
C. Cloud Deployment Manager is unfamiliar to the company's engineers: If the company's engineers are not familiar with Cloud
Deployment Manager, there could be a learning curve and potential delays during the migration process. Training and adaptation
time could affect productivity and introduce risks associated with potential mistakes or inefficiencies during the transition.
upvoted 
4 
times
Armne96X
Armne96X
 
4 months, 3 weeks ago
B. Cloud deployment manager is being deprecated.
F. Cloud Deployment Manager only supports automation of Google Cloud resources
The question is about business risks - it's not about technical risks 
A C D E options are technical aspects of the Deployment managerA C D E options are technical aspects of the Deployment manager
upvoted 
2 
times
vbondoo7
vbondoo7
 
5 months ago
Should be B & C
upvoted 
4 
times
Choopaower
Choopaower
 
6 months, 1 week ago
Selected Answer: 
BC
My answer is BC.
I think the correct answers should be related to the engineer's custom tool.
The choices should answer the question "Why should we migrate Lead Engineer's tool to the new cloud environment".
It's tried and tested Custom Tool versus the new Deployment Manager.
B, because it can be deprecated, custom tool has been used for a long time.
C, because their engineers don't know it yet.
upvoted 
4 
times
Sephethus
Sephethus
 
6 months, 3 weeks ago
Chat GPT tells me B and C and it gets it right maybe 98% of the time so far. It seems nobody on this discussion can agree on an
answer but if we're going to look at keywords of the questions and over-scrutinize them, then yes, risk is a key word and I don't see
F as a risk. E is a risk but not even remotely unique to this situation. C is a mild risk but really the only one that makes sense in
light of eliminating the others. B is definitely a risk... but then it also isn't unique to this service so we're back at the same issue with
E. This question is garbage.
upvoted 
2 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
Selected Answer: 
CF
for me C and F
upvoted 
2 
times
de1001c
de1001c
 
7 months, 1 week ago
Selected Answer: 
BF
B: Cloud deployment manager is being deprecated.
F: Mentions on-premise resources and managing GCP which will make cloud deployment manager not the best tool for this.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #31
A development manager is building a new application. He asks you to review his requirements and identify what cloud
technologies he can use to meet them. The application must: 
1. Be based on open-source technology for cloud portability 
2. Dynamically scale compute capacity based on demand 
3. Support continuous software delivery 
4. Run multiple segregated copies of the same application stack 
5. Deploy application bundles using dynamic templates 
6. Route network traffic to specific services based on URL 
Which combination of technologies will meet all of his requirements? 
A. 
Google Kubernetes Engine, Jenkins, and Helm 
Most Voted
B. 
Google Kubernetes Engine and Cloud Load Balancing
C. 
Google Kubernetes Engine and Cloud Deployment Manager
D. 
Google Kubernetes Engine, Jenkins, and Cloud Load Balancing
Correct Answer:
 
A 
Comments
rsamant
rsamant
 
Highly Voted
 
3 years, 6 months ago
it should be A .. helm is needed for "Deploy application bundles using dynamic templates"
Load Balancing should be part of GKE Already
upvoted 
67 
times
satish4exam
satish4exam
 
2 days, 23 hours ago
Then jenkins is of no use
upvoted 
1 
times
raf2121
raf2121
 
3 years, 4 months ago
Kubernetes Engine offers integrated support for two types of Cloud Load Balancing (Ingress and External Network Load
Community vote distribution
A (65%)
D (35%)Kubernetes Engine offers integrated support for two types of Cloud Load Balancing (Ingress and External Network Load
Balancing) , hence Option A
Reference : https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer
upvoted 
4 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A should be fine
upvoted 
3 
times
Prakzz
Prakzz
 
1 year, 3 months ago
Load balancing is not a part of GKE untill it's created explicitly
upvoted 
2 
times
victory108
victory108
 
Highly Voted
 
3 years, 6 months ago
D. Google Kubernetes Engine, Jenkins, and Cloud Load Balancing
upvoted 
42 
times
MikeMike7
MikeMike7
 
Most Recent
 
4 weeks, 1 day ago
Selected Answer: 
D
D: load balancing is needed for this requirement: 6. Route network traffic to specific services based on URL
upvoted 
1 
times
drinkwater
drinkwater
 
1 month, 1 week ago
A & D are both right 
Why D Might Still Be Preferred:
While A is a valid choice, D (Google Kubernetes Engine, Jenkins, and Cloud Load Balancing) might be preferred for the following
reasons:
Cloud Load Balancing provides a more feature-rich, fully managed solution for routing traffic across multiple regions and services,
including advanced load balancing, SSL termination, and support for more sophisticated network traffic management.
It integrates well with GKE and offers additional scalability and flexibility that might be important as your system grows
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 3 weeks ago
Selected Answer: 
A
1. Open-source technology: Kubernetes Engine, Jenkins, and Helm.
2. Dynamically scale compute capacity: Kubernetes Engine provides autoscaling to adjust the number of nodes based on
demand.
3. Support continuous software delivery: Jenkins enables CI/CD pipelines for automated building, testing, and deployment of
applications.
4. Run multiple segregated copies: Kubernetes Engine allows deploying multiple instances of the application in isolated
environments (namespaces) within the same cluster.
5. Deploy application bundles using dynamic templates: Helm uses charts (templates) to define, install, and upgrade Kubernetes
applications.
6. Route network traffic based on URL: 
Kubernetes Engine's service objects and ingress controllers can route traffic to specific
services based on URLs and other criteria.
upvoted 
7 
times
VedaSW
VedaSW
 
4 months ago
Selected Answer: 
A
Based on: "Be based on open-source technology for cloud portability", I go for A, because it is more portable, as compare to D.
upvoted 
3 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Choose A
upvoted 
3 
times
desertlotus1211
desertlotus1211
 
5 months ago
Yes, Kubernetes can route network traffic to specific services based on URL using Ingress and Ingress controllers
Answer is AAnswer is A
upvoted 
1 
times
Chris_21
Chris_21
 
6 months, 1 week ago
Selected Answer: 
D
Load Balancer is required as per point 6.
Jenkins satisfies point 3.
D is correct
upvoted 
1 
times
Ric350
Ric350
 
5 months, 2 weeks ago
Helm is needed for the dynamic templates requirement. 
The question is vague in whether the application is internally or externally
facing which would clarify things a lot more for us. 
However, in its ambiguity option A has the techonologies neede to the
requirments and thus deduce or infer that the application is internally facing and the ingress controllers will handle the routing of
traffic. 
It's ambiguous on purpose which I hate in these exams. More of a test of how well we can read and interpret the questions
vs our knowledge of material.
upvoted 
3 
times
Rehamss
Rehamss
 
9 months, 3 weeks ago
Selected Answer: 
D
D is correct
upvoted 
1 
times
kahinah
kahinah
 
9 months, 4 weeks ago
Selected Answer: 
D
Option A (GKE, Jenkins, Helm) meets most requirements except for explicit URL-based routing, though Kubernetes Ingress
(which can be managed through Helm charts) implicitly covers this.
Option D (GKE, Jenkins, Cloud Load Balancing) directly meets every requirement, including URL-based routing without needing to
infer capabilities or integrate additional tools beyond the scope of what's listed. Jenkins supports continuous delivery, GKE
supports dynamic scaling, segregated application stacks, and cloud portability. Cloud Load Balancing directly addresses the URL-
based routing requirement.
upvoted 
4 
times
Sephethus
Sephethus
 
6 months, 3 weeks ago
Except none of that meets the needs for deployment templates.
upvoted 
1 
times
Ric350
Ric350
 
5 months, 2 weeks ago
My point exactly and my response to Chris_21. By process of elimination, you need Helm for the dynamic templates and you
need Jenkins. Thus, you have to assume the application is internally facing and the ingress controllers will handle the traffic just
fine.
upvoted 
1 
times
VidhyaBupesh
VidhyaBupesh
 
10 months, 2 weeks ago
Selected Answer: 
D
D is OK
upvoted 
1 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
1. Be based on open-source technology for cloud portability: GKE
2. Dynamically scale compute capacity based on demand: GKE
3. Support continuous software delivery: Jenkins
4. Run multiple segregated copies of the same application stack: GKE
5. Deploy application bundles using dynamic templates -> Jenkins
6. Route network traffic to specific services based on URL -> Only HTTPs load balancer can meet this requirement: Next best is
Cloud balancing (that can be either Network or HTTPs), 
So D makes sense to me.
upvoted 
5 
times
kip21
kip21
 
11 months, 3 weeks agoD - Correct
upvoted 
1 
times
MMuzammil
MMuzammil
 
1 year ago
I thought that answer A was correct but after researching HELM 
I think now the option D is correct.
Helm is not a cloud service on its own, and it is not built into Google Kubernetes Engine (GKE). Helm is an open-source package
manager for Kubernetes that simplifies the deployment and management of applications on Kubernetes clusters.
Here's a brief overview:
Helm:
Helm allows you to define, install, and upgrade even the most complex Kubernetes applications using packages called charts. A
Helm chart includes pre-configured Kubernetes resources that define the structure of an application. Helm provides a convenient
way to package, version, and deploy applications on Kubernetes.
upvoted 
3 
times
Sephethus
Sephethus
 
6 months, 3 weeks ago
A is the real-world answer, D is the "Google (TM)" answer I think. It's obnoxious how bad these questions are.
upvoted 
1 
times
adoyt
adoyt
 
1 year ago
Selected Answer: 
A
Kubernetes natively supports routing to different services based on URLs via the ingress gateway regardless of wether a LB is
used...
upvoted 
1 
times
simiramis221
simiramis221
 
1 year ago
The correct answer is A, you can route network traffic to specific services based on URL using Google Kubernetes Engine (GKE)
using Ingress
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #32
You have created several pre-emptible Linux virtual machine instances using Google Compute Engine. You want to properly
shut down your application before the virtual machines are preempted. 
What should you do? 
A. 
Create a shutdown script named k99.shutdown in the /etc/rc.6.d/ directory
B. 
Create a shutdown script registered as a xinetd service in Linux and configure a Stackdriver endpoint check to call the
service
C. 
Create a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud
Platform Console when you create the new virtual machine instance 
Most Voted
D. 
Create a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute instances add-metadata
command to specify the service URL as the value for a new metadata entry with the key shutdown-script-url
Correct Answer:
 
C 
Comments
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
https://cloud.google.com/compute/docs/shutdownscript ... So C
upvoted 
39 
times
nitinz
nitinz
 
3 years, 10 months ago
C, statup/shutdown script = metadata
upvoted 
4 
times
VishalB
VishalB
 
3 years, 5 months ago
Since the instance is already created Option C gets eliminated. "gcloud compute instances addmetadata”
command can be used to add or update the metadata of a virtual machine instance"
upvoted 
12 
times
Gini
Gini
 
Highly Voted
 
4 years, 7 months ago
I have doubts with the answer C because the question states that "You have created the instances" so C works too but the
Community vote distribution
C (65%)
D (35%)I have doubts with the answer C because the question states that "You have created the instances" so C works too but the
solution cannot apply to the already created instances. D seems correct to me...
Reference:
https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances
upvoted 
28 
times
[Removed]
[Removed]
 
1 year ago
I think C should be correct over D, because
https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances
upvoted 
2 
times
NG123
NG123
 
2 years, 6 months ago
I also feel so because the virtual machines are already created.
upvoted 
2 
times
dsnaghxhinwtsvvmip
dsnaghxhinwtsvvmip
 
1 year, 8 months ago
xinetd. Xinet makes the D answer be nonsense
upvoted 
5 
times
pepYash
pepYash
 
4 years, 1 month ago
Yes. The correct answer should be D.
To add a shutdown script to a running instance, follow the instructions in the Applying a startup script to running instances
documentation but replace the metadata keys with one of the following keys:
shutdown-script: Supply the shutdown script contents directly with this key. Using the gcloud command-line tool, you can provide
the path to a shutdown script file, using the --metadata-from-file flag and the shutdown-script metadata key.
shutdown-script-url: Supply a Cloud Storage URL to the shutdown script file with this key.
upvoted 
4 
times
pepYash
pepYash
 
4 years, 1 month ago
changed my mind. preemptible vms can be stopped and started anytime. with that flexibility, C is ok.
upvoted 
6 
times
RobertArnaud
RobertArnaud
 
Most Recent
 
3 weeks, 1 day ago
Selected Answer: 
C
"when you create the new virtual machine instance" means it is too late, then C seems disqualified :( but the other choices are
worse ?
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
Selected Answer: 
C
1. Preemptible instances and shutdown scripts: Google Cloud Platform's preemptible instances are cost-effective but can be
terminated with short notice. To gracefully handle this, you need a shutdown script that runs before the instance is preempted.
2. Metadata and shutdown-script: 
Google Cloud allows you to add custom metadata to your instances. When you create a
preemptible instance, you can include a shutdown-script metadata entry. The value of this entry should be your script, which will
be executed automatically before the instance is preempted.
upvoted 
1 
times
Barry123456
Barry123456
 
2 months, 2 weeks ago
Selected Answer: 
D
It's not C. 
C says "when you create the new virtual machine instance". 
But in the question, the instances have already been
created. 
Thus, D. 
In D, "service URL" obviously means cloud storage.
upvoted 
1 
times
snehaso
snehaso
 
4 months, 3 weeks ago
Among Option C & D, option D uses shutdown-script-url but shutdown-script-url should be a Cloud storage url and not a linux
service URL. That brings us to option C
upvoted 
1 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Choose C
upvoted 
1 
times
nicksb19
nicksb19
 
6 months, 1 week ago
C is correct since xinetd does not make sense.
upvoted 
1 
times
lisabisa
lisabisa
 
10 months, 2 weeks ago
Selected Answer: 
C
Every virtual machine instance in GCP has access to a metadata server, which provides information about the instance and
allows you to configure various settings, including startup and shutdown scripts.
Startup and shutdown scripts are specified using special metadata keys in the metadata server. 
shutdown-script specifies the shutdown script that should be executed when the instance is being shut down.
upvoted 
2 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
Selected Answer: 
C
All other options are related to play with Linux files or services. With Preemptible VMs ,these operations are overhead. Hence it
makes sense to Automate such tasks.
upvoted 
1 
times
Mo7y
Mo7y
 
11 months, 3 weeks ago
Selected Answer: 
C
The answer is either C or D
I exclude D because shutdown-script-url only works with a shutdown script hosted on a cloud storage. Option D wants you to use
shutdown-script-url for a locally hosted shutdown script, thus it's not the correct answer.
upvoted 
2 
times
kip21
kip21
 
11 months, 3 weeks ago
C
https://cloud.google.com/compute/docs/shutdownscript
upvoted 
1 
times
Terryhsieh
Terryhsieh
 
1 year ago
The answer should be C. reference to
https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances
Regarding the answer D, it is not the option becasue no need to touch xinetd servie inside Linux.
upvoted 
2 
times
RLsh
RLsh
 
1 year, 2 months ago
Selected Answer: 
D
I believe the answer should be D since the VMs are already created
upvoted 
1 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
Selected Answer: 
C
C is the right answer. See, there is one tip. In GCP, things like these are given to the customers as a solution - like give a
shutdown script. GCP won't trouble the users to know all those geeky linux stuffs. So the answer is simply C
upvoted 
1 
times
wukoon
wukoon
 
1 year, 2 months ago
Option D: Creating a shutdown script, registered as a xinetd service in Linux, and using the gcloud compute instances add-
metadata command to specify the service URL as the value for a new metadata entry with the key shutdown-script-url is not as
reliable as option C because it requires the gcloud command-line tool to be installed and configured on the virtual machinereliable as option C because it requires the gcloud command-line tool to be installed and configured on the virtual machine
instance.
upvoted 
1 
times
vc1011
vc1011
 
1 year, 2 months ago
Selected Answer: 
D
Reference:
https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #33
Your organization has a 3-tier web application deployed in the same network on Google Cloud Platform. Each tier (web, API,
and database) scales independently of the others. Network traffic should flow through the web to the API tier and then on to
the database tier. Traffic should not flow between the web and the database tier. 
How should you configure the network? 
A. 
Add each tier to a different subnetwork
B. 
Set up software based firewalls on individual VMs
C. 
Add tags to each tier and set up routes to allow the desired traffic flow
D. 
Add tags to each tier and set up firewall rules to allow the desired traffic flow 
Most Voted
Correct Answer:
 
D 
Comments
shandy
shandy
 
Highly Voted
 
5 years, 1 month ago
D. refer to target filtering. https://cloud.google.com/solutions/best-practices-vpc-design
upvoted 
36 
times
tartar
tartar
 
4 years, 4 months ago
D is ok
upvoted 
8 
times
pepYash
pepYash
 
4 years, 1 month ago
Thank you for the link. 
Precisely:
https://cloud.google.com/solutions/best-practices-vpc-design#target_filtering
upvoted 
8 
times
[Removed]
[Removed]
 
2 years ago
perfect! the example in that section is the exact question statement
upvoted 
2 
times
Community vote distribution
D (100%)nitinz
nitinz
 
3 years, 10 months ago
D, firewalls can be done on ip or network tags or service accounts in GCE.
upvoted 
4 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
D is right
upvoted 
1 
times
amxexam
amxexam
 
Highly Voted
 
3 years, 4 months ago
Let's go with option elimination
A. Add each tier to a different subnetwork
>> Adding tiers to different subnets does not prevent or block them from accessing each other. Until specific firewall rules on VM
or subnet allow access traffic on a specific port in the rule.
B. Set up software-based firewalls on individual VMs
>> Not a recommended practice will have to enable firewall anyway.
C. Add tags to each tier and set up routes to allow the desired traffic flow
>> Can be done but. 
D. Add tags to each tier and set up firewall rules to allow the desired traffic flow
>> Recommended way
Hence D
upvoted 
11 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
D
1. Firewall rules for security: Firewall rules provide the most granular and robust control over network traffic. By using tags to
identify instances in each tier (web, API, database), you can create firewall rules that explicitly allow or deny traffic between these
tiers.
2. Controlling traffic flow: 
You can create rules that:
- Allow traffic from the web tier to the API tier.
- Allow traffic from the API tier to the database tier.
- Explicitly deny traffic between the web and database tiers.
3. Scalability and Flexibility: This approach works well even when your tiers scale independently. 
As new instances are added, they
inherit the tags and automatically adhere to the defined firewall rules.
upvoted 
1 
times
ddatta
ddatta
 
2 months, 2 weeks ago
D is correct
upvoted 
1 
times
lisabisa
lisabisa
 
10 months, 2 weeks ago
Selected Answer: 
D
Routes are typically used for directing traffic between networks rather than within the same network. While tags can be used for
identifying resources, they are typically used in conjunction with firewall rules for controlling traffic flow.
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
D
Why to implement anything else when Firewall is built-in within VPC and works based on Tags associated with resources.
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
separate vnet is ruled out as they are on same network.
upvoted 
1 
times
red_panda
red_panda
 
1 year, 7 months agored_panda
red_panda
 
1 year, 7 months ago
Selected Answer: 
D
For me most suitable answer is D
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
It's D
To configure the network so that traffic flows through the web to the API tier and then on to the database tier, but does not flow
between the web and the database tier, you can add tags to each tier and set up firewall rules to allow the desired traffic flow. By
adding tags to each tier, you can identify the VMs that belong to each tier and create firewall rules that allow traffic between the
tiers as needed. For example, you can create a firewall rule that allows traffic from the web tier to the API tier, and another rule that
allows traffic from the API tier to the database tier. This will ensure that traffic flows through the desired path and is not allowed
between the web and database tiers.
Other options, such as adding each tier to a different subnetwork or setting up software-based firewalls on individual VMs, may not
provide the necessary level of control over the traffic flow between the tiers. Setting up routes to allow the desired traffic flow may
not be sufficient to prevent traffic between the web and database tiers.
upvoted 
5 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
D
D is right answer
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
D
Having 3-tier web application deployed in the same network is wrong to begin with. However, even in different subnets you will
need to apply firewall rules to prevent traffic between selected subnets. In this case they will probably be better of with D.
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
Did this appear in the exam?
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
use firewall rules
upvoted 
1 
times
amxexam
amxexam
 
2 years, 7 months ago
Selected Answer: 
D
A per my comment below .
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
D is the correct answer
upvoted 
3 
times
haroldbenites
haroldbenites
 
3 years, 1 month ago
Go for D
upvoted 
2 
times
unnikrisb
unnikrisb
 
3 years, 2 months ago
From Google practice exam question : 
D is correct because as instances scale, they will all have the same tag to identify the tier. These tags can then be leveraged inD is correct because as instances scale, they will all have the same tag to identify the tier. These tags can then be leveraged in
firewall rules to allow and restrict traffic as required, because tags can be used for both the target and source.
https://cloud.google.com/vpc/docs/using-vpc
https://cloud.google.com/vpc/docs/routes
https://cloud.google.com/vpc/docs/add-remove-network-tags
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #34
Your development team has installed a new Linux kernel module on the batch servers in Google Compute Engine (GCE) virtual
machines (VMs) to speed up the nightly batch process. Two days after the installation, 50% of the batch servers failed the
nightly batch run. You want to collect details on the failure to pass back to the development team. 
Which three actions should you take? (Choose three.) 
A. 
Use Stackdriver Logging to search for the module log entries 
Most Voted
B. 
Read the debug GCE Activity log using the API or Cloud Console
C. 
Use gcloud or Cloud Console to connect to the serial console and observe the logs 
Most Voted
D. 
Identify whether a live migration event of the failed server occurred, using in the activity log
E. 
Adjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics 
Most Voted
F. 
Export a debug VM into an image, and run the image on a local server where kernel log messages will be displayed on
the native screen
Correct Answer:
 
ACE 
Comments
rishab86
rishab86
 
Highly Voted
 
3 years, 7 months ago
ACE 
A. Use Stackdriver Logging to search for the module log entries = Check logs 
C. Use gcloud or Cloud Console to connect to the serial console and observe the logs = Check grub messages, remember new
kernel module was installed. 
E. Adjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics = Zoom into the time
window when problem happened.
upvoted 
46 
times
Pokchok
Pokchok
 
3 years, 6 months ago
But the assumption you made is that stack driver was already installed on the vms. 
What if it was not there? Would there be any
scope to install later and retrieve the logs?
upvoted 
3 
times
[Removed]
[Removed]
 
2 years ago
Community vote distribution
ACE (52%)
ABE (40%)
ACD (8%)[Removed]
[Removed]
 
2 years ago
But isn't it the same with B? it is talking about 'reading' the logs.
upvoted 
1 
times
AmitAr
AmitAr
 
2 years, 7 months ago
A, B, E
C - doesn't look correct as it ends with "observe the logs" - question is on sharing the details to development team, not to look for
cause
upvoted 
1 
times
tocsa
tocsa
 
7 months ago
E observe the logs too. One of my problem is that several of the options go on and do some observation instead of just
delivering.
upvoted 
1 
times
haroldbenites
haroldbenites
 
Highly Voted
 
3 years, 1 month ago
Go for A,B,E.
C is when the VM is running , but in this case the sentence says “recollect”. It means that “error ever” already happened.
upvoted 
11 
times
pddddd
pddddd
 
2 years, 12 months ago
and how will activity log help?
upvoted 
1 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 6 hours ago
Selected Answer: 
ACE
I will go for ACE.
upvoted 
1 
times
Ishu_awsguy
Ishu_awsguy
 
6 days, 17 hours ago
Selected Answer: 
ABE
B vs C is the question.
I vote for B , since activity logs now have been moved to audit logs and they provode system event information.
C is moee real time, it will be a task to extract all the logs and put it to a file and share with dev team.
Anyways audit logs is already abstracting that work for you.
Hence ABE is the right answer as per me
upvoted 
1 
times
mahi_h
mahi_h
 
2 weeks, 3 days ago
Selected Answer: 
ABE
I chose B over C as both options trying to read/observe logs. But C looks like reading at the runtime. Give that, it's a nightly batch
process, B seems to suitable post error occurrence.
upvoted 
1 
times
deep316
deep316
 
3 weeks, 2 days ago
Selected Answer: 
ACD
A. Use Stackdriver Logging to search for the module log entries: This will help you identify any errors or issues related to the new
kernel module that were logged during the batch process.
C. Use gcloud or Cloud Console to connect to the serial console and observe the logs: The serial console logs can provide
detailed information about the boot process and any kernel-related messages that might indicate why the batch servers failed.
D. Identify whether a live migration event of the failed server occurred, using the activity log: Live migration events can sometimes
cause disruptions. Checking the activity log will help you determine if this was a factor in the failures.
upvoted 
2 
times
deep316
deep316
 
3 weeks, 2 days ago
Selected Answer: 
ACEA. Use Stackdriver Logging to search for the module log entries: This will help you identify any errors or issues related to the new
kernel module that were logged during the batch process.
C. Use gcloud or Cloud Console to connect to the serial console and observe the logs: The serial console logs can provide
detailed information about the boot process and any kernel-related messages that might indicate why the batch servers failed.
D. Identify whether a live migration event of the failed server occurred, using the activity log: Live migration events can sometimes
cause disruptions. Checking the activity log will help you determine if this was a factor in the failures.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
Selected Answer: 
ACE
A. Use Stackdriver Logging to search for the module log entries: This is crucial. Stackdriver Logging aggregates logs from various
sources, including your VMs. You can search for specific entries related to the new kernel module to identify errors, warnings, or
unusual behavior that might explain the failures.
C. Use gcloud or Cloud Console to connect to the serial console and observe the logs: 
The serial console provides access to the
VM's output even if the system is unresponsive. This can be invaluable for capturing kernel panic messages, boot errors, or other
critical information that might not be available through standard logging channels.
E. Adjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics: Stackdriver Monitoring
provides detailed performance metrics for your VMs. By aligning the timeline with the failures, you can analyze CPU usage,
memory consumption, disk I/O, and network activity.
upvoted 
3 
times
SerGCP
SerGCP
 
1 month, 4 weeks ago
Selected Answer: 
ABE
considering that logs from the serial console might not be useful after two days:
A. Use Stackdriver Logging to search for the module log entries: This will help you identify any errors or issues logged by the new
kernel module.
B. Read the debug GCE Activity log using the API or Cloud Console: This can provide information on significant events like reboots
or live migrations that might have affected the batch process.
E. Adjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics: This helps correlate the
failure with any anomalies in the server metrics, providing insights into what might have gone wrong.
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Answer is ACE
upvoted 
1 
times
Hungdv
Hungdv
 
4 months, 4 weeks ago
Vote ACE
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 1 week ago
Selected Answer: 
ABE
I'm not sure but I vote for ABE
upvoted 
2 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
ACE makes sense. 
A and E dont have any doubts. Questions is that if it is B, C or F. 
Whats' use of serial console if team does not use it for logging especially kernal related updates. 
that makes sense to choose C.
upvoted 
1 
times
CloudDom
CloudDom
 
1 year, 1 month ago
Selected Answer: 
ABE
Most automated way, with C, collecting from VMs involves a lot of manual efforts
upvoted 
3 
times
 
1 year, 5 months agopablobairat
pablobairat
 
1 year, 5 months ago
Selected Answer: 
ABE
ABE is correct
upvoted 
2 
times
Ozymandiax
Ozymandiax
 
2 years ago
I'm really not sure here. A and E are just OK, and for me the final point is between B o C. many ppl is saying C, but, the question
says that the VM's already failed and you're investigating what happened in the past. 
Anyway, there are 2 ways to interpret this, from my point of view:
1) The failure happened and it's going to happen again. In this case ACE would be maybe the best option
BUt
2) 
The failure happened and you want to investigate this failure, which happened in the past. Therefore ABE would be the right one,
as you are "splunking" in the logs of the past, not having a review of the logs as they happen.
from my personal interpretation I'd go with ABE
upvoted 
9 
times
omermahgoub
omermahgoub
 
2 years ago
ACE
To collect details on the failure of the batch servers in GCE VMs, you can take the following actions:
A: Stackdriver Logging can help you identify any issues related to the new Linux kernel module by searching for log entries related
to the module.
C: Connecting to the serial console allows you to view the logs in real-time as the batch servers are running. This can help you
identify any issues related to the new kernel module.
E: By adjusting the timeline in Stackdriver to match the failure time, you can view the batch server metrics during the time when
the failures occurred. This can help you identify any issues related to the new kernel module.
Other options, such as reading the debug GCE Activity log using the API or Cloud Console, identifying whether a live migration
event of the failed server occurred, or exporting a debug VM into an image and running the image on a local server, may not
provide the necessary information to understand
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #35
Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log data to the cloud
and test the analytics features available to them there, while also retaining that data as a long-term disaster recovery backup. 
Which two steps should you take? (Choose two.) 
A. 
Load logs into Google BigQuery 
Most Voted
B. 
Load logs into Google Cloud SQL
C. 
Import logs into Google Stackdriver
D. 
Insert logs into Google Cloud Bigtable
E. 
Upload log files into Google Cloud Storage 
Most Voted
Correct Answer:
 
AE 
Comments
rishab86
rishab86
 
Highly Voted
 
3 years, 7 months ago
Answer is A as they want to load logs for analytics and E for storing data in buckets for long term.
upvoted 
33 
times
omermahgoub
omermahgoub
 
Highly Voted
 
2 years ago
AE
To archive approximately 100 TB of log data to the cloud and test the analytics features available while also retaining the data as a
long-term disaster recovery backup, you can take the following steps:
E: Upload log files into Google Cloud Storage: Google Cloud Storage is a scalable, durable, and fully-managed cloud storage
service that can be used to store large amounts of data. You can upload your log files to Cloud Storage to archive them in the
cloud.
A: Load logs into Google BigQuery: Google BigQuery is a fully-managed, cloud-native data warehouse that can be used to analyze
large amounts of data quickly and efficiently. You can load your log data into BigQuery to perform analytics on it and test the
available analytics features.
Other options, such as loading logs into Google Cloud SQL, importing logs into Google Stackdriver, or inserting logs into Google
Cloud Bigtable, may not provide the necessary functionality for archiving and analyzing the log data.
upvoted 
12 
times
Community vote distribution
AE (88%)
CE (12%)upvoted 
12 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 6 hours ago
Selected Answer: 
AE
I will go for AE.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
Selected Answer: 
AE
A. Load logs into Google BigQuery: BigQuery is Google Cloud's serverless, highly scalable, and cost-effective multicloud data
warehouse designed for data analytics. It's ideal for storing and analyzing large volumes of log data (100 TB in this case). You can
use BigQuery's powerful SQL capabilities to run queries, generate reports, and gain insights from your logs.
E. Upload log files into Google Cloud Storage: Cloud Storage provides durable, scalable, and secure object storage. It's perfect for
storing your log data as a long-term disaster recovery backup. Cloud Storage offers different storage classes to optimize costs
based on your data access frequency and retention needs.
upvoted 
3 
times
ionescuandrei
ionescuandrei
 
1 year, 9 months ago
Selected Answer: 
AE
This looks right.
upvoted 
2 
times
CMata
CMata
 
2 years, 1 month ago
Selected Answer: 
AE
If you want to analize those logs its recommended Big Query. For storing and backup Cloud Storage is your option, so AE
upvoted 
3 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A and E can do the required task
upvoted 
2 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
AE
AE are the only options for analysis and archiving
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
A load in Big query for analytics and E for cloud storage
upvoted 
1 
times
Ramheadhunter
Ramheadhunter
 
2 years, 4 months ago
Selected Answer: 
AE
The key word is 'Analytics' here the main reason for moving logs to GCP is to perform Analytics on the data. BigQuery is the best
suite for it. For long term storage it wiould be GC
upvoted 
1 
times
raaj_p
raaj_p
 
2 years, 4 months ago
Selected Answer: 
CE
Answer is C and E
Key features
Real-time log management and analysis
Cloud Logging is a fully managed service that performs at scale and can ingest application and platform log data, as well as
custom log data from GKE environments, VMs, and other services inside and outside of Google Cloud. Get advanced
performance, troubleshooting, security, and business insights with Log Analytics, integrating the power of BigQuery into Cloud
Logging. 
- 
https://cloud.google.com/products/operations
upvoted 
2 
times
AMohanty
AMohanty
 
2 years, 4 months agoyou don't do realtime log management on 10 TB data.
You only perform analytics on it. 
So A for Analytics
E for storage.
upvoted 
6 
times
tocsa
tocsa
 
7 months ago
Is it even possible to import 10TB of logs into StackDriver?
upvoted 
1 
times
Ramheadhunter
Ramheadhunter
 
2 years, 4 months ago
The key word is 'Analytics' here the main reason for moving logs to GCP is to perform Analytics on the data. BigQuery is the best
suite for it. For long term storage it wiould be GCS
upvoted 
1 
times
faagee01
faagee01
 
2 years, 5 months ago
Selected Answer: 
AE
Big Query for analytics, Cloud Storage for long term archive
upvoted 
3 
times
Dhiraj03
Dhiraj03
 
2 years, 6 months ago
For Storage GCS is the best option and for analyzing the data BIg Query makes sense
upvoted 
1 
times
Nirca
Nirca
 
2 years, 8 months ago
A E are ok
upvoted 
2 
times
vincy2202
vincy2202
 
3 years ago
AE are the correct answers
upvoted 
3 
times
andeu
andeu
 
3 years ago
Answers: A is correct because BigQuery is the fully managed cloud data warehouse for analytics and supports the analytics
requirement.
E is correct because Cloud Storage provides the Coldline storage class to support long-term storage with infrequent access,
which would support the long-term disaster recovery backup requirement.
https://cloud.google.com/bigquery/
https://cloud.google.com/stackdriver/
https://cloud.google.com/storage/docs/storage-classes#coldline
https://cloud.google.com/sql/
https://cloud.google.com/bigtable/
upvoted 
4 
times
MQQ
MQQ
 
2 years, 7 months ago
But BigQuery is for SQL DATA, the logs are nosql ?
Why not choose stackdriver?
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years, 1 month ago
Go for A,E
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #36
You created a pipeline that can deploy your source code changes to your infrastructure in instance groups for self-healing. One
of the changes negatively affects your key performance indicator. You are not sure how to fix it, and investigation could take
up to a week. 
What should you do? 
A. 
Log in to a server, and iterate on the fox locally
B. 
Revert the source code change, and rerun the deployment pipeline 
Most Voted
C. 
Log into the servers with the bad code change, and swap in the previous code
D. 
Change the instance group template to the previous one, and delete all instances
Correct Answer:
 
B 
Comments
amxexam
amxexam
 
Highly Voted
 
3 years, 4 months ago
Let's go with option elimination
A. Log in to a server, and iterate on the fix locally
>> Long step, hence eliminate
B. Revert the source code change and rerun the deployment pipeline
>> This revert will be logged in the source repo. Will go with this way although D also is correct.
C. login to the servers with the bad code change, and swap in the previous code
>> C is manually doing what can be automatically done by B and C, hence eliminate.
D. Change the instance group template to the previous one and delete all instances
>> This is similar to B but why manually do something which is automated. Hence eliminate. But is also correct. But B is better
from code lifecycle perspective.
Hence B
upvoted 
75 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
Community vote distribution
B (62%)
D (36%)
A
(1%)The question itself looks the madeup. Not a real scenario ..."You created a pipeline that can deploy your source code changes to
your infrastructure in instance groups for self-healing. One of the changes negatively affects your key performance indicator. 
"
How a self healing code is affecting KPI. 
What was KPI, we dont know. Was the self healing done? we dont know. Dont know
who make this questions. 
Even if we go whatever they try to ask, with 
options available, B is safest. However this option is just
answer to any troubleshooting step. I m not convinced for the person who wrote this question
upvoted 
3 
times
ewredtrfygi
ewredtrfygi
 
Highly Voted
 
4 years, 5 months ago
Too many responses saying B is the answer - I wonder if GCP pays people to provide the wrong answers on this website. It's
clearly D, MIG templates support versioning, they were created to solve this exact problem. You simply select the previous
template version, set that as the new deployment, and it will roll back the KPI depriving deployment and roll out the previous
working deployment. The only part of D I don't like is the "terminate all instances" since you should engage in a rolling deployment,
but if it's not a live website I suppose that would be fine.
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups
upvoted 
62 
times
mexblood1
mexblood1
 
4 years, 2 months ago
If you can deploy your source code changes to the infrastructure in instance group for self-healing, it means you're not using
Manage Instance Groups. Otherwise you would be creating a new template with the code changes. Further more, you would not
delete instances on a MIG, you would be rolling out the previous template again in a controlled manner using maxsurge,
maxunavailable, etc. For those reasons I'll choose B.
upvoted 
21 
times
AmitAr
AmitAr
 
2 years, 7 months ago
B. keyword is "self-healing" not "auto-healing" - which means MIG not used. So correct answer is B
upvoted 
2 
times
Bill831231
Bill831231
 
3 years, 2 months ago
seems with approach, there will be a mismatch in pipeline
upvoted 
4 
times
Meyucho
Meyucho
 
2 years, 11 months ago
If you change manually the template.. why are using pipelines? B is the best answer because is automated!!! Why Google will be
interested to vote the wrong answers??? They want more professionals with GCP certifications!!!!
upvoted 
7 
times
Davidik79
Davidik79
 
2 years, 9 months ago
"....One of the changes has impacted negatively your PKI". Why is the question about pipeline? It is about how to do
investigations and keep your PKI at the proper SLA/SLO.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
B
1. Quickest path to recovery: Reverting the code change that caused the performance degradation is the fastest way to restore
your key performance indicator (KPI) to its previous level. Your pipeline is designed to automate deployments, so re-deploying the
known good version should be straightforward.
2. Minimizes downtime: 
While investigating the issue is important, it can take time. Reverting first minimizes the duration of the
negative impact on your KPI.
3. Clean and controlled: This approach avoids making ad-hoc changes directly on servers (options A and C), which can lead to
inconsistencies and make it harder to track the problem's source.
4. Maintains instance group integrity: Option D involves deleting instances, which can disrupt services and lead to unnecessary re-
creation of resources.
upvoted 
2 
times
sim7243
sim7243
 
1 month, 3 weeks ago
option B
upvoted 
1 
times
dija123
dija123
 
9 months agoSelected Answer: 
B
Agree with B
upvoted 
1 
times
lisabisa
lisabisa
 
10 months, 1 week ago
D is infrastructure change. 
B is application change. 
So B is correct.
upvoted 
3 
times
kip21
kip21
 
11 months, 3 weeks ago
D
The question is talking abt MIG and you can revert Inst Template same as B. Since this is about MIG's I will choose D
upvoted 
1 
times
adoyt
adoyt
 
1 year ago
Selected Answer: 
D
D. This is a question about instance groups and so modifying templates should be what we're looking for.
upvoted 
1 
times
simiramis221
simiramis221
 
1 year ago
This a B for sure
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
B
The popped up with source code changes, hence reverting the change and deployment will solve the issue.
B. Revert the source code change, and rerun the deployment pipeline
upvoted 
2 
times
yilexar
yilexar
 
1 year, 3 months ago
D is incorrect:
- MIG instance group template is immutable, there is no version concept. The context never mentioned that team created multiple
instance group templates.
- Software code change might not all ended up in the instance group templates, it depends on how the deployment pipeline is
configured. 
Regardless, B is a best practices, ensure that your infrastructure is synced with your source control system.
upvoted 
1 
times
rusll
rusll
 
1 year, 3 months ago
Selected Answer: 
D
You don't need to touch your code, just deploy and older version and fix the code, then deploy the fixed version
upvoted 
2 
times
jalberto
jalberto
 
1 year, 4 months ago
Selected Answer: 
D
The most secure option is D
Revert a source code change could be complex (if change was made in various components)
upvoted 
2 
times
chrismar
chrismar
 
1 year, 6 months ago
Selected Answer: 
B
Because the question starting from source code
upvoted 
2 
times
oriori123123
oriori123123
 
1 year, 6 months ago
Selected Answer: 
BSelected Answer: 
B
by bard:
The correct answer is B. Revert the source code change, and rerun the deployment pipeline.
upvoted 
1 
times
JohnWick2020
JohnWick2020
 
1 year, 7 months ago
B. 
The keyword here is source code not VM configuration. If it was the later then instance group templates is the answer. But in this
case simply rollback your source code change and rerun to last workable version. Simples!
upvoted 
2 
times
red_panda
red_panda
 
1 year, 7 months ago
Selected Answer: 
B
B is ok for me
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #37
Your organization wants to control IAM policies for different departments independently, but centrally. 
Which approach should you take? 
A. 
Multiple Organizations with multiple Folders
B. 
Multiple Organizations, one for each department
C. 
A single Organization with Folders for each department 
Most Voted
D. 
A single Organization with multiple projects, each with a central owner
Correct Answer:
 
C 
Comments
AWS56
AWS56
 
Highly Voted
 
5 years, 1 month ago
https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
I will stick with C
upvoted 
27 
times
CamiloJrJr
CamiloJrJr
 
1 month, 1 week ago
https://cloud.google.com/architecture/identity/best-practices-for-
planning#use_organizations_to_delineate_administrative_authority
upvoted 
1 
times
CamiloJrJr
CamiloJrJr
 
1 month, 1 week ago
https://cloud.google.com/architecture/identity/best-practices-for-planning#use_a_separate_staging_organization
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is recommended approach
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
Highly Voted
 
2 years, 3 months ago
Community vote distribution
C (100%)The reason it isn't D is that a Dept modelled as a project puts a massive constraint on the dept that they can only have a single
project, it's likely a department will want many projects.
upvoted 
7 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
C
1. Centralized Control: A single organization provides a central point for managing all your Google Cloud resources and IAM
policies. This simplifies administration and ensures consistency across your organization.
2. Independent Department Management: Folders allow you to group projects within your organization and delegate administrative
control to different departments. Each department can manage its own IAM policies within its assigned folder, providing the
necessary independence.
3. Hierarchical Structure: 
Folders provide a hierarchical structure for organizing your resources. You can create sub-folders within
departments for further granularity and control.
4. Efficient Resource Management: This structure makes it easier to manage resources, track costs, and enforce security policies
across your organization.
upvoted 
4 
times
Shasha1
Shasha1
 
2 months, 3 weeks ago
B
upvoted 
1 
times
vyomkeshbakshi
vyomkeshbakshi
 
1 year, 5 months ago
Selected Answer: 
C
Clearly C
upvoted 
1 
times
red_panda
red_panda
 
1 year, 7 months ago
Selected Answer: 
C
Is obviously C
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
C. A single Organization with Folders for each department
To control IAM policies for different departments independently but centrally, you should create a single organization and use
folders to organize the policies for each department. This approach allows you to centralize the management of IAM policies for all
departments within a single organization, while also allowing you to set up different policies for each department as needed.
Option A, multiple organizations with multiple folders, would not be an effective solution because it would create unnecessary
complexity and make it more difficult to centralize the management of IAM policies. Option B, multiple organizations, one for each
department, would also not be an effective solution because it would create unnecessary complexity and make it more difficult to
centralize the management of IAM policies. Option D, a single organization with multiple projects, each with a central owner, would
not be an effective solution because it would not allow you to set up different policies for each department as needed.
upvoted 
2 
times
MarcoEscanor
MarcoEscanor
 
2 years, 2 months ago
C. It's a best practice and I've done this with my previous and current company :)
upvoted 
3 
times
Prashant2022
Prashant2022
 
2 years, 3 months ago
Selected Answer: 
C
.............
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
C single org and multiple folders
upvoted 
1 
times
cmamiusa
cmamiusa
 
2 years, 9 months agocmamiusa
cmamiusa
 
2 years, 9 months ago
Selected Answer: 
C
C is the right answer
upvoted 
1 
times
mygcpjourney2712
mygcpjourney2712
 
2 years, 9 months ago
Selected Answer: 
C
Will go for C
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years, 1 month ago
Go for C
upvoted 
2 
times
vincy2202
vincy2202
 
3 years, 1 month ago
C is the right answer
upvoted 
1 
times
nansi
nansi
 
3 years, 3 months ago
C shall be the correct answer
upvoted 
1 
times
rikoko
rikoko
 
3 years, 4 months ago
C. Seems to be best practice (cf AWS56). And I believe that D should be excluded because it says "Project owner" - it is not best
practice since it's a basic role + it's not even stated as a requisite
upvoted 
1 
times
victory108
victory108
 
3 years, 7 months ago
C. A single Organization with Folders for each department
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #38
You deploy your custom Java application to Google App Engine. It fails to deploy and gives you the following stack trace. 
What should you do? 
 
A. 
Upload missing JAR files and redeploy your application.
B. 
Digitally sign all of your JAR files and redeploy your application 
Most Voted
C. 
Recompile the CLoakedServlet class using and MD5 hash instead of SHA1C. 
Recompile the CLoakedServlet class using and MD5 hash instead of SHA1
Correct Answer:
 
B 
Comments
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
Signing the JAR files grants it permissions. (https://docs.oracle.com/javase/tutorial/deployment/jar/signindex.html)
upvoted 
23 
times
tartar
tartar
 
4 years, 5 months ago
B is ok
upvoted 
9 
times
Urban_Life
Urban_Life
 
3 years, 2 months ago
Where do you go? when we need you for other questions. Plz ans other q's if you have time
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
B, SHA1 Digest error in the first line in the error code. With Java errors, always focus on the first line in the error code, rest of the
lines are garbage **mostly**.
upvoted 
18 
times
omermahgoub
omermahgoub
 
Highly Voted
 
2 years ago
The most likely cause of the error is that one of the JAR files in your application has been tampered with or is corrupt. The SHA1
digest error indicates that the JAR file's signature does not match the expected value, which could be due to tampering or
corruption.
To fix the issue, you should try uploading missing JAR files and redeploying your application. If the issue persists, you may need to
digitally sign all of your JAR files and redeploy your application to ensure that the signatures are valid. You should not try to
recompile the Cloaked
upvoted 
14 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
B
1. JAR signing and integrity: Digitally signing your JAR files ensures their authenticity and integrity. It adds a digital signature that
verifies the origin and confirms that the file hasn't been tampered with. This is crucial for security and prevents issues like the
SHA1 digest error you're encountering.
2. App Engine requirement: Google App Engine enforces JAR signing for security reasons. All deployed applications must have
properly signed JAR files.
upvoted 
4 
times
chrissamharris
chrissamharris
 
1 month, 3 weeks ago
Selected Answer: 
A
A SHA1 digest error during the deployment of a Java application to Google App Engine (GAE) typically indicates an issue with the
integrity of your JAR files. This error can arise due to corrupted, modified, or missing JAR files that are essential for your
application to run correctly.
upvoted 
1 
times
lisabisa
lisabisa
 
10 months, 1 week ago
Selected Answer: 
A
A missing JAR (Java ARchive) file indicates a problem with the code to be deployed.
B Digitally signing all of your JAR files indicates a problem with the signature.
A is better
upvoted 
3 
times
Community vote distribution
B (73%)
A (27%)upvoted 
3 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
B
"JavaVerifier.Java.428" is the key here
upvoted 
1 
times
SantoshPanigrahi
SantoshPanigrahi
 
1 year, 4 months ago
Selected Answer: 
B
B is the correct answer.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
A
A. Upload missing JAR files and redeploy your application.
The error message indicates that there is a problem with the SHA1 digest for the "com/altostrat/cloakedservlet.class" file. This can
be caused by a corrupted or incomplete JAR file. Therefore, the best course of action is to upload any missing JAR files and
redeploy the application.
upvoted 
2 
times
tocsa
tocsa
 
7 months ago
Corrupted or incomplete. But not missing.
upvoted 
1 
times
Racinely
Racinely
 
2 years, 1 month ago
B is correct
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
Ok B but how is this question related to a GCP exam? I guess a google search will be faster than reading the theory around Java
(unless you are a developer).
upvoted 
7 
times
zr79
zr79
 
2 years, 2 months ago
have you done any Azure exams? you will thank Google
upvoted 
5 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
nothing to do with GCP however basic troubleshooting skills required as a DevOps or Architect, B is fine
upvoted 
3 
times
[Removed]
[Removed]
 
2 years ago
I see your point, but for basic troubleshooting of apps, i will usually have access to google (aka stackoverflow homepage). This
could have been a cloud developer question that they repurposed.
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
B is the right answer
upvoted 
1 
times
avinashvidyarthi
avinashvidyarthi
 
2 years, 7 months ago
Selected Answer: 
B
B is Correct!
upvoted 
1 
times
Munna19
Munna19
 
2 years, 8 months agoMunna19
Munna19
 
2 years, 8 months ago
B is the right answer
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
B is the correct answer
upvoted 
2 
times
duocnh
duocnh
 
3 years, 1 month ago
Selected Answer: 
B
vote B
upvoted 
1 
times
TheCloudBoy77
TheCloudBoy77
 
3 years, 1 month ago
Selected Answer: 
B
B is correct answer
upvoted 
1 
times
bala786
bala786
 
3 years, 5 months ago
Option B. Digitally sign all of your JAR files and redeploy your application
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #39
You are designing a mobile chat application. You want to ensure people cannot spoof chat messages, by providing a message
were sent by a specific user. 
What should you do? 
A. 
Tag messages client side with the originating user identifier and the destination user.
B. 
Encrypt the message client side using block-based encryption with a shared key.
C. 
Use public key infrastructure (PKI) to encrypt the message client side using the originating user's private key. 
Most Voted
D. 
Use a trusted certificate authority to enable SSL connectivity between the client application and the server.
Correct Answer:
 
C 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
I am not sure about this one. D works if SSL client authentication is enabled.
C works as well if client encrypts message with private key and server decrypt with public key.
I prefer C.
upvoted 
38 
times
JoeShmoe
JoeShmoe
 
5 years, 1 month ago
Agree with C
upvoted 
5 
times
asfar
asfar
 
4 years, 11 months ago
I agree with C on this one.
upvoted 
5 
times
Tobbe
Tobbe
 
Highly Voted
 
3 years, 10 months ago
Encrypting each block and tagging each message at the client side is an overhead on the application. Best method which has
been adopted since years is contacting SSL provider and use public certificate to encrypt the traffic between client and server. 
D is correct
Community vote distribution
C (79%)
D (21%)upvoted 
14 
times
Meyucho
Meyucho
 
2 years, 11 months ago
If you use server public key you aren't meeting the goal. Don't miss the "for specific user" in the statement
upvoted 
1 
times
Alekshar
Alekshar
 
3 years, 10 months ago
If you use the server's public certificate to encrypt your data you only ensure the right server is the only one to read you. 
But anyone can use the same encryption key as you did and pretend to be you. Hence it does not solve our authentication
problematic
upvoted 
6 
times
Tobbe
Tobbe
 
3 years, 10 months ago
thanks for your insight! C is correct.
upvoted 
2 
times
stefanop
stefanop
 
1 year, 1 month ago
Can you explain why you think this?
upvoted 
1 
times
lynx256
lynx256
 
3 years, 9 months ago
I cannot agree with you. Before one be able to pretend to be someone else, he should know his (someone's) password on the
Chat Server...
upvoted 
1 
times
PeppaPig
PeppaPig
 
3 years, 5 months ago
SSL doesn't use server's public key to encrypt data. This is definitely wrong. Please read SSL specs. 
SSL uses a separate
session key for message encryption. 
This session key is temporary and will be rotated for every single session.
upvoted 
4 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
C
1. Digital Signatures and Non-Repudiation: PKI provides the foundation for digital signatures. When a user sends a message, it's
encrypted with their private key. 
The recipient can then use the sender's public key to decrypt it. This ensures:
- Authenticity: The message truly originated from the claimed sender.
- Non-repudiation: The sender cannot deny sending the message.
- Integrity: The message hasn't been tampered with in transit.
2. How it prevents spoofing: 
Since only the sender has access to their private key, no one else can create a message that would
decrypt correctly with their public key. This effectively prevents spoofing.
upvoted 
3 
times
Chojrak
Chojrak
 
7 months ago
The "C" answer is either messed up on purpose, or somebody dumped it wrong. 
When you use PKI (Public Key Infrastructure), you encrypt using a _public_ key of the recipient, and the recipient decrypts using
their _private_ key. Sample reference that this is correct: https://www.keyfactor.com/education-center/what-is-pki/
On the contrary, when a messages is _digitally signed_, the originator is using their _private_ key to sign the message, and the
recipient is verifying it using _public key_ of the _originator_.
I still don't know which answer would I choose on the actual exam.
upvoted 
3 
times
yas_cloud
yas_cloud
 
11 months, 1 week ago
Option A is not secure because anyone who intercepts the message could modify the user identifiers. Option B does not provide a
way to verify the sender’s identity. Option D is important for securing the connection between the client and server, but it does not
prevent message spoofing by itself.
Hence C.
upvoted 
1 
timesupvoted 
1 
times
02fc23a
02fc23a
 
1 year, 1 month ago
Selected Answer: 
C
I was considering D, but nope, C:
https://support.google.com/messages/answer/10262381#:~:text=your%20device%20and%20the%20device%20you%20message
upvoted 
1 
times
_kartik_raj
_kartik_raj
 
1 year, 2 months ago
Answer : D
Let me clarify , what PKI is saying i think first the answer is D , reason , Just understand what it says , i.e. option c - Using PKI to
encrypt messages using the originating user's private key, now couple people are saying that it is good and then the server will
decrypt the msg using public key, but can't you see anyone in the whole world will be able to see the messages as public key is
available 
publicly. ideally what should have been the solution, Using the public key of receiver the messages should have been
encrypted then the receiver would have decrypted 
using his private key, which absolutely makes sense, Talking about ssl i 
think its
one of the widely used secure tech for communication between client and server
upvoted 
1 
times
ArtistS
ArtistS
 
1 year, 1 month ago
Thanks for your explanation, so I choose C.....
upvoted 
1 
times
stefanop
stefanop
 
1 year, 1 month ago
Why? Can you explain that?
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
C
Option C - Use public key infrastructure (PKI) to encrypt the message client-side using the originating user's private key: Using PKI
to encrypt messages using the originating user's private key provides end-to-end encryption, which means only the intended
recipient can decrypt the message. This option also ensures that the message's authenticity is protected. If a malicious user
changes the sender's name, the recipient will not be able to decrypt the message since it was not encrypted using the correct
private key. This option is a strong method for securing chat messages.
upvoted 
4 
times
patricklin1105
patricklin1105
 
1 year, 5 months ago
C is wrong because private can only be used for signing, not encrypting. Public key is used for encrypting, private key is used for
decrypting.
upvoted 
3 
times
stefanop
stefanop
 
1 year, 1 month ago
But still, spoofing is not about reading data in clear but impersonating someone else. 
Using D you can sign the message by confirming your identity.
upvoted 
1 
times
BigfootPanda
BigfootPanda
 
1 year, 6 months ago
Selected Answer: 
C
As question is about ensuring a specific user sent a message, answer could not be D, which would ensure secure message
transmission, but not message origin (which can only be done by using asymmetric key)
upvoted 
3 
times
stefanop
stefanop
 
1 year, 1 month ago
So the sender is signing the message while encrypting it with the private key?
upvoted 
1 
times
oriori123123
oriori123123
 
1 year, 6 months ago
bard say D.
and ChatGTP say C..
upvoted 
1 
timesionescuandrei
ionescuandrei
 
1 year, 9 months ago
Selected Answer: 
C
I noticed this question in other tests and the suggested answer was C.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
C
To prevent message spoofing, it is important to ensure that messages cannot be altered or forged by anyone other than the
originating user. One way to accomplish this is by using public key infrastructure (PKI) to encrypt messages using the originating
user's private key.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
To ensure that chat messages cannot be spoofed and that the messages are truly sent by a specific user, the best option would
be to use public key infrastructure (PKI) to encrypt the message client side using the originating user's private key. This would
allow the recipient to verify the authenticity of the message by using the originating user's public key to decrypt the message.
Option A, tagging the message with the originating user identifier and the destination user, would not ensure the authenticity of the
message, as it could potentially be forged by an attacker.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
Option B, encrypting the message using block-based encryption with a shared key, would also not ensure the authenticity of the
message, as the shared key could potentially be compromised by an attacker.
Option D, using a trusted certificate authority to enable SSL connectivity between the client application and the server, would help
to secure the communication channel between the client and the server, but it would not necessarily ensure the authenticity of
the chat messages themselves.
Overall, using PKI and the originating user's private key to encrypt the message would be the most effective way to ensure the
authenticity of the chat messages in your mobile chat application.
upvoted 
2 
times
FateSpringfield
FateSpringfield
 
2 years, 1 month ago
C is the answer, The requirement is the integrity of messages sent in CIA security (Confidentiality, Integrity, and Availability). For
Confidentiality, using PublicKey of receiver, for Integrity, using PrivateKey of sender. D works in case of SSL client authentication.
upvoted 
3 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
AjayPandit
AjayPandit
 
2 years, 2 months ago
Selected Answer: 
C
PKI uses X.509 certificates and Public Keys, where the key is used for end-to-end encrypted communication, so that both parties
can trust each other and test their authenticity. PKI is mostly used in TLS/SSL to secure connections between the user and the
server, while the user tests the server’s authenticity to make sure it’s not spoofed
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #40
As part of implementing their disaster recovery plan, your company is trying to replicate their production MySQL database from
their private data center to their 
GCP project using a Google Cloud VPN connection. They are experiencing latency issues and a small amount of packet loss
that is disrupting the replication. 
What should they do? 
A. 
Configure their replication to use UDP.
B. 
Configure a Google Cloud Dedicated Interconnect. 
Most Voted
C. 
Restore their database daily using Google Cloud SQL.
D. 
Add additional VPN connections and load balance them.
E. 
Send the replicated transaction to Google Cloud Pub/Sub.
Correct Answer:
 
B 
Comments
mawsman
mawsman
 
Highly Voted
 
4 years, 10 months ago
It's latency issues. That won't be solved by adding another VPN tunnel. If it was just a throughput issue then VPN would do,
however to improve latency you need to go layer 2. Answer is B
upvoted 
36 
times
chiar
chiar
 
Highly Voted
 
5 years, 1 month ago
I think B is correct. I think it is more reliable.
upvoted 
30 
times
desertlotus1211
desertlotus1211
 
Most Recent
 
1 month ago
Selected Answer: 
B
Adding VPN connections may improve bandwidth but does not resolve latency or packet loss issues caused by public internet
routing... though not mentioned, we must 'think' beyond the scope of the question and ask 'what is causing the latency'... 
Answer B.
upvoted 
1 
times
Community vote distribution
B (67%)
D (25%)
E (8%)upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
Selected Answer: 
B
1. Dedicated Interconnect for high performance: Dedicated Interconnect provides a direct physical connection between your on-
premises network and Google Cloud. This offers significantly lower latency, higher bandwidth, and greater reliability compared to a
VPN. It's ideal for demanding workloads like database replication.
2. Reduced latency and packet loss: By bypassing the public internet, Dedicated Interconnect minimizes latency and packet loss,
ensuring consistent and efficient data transfer for your MySQL replication.
3. Enhanced reliability: Dedicated Interconnect provides a more stable and predictable connection compared to a VPN, which can
be affected by internet traffic fluctuations.
Why D is not correct: Add additional VPN connections and load balance them: This might improve bandwidth slightly but won't
address the fundamental latency and packet loss issues inherent with VPNs over the public internet.
upvoted 
3 
times
19040e5
19040e5
 
7 months, 2 weeks ago
Selected Answer: 
B
It's B, Interconnect.
D is wrong in this case: While this might help distribute traffic, it won't solve the underlying issue of latency and packet loss caused
by the inherent limitations of VPNs.
upvoted 
2 
times
AWS_Sam
AWS_Sam
 
12 months ago
Dedicated interconnect is the answer. A second VPN will give you an HA solution, not going to resolve the latency.
upvoted 
2 
times
[Removed]
[Removed]
 
1 year ago
Selected Answer: 
E
Have you mind the budget you'll need to improve network infrastructure - whether it's dedicated interconnect or duplicating VPN
connection? Mega IT corps may can afford, but I won't approve the work just for disaster recovery plan, if I were the authority. It's
definatedly overkill.
Main problem here is a latency issue and/or packet loss, yet the reason hasn't clearly configured. Whether it's occational, or
repeatetive, and/or by DB engine or by network, mostly unknown. But you don't have to solve the problem if there's better bypass.
Simply retry and test it. There C also can be a solution (which likely being placed already), but it doesn't have significant feature for
logging a transaction success/fail. If you take advance Pub/Sub, you can track each transaction processes. Ordering, can adding
another issue, but it worth a try - cost effectively.
upvoted 
2 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
B
Correct answer is B as its a latency issue.
upvoted 
2 
times
stefanop
stefanop
 
1 year ago
Selected Answer: 
B
I go for B.
Latency won't be solved by adding new VPN tunnels.
upvoted 
2 
times
Demo_Helloworld
Demo_Helloworld
 
1 year, 3 months ago
Selected Answer: 
D
We have something called as HAVPN which uses 2 VPN Connections at a time. As this Question is old. we dont have this Option
called use HAVPN. Now its Updated so the answer will be D. As its just a replication for disaster recovery and they are facing very
minimal challenges
upvoted 
6 
times
SandipGhosal
SandipGhosal
 
1 year, 10 months ago
I think the best option is E, using PubSub. In question the main issue is " a small amount of packet loss". As per google PubSub
documentation, data replication among databases is one of the common use case of PubSub. The asynchronouslydocumentation, data replication among databases is one of the common use case of PubSub. The asynchronously
communication of PubSub can overcome small latency issues. Setting up dedicated interconnect would be very costly and
required many pre-requisites.
upvoted 
3 
times
Shawnn
Shawnn
 
1 year, 10 months ago
You could technically use your private key to encrypt a message, but it would not be secure because anyone who has your public
key could decrypt the message. The recommended practice is to use your private key only for decryption and to use the recipient's
public key for encryption.
I vote for D
upvoted 
1 
times
SirajShan
SirajShan
 
1 year, 10 months ago
Configuring a Google Cloud Dedicated Interconnect 
requirement for company is a proximity to colocation facility and meeting
condition to have dedicated interconnect. Had this was possible why did they used Cloud VPN in the first place ? 
I think answer
should be D.
upvoted 
1 
times
n_nana
n_nana
 
1 year, 11 months ago
If i face this issue, I will give a try with additional VPN, decision using VPN, maybe because they need encryption as well. With
switching to dedicated interconnect, you have to implement your own VPN solution or application encryption. so it need more
anaylsis to just skip VPN and use dedicated interconnect solution.
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
The company should consider configuring a Google Cloud Dedicated Interconnect. A Google Cloud Dedicated Interconnect
provides a private connection between the company's on-premises data center and GCP, which can help to reduce latency and
improve the reliability of the connection. This can be particularly useful for replicating large amounts of data or for applications that
require low-latency connectivity.
Option A, configuring the replication to use UDP, would not necessarily improve the reliability of the connection, as UDP is a
connectionless protocol that does not guarantee delivery of packets.
Option C, restoring the database daily using Google Cloud SQL, would not address the underlying issues with the replication
process.
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
Option D, adding additional VPN connections and load balancing them, may help to improve the reliability of the connection by
providing redundancy, but it may not necessarily address latency issues.
Option E, sending the replicated transaction to Google Cloud Pub/Sub, could potentially help to improve the reliability of the
replication process by allowing the company to handle failures and retries in a more structured way, but it would not necessarily
address latency issues.
Overall, configuring a Google Cloud Dedicated Interconnect is likely to be the most effective solution for addressing latency
issues and packet loss in the replication process.
upvoted 
1 
times
VSMu
VSMu
 
1 year, 11 months ago
Why focus on latency when the solution is for disaster recovery? The main issue is packet loss. While this can be solved with
Dedicated Interconnect or Cloud Pub/Sub, PubSub seems like a cheaper alternative that prevents data loss and achieves
reliability. 
I wouldn't care about latency as the backup is only for DA.. so how does it matter if it goes slowly?
upvoted 
3 
times
jrisl1991
jrisl1991
 
1 year, 2 months ago
Because latency and packet loss are probably coming from traffic going over public internet. This is the Cloud VPN definition
from the public documentation:
"Cloud VPN securely extends your peer network to Google's network through an IPsec VPN tunnel. Traffic is encrypted and
travels between the two networks over the public internet. Cloud VPN is useful for low-volume data connections."
There are documents showing what happens when public internet is used, but basically there's no way to prevent informationThere are documents showing what happens when public internet is used, but basically there's no way to prevent information
from going through multiple hops over the internet, which is why Dedicated Interconnect should work. Plus, VPN is
recommended for low-volume data connections, and I highly doubt that replicating a database is considered a low-volume
operation. 
I'm going with B: using Cloud Dedicated Interconnect.
upvoted 
2 
times
ashrafh
ashrafh
 
2 years, 1 month ago
so just to 
solve this 
issue we are going over a Dedicated Interconnect imagine saying this to a your project head.
upvoted 
3 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
ok for B
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #41
Your customer support tool logs all email and chat conversations to Cloud Bigtable for retention and analysis. What is the
recommended approach for sanitizing this data of personally identifiable information or payment card information before
initial storage? 
A. 
Hash all data using SHA256
B. 
Encrypt all data using elliptic curve cryptography
C. 
De-identify the data with the Cloud Data Loss Prevention API 
Most Voted
D. 
Use regular expressions to find and redact phone numbers, email addresses, and credit card numbers
Correct Answer:
 
C 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
C is the answer
upvoted 
23 
times
nitinz
nitinz
 
3 years, 10 months ago
C, data sanitization = DLP
upvoted 
8 
times
tartar
tartar
 
4 years, 4 months ago
C is ok
upvoted 
8 
times
omermahgoub
omermahgoub
 
Highly Voted
 
2 years ago
The recommended approach for sanitizing data of personally identifiable information or payment card information before storing it
in Cloud Bigtable is option C: De-identify the data with the Cloud Data Loss Prevention API.
The Cloud Data Loss Prevention (DLP) API is a powerful tool that allows you to automatically discover, classify, and redact
sensitive data in your organization. It uses advanced machine learning techniques to accurately identify and protect a wide range
of sensitive data types, including personal information such as names, addresses, phone numbers, and payment card information.
Community vote distribution
C (100%)Using the DLP API to de-identify your data before storing it in Cloud Bigtable is the most effective way to ensure that sensitive
information is protected and not accessible to unauthorized users.
upvoted 
13 
times
omermahgoub
omermahgoub
 
2 years ago
Option A: Hashing data using SHA256 is not sufficient for protecting sensitive information, as hashes can be reversed using
various techniques.
Option B: Encrypting data using elliptic curve cryptography is a good option for protecting data, but it requires that you have a
secure way to store and manage the encryption keys. If the keys are lost or compromised, the data will be inaccessible.
Option D: Using regular expressions to find and redact phone numbers, email addresses, and credit card numbers can be
effective in some cases, but it requires that you have a complete and up-to-date list of all the data patterns that you want to
protect. It is also prone to errors and may not be able to detect all instances of sensitive data.
upvoted 
3 
times
Kiroo
Kiroo
 
1 year, 8 months ago
About D, usually is recommended that you don´t reinvent the wheel specially when talking about security .
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
C
1. Accurate and comprehensive: The Cloud DLP API is specifically designed to identify and redact sensitive information like PII
(personally identifiable information) and payment card data. It uses advanced techniques like machine learning to accurately detect
sensitive data, even in complex and unstructured text.
2. Context-aware: 
DLP understands the context of the data. It can differentiate between a credit card number and a similar-looking
sequence of numbers that's not a credit card. This reduces false positives and ensures accurate sanitization.
3. Flexible and customizable: You can configure DLP to detect specific types of sensitive data, define your own detection rules,
and choose how to de-identify the data (e.g., redaction, masking, tokenization).
4. Scalable and efficient: DLP can handle large volumes of data and integrates seamlessly with other GCP services like Cloud
Storage and BigQuery.
upvoted 
3 
times
sim7243
sim7243
 
1 month, 3 weeks ago
Selected Answer: 
C
C option
upvoted 
1 
times
Palan
Palan
 
1 year, 3 months ago
Without any doubt C is the right answer as the DLP API is a flexible and robust tool that helps identify sensitive data like credit card
numbers, social security numbers, names and other forms of personally identifiable information (PII).
upvoted 
1 
times
shutupbot
shutupbot
 
1 year, 9 months ago
Cloud Data Loss Prevention API provides obfuscation and de-identification methods like masking and tokenization. Especially for
credit card transactions, the card numbers are supposed to be tokenized. Therefore, this API is helpful.
upvoted 
1 
times
mbrochard
mbrochard
 
2 years, 1 month ago
Selected Answer: 
C
C for sure !
upvoted 
1 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
C
C is correct, DLP is the solution
upvoted 
1 
timesmegumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
ok for C
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
haroldbenites
haroldbenites
 
3 years ago
Go for C
upvoted 
3 
times
haroldbenites
haroldbenites
 
3 years ago
https://cloud.google.com/dlp
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is C
upvoted 
1 
times
victory108
victory108
 
3 years, 7 months ago
C. De-identify the data with the Cloud Data Loss Prevention API
upvoted 
3 
times
un
un
 
3 years, 7 months ago
C is correct
upvoted 
1 
times
sidhappy
sidhappy
 
3 years, 8 months ago
Effectively reduce data risk with de-identification methods like masking and tokenization
https://cloud.google.com/dlp
upvoted 
3 
times
Ausias18
Ausias18
 
3 years, 9 months ago
Answer is C
upvoted 
1 
times
lynx256
lynx256
 
3 years, 9 months ago
C is ok
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #42
You are using Cloud Shell and need to install a custom utility for use in a few weeks. Where can you store the file so it is in the
default execution path and persists across sessions? 
A. 
~/bin 
Most Voted
B. 
Cloud Storage
C. 
/google/scripts
D. 
/usr/local/bin
Correct Answer:
 
A 
Comments
ffk
ffk
 
Highly Voted
 
5 years, 2 months ago
A 
is correct
https://cloud.google.com/shell/docs/how-cloud-shell-works
Cloud Shell provisions 5 GB of free persistent disk storage mounted as your $HOME directory on the virtual machine instance.
This storage is on a per-user basis and is available across projects. Unlike the instance itself, this storage does not time out on
inactivity. All files you store in your home directory, including installed software, scripts and user configuration files like .bashrc and
.vimrc, persist between sessions. Your $HOME directory is private to you and cannot be accessed by other users.
upvoted 
75 
times
Jambalaja
Jambalaja
 
3 years, 8 months ago
Maybe also to mention is that ~/bin is located in the $HOME directory
upvoted 
17 
times
zanfo
zanfo
 
3 years, 3 months ago
cd ~/ 
is egual at 
cd $HOME
~/bin 
is egual a 
cd $HOME/bin
the persistent disk in cloud shell is for $HOME
upvoted 
9 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Agree. A is right
Community vote distribution
A (86%)
D (14%)Agree. A is right
upvoted 
1 
times
akoti
akoti
 
4 years, 1 month ago
$HOME is not ~/bin. So 'C' is the answer.
upvoted 
1 
times
zanfo
zanfo
 
3 years, 3 months ago
cd ~/ is egual at cd $HOME
~/bin is egual a cd $HOME/bin
the persistent disk in cloud shell is for $HOME
upvoted 
10 
times
Shabje
Shabje
 
4 years, 8 months ago
Won’t the persistent disk be auto-delete enabled by default, whereby the work maybe lost. Would that not be sufficient reason to
consider Cloud storage instead. Thanks
upvoted 
2 
times
kaush
kaush
 
4 years, 6 months ago
The virtual machine instance that backs your Cloud Shell session is not permanently allocated to a Cloud Shell session and
terminates if the session is inactive for an hour. After the instance is terminated, any modifications that you made to it outside
your $HOME are lost.
upvoted 
3 
times
zanfo
zanfo
 
3 years, 3 months ago
Cloud Shell provisions 5 GB of free persistent disk storage mounted as your $HOME
upvoted 
1 
times
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
Well, I just double checked and if they were referring to the PATH variable then /usr/local/bin is also a correct
answer........................................
upvoted 
21 
times
RobertArnaud
RobertArnaud
 
Most Recent
 
3 weeks, 1 day ago
Selected Answer: 
A
If "few weeks" means less than 120 days
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
Selected Answer: 
A
1. Cloud Shell's home directory: ~/bin is a subdirectory within your Cloud Shell home directory. Files placed in this directory are
automatically added to your PATH environment variable, making them accessible from anywhere in your Cloud Shell session.
2. Persistence: Your Cloud Shell home directory is persistent across sessions. This means any files you store there, including
those in ~/bin, will remain available even if you close and reopen Cloud Shell.
3. Convenience: 
This option is the most straightforward. It doesn't require any extra configuration or interaction with other services.
Why D i snot correct? 
because 
/usr/local/bin: 
is a common directory for system-wide binaries, but in Cloud Shell, your home
directory's
upvoted 
4 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
D
/usr/local/bin is the place where the files will persist across sessions.
Hence, D.
upvoted 
2 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
A is correct!A is correct!
upvoted 
1 
times
moota
moota
 
1 year, 11 months ago
I tested this. Although ~/bin is not in the default $PATH, choice D is definitely not persisting across sessions.
upvoted 
2 
times
Flight1976
Flight1976
 
1 year, 6 months ago
1. When logging in to cloud shell for the first time, the ~/bin directory does not exist
2. mkdir ~/bin
3. After re-login to the cloud shell, $PATH will automatically add ~/bin
So A is the correct answer
upvoted 
4 
times
FI22
FI22
 
2 years ago
At this moment default directory cant be set as Cloud storage bucket, so no C.
A will be correct as zonal PD with preinstalled tools 5gb 
available that does not timeout!
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
The recommended location for storing a custom utility file that you want to use in Cloud Shell and that should be in the default
execution path and persist across sessions is option A: ~/bin.
The ~/bin directory is a personal directory that is in the default execution path for all users in Cloud Shell. Any executable files that
you place in this directory will be available to you whenever you log in to Cloud Shell, and they will persist across sessions.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
Option B: Cloud Storage is not a suitable location for storing a custom utility file that you want to use in Cloud Shell, as it is not in
the default execution path and would require additional steps to make it accessible.
Option C: The /google/scripts directory is not a suitable location for storing a custom utility file, as it is not in the default execution
path and is intended for use by Google Cloud system processes.
Option D: The /usr/local/bin directory is a system directory that is in the default execution path for all users, but it is not a suitable
location for storing a custom utility file, as any files that you place in this directory may be deleted or overwritten during system
updates.
upvoted 
3 
times
Jailbreaker
Jailbreaker
 
2 years, 1 month ago
Selected Answer: 
A
For sure correct answer is A
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
A
A. ~/bin
upvoted 
1 
times
vpatiltech
vpatiltech
 
2 years, 10 months ago
Selected Answer: 
A
Cloud Shell provisions 5 GB of persistent disk storage mounted as your $HOME directory on the Cloud Shell instance. All files you
store in your home directory, including scripts and user configuration files like .bashrc and .vimrc, persist between sessions.
Reference- https://cloud.google.com/shell/?utm_source=google&utm_medium=cpc&utm_campaign=japac-IN-all-en-dr-bkwsrmkt-
all-all-trial-e-dr-1009882&utm_content=text-ad-none-none-DEV_c-CRE_442449534611-ADGP_Hybrid%20%7C%20BKWS%20-all-all-trial-e-dr-1009882&utm_content=text-ad-none-none-DEV_c-CRE_442449534611-ADGP_Hybrid%20%7C%20BKWS%20-
%20EXA%20%7C%20Txt%20~%20Management%20Tools%20~%20Cloud%20Shell_cloud%20shell-general%20-%20Products-
KWID_43700054972141701-kwd-837034669893&userloc_9302140-
network_g&utm_term=KW_gcp%20cloud%20shell&gclsrc=ds&gclsrc=ds
upvoted 
4 
times
OrangeTiger
OrangeTiger
 
3 years ago
I think D is correct.ummm
upvoted 
2 
times
vincy2202
vincy2202
 
3 years ago
A is the correct answer
upvoted 
1 
times
exam_war
exam_war
 
3 years, 2 months ago
A is for sure. ~ stands for user's home
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #43
You want to create a private connection between your instances on Compute Engine and your on-premises data center. You
require a connection of at least 20 
Gbps. You want to follow Google-recommended practices. How should you set up the connection? 
A. 
Create a VPC and connect it to your on-premises data center using Dedicated Interconnect. 
Most Voted
B. 
Create a VPC and connect it to your on-premises data center using a single Cloud VPN.
C. 
Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises data center using Dedicated
Interconnect.
D. 
Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises datacenter using a single
Cloud VPN.
Correct Answer:
 
A 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
Cloud VPN supports unto 3 Gbps where as Interconnect can support unto 100 gbps... I'll go with A
upvoted 
45 
times
tartar
tartar
 
4 years, 5 months ago
A is ok
upvoted 
7 
times
fraloca
fraloca
 
3 years, 11 months ago
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#network-bandwidth
upvoted 
3 
times
nitinz
nitinz
 
3 years, 10 months ago
A, 20Gbps dedicated interconnect is the way.
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Community vote distribution
A (100%)AzureDP900
AzureDP900
 
2 years, 2 months ago
A is required for consistent speed and VPN not supports that speed
upvoted 
2 
times
omermahgoub
omermahgoub
 
Highly Voted
 
2 years ago
Answer is A: Dedicated Interconnect is a service that allows you to create a dedicated, high-bandwidth network connection
between your on-premises data center and Google Cloud. It is the recommended solution for creating a private connection
between your on-premises data center and Google Cloud when you require a connection of at least 20 Gbps.
Option B: Using a single Cloud VPN to connect your VPC to your on-premises data center is not suitable for a connection of at
least 20 Gbps, as Cloud VPN has a maximum capacity of 30 Gbps.
Option C: The Cloud Content Delivery Network (Cloud CDN) is a globally distributed network of caching servers that speeds up the
delivery of static and dynamic web content. It is not suitable for creating a private connection between your instances on Compute
Engine and your on-premises data center.
Option D: Connecting your Cloud CDN to your on-premises data center using a single Cloud VPN is not suitable for a connection
of at least 20 Gbps, as Cloud VPN has a maximum capacity of 30 Gbps.
upvoted 
13 
times
MJCLOUD
MJCLOUD
 
1 year, 10 months ago
Very nice answer, I think you meant 3 Gbps for Cloud VPN.
upvoted 
4 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
A
1. Dedicated Interconnect for high bandwidth: Dedicated Interconnect is designed for high-bandwidth, private connections between
your on-premises network and Google Virtual Private Cloud (VPC). It offers speeds of 10 Gbps up to 100 Gbps, meeting your
requirement of at least 20 Gbps.
2. Private and secure connection: Dedicated Interconnect provides a direct physical connection, bypassing the public internet. This
ensures a secure and private connection for your sensitive data.
3. Google-recommended practice: For demanding workloads that require high bandwidth and low latency, Google recommends
Dedicated Interconnect over VPN.
upvoted 
3 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
high bandwidth means A
upvoted 
1 
times
greyhats13
greyhats13
 
2 years ago
Selected Answer: 
A
the question mention 20gbs for the least, it should be Dedicated Interconnect. The answer is A
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
Jay_Krish
Jay_Krish
 
2 years, 4 months ago
Selected Answer: 
A
Any connection between On-Prem and GCP and requires high speed I'd choose dedicated interconnect
upvoted 
1 
times
avinashvidyarthi
avinashvidyarthi
 
2 years, 7 months ago
A is correct
upvoted 
1 
times
vincy2202
vincy2202
 
3 years agovincy2202
vincy2202
 
3 years ago
A is the correct answer
upvoted 
2 
times
haroldbenites
haroldbenites
 
3 years ago
Go for A
upvoted 
2 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
2 
times
duocnh
duocnh
 
3 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
3 
times
unnikrisb
unnikrisb
 
3 years, 2 months ago
A is good option (easily elminate C & D) 
and B with connection speed.
10Gbps per link for Dedicated Interconnect and Direct Peering
1.5-3Gbps per tunnel for Cloud VPN
50Mbps to 10Gbps per connection - Partner Interconnect
noSLA - Carrier Peering
upvoted 
1 
times
amxexam
amxexam
 
3 years, 4 months ago
Let's go with option elimination
A. Create a VPC and connect it to your on-premises data centre using Dedicated Interconnect.
>> Is the only remaining best option after elimination. As per the document, its partner interconnects with VPN. Interconnect is
between GCP and on-prem. URL1
B. Create a VPC and connect it to your on-premises data centre using a single Cloud VPN.
>> max 3 gigabits per second (Gbps) eliminate the option.
C. Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises data centre using Dedicated
Interconnect.
>> CDN is for egress traffic or static content hosting hence eliminate the option URL2
D. Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises datacenter using a single Cloud
VPN.
>> CDN is for egress traffic or static content hosting hence eliminate the option URL2
Hence A
URL1 https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview
URL2 https://cloud.google.com/network-connectivity/docs/cdn-interconnect
upvoted 
3 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is A
upvoted 
1 
times
aviratna
aviratna
 
3 years, 6 months ago
A is correct. Dedicated Interconnect supports upto 80 GBPS
upvoted 
1 
times
victory108
victory108
 
3 years, 7 months ago
A. Create a VPC and connect it to your on-premises data center using Dedicated Interconnect.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #44
You are analyzing and defining business processes to support your startup's trial usage of GCP, and you don't yet know what
consumer demand for your product will be. Your manager requires you to minimize GCP service costs and adhere to Google
best practices. What should you do? 
A. 
Utilize free tier and sustained use discounts. Provision a staff position for service cost management.
B. 
Utilize free tier and sustained use discounts. Provide training to the team about service cost management. 
Most Voted
C. 
Utilize free tier and committed use discounts. Provision a staff position for service cost management.
D. 
Utilize free tier and committed use discounts. Provide training to the team about service cost management.
Correct Answer:
 
B 
Comments
ehgm
ehgm
 
Highly Voted
 
3 years ago
Sustained are automatic discounts for running specific GCE a significant portion of the billing month:
https://cloud.google.com/compute/docs/sustained-use-discounts
Committed is for workloads with predictable resource needs between 1 year or 3 year, discount is up to 57% for most resources:
https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts
upvoted 
43 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
Adding to it. 
Provide training to the team about service cost management: Because we are still using free tier, and no need a separate
position even if that need arises. Its startup Company any way, and they are trained to work beyond hours :-)
upvoted 
2 
times
squishy_fishy
squishy_fishy
 
2 years, 11 months ago
Best answer!
upvoted 
5 
times
crypt0
crypt0
 
Highly Voted
 
5 years, 2 months ago
I would choose "B"
Community vote distribution
B (100%)I would choose "B"
upvoted 
40 
times
tartar
tartar
 
4 years, 5 months ago
B is ok
upvoted 
11 
times
nitinz
nitinz
 
3 years, 10 months ago
B reason minimize GCP service costs
upvoted 
3 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
B
1. Free Tier: Google Cloud's Free Tier allows you to use certain services for free, up to specified limits. This is ideal for a startup
testing the platform and controlling costs during the trial phase.
2. Sustained Use Discounts: These discounts are automatically applied to your bill when you use eligible services for a significant
portion of the billing month. This is beneficial for a startup that might have unpredictable usage patterns during the trial phase, as it
doesn't require any upfront commitment.
3. Training for cost management: 
Educating your team about cost management best practices empowers them to make cost-
effective decisions from the start. This includes understanding pricing models, monitoring resource usage, and optimizing
configurations.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
The recommended approach for minimizing GCP service costs and adhering to Google best practices are: 
- Free tier: Google Cloud offers a free tier of services that allows you to try out many of its products for free, up to certain usage
limits. Utilizing the free tier can help you minimize your GCP service costs while you are in the trial usage phase.
- Committed use discounts: Committed use discounts are a type of discount that you can apply to certain GCP products by
committing to a certain level of usage over a one- or three-year period. Committed use discounts can help you save on your GCP
service costs, but they require you to commit to a certain level of usage, which may not be suitable if you are unsure of your future
demand.
- Providing training to the team about service cost management: It is important that your team is aware of the different options
available for minimizing GCP service costs and understands how to manage and monitor their usage of GCP services. Providing
training on service cost management can help your team make informed decisions about how to use GCP services in the most
cost-effective way.
upvoted 
9 
times
omermahgoub
omermahgoub
 
2 years ago
The recommended approach for minimizing GCP service costs and adhering to Google best practices while your startup is in the
trial usage phase and you don't yet know what consumer demand for your product will be is option D: Utilize free tier and
committed use discounts. Provide training to the team about service cost management.
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
Sustained use discounts are based on your usage of GCP services over a certain period, and are not available for all GCP
products. 
Provisioning a staff position for service cost management may not be necessary if you provide training to the team about service
cost management.
upvoted 
2 
times
SureshbabuK
SureshbabuK
 
2 years, 1 month ago
Selected Answer: 
B
Sustained use discounts - Compute Engine - Google Cloudhttps://cloud.google.com › ... › Documentation
Compute Engine offers sustained use discounts on resources that are used for more than 25% of a billing month 
- There trial
could be more than 7 or 8 days , at this point commitment of use can not be provided due to trails stage of gcp use
upvoted 
1 
times
vranjan
vranjan
 
2 years, 1 month agoThe answer is B, because Sustained use discount can give up to 30% and requires no commitment.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
ok for B
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
committed use discounts are for long-run discounts which in the case of startup they're trying GCP. So options C and D are out
B is the correct answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
I will choose B, D is only long term commitment
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
B. Utilize free tier and sustained use discounts. Provide training to the team about service cost management.
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
2 years, 3 months ago
Sustained use discount makes sense over committed as not enough info to know what to comit to. 
Provide staff training on how to
keep things cheap gonna further keep cost down.
upvoted 
1 
times
cmamiusa
cmamiusa
 
2 years, 9 months ago
Selected Answer: 
B
B is the correct option
upvoted 
1 
times
gcmrjbr
gcmrjbr
 
2 years, 10 months ago
free tier (monthly discounts) does not make sense combined with committed use discounts - anual base, dont't you think so?
upvoted 
2 
times
vincy2202
vincy2202
 
3 years ago
B is the correct answer
upvoted 
1 
times
Israel
Israel
 
3 years, 3 months ago
Answer is B
upvoted 
1 
times
VishalB
VishalB
 
3 years, 5 months ago
Answer B
Sustained use discounts are applied on incremental use after you reach certain usage thresholds. This means that you pay only
for the number of minutes that you use an instance, and Compute Engine automatically gives you the best price. There's no
reason to run an instance for longer than you need it.
- https://cloud.google.com/compute/docs/sustained-use-discounts
Committed use discounts are ideal for workloads with predictable resource needs. When you purchase a committed use contract,
you purchase compute resource (vCPUs, memory, GPUs, and local SSDs) at a discounted price in return for committing to
paying for those resources for 1 year or 3 years. The discount is up to 57% for most resources like machine types or GPUs. The
discount is up to 70% for memory-optimized machine types. For committed use prices for different machine types, see VM
instances pricing.
- https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts
upvoted 
5 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months agoAnswer is B
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #45
You are building a continuous deployment pipeline for a project stored in a Git source repository and want to ensure that code
changes can be verified before deploying to production. What should you do? 
A. 
Use Spinnaker to deploy builds to production using the red/black deployment strategy so that changes can easily be
rolled back.
B. 
Use Spinnaker to deploy builds to production and run tests on production deployments.
C. 
Use Jenkins to build the staging branches and the master branch. Build and deploy changes to production for 10% of
users before doing a complete rollout.
D. 
Use Jenkins to monitor tags in the repository. Deploy staging tags to a staging environment for testing. After testing, tag
the repository for production and deploy that to the production environment. 
Most Voted
Correct Answer:
 
D 
Comments
Googler2
Googler2
 
Highly Voted
 
4 years, 8 months ago
I believe the best answer is D, because the tagging is a best practice that is recommended on Jenkins/Spinnaker to deploy the
right code and prevent accidentally (or intentionally) push of wrong code to production environments. See
https://stackify.com/continuous-delivery-git-jenkins/
upvoted 
59 
times
Ziegler
Ziegler
 
4 years, 7 months ago
Agreed with D as the right answer. The url provided explains it better
upvoted 
10 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
yes, D is correct
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
How can I join Google
upvoted 
6 
times
Community vote distribution
D (100%)upvoted 
6 
times
Anish17
Anish17
 
Highly Voted
 
4 years, 4 months ago
I got this question in real exam. This question states "before deploying to production" environment. So i picked D . I passed the
exam.
upvoted 
53 
times
bnlcnd
bnlcnd
 
3 years, 11 months ago
that resolved the puzzle :)
upvoted 
6 
times
GunaGCP123
GunaGCP123
 
3 years, 2 months ago
congrats for passing the exam. practising all 255 questions is sufficient for passing the exam? how much percentage of
questions you got from here roughly?
upvoted 
2 
times
zr79
zr79
 
2 years, 2 months ago
He won't answer, he already passed the exam
upvoted 
10 
times
Rzla
Rzla
 
3 years, 4 months ago
Agree, its the only answer that meets the requirement of "before deploying to production"
upvoted 
2 
times
winset
winset
 
3 years, 10 months ago
not only 1 Q passed! C is beeter
upvoted 
2 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
D
1. Clear separation of environments: Using separate staging and production environments ensures that code changes are
thoroughly tested before they reach your users. This minimizes the risk of introducing bugs or regressions into production.
2. Controlled deployments: Tagging the repository for different stages (staging, production) provides a clear and auditable way to
track which code versions are deployed in each environment. This makes it easier to roll back to a previous version if necessary.
3. Jenkins for automation: Jenkins is a powerful automation server that can monitor your Git repository for new tags, automatically
build and deploy the code to the appropriate environment, and even trigger automated tests.
4. Comprehensive testing: 
A staging environment allows you to perform comprehensive testing, including integration testing, user
acceptance testing (UAT), and performance testing, before deploying to production.
upvoted 
3 
times
sim7243
sim7243
 
1 month, 3 weeks ago
Selected Answer: 
D
D is the best choice.
upvoted 
1 
times
bkovari
bkovari
 
10 months, 2 weeks ago
ABC is about to deploy to production without prior testing. Hence D is the only reasonable choice.
upvoted 
1 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
A B and C are very risky options and irrelevant as involving production. Whereas question is to test before rolling out to production.
Hence D
upvoted 
1 
times
blackhawk86
blackhawk86
 
1 year, 3 months ago
Selected Answer: 
DNo discussion here.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
ok for D
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
2 years, 3 months ago
Given the question D is the only answer. 
Everything else pushes to production immediately.
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 9 months ago
I don't think it was a good idea when new edtion be created and directly deploy to the production ENV without any testing stage
even using Canary deployment.
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 9 months ago
D is better.
upvoted 
1 
times
Davidik79
Davidik79
 
2 years, 9 months ago
Selected Answer: 
D
The question states: "... code changes can be verified BEFORE deploying to production", it eliminates option C.
The approach of tagging is the correct practise that DevOps use
upvoted 
2 
times
[Removed]
[Removed]
 
2 years, 11 months ago
Selected Answer: 
D
Correct answer is D. Question talks about 'before deploying to production'. C talks about after deploying to production.
upvoted 
4 
times
hantanbl
hantanbl
 
2 years, 11 months ago
C is the closest answer
If question is asking 'what's Jenkin best practise' then D is the answer
upvoted 
1 
times
Davidik79
Davidik79
 
2 years, 9 months ago
C involves deploying into production. the question specifies: "BEFORE deploying to production"
upvoted 
2 
times
blackhawk86
blackhawk86
 
1 year, 3 months ago
Incorrect. D is the correct answer. 1. It should test BEFORE deploy to production. 2. It's a DevOps practice
upvoted 
1 
times
lxgywil
lxgywil
 
2 years, 12 months ago
I choose D as we want to ensure that code changes can be verified BEFORE deploying to production. Option C suggests that we
build and deploy changes to production for 10% of users.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
3 years ago
Selected Answer: 
D
Vote D.
C is canary deploy.
But the sentence in question has no word to mean "tested by a small number of users"
upvoted 
3 
times
 
3 years agoOrangeTiger
OrangeTiger
 
3 years ago
The revelal solution on this site is wrong, isn't it? I'm getting anxious.
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
D is the correct answer
upvoted 
1 
times
ABO_Doma
ABO_Doma
 
3 years ago
Selected Answer: 
D
clearly
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #46
You have an outage in your Compute Engine managed instance group: all instances keep restarting after 5 seconds. You have
a health check configured, but autoscaling is disabled. Your colleague, who is a Linux expert, offered to look into the issue. You
need to make sure that he can access the VMs. What should you do? 
A. 
Grant your colleague the IAM role of project Viewer
B. 
Perform a rolling restart on the instance group
C. 
Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys 
Most Voted
D. 
Disable autoscaling for the instance group. Add his SSH key to the project-wide SSH Keys
Correct Answer:
 
C 
Comments
Narinder
Narinder
 
Highly Voted
 
2 years, 11 months ago
C, is the correct answer. As per the requirement linux expert would need access to VM to troubleshoot the issue. With health
check enabled, old VM will be terminated as soon as health-check fails for the VM and new VM will be auto-created. So, this
situation will prevent linux expert to troubleshoot the issue. Had it been the case that stack-drover logging is enabled and the expert
just want to view the logs from the Cloud-logs than role to project-viewer could help. But it is specifically mentioned that expert will
login into VM to troubleshoot the issue and not looking at the cloud Logs. So, Option-C is the correct answer.
upvoted 
95 
times
twistyfries
twistyfries
 
2 years, 10 months ago
great answer
upvoted 
2 
times
AmitAr
AmitAr
 
2 years, 7 months ago
This looks best justification between A and C.. So, C should be correct answer.
upvoted 
2 
times
devjuliusoh
devjuliusoh
 
2 years, 4 months ago
Good explanation
upvoted 
2 
times
Community vote distribution
C (86%)
A (14%)upvoted 
2 
times
ashishdwi007
ashishdwi007
 
11 months, 2 weeks ago
The key element in C is "Disable the Health check.", so that server wont restart automatically.
But before that the actual troubleshooting step is to check Cloud console -> Instance template -> Metadata-> and see if any
startup script is there, if yes review it and possibly remove it. 
[Consider the case, a script is causing restarting the VM, (possibly
in Metadata). 
]
upvoted 
1 
times
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
C should be correct answer.
upvoted 
39 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
C
1. Access for troubleshooting: Disabling the health check prevents the VMs from constantly restarting, allowing the expert to log in
and investigate the root cause.
2. SSH access: Adding the expert's SSH key grants them the necessary access to the VMs.
3. Temporary measure: This is a temporary measure for troubleshooting. Once the issue is resolved, the health check should be
re-enabled.
upvoted 
3 
times
wilson1005
wilson1005
 
9 months, 3 weeks ago
Selected Answer: 
C
C is correct!
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
I like C
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
C
To allow your colleague, who is a Linux expert, to access the VMs and troubleshoot the issue, you should disable the health check
for the instance group. This will prevent the instance group from automatically removing and replacing unhealthy instances.
You should also add your colleague's SSH key to the project-wide SSH Keys to allow him to SSH into the instances and perform
troubleshooting. This can be done through the Google Cloud Console or the gcloud command-line tool.
upvoted 
3 
times
MestreCholas
MestreCholas
 
1 year, 10 months ago
Selected Answer: 
C
C. Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys.
Granting the IAM role of project Viewer (Option A) would allow your colleague to view the project resources but not necessarily give
them access to the specific VM instances. Performing a rolling restart on the instance group (Option B) would not resolve the
issue, as the instances keep restarting after 5 seconds. Disabling autoscaling for the instance group (Option D) is not relevant
since autoscaling is already disabled.
Disabling the health check for the instance group will prevent the managed instance group from automatically recreating the
instances. Adding your colleague's SSH key to the project-wide SSH Keys will allow them to access the VMs and troubleshoot the
issue. Once the issue is resolved, you can re-enable the health check for the instance group.
upvoted 
5 
times
urssiva
urssiva
 
1 year, 11 months ago
Selected Answer: 
C
C is the answer
upvoted 
1 
times
roaming_panda
roaming_panda
 
2 years agoC as the machines will keep restarting if hc is not disabled , and then our expert can look around for RCA
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
To allow your colleague access the instances in MIG you should do option D: Disable autoscaling for the instance group. Add his
SSH key to the project-wide SSH Keys.
Disabling autoscaling for the instance group will prevent new instances from being created or terminated while your colleague is
trying to troubleshoot the issue. This will give him a stable environment to work in and will ensure that he can access the instances
that are currently in the instance group.
Adding his SSH key to the project-wide SSH Keys will allow him to connect to the instances using Secure Shell (SSH) without
requiring a password. This is a convenient way to give him access to the instances and can help him troubleshoot the issue more
efficiently.
upvoted 
2 
times
n_nana
n_nana
 
1 year, 11 months ago
autoscaling is already disabled. Answer D is not make sense here. i voted it wrongly instead of reply.
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
Option A: Granting your colleague the IAM role of project Viewer will not allow him to access the instances in the managed
instance group. The project Viewer role does not include any permissions to access Compute Engine resources.
Option B: Performing a rolling restart on the instance group will not solve the issue and may even make it worse if the instances
are not able to start up properly.
Option C: Disabling the health check for the instance group will not solve the issue and may even make it worse if the instances
are not able to start up properly. Adding his SSH key to the project-wide SSH Keys will allow him to access the instances, but it is
not a sufficient solution on its own.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
ok for C
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
C. Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys. This will allow the engineer to
logon to the VM's and check the logs. Disabling health checks will prevent rebooting of the VM's.
upvoted 
2 
times
holerina
holerina
 
2 years, 3 months ago
C should be the correct answer
upvoted 
1 
times
abirroy
abirroy
 
2 years, 4 months ago
Selected Answer: 
C
Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys
upvoted 
1 
times
backhand
backhand
 
2 years, 4 months ago
vote C
what can project viewer do without access vm by linux expert?
upvoted 
1 
timestricky_learner
tricky_learner
 
2 years, 5 months ago
Selected Answer: 
C
I believe that answer C is correct.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #47
Your company is migrating its on-premises data center into the cloud. As part of the migration, you want to integrate Google
Kubernetes Engine (GKE) for workload orchestration. Parts of your architecture must also be PCI DSS-compliant. Which of the
following is most accurate? 
A. 
App Engine is the only compute platform on GCP that is certified for PCI DSS hosting.
B. 
GKE cannot be used under PCI DSS because it is considered shared hosting.
C. 
GKE and GCP provide the tools you need to build a PCI DSS-compliant environment. 
Most Voted
D. 
All Google Cloud services are usable because Google Cloud Platform is certified PCI-compliant.
Correct Answer:
 
C 
Comments
rishab86
rishab86
 
Highly Voted
 
3 years, 7 months ago
Link : https://cloud.google.com/security/compliance/pci-dss 
Clearly mention GKE as PCI DSS-Compliant but not all GCP service are PCI DSS-Compliant so answer is definitely C.
upvoted 
47 
times
Mikado211
Mikado211
 
2 years, 5 months ago
In 2022, GCP is now fully PCI-DSS compliant, so technically D is perfectly true.
But you still have to check that your application is PCI-DSS compliant.
so C is still the best answer.
upvoted 
8 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
C – Kubernetes Engine provides tools you need to build to PCI-DSS compliant environment.
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
But, The paragraph 3 says that all products of google are certified by PCI.
upvoted 
2 
times
Community vote distribution
C (100%)upvoted 
2 
times
aviratna
aviratna
 
Highly Voted
 
3 years, 6 months ago
C: GKE & Compute Engine is PCI DSS compliant while Cloud Function, App Engine are not PC compliant
upvoted 
5 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
C
1. GKE and PCI DSS: While GKE itself isn't inherently PCI DSS compliant, it provides the infrastructure and tools you need to build
a compliant environment. You'll need to configure it correctly, implement security measures, and follow best practices.
2. Shared Responsibility Model: Google Cloud Platform operates under a shared responsibility model. Google is responsible for
securing the underlying infrastructure, while you are responsible for securing your applications and data within that environment.   
3. Flexibility for Compliance: GKE offers features like private clusters, network policies, and integration with security tools that help
you meet PCI DSS requirements.
upvoted 
3 
times
eka_nostra
eka_nostra
 
1 year, 5 months ago
Selected Answer: 
C
We still have to configure our env to comply with PCI/DSS. https://cloud.google.com/architecture/pci-dss-compliance-in-
gcp#kubernetes_engine
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
The most accurate statement is option C: GKE and GCP provide the tools you need to build a PCI DSS-compliant environment.
Google Kubernetes Engine (GKE) is a fully managed service that allows you to deploy and manage containerized applications on
Google Cloud. It is not specifically certified for PCI DSS hosting, but it can be used as part of a PCI DSS-compliant environment if
the necessary controls and safeguards are in place.
Google Cloud Platform (GCP) provides a range of tools and services that can be used to build a PCI DSS-compliant environment,
including Cloud Identity and Access Management (IAM) for controlling access to resources, Cloud Key Management Service
(KMS) for managing encryption keys, and Cloud Security Command Center for monitoring and detecting security threats.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
Option A: App Engine is a fully managed platform for building and deploying web and mobile applications, but it is not the only
compute platform on GCP that is certified for PCI DSS hosting. Other compute platforms such as Compute Engine and Google
Kubernetes Engine can also be used as part of a PCI DSS-compliant environment.
Option B: GKE is not considered shared hosting and can be used as part of a PCI DSS-compliant environment if the necessary
controls and safeguards are in place.
Option D: While Google Cloud Platform is certified PCI-compliant, not all of its services are automatically usable in a PCI DSS-
compliant environment. It is up to the user to ensure that they are using the appropriate controls and safeguards to meet the
requirements of the PCI DSS.
upvoted 
3 
times
abirroy
abirroy
 
2 years, 4 months ago
Selected Answer: 
C
C is the right answer
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 10 months ago
Selected Answer: 
C
I got similar question on my exam.
upvoted 
3 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
timesupvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for C.
upvoted 
1 
times
SHOURYA_SOOD
SHOURYA_SOOD
 
3 years, 1 month ago
Selected Answer: 
C
C- All of them: GKE, GCE, and GAE ate PCI-DSS-Compliant but A & B says it's only GAE and GCE respectively so cancel them
out.
D says all of GCP is PCI DSS-Compliant but it's not true.
So, C seems to be the right answer.
upvoted 
1 
times
imranmani
imranmani
 
3 years, 2 months ago
C is the right answer
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is C
upvoted 
3 
times
victory108
victory108
 
3 years, 6 months ago
C. GKE and GCP provide the tools you need to build a PCI DSS-compliant environment.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #48
Your company has multiple on-premises systems that serve as sources for reporting. The data has not been maintained well
and has become degraded over time. 
You want to use Google-recommended practices to detect anomalies in your company data. What should you do? 
A. 
Upload your files into Cloud Storage. Use Cloud Datalab to explore and clean your data.
B. 
Upload your files into Cloud Storage. Use Cloud Dataprep to explore and clean your data. 
Most Voted
C. 
Connect Cloud Datalab to your on-premises systems. Use Cloud Datalab to explore and clean your data.
D. 
Connect Cloud Dataprep to your on-premises systems. Use Cloud Dataprep to explore and clean your data.
Correct Answer:
 
B 
Comments
JohnWick2020
JohnWick2020
 
Highly Voted
 
3 years, 8 months ago
Answer is B:
Keynotes from question:
1- On-premise data sources
2- Unfit data; not well maintained and degraded
3- Google-recommended best practice to "detect anomalies" <<-Very important.
Explanation:
A & C - incorrect; Datalab does not provide anomaly detection OOTB. It is used more for data science scenarios like interactive
data analysis and build ML models. 
B - CORRECT; DataPrep OOTB provides for fast exploration and anomaly detection and lists cloud storage as an ingestion
medium. Refer to ELT pipeline architecture here = https://cloud.google.com/dataprep
D - incorrect; At this time DataPrep cannot connect to SaaS or on-premise source. Not to be confused for DataFlow which can!
upvoted 
60 
times
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
Both B and D work, because the question says "Google's Best Practices" uploading the files first would keep the original copies
Google encrypted and stored.
upvoted 
12 
times
skywalker
skywalker
 
4 years, 7 months ago
Community vote distribution
B (100%)skywalker
skywalker
 
4 years, 7 months ago
Both of them works....
upvoted 
1 
times
Musk
Musk
 
4 years, 5 months ago
You can't connect DataPrep to your on-prem systems. You simply upload a file, but that is not connecting it to your systems.
Because of that, I'd discard D and stay with B.
upvoted 
9 
times
tartar
tartar
 
4 years, 5 months ago
B is ok
upvoted 
9 
times
nitinz
nitinz
 
3 years, 10 months ago
B, dataprep = visually explore, clean, and prepare data for analysis
upvoted 
7 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
B is better choice
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
B
Why Cloud Storage is important ?
1. Centralized repository: Cloud Storage provides a secure and scalable place to store your data. This makes it accessible to
various GCP services.
2. Data lake concept: This aligns with the idea of a data lake, where you bring raw data into a central location before processing
and refining it.
Why Cloud Dataprep is a good fit ?
1. Visual data exploration: Dataprep excels at helping you quickly understand your data through visualizations and profiling. This is
crucial for identifying anomalies.
2. Data cleaning and transformation: Dataprep makes it easy to clean and standardize your data, which is essential before
anomaly detection. Inconsistent formats, missing values, and errors can skew your analysis.
3. Built-in anomaly detection: Dataprep has features specifically designed to help you find anomalies. It can highlight unusual
values, outliers, and patterns.
upvoted 
3 
times
snehaso
snehaso
 
4 months, 3 weeks ago
Datalab was shutdown. Its replacement is vertex AI. Read question accordingly
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Cloud Datalab is a powerful interactive tool created to explore, analyze, transform, and visualize data and build machine learning
models on Google Cloud Platform.
Dataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data
for analysis, reporting, and machine learning.
Dataprep do not have an integration for on-prem: https://console.cloud.google.com/marketplace/product/endpoints/cloud-dataprep-
editions-v2?project=fast-art-401415
So, clearly, the only option left is B.
upvoted 
4 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
B is correct.
upvoted 
1 
times
n_nana
n_nana
 
1 year, 10 months ago
Today, data ingestion to DataPrep can be Application, file upload, database.
so B is also now valid
upvoted 
1 
timesupvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
The recommended approach for detecting anomalies in your company data using Google-recommended practices is option B:
Upload your files into Cloud Storage. Use Cloud Dataprep to explore and clean your data.
Cloud Storage is a highly scalable, durable, and secure object storage service that can be used to store and retrieve data from
anywhere on the web. You can use Cloud Storage to store your company data files and make them available for analysis.
Cloud Dataprep is a fully managed data preparation service that allows you to quickly and easily explore, clean, and transform your
data for analysis. It can help you detect anomalies in your data by providing features such as data profiling, data cleansing, and
data transformation.
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
Option A: Using Cloud Datalab to explore and clean your data is not a recommended approach, as Cloud Datalab is a
collaborative data exploration and visualization platform that is not specifically designed for data preparation tasks such as
cleansing and transformation.
Option C: Connecting Cloud Datalab to your on-premises systems is not a recommended approach, as Cloud Datalab is a
collaborative data exploration and visualization platform and is not designed for data preparation tasks such as cleansing and
transformation.
Option D: Connecting Cloud Dataprep to your on-premises systems is not necessary, as you can use Cloud Dataprep to explore
and clean data stored in Cloud Storage.
upvoted 
1 
times
allen_y_q_huang
allen_y_q_huang
 
2 years ago
ok for B & D, but B is suitable to gcp
upvoted 
1 
times
Smaks
Smaks
 
2 years, 1 month ago
Selected Answer: 
B
Datalab is deprecated : https://cloud.google.com/datalab/docs
New Cloud Dataprep options will give connectivity to relational databases, business applications and extend our integrations
across Google Cloud with Google Sheets: https://www.trifacta.com/blog/cloud-dataprep-trifacta/
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
ok for B
upvoted 
1 
times
Cloudexplorer
Cloudexplorer
 
2 years, 5 months ago
Could anyone provide a link where it explicitly says that Datprep does not connect to on-premises data sources. 
In the ingestion layer on the diagram at https://cloud.google.com/dataprep it shows databases as a source. 
I can't see anywhere that there is a limitation connecting to on-premises. Would be great if someone could share that.
upvoted 
3 
times
BigSteveO
BigSteveO
 
2 years, 6 months ago
Selected Answer: 
B
It's gotta be B.
upvoted 
1 
times
Dhiraj03
Dhiraj03
 
2 years, 6 months ago
Keyword : Anamolies Data prep is the only product ... So options A and C is eliminated ... Cost effective is storing the data in GCS
Cloud storage ... So option is B
upvoted 
1 
times
nkit
nkit
 
2 years, 8 months ago
Selected Answer: 
BDataprep to detect anomalies in Data is the right choice.
upvoted 
1 
times
GMats
GMats
 
2 years, 11 months ago
B...It supports only CloudStorage and Bigquery..."So you can start transforming datasets, you hereby instruct Google to allow
Trifacta, who provides the service Dataprep in collaboration with Google, to view and modify project data in Cloud Storage and
BigQuery, run Dataflow jobs, and use all project service accounts."
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for B.
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
The question says “best practice”. In GCP , a best practice for many use cases is load to cloud storage and then processing
data.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #49
Google Cloud Platform resources are managed hierarchically using organization, folders, and projects. When Cloud Identity and
Access Management (IAM) policies exist at these different levels, what is the effective policy at a particular node of the
hierarchy? 
A. 
The effective policy is determined only by the policy set at the node
B. 
The effective policy is the policy set at the node and restricted by the policies of its ancestors
C. 
The effective policy is the union of the policy set at the node and policies inherited from its ancestors 
Most Voted
D. 
The effective policy is the intersection of the policy set at the node and policies inherited from its ancestors
Correct Answer:
 
C 
Comments
passnow
passnow
 
Highly Voted
 
5 years ago
The effective policy for a resource is the union of the policy set at that resource and the policy inherited from its
parent.https://cloud.google.com/iam/docs/resource-hierarchy-access-control
upvoted 
32 
times
ghadxx
ghadxx
 
Highly Voted
 
2 years, 11 months ago
You can set IAM policies at the level of the node, in addition to policies inherited from its parent. Hence, it is a union.
upvoted 
13 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
C
Here's how IAM policies work in GCP's hierarchical structure:
1. Hierarchy: 
GCP resources are organized in a hierarchy:
- Organization: The root node representing your company.
- Folders: Used to organize projects within the organization.
- Projects: Containers for your resources (VMs, databases, etc.).
2. Inheritance: IAM policies are inherited down the hierarchy. This means a policy set at the Organization level applies to all folders
and projects within it.
3. Union of Policies: 
When you have policies at different levels, the effective policy at a particular node (e.g., a project) is the
combination (union) of:
- The policy set directly at that node.
Community vote distribution
C (100%)- The policy set directly at that node.
- All the policies inherited from its parent folder and the organization.
Example: If a user has "Viewer" access at the Organization level and "Editor" access at the Project level, their effective permission
on that project is "Editor" (the higher permission).
upvoted 
2 
times
Di4sa
Di4sa
 
1 year, 4 months ago
Selected Answer: 
C
From google doc: Google Cloud resources are organized hierarchically, where the organization node is the root node in the
hierarchy, the projects are the children of the organization, and the other resources are descendants of projects. You can set allow
policies at different levels of the resource hierarchy. Resources inherit the allow policies of the parent resource. The effective allow
policy for a resource is the union of the allow policy set at that resource and the allow policy inherited from its parent.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
The effective policy at a particular node in the resource hierarchy in GCP is determined by the intersection of the policy set at the
node and policies inherited from its ancestors, as described in option D
Cloud IAM policies in GCP are hierarchical, meaning that policies set at higher levels of the resource hierarchy can be inherited by
lower levels. When a user or service account attempts to access a resource, the effective policy at that resource is determined by
evaluating the policies set at the resource itself and all of its ancestors in the hierarchy. If any of the policies deny access, the user
or service account will be denied access.
For example, consider the following resource hierarchy:
Organization => Folder => Project => Compute Engine instance
If an IAM policy is set at the organization level that allows read access to all Compute Engine instances, and a policy is set at the
project level that denies read access to a specific Compute Engine instance, the effective policy for that instance will be the
intersection of the two policies, which will be to deny read access to the instance.
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
Option A: The effective policy is not determined only by the policy set at the node, as policies set at higher levels in the hierarchy
can also have an impact on the effective policy.
Option B: The effective policy is not restricted by the policies of its ancestors, as the policies of its ancestors can also be
included in the effective policy if they allow access.
Option C: The effective policy is not the union of the policy set at the node and policies inherited from its ancestors, as the
intersection of the policies is used to determine the effective policy.
upvoted 
1 
times
habros
habros
 
2 years, 1 month ago
Selected Answer: 
C
C. Is a skewed wording question. Cannot be comprehended right away.
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
ok for C
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
C is correct answer
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
English as a second language will struggle here. Good luck to us
upvoted 
5 
times
BiddlyBdoyng
BiddlyBdoyng
 
2 years, 3 months agoA: Would mean polcies set at the project or higher meant nothing, this is obviously wrong
B: would mean you could not grant a permissions to a single VM, it would need to be at project or above (you restrict by not giving
the permission)
C : The permission is the sum of all the permissions you are given through the hierarchy, this is correct, you cannot restrict once it
is given at a higher level.
D: Would mean you would need the permission set at ancestor and the node, this would mean to get access to a single VM you
would need to be given access to all VMs at the project level.
upvoted 
3 
times
holerina
holerina
 
2 years, 3 months ago
C is correct answer as it inheritance is the basic model of IAM
upvoted 
2 
times
avinashvidyarthi
avinashvidyarthi
 
2 years, 7 months ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
Atnafu
Atnafu
 
3 years ago
C
Google Cloud resources are organized hierarchically, where the organization node is the root node in the hierarchy, the projects
are the children of the organization, and the other resources are descendants of projects. You can set Identity and Access
Management (IAM) policies at different levels of the resource hierarchy. Resources inherit the policies of the parent resource. The
effective policy for a resource is the union of the policy set at that resource and the policy inherited from its parent.
upvoted 
3 
times
vincy2202
vincy2202
 
3 years ago
C is the correct answer
upvoted 
2 
times
haroldbenites
haroldbenites
 
3 years ago
Go for C.
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is C
upvoted 
3 
times
victory108
victory108
 
3 years, 7 months ago
C. The effective policy is the union of the policy set at the node and policies inherited from its ancestors
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #50
You are migrating your on-premises solution to Google Cloud in several phases. You will use Cloud VPN to maintain a
connection between your on-premises systems and Google Cloud until the migration is completed. You want to make sure all
your on-premise systems remain reachable during this period. How should you organize your networking in Google Cloud? 
A. 
Use the same IP range on Google Cloud as you use on-premises
B. 
Use the same IP range on Google Cloud as you use on-premises for your primary IP range and use a secondary range
that does not overlap with the range you use on-premises
C. 
Use an IP range on Google Cloud that does not overlap with the range you use on-premises 
Most Voted
D. 
Use an IP range on Google Cloud that does not overlap with the range you use on-premises for your primary IP range and
use a secondary range with the same IP range as you use on-premises
Correct Answer:
 
C 
Comments
newbie2020
newbie2020
 
Highly Voted
 
4 years, 11 months ago
Ans is C, 
https://cloud.google.com/vpc/docs/using-vpc
"Primary and secondary ranges can't conflict with on-premises IP ranges if you have connected your VPC network to another
network with Cloud VPN, Dedicated Interconnect, or Partner Interconnect."
upvoted 
131 
times
Smart
Smart
 
4 years, 10 months ago
Agreed!
upvoted 
2 
times
AD2AD4
AD2AD4
 
4 years, 7 months ago
Perfect.. Exact find in link.
upvoted 
2 
times
elaineshi
elaineshi
 
2 years, 7 months ago
Community vote distribution
C (83%)
B (17%)agree, any ip range, shall use filewall rule to communicate, instead of setting same IP range, which is a mess to control.
upvoted 
2 
times
Sundeepk
Sundeepk
 
4 years, 6 months ago
from the above link - it clearly states - "Primary and secondary ranges for subnets cannot overlap with any allocated range, any
primary or secondary range of another subnet in the same network, or any IP ranges of subnets in peered networks." once we
create a VPN, they all are part of the same network . Hence option C is correct
upvoted 
13 
times
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
I think C is correct.
upvoted 
21 
times
JoeShmoe
JoeShmoe
 
5 years, 1 month ago
Agree with C. Secondary IP range still can't overlap
upvoted 
10 
times
AWS56
AWS56
 
4 years, 11 months ago
".... and Google Cloud until the migration is completed." Taking this as the key, the intention is to remove the connection b/w on-
prem and GCP once the migration is done. and then the secondary IPs will act as primary. So I will choose D
upvoted 
3 
times
tartar
tartar
 
4 years, 5 months ago
C is ok
upvoted 
10 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
B, The key points here:
- migrating in several phases
- use Cloud VPN until the migration is completed
- all your on-premise systems remain reachable during this period
upvoted 
2 
times
zanfo
zanfo
 
3 years, 3 months ago
how to manage the routing table in VPC if is present a subnet with the same network of vpn remote net? the correct answer is
C
upvoted 
1 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Yes C it is
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
C, no brainer. You have on-prem <--> VPN <---> GCP only way this data flow to work in non-over lapping subnets. You can
stretch subnets at layer 7 but you wont be able to route it via VPN.
upvoted 
4 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
C
1. IP Address Conflicts: When you have overlapping IP ranges between your on-premises network and your Google Cloud
network, you'll run into routing conflicts. Devices won't know where to send traffic, leading to connectivity problems and
unreachable systems.
2. Cloud VPN and Routing: Cloud VPN creates a secure tunnel between your on-premises network and your Google Cloud Virtual
Private Cloud (VPC). 
To ensure proper routing, each side of the connection needs to have distinct, non-overlapping IP address
spaces.
3. Best Practice for Hybrid Networks: Using different IP ranges is a standard best practice for hybrid cloud setups. It prevents
ambiguity and ensures that traffic flows correctly between your on-premises and cloud environments.ambiguity and ensures that traffic flows correctly between your on-premises and cloud environments.
upvoted 
3 
times
gracjanborowiak
gracjanborowiak
 
5 months, 3 weeks ago
Selected Answer: 
B
question is tricky. as network architect knowing gcp i have exp that you can use non-overlapping secondary ranges for vpn as well.
in many migrations it is not possible to make new addressing hence you need to make them overlapping. this is why 2nd ranges
are so useful. 
B is better choice. more realistic and possible in gcp. 
from overall perspective i 
agree to have non-overlapping but do not forget this is migration and you need to have full connectivity all
the time. it is also not mentioning about what ips should be used
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
5 months ago
When migrating to the cloud, best practices for IP schema generally involve avoiding duplicate IP addresses and keeping cloud
and on-premise IP ranges separate
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
C is correct
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
B
Using an IP range on Google Cloud that does not overlap with the range used on-premises (option C) is a good choice to avoid IP
address conflicts. However, it is important to use the same IP range as the on-premises applications for the primary IP range to
ensure that the on-premises systems remain accessible. Therefore, using the same IP range on Google Cloud as on-premises for
the primary IP range and using a secondary range that does not overlap with the range used on-premises can avoid IP address
duplication and ensure that the on-premises systems remain accessible. Hence, option B is the better choice.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
The recommended approach for organizing your networking in Google Cloud to ensure that all your on-premises systems remain
reachable during the migration is option C: Use an IP range on Google Cloud that does not overlap with the range you use on-
premises.
When using Cloud VPN to establish a connection between your on-premises systems and Google Cloud, it is important to ensure
that the IP ranges used in your on-premises systems and Google Cloud do not overlap. If the IP ranges overlap, it can cause
conflicts and make it difficult to route traffic between your on-premises systems and Google Cloud.
To avoid IP range conflicts, you should use an IP range on Google Cloud that is different from the range you use on-premises. This
will ensure that all your on-premises systems remain reachable during the migration.
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
Option A: Using the same IP range on Google Cloud as you use on-premises is not a recommended approach, as it can cause
IP range conflicts and make it difficult to route traffic between your on-premises systems and Google Cloud.
Option B: Using the same IP range on Google Cloud as you use on-premises for your primary IP range and a secondary range
that does not overlap with the range you use on-premises is not a recommended approach, as it can still cause IP range conflicts
and make it difficult to route traffic between your on-premises systems and Google Cloud.
Option D: Using an IP range on Google Cloud that does not overlap with the range you use on-premises for your primary
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
ok for C
upvoted 
1 
timeszr79
zr79
 
2 years, 2 months ago
no overlapping
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C. Use an IP range on Google Cloud that does not overlap with the range you use on-premises
upvoted 
1 
times
marksie1988
marksie1988
 
2 years, 4 months ago
Selected Answer: 
C
C, IP should never overlap if avoidable. double nat is nasty
upvoted 
1 
times
ZLT
ZLT
 
2 years, 6 months ago
Selected Answer: 
C
The correct answer is C
upvoted 
2 
times
Barry123456
Barry123456
 
2 years, 6 months ago
Selected Answer: 
C
C
Why would you ever create an IP overlap?
upvoted 
1 
times
jonty4gcp
jonty4gcp
 
2 years, 8 months ago
Selected Answer: 
C
Answer is C
upvoted 
1 
times
Davidik79
Davidik79
 
2 years, 9 months ago
Selected Answer: 
C
From here: https://cloud.google.com/vpc/docs/create-modify-vpc-networks
"Primary and secondary ranges can't conflict with on-premises IP ranges if you have connected your VPC network to another
network with 
Cloud VPN, Dedicated Interconnect, or Partner Interconnect."
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 10 months ago
Selected Answer: 
C
I got similar question on my exam.
upvoted 
3 
times
Sreedharveluru
Sreedharveluru
 
2 years, 11 months ago
ANS - C
Primary and secondary ranges for subnets cannot overlap with any allocated range, any primary or secondary range of another
subnet in the same network, or any IPv4 ranges of subnets in peered networks.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #51
You have found an error in your App Engine application caused by missing Cloud Datastore indexes. You have created a YAML
file with the required indexes and want to deploy these new indexes to Cloud Datastore. What should you do? 
A. 
Point gcloud datastore create-indexes to your configuration file 
Most Voted
B. 
Upload the configuration file to App Engine's default Cloud Storage bucket, and have App Engine detect the new indexes
C. 
In the GCP Console, use Datastore Admin to delete the current indexes and upload the new configuration file
D. 
Create an HTTP request to the built-in python module to send the index configuration file to your application
Correct Answer:
 
A 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
Correct A, you have to recreate the indexes
upvoted 
31 
times
nitinz
nitinz
 
3 years, 10 months ago
A, if index is missing then create it.
upvoted 
4 
times
tartar
tartar
 
4 years, 5 months ago
A is ok
upvoted 
11 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Yes, use this command in cloud sell to create indexes. 
gcloud datastore create indexes
upvoted 
4 
times
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
A is incorrect because the command is actually gcloud datastore indexes create.
(https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create).
Community vote distribution
A (100%)(https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create).
upvoted 
17 
times
bogd
bogd
 
3 years, 10 months ago
It might have changed recently - I was able to find documentation mentioning "datastore create-indexes":
https://cloud.google.com/appengine/docs/standard/python/datastore/indexes
upvoted 
15 
times
sim7243
sim7243
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
A
A, option
upvoted 
1 
times
james2033
james2033
 
7 months, 1 week ago
Selected Answer: 
A
To deploy new Cloud Datastore indexes using the YAML file you created, you should use the `gcloud` command-line tool.
Specifically, the correct option is:
**A. Point gcloud datastore create-indexes to your configuration file**
Here’s how you can do it:
1. **Ensure you have the `gcloud` CLI installed**: You need the Google Cloud SDK installed and set up on your local machine. If
you haven't done this yet, follow the [installation guide](https://cloud.google.com/sdk/docs/install).
2. **Navigate to the directory containing your `index.yaml` file**: This file contains the definitions of the indexes you want to deploy.
3. **Run the following command**:
```bash
gcloud datastore indexes create index.yaml
```
This command will deploy the indexes defined in the `index.yaml` file to Cloud Datastore.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
A is correct: https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create
upvoted 
3 
times
jlambdan
jlambdan
 
1 year, 9 months ago
A)
https://cloud.google.com/datastore/docs/tools/indexconfig#Datastore_Updating_indexes
upvoted 
2 
times
MestreCholas
MestreCholas
 
1 year, 10 months ago
Selected Answer: 
A
A. Point gcloud datastore create-indexes to your configuration file.
To deploy new indexes to Cloud Datastore, you can use the gcloud datastore create-indexes command and point it to the YAML
configuration file containing the required indexes. This command will create the new indexes in Cloud Datastore for your
application.
Option B is not correct because App Engine does not automatically detect and create indexes from uploaded configuration files in
Cloud Storage.
Option C is also not correct because deleting current indexes in Datastore Admin is not necessary to upload new indexes.
Option D is not correct because there is no built-in Python module that can send the index configuration file to your application
upvoted 
11 
times
omermahgoub
omermahgoub
 
2 years ago
To deploy new indexes to Cloud Datastore, you should use the gcloud datastore create-indexes command and point it to your
configuration file. The correct option is therefore A: Point gcloud datastore create-indexes to your configuration file.Here's the general format of the gcloud datastore create-indexes command:
Copy code
gcloud datastore create-indexes [FILE]
Where [FILE] is the path to your configuration file. The configuration file should be in YAML format and contain a list of indexes that
you want to create.
For example:
Copy code
gcloud datastore create-indexes index.yaml
This command will create the indexes specified in the index.yaml file in Cloud Datastore.
upvoted 
7 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A is correct answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A is correct
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
A
gcloud app deploy index.yaml
upvoted 
2 
times
AMEJack
AMEJack
 
2 years, 2 months ago
Answer is A.
Option C: no create index from configuration file in the firestore console.
upvoted 
1 
times
Najmuddoja
Najmuddoja
 
2 years, 3 months ago
Is A still the valid answer, many other website resources saying C is the right answer
upvoted 
1 
times
holerina
holerina
 
2 years, 3 months ago
A looks like more logical answer
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 9 months ago
A is incorrect.
There is no correct answer, in fact. 
In 2022, the gcloud parameter "create-index" doesn't exist anymore. However, that was in
2017*. Today, the right CLI command** should be gcloud datastore "indexes create".
Evidences: 
* 
https://stackoverflow.com/questions/43041126/how-to-create-datastore-composite-indexes-with-just-cloud-functions)
** https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create
upvoted 
7 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #52
You have an application that will run on Compute Engine. You need to design an architecture that takes into account a disaster
recovery plan that requires your application to fail over to another region in case of a regional outage. What should you do? 
A. 
Deploy the application on two Compute Engine instances in the same project but in a different region. Use the first
instance to serve traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster.
B. 
Deploy the application on a Compute Engine instance. Use the instance to serve traffic, and use the HTTP load balancing
service to fail over to an instance on your premises in case of a disaster.
C. 
Deploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use
the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group
in case of a disaster. 
Most Voted
D. 
Deploy the application on two Compute Engine instance groups, each in a separate project and a different region. Use
the first instance group to serve traffic, 
and use the HTTP load balancing service to fail over to the standby instance group
in case of a disaster.
Correct Answer:
 
C 
Comments
Eroc
Eroc
 
Highly Voted
 
4 years, 8 months ago
Groups are better for management that non-groups so A and B are eliminated. Keeping the the instances in the same project will
help maintain consistency, so C is better than D.
upvoted 
38 
times
nitinz
nitinz
 
3 years, 4 months ago
C, because external LB needs **IG** period. It can either be managed or un-managed. You can not do External HTTP LB on
instances. Also External HHTP LB is a Regional resource.
upvoted 
14 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 1 month ago
make sense.
upvoted 
1 
times
Community vote distribution
C (100%)upvoted 
1 
times
gigibit
gigibit
 
Highly Voted
 
2 years, 9 months ago
Yes, but why not choose a cost-effective solution like A, preferring a not required performance optimization solution like C? The
question it's just asking for a simple fail over
upvoted 
13 
times
Terryhsieh
Terryhsieh
 
6 months, 1 week ago
Use two groups, each group contain one instance would be more flixible than answer A.
upvoted 
2 
times
hellosam
hellosam
 
11 months, 1 week ago
LB does not work without MIG
upvoted 
4 
times
todos213
todos213
 
2 years, 3 months ago
In real word situation, I'd choose A for a customer. The question doesn't mention performance to be enhanced or scalability.
upvoted 
6 
times
yilexar
yilexar
 
Most Recent
 
8 months, 3 weeks ago
Now you can use regional load balancer to route traffic to instance groups in different GCP projects. Welcome to cloud :-)
https://cloud.google.com/blog/products/networking/cloud-load-balancing-gets-cross-project-service-referencing
upvoted 
6 
times
heretolearnazure
heretolearnazure
 
10 months, 2 weeks ago
C is correct
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 3 months ago
To set up a load balancer with a Compute Engine backend, your VMs need to be in an instance group. The managed instance
group provides VMs running the backend servers of an external HTTP load balancer
Therefore C
upvoted 
6 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 1 month ago
good catch .
upvoted 
2 
times
BeCalm
BeCalm
 
1 year, 3 months ago
Why can't this be A since there is no mention of scaling?
upvoted 
1 
times
hellosam
hellosam
 
11 months, 1 week ago
LB does not work without MIG
upvoted 
1 
times
axle818
axle818
 
7 months ago
You can use either MIG or unmanaged..
https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances
upvoted 
2 
times
n_nana
n_nana
 
1 year, 5 months ago
Selected Answer: 
C
Google recommend using MIG for Zonal outage and multiple MIG for regional outage
https://cloud.google.com/architecture/disaster-recovery#compute-engine
sentence says:
Compute Engine instances are zonal resources, so in the event of a zone outage instances are unavailable by default. Compute
Engine does offer managed instance groups (MIGs) which can automatically scale up additional VMs from pre-configured instanceEngine does offer managed instance groups (MIGs) which can automatically scale up additional VMs from pre-configured instance
templates, both within a single zone and across multiple zones within a region. MIGs are ideal for applications that require
resilience to zone loss and are stateless, but require configuration and resource planning. Multiple regional MIGs can be used to
achieve region outage resilience for stateless applications.
upvoted 
8 
times
roaming_panda
roaming_panda
 
1 year, 6 months ago
Selected Answer: 
C
MIG , in 1 project
upvoted 
1 
times
sameer2803
sameer2803
 
1 year, 6 months ago
the only solution that they want is to address regional outage. scalability or performance is not a concern at this point. MIG is better
but that not what is solving the regional outage.
upvoted 
1 
times
n_nana
n_nana
 
1 year, 5 months ago
According to this link, MIG is not only for performance and scalability. It is also for reliability
https://cloud.google.com/architecture/disaster-recovery#compute-engine
sentence says: 
Compute Engine instances are zonal resources, so in the event of a zone outage instances are unavailable by default. Compute
Engine does offer managed instance groups (MIGs) which can automatically scale up additional VMs from pre-configured
instance templates, both within a single zone and across multiple zones within a region. MIGs are ideal for applications that
require resilience to zone loss and are stateless, but require configuration and resource planning. Multiple regional MIGs can be
used to achieve region outage resilience for stateless applications.
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The correct answer is D: Deploy the application on two Compute Engine instance groups, each in a separate project and a
different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby
instance group in case of a disaster.
To implement a disaster recovery plan that requires your application to fail over to another region in case of a regional outage, you
should deploy the application on two Compute Engine instance groups, each in a separate project and a different region. This will
ensure that the application is running in at least two regions, so that if one region experiences an outage, the application can still be
accessed from the other region.
You can use the HTTP load balancing service to distribute traffic between the two instance groups and to fail over to the standby
instance group in case of a disaster. This will ensure that your application is always available, even in the event of a regional
outage.
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option A: Deploying the application on two Compute Engine instances in the same project but in a different region will not provide
enough redundancy, as the instances are still in the same project and could be affected by the same regional outage.
Option B: Deploying the application on a Compute Engine instance and using the HTTP load balancing service to fail over to an
instance on your premises in case of a disaster is not a valid option, as it does not provide the required disaster recovery
capability.
Option C: Deploying the application on two Compute Engine instance groups, each in the same project but in a different region, is
not a valid option, as it does not provide the required disaster recovery capability.
upvoted 
2 
times
oms_muc
oms_muc
 
1 year, 6 months ago
IMHO A would also work (test env). In production relying on instances from other regions (introducing latency) just for possible
expected zonal outages, would not be my best practice.
upvoted 
1 
times
AniketD
AniketD
 
1 year, 7 months ago
Selected Answer: 
C
Correct answer is C. No need to host MIG in separate projects.
upvoted 
2 
timesmegumin
megumin
 
1 year, 8 months ago
Selected Answer: 
C
ok for C
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
C is right choice
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
C
C - using instance groups when planning for DR is better than having single vm's (https://cloud.google.com/architecture/disaster-
recovery). Having the resources in the same project is probably good for resource management.
upvoted 
1 
times
arjunvijayvargiya
arjunvijayvargiya
 
1 year, 9 months ago
Answer should have been A as autoscaling requirements are not mentioned.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 9 months ago
I think you need an group for the load balancer so A no good?
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #53
You are deploying an application on App Engine that needs to integrate with an on-premises database. For security purposes,
your on-premises database must not be accessible through the public internet. What should you do? 
A. 
Deploy your application on App Engine standard environment and use App Engine firewall rules to limit access to the
open on-premises database.
B. 
Deploy your application on App Engine standard environment and use Cloud VPN to limit access to the on-premises
database.
C. 
Deploy your application on App Engine flexible environment and use App Engine firewall rules to limit access to the on-
premises database.
D. 
Deploy your application on App Engine flexible environment and use Cloud VPN to limit access to the on-premises
database. 
Most Voted
Correct Answer:
 
D 
Comments
MyPractice
MyPractice
 
Highly Voted
 
5 years ago
Agree with D 
- "When to choose the flexible environment" 
"Accesses the resources or services of your Google Cloud project that
reside in the Compute Engine network."
https://cloud.google.com/appengine/docs/the-appengine-environments
upvoted 
54 
times
AWS56
AWS56
 
4 years, 11 months ago
Why not B ? https://cloud.google.com/appengine/docs/flexible/python/using-third-party-databases
upvoted 
7 
times
areza
areza
 
3 years, 6 months ago
because app engine standard cant connect to on-prem db
upvoted 
29 
times
VSMu
VSMu
 
1 year, 11 months ago
Community vote distribution
D (49%)
B (45%)
C (6%)Where does it say appengine cannot connect to on-prem db? With CloudVPN, it shoudl connect as per this
https://cloud.google.com/appengine/docs/flexible/storage-options#on_premises
Also going with D will require app to be containerized. That is not listed in the requirement.
upvoted 
6 
times
Cloudcrawler
Cloudcrawler
 
1 year, 4 months ago
This is the link for Standard Env
https://cloud.google.com/appengine/docs/standard/storage-options
Both standard and Flexible can connect to a VPC with Serverless VPC connector. Once it connects to a VPC, connecting to
onprem is same for any service.
upvoted 
4 
times
jrisl1991
jrisl1991
 
1 year, 2 months ago
I just had the same confusion. Serverless VPC Connector is something relatively newer than this question on the exam, so
probably it's safer to assume that a VPC connection is not supported (at least directly) by App Engine Standard. 
Besides, this would add extra overhead, and would also increase the costs for the solution. 
Most of these questions haven't been updated or repurposed according to newer products and services. For this particular
question, using a Serverless VPC Connector would add unnecessary complexity and the solution would become more
expensive. 
I swore to god it was B lol, but after a few hours of reading the documentation, I changed my mind and switched to option D.
You might want to do the same.
upvoted 
4 
times
elaineshi
elaineshi
 
2 years, 7 months ago
Isn't the question said "not public internet access"?
upvoted 
2 
times
mnsait
mnsait
 
7 months, 2 weeks ago
Yes, that phrase in the question bothers me too. However, when I check this:
https://cloud.google.com/appengine/docs/flexible/storage-options#:~:text=On%20premises,-
If%20you%20have&text=Because%20App%20Engine%20and%20Compute,database%20server's%20internal%20IP%20addre
ss.
it says "If you have existing on-premises databases that you want to make accessible to your App Engine app, you can either
configure your internal network and firewall to give the database a public IP address or connect using a VPN."
So I think the question should have skipped the words "not public internet access" if they want us to choose VPN.
upvoted 
2 
times
haroldbenites
haroldbenites
 
3 years ago
In a forum mentions that GCE and CAP flex are designed for connect to VPC . With GAP standard is needed a proxy .
https://stackoverflow.com/questions/47537204/how-to-connect-app-engine-and-on-premise-server-through-vpn
upvoted 
5 
times
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
Right is D:
https://stackoverflow.com/questions/37137914/is-it-possible-to-use-google-app-engine-with-google-cloud-vpn
upvoted 
18 
times
amxexam
amxexam
 
2 years, 7 months ago
Question is can we restrict acess with VP N ?
upvoted 
5 
times
moiradavis
moiradavis
 
2 years, 5 months ago
The stackoverflow reference if older that the answer (6 years) I think that has changed.The stackoverflow reference if older that the answer (6 years) I think that has changed.
upvoted 
1 
times
deep316
deep316
 
Most Recent
 
3 weeks, 2 days ago
Selected Answer: 
D
Standard requires more setup compared to Flexible. 
Standard Environment:
To connect from the standard environment, you primarily use "Serverless VPC Access" which allows your App Engine app to
reach your VPC network over private IP addresses without exposing it directly to the public internet. 
Flexible Environment:
In the flexible environment, you can directly connect to your VPC network by deploying your app within the same VPC as your
Cloud VPN gateway, enabling a more seamless connection using the private IP addresses of your network resources.
upvoted 
2 
times
Nimeshv
Nimeshv
 
1 month ago
Selected Answer: 
D
App Engine Standard Environment:
Limited Customization: It runs on predefined runtime environments with limited flexibility.
No Access to Compute Engine Network: It does not support integration with the VPC network, which is necessary for setting up
VPN connections.
App Engine Flexible Environment:
Customizable Runtimes: Allows you to run custom runtimes and use your own Docker containers.
VPC Integration: Fully supports VPC networking, enabling you to set up Cloud VPN to securely connect to your on-premises
database.
Greater Flexibility: More control over the instance types, scaling, and networking options.
upvoted 
2 
times
desertlotus1211
desertlotus1211
 
1 month ago
Selected Answer: 
D
Per google: 
For using Cloud VPN, App Engine Flexible is generally considered better than App Engine Standard because it offers more
customization and control over your virtual machine environment, allowing you to configure network settings and access on-
premises resources more easily through the VPN
Answer is D
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
1 month, 1 week ago
Selected Answer: 
D
The answer is BOTH B&D...
Answer: BD
I'm not sure what is the point of the question or the problem.
The standard environment can scale from zero instances up to thousands very quickly. In contrast, the flexible environment must
have at least one instance running for each active version and can take longer to scale out in response to traffic. Standard
environment uses a custom-designed autoscaling algorithm.
But the question doesn't address this.
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
1 month, 1 week ago
Sorry I have to mark an answer. I wanted to choose B&D, but it won't let me
upvoted 
1 
times
icarogsm
icarogsm
 
1 month, 1 week ago
Selected Answer: 
D
D, just App Engine Flex can connect to onprem.
upvoted 
1 
times
 
1 month, 2 weeks agoEkramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
Selected Answer: 
D
The flexible environment gives you more control over the networking configuration of your application. This is crucial for setting up
a secure connection to your on-premises database.
upvoted 
2 
times
yocixim836
yocixim836
 
1 month, 3 weeks ago
Selected Answer: 
B
https://cloud.google.com/appengine/docs/standard/storage-options#on_premises
upvoted 
2 
times
dpttpd
dpttpd
 
2 months, 1 week ago
Selected Answer: 
B
https://cloud.google.com/appengine/docs/standard/storage-options#on_premises
upvoted 
2 
times
raghupothula
raghupothula
 
3 months, 1 week ago
B and C are not appropriate and wherein in App Engine flex resources reside in VPC NEtwo
https://cloud.google.com/appengine/docs/the-appengine-environments#app-engine-environments
D
upvoted 
1 
times
maxdanny
maxdanny
 
4 months ago
Selected Answer: 
D
App Engine flexible environment provides more flexibility and supports VPC (Virtual Private Cloud) connectivity, which allows you to
set up a Cloud VPN connection. The VPN can be used to securely connect your App Engine application to the on-premises
database without exposing it to the public internet.
upvoted 
2 
times
joecloud12
joecloud12
 
5 months ago
Selected Answer: 
B
flexible is more expensive. standard will suffice
upvoted 
1 
times
janji456
janji456
 
5 months, 1 week ago
D
et up a VPN connection between your on-premises network and Google Cloud.
This establishes a secure tunnel for communication.
Your App Engine
upvoted 
1 
times
neha_pallod
neha_pallod
 
6 months ago
Selected Answer: 
B
right answer is B
upvoted 
1 
times
nhatne
nhatne
 
6 months, 1 week ago
Selected Answer: 
D
"your on-premises database must not be accessible through the public internet" 
=> definitely C
upvoted 
1 
times
nhatne
nhatne
 
6 months, 1 week ago
sorry was a typo, It's D
upvoted 
1 
times
Toothpick
Toothpick
 
5 months, 1 week agoToothpick
Toothpick
 
5 months, 1 week ago
B and D both use public internet, ie, VPN. So B is the easier option as per 
https://cloud.google.com/appengine/docs/standard/connecting-vpc
upvoted 
1 
times
gustangelo
gustangelo
 
7 months, 1 week ago
Selected Answer: 
B
The documentation mentions that App Engine Standard can connect to on-prem database using VPN. Link of the documentation:
https://cloud.google.com/appengine/docs/standard/storage-options
upvoted 
5 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #54
You are working in a highly secured environment where public Internet access from the Compute Engine VMs is not allowed.
You do not yet have a VPN connection to access an on-premises file server. You need to install specific software on a Compute
Engine instance. How should you install the software? 
A. 
Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access
subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil. 
Most Voted
B. 
Upload the required installation files to Cloud Storage and use firewall rules to block all traffic except the IP address
range for Cloud Storage. Download the files to the VM using gsutil.
C. 
Upload the required installation files to Cloud Source Repositories. Configure the VM on a subnet with a Private Google
Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gcloud.
D. 
Upload the required installation files to Cloud Source Repositories and use firewall rules to block all traffic except the IP
address range for Cloud Source Repositories. Download the files to the VM using gsutil.
Correct Answer:
 
A 
Comments
zaki_b
zaki_b
 
Highly Voted
 
5 years, 2 months ago
Internet access is not allowed so it should be A. CMIIW
upvoted 
55 
times
tartar
tartar
 
4 years, 5 months ago
A is ok
upvoted 
10 
times
kumarp6
kumarp6
 
4 years, 2 months ago
A is the answer
upvoted 
3 
times
nitinz
nitinz
 
3 years, 10 months ago
A is the best answer.
Community vote distribution
A (82%)
B (18%)A is the best answer.
upvoted 
3 
times
KNG
KNG
 
Highly Voted
 
4 years, 10 months ago
Should be A
https://cloud.google.com/vpc/docs/configure-private-services-access
Note: Even though the IP addresses for Google APIs and services are public, the traffic path from instances that are using Private
Google Access to the Google APIs remains within Google's network.
upvoted 
20 
times
plumbig11
plumbig11
 
Most Recent
 
2 days, 9 hours ago
Selected Answer: 
A
Private Google Access subnet with cloud storage. A
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
Selected Answer: 
A
Private Google Access allows your VM to access Google APIs and services (like Cloud Storage) without needing a public IP
address. This is crucial in your restricted environment.
upvoted 
1 
times
maxdanny
maxdanny
 
4 months ago
Selected Answer: 
A
Private Google Access ensures the VM can reach Cloud Storage using its internal IP, while still restricting public internet access.
upvoted 
3 
times
arotesa
arotesa
 
4 months ago
The Answer is D
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
A
A. Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet.
Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.
upvoted 
1 
times
ppandher
ppandher
 
2 years ago
Those who are opting for B, Can please explain without Internet access and without Private Google Access enabled how will they
communicate with Cloud Storage ? :)
upvoted 
5 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is A: Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private
Google Access subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.
To install specific software on a Compute Engine instance in a highly secured environment where public Internet access is not
allowed, you can follow these steps:
Upload the required installation files to Cloud Storage.
Configure the VM on a subnet with a Private Google Access subnet. This will allow the VM to access Google APIs and services,
such as Cloud Storage, without requiring a public IP address or internet access.
Assign only an internal IP address to the VM. This will ensure that the VM is not accessible from the public internet.
Download the installation files to the VM using gsutil, which is a command-line tool that allows you to access Cloud Storage from
the VM.
upvoted 
8 
times
omermahgoub
omermahgoub
 
2 years ago
Option B: Uploading the required installation files to Cloud Storage and using firewall rules to block all traffic except the IP address
range for Cloud Storage is not a valid option, as it does not allow the VM to access the installation files without public internet
access.Option C: Uploading the required installation files to Cloud Source Repositories and using gcloud to download the files to the VM
is not a valid option, as Cloud Source Repositories does not support storing large binary files such as installation files.
Option D: Uploading the required installation files to Cloud Source Repositories and using firewall rules to block all traffic except
the IP address range for Cloud Source Repositories is not a valid option, as it does not allow the VM to access the installation
files without public internet access.
upvoted 
6 
times
habros
habros
 
2 years, 1 month ago
Selected Answer: 
A
Eliminate B&D as it connect via public networks despite it being a Google Cloud service.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
stevehlw
stevehlw
 
2 years, 1 month ago
With private Google access subnet, the vm can reach external network. With this setting, it violates “public Internet access from
the Compute Engine VMs is not allowed”. Can someone explain why it’s not B instead?
upvoted 
2 
times
ppandher
ppandher
 
2 years, 1 month ago
Private Google access means - refer to https://www.youtube.com/watch?v=yd5FtV8aJkk
upvoted 
3 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A is good
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
A
A. Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet.
Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.
upvoted 
1 
times
muneebarshad
muneebarshad
 
2 years, 3 months ago
Selected Answer: 
B
Configuring Private Google Access is the best way to access Google Services for VM that does not have access to the internet. In
order to access Google Private APIs egress should be opened to the following IP Address restricted.googleapis.com
(199.36.153.4/30). VM will leverage internal networking to access Cloud Storage 
https://cloud.google.com/vpc/docs/configure-private-google-access
upvoted 
4 
times
6721sora
6721sora
 
2 years, 4 months ago
C because Cloud repositories is a private Git within Google cloud. Hence it is ideal for simple pull, push, clone type "git" operations.
As this is within Google cloud and is a private git, you do not need public internet access
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
2 years, 3 months ago
I think it's not this because Clouse Source Repositories is for source code. 
Sounds like we are looking for an executable?
upvoted 
1 
times
amxexam
amxexam
 
2 years, 7 months ago
Selected Answer: 
A
C&D we are all eliminating becoz of source storage repo
Between A& B B looks more tempting to select because it mentions fire wallrule But the problem with B is the statement is wrongBetween A& B B looks more tempting to select because it mentions fire wallrule But the problem with B is the statement is wrong
the access will happen from VM to storage and the statement mentions traffic from storage to Vm.
Hence A
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #55
Your company is moving 75 TB of data into Google Cloud. You want to use Cloud Storage and follow Google-recommended
practices. What should you do? 
A. 
Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.
Most Voted
B. 
Move your data onto a Transfer Appliance. Use Cloud Dataprep to decrypt the data into Cloud Storage.
C. 
Install gsutil on each server that contains data. Use resumable transfers to upload the data into Cloud Storage.
D. 
Install gsutil on each server containing data. Use streaming transfers to upload the data into Cloud Storage.
Correct Answer:
 
A 
Comments
AshishK
AshishK
 
Highly Voted
 
5 years ago
It should be 'A'
Transfer Appliance lets you quickly and securely transfer large amounts of data to Google Cloud Platform via a high capacity
storage server that you lease from Google and ship to our datacenter. Transfer Appliance is recommended for data that exceeds
20 TB or would take more than a week to upload.
upvoted 
35 
times
MyPractice
MyPractice
 
5 years ago
where did u get that 20 TB number - can help to share link?
upvoted 
1 
times
onashwani
onashwani
 
4 years ago
Here is the link:
https://cloud.google.com/transfer-appliance/docs/2.2/overview
upvoted 
4 
times
gcp_learner
gcp_learner
 
3 years ago
But that link mentions a few hundred terabytes to 1 petabyte not 20TB or did I read that incorrectly?
upvoted 
2 
times
Community vote distribution
A (85%)
C (15%)upvoted 
2 
times
mindhoc
mindhoc
 
1 year, 2 months ago
The request transfer appliance UI seems to suggest that it is not cost effective under 20TB of data.
upvoted 
2 
times
Yahowmy
Yahowmy
 
4 years, 5 months ago
To this date Transfer Appliance supported locations are only
United States 
Canada 
European Union 
Norway 
Switzerland.
What if data reside in a location other than this?
C is the most convenience for this scenario.
upvoted 
11 
times
ccpmad
ccpmad
 
6 months, 2 weeks ago
stupid answer
upvoted 
2 
times
NoCrapEva
NoCrapEva
 
11 months, 1 week ago
There is NO mention of region - you dont assume anything NOT mentioned in the question therefore - Ans =A
upvoted 
1 
times
Ramheadhunter
Ramheadhunter
 
2 years, 4 months ago
Why assume a scenario no provided in the question. We need to choose the best case scenario based on available information
instead of making assumptions. So A should be good.
upvoted 
5 
times
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
Why not A?
upvoted 
30 
times
tartar
tartar
 
4 years, 4 months ago
A is ok
upvoted 
9 
times
kumarp6
kumarp6
 
4 years, 2 months ago
It is A
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
A, anything over 10TB goes via appliance.
upvoted 
14 
times
Begum
Begum
 
2 years, 3 months ago
I have moved 120 TB using gsutil- cost effectively!
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
takes longer though
upvoted 
1 
times
Koushick
Koushick
 
3 years, 8 months ago
Answer is A.
upvoted 
4 
timesplumbig11
plumbig11
 
Most Recent
 
2 days, 9 hours ago
Selected Answer: 
A
75tb Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.
upvoted 
1 
times
JonathanSJ
JonathanSJ
 
5 days, 5 hours ago
Selected Answer: 
A
I will go for A.
upvoted 
1 
times
kip21
kip21
 
11 months, 3 weeks ago
A - Correct
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
A
Sending 75 TB of data on reliable 1.5 Gbps line will take about 6 days to complete data transfer online at the same time it will
consume entire bandwidth. Hence use of Transfer appliace is required.
upvoted 
1 
times
MaheshKaswan
MaheshKaswan
 
1 year, 6 months ago
Selected Answer: 
C
Opion A is partially correct as you would not use a Transfer Appliance Rehydrator to decrypt the data. The Transfer Appliance itself
is used to encrypt and decrypt the data. Option C is correct.
upvoted 
2 
times
RVivek
RVivek
 
1 year, 11 months ago
Selected Answer: 
A
gsutil is recommanded for data size less than a TB. 
That rules out C and D
B says decrypt data using Dataprep not sure this is possible. 
https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#transfer-options
upvoted 
3 
times
n_nana
n_nana
 
1 year, 11 months ago
Selected Answer: 
A
It will be more precise with info about the bandwidth
Google says: 
The two main criteria to consider with Transfer Appliance are cost and speed. With reasonable network connectivity (for example,
1 Gbps), transferring 100 TB of data online takes over 10 days to complete. If this rate is acceptable, an online transfer is likely a
good solution for your needs. If you only have a 100 Mbps connection (or worse from a remote location), the same transfer takes
over 100 days. At this point, it's worth considering an offline-transfer option such as Transfer Appliance.
So even for such 100 TB google choose between transfer appliance or online transfer. not going with gsutil at all. it is clear gsutil is
suitable for small to medium size (less than 1 TB)
so with no more details, google recommendation is A
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is A: Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into
Cloud Storage.
To move large amounts of data into Google Cloud, it is recommended to use Transfer Appliance. Transfer Appliance is a physical
storage device that you can use to transfer large amounts of data to Google Cloud quickly and securely. Once you have moved
your data onto a Transfer Appliance, you can use a Transfer Appliance Rehydrator to decrypt the data and load it into Cloud
Storage.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
Option B: Using Cloud Dataprep to decrypt the data into Cloud Storage is not a valid option, as Cloud Dataprep is a dataOption B: Using Cloud Dataprep to decrypt the data into Cloud Storage is not a valid option, as Cloud Dataprep is a data
preparation tool that does not support data transfer or decryption.
Option C: Using resumable transfers to upload the data into Cloud Storage is not a recommended option for moving large
amounts of data, as resumable transfers are designed for smaller data sets and may not be efficient for transferring large
amounts of data.
Option D: Using streaming transfers to upload the data into Cloud Storage is not a recommended option for moving large
amounts of data, as streaming transfers are designed for transferring real-time data streams and may not be efficient for
transferring large amounts of data.
Therefore, the correct answer is A: Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt
the data into Cloud Storage.
upvoted 
6 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Option A Use a Transfer Appliance Rehydrator
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
A
A. Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.
upvoted 
1 
times
sgofficial
sgofficial
 
2 years, 5 months ago
Selected Answer: 
A
A is correct ...
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options 
1. gsutil is for lessthan <1 TB data with enough bandwidth, so C and D can be eliminated
2. option b can be eliminated since dataprep for decription is not correct
3. so only left over is a and its offline transfer, since the question did not give any time line when the transfer to be completed
upvoted 
3 
times
[Removed]
[Removed]
 
2 years, 7 months ago
Selected Answer: 
A
https://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability
upvoted 
1 
times
amxexam
amxexam
 
2 years, 7 months ago
Selected Answer: 
A
I would eliminate B & C as the question clearly methos google recomendations.About 10 TB or canal to we need to me transfer
appliance. Let's not worry about what regions Ohk for now.
Between A & B B is over kill as transfer applion allows decryption. Hence A
upvoted 
1 
times
meokey
meokey
 
2 years, 8 months ago
Selected Answer: 
A
Is Transfer Appliance suitable for me?
"Your data size is greater than or equal to 10TB."
https://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #56
You have an application deployed on Google Kubernetes Engine using a Deployment named echo-deployment. The deployment
is exposed using a Service called echo-service. You need to perform an update to the application with minimal downtime to the
application. What should you do? 
A. 
Use kubectl set image deployment/echo-deployment <new-image> 
Most Voted
B. 
Use the rolling update functionality of the Instance Group behind the Kubernetes cluster
C. 
Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and
kubectl create 
ג
"€f <yaml-file>
D. 
Update the service yaml file which the new container image. Use kubectl delete service/echo-service and kubectl create
ג
"€f <yaml-file>
Correct Answer:
 
A 
Comments
ffk
ffk
 
Highly Voted
 
5 years, 2 months ago
A 
is correct. 
B is funny
upvoted 
47 
times
AmitAr
AmitAr
 
2 years, 7 months ago
B looks most sensible (not funny). rolling update is a deployment strategy, which will deploy on pods 1 by 1,. i.e. by the time first
pod is getting newer version of application, other pods are running with older version... In this way, there will be no downtime of
application.. which is real ask from this question.
I recommend B
upvoted 
6 
times
monopfm
monopfm
 
8 months, 1 week ago
Use the rolling update functionality of the >[Instance Group behind the Kubernetes cluster]<. It's not a rolling update of a
Deployment. Read carefully.
upvoted 
4 
times
Community vote distribution
A (93%)
C (7%)upvoted 
4 
times
tartar
tartar
 
4 years, 5 months ago
A is ok
upvoted 
14 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Yes A is correct
upvoted 
3 
times
nitinz
nitinz
 
3 years, 10 months ago
Only logical answer is A.
upvoted 
1 
times
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
Correct is A
upvoted 
13 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 4 hours ago
Selected Answer: 
A
I will go for A.
upvoted 
1 
times
sim7243
sim7243
 
1 month, 3 weeks ago
Selected Answer: 
A
option A
upvoted 
1 
times
isa_pr
isa_pr
 
5 months, 4 weeks ago
It's A. As per K8s documentation:
To update the image of the application to version 2, use the set image subcommand, followed by the deployment name and the
new image version:
kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2
The command notified the Deployment to use a different image for your app and initiated a rolling update."
"
upvoted 
3 
times
kip21
kip21
 
11 months, 3 weeks ago
Option C - 
is the best option to perform an update to an application deployed on Google Kubernetes Engine with minimal downtime
because it provides control over the update process, ensures high availability, and minimizes disruption. Rolling update
functionality can also be used but requires more effort to implement. 
Option A and Option D may result in downtime if the new image is incompatible with the existing application.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
A
https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application
upvoted 
4 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
C
C. Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment and kubectl
create 
ג
"€f <yaml-file>
I agreed with omermahgoub with his explanation.
upvoted 
1 
times
cacharritos
cacharritos
 
1 year agodelete.. minimal downtime.. A is correct ^_^U
upvoted 
1 
times
vamgcp
vamgcp
 
1 year, 11 months ago
To perform an update to the application with minimal downtime on Google Kubernetes Engine (GKE), you can use a rolling update
strategy, which involves updating the application incrementally, one pod at a time, while ensuring that the updated pods are
functioning properly before updating the next set. Here's the general process:
kubectl set image deployment/echo-deployment echo=<new_image_tag>
upvoted 
4 
times
roaming_panda
roaming_panda
 
2 years ago
Selected Answer: 
A
https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps 
says rolling updates and mentions same command . 
So 100 % A
upvoted 
9 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is C: Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-
deployment and kubectl create –f <yaml-file>.
To perform an update to an application deployed on Google Kubernetes Engine with minimal downtime, you can follow these
steps:
Update the deployment yaml file with the new container image.
Use the kubectl delete deployment/echo-deployment command to delete the existing deployment.
Use the kubectl create –f <yaml-file> command to create a new deployment using the updated yaml file.
This process, known as a rolling update, allows you to update your application with minimal downtime by replacing the old version
of the application with the new version one pod at a time, while ensuring that there is always at least one pod available to serve
traffic.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
using kubectl set image deployment/deployment <new-image> will not allow you to perform an update to the application with
minimal downtime, even if the deployment is exposed using a Service.
This command will update the image of the containers in the deployment, but it will not perform a rolling update. A rolling update
allows you to update your application with minimal downtime by replacing the old version of the application with the new version
one pod at a time, while ensuring that there is always at least one pod available to serve traffic. Without a rolling update, all of the
pods in the deployment will be replaced at the same time, which may result in downtime for the application.
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
Option A: Using kubectl set image deployment/echo-deployment <new-image> will update the image of the containers in the
deployment, but it will not perform a rolling update and may result in downtime for the application.
Option B: Using the rolling update functionality of the Instance Group behind the Kubernetes cluster is not a valid option, as the
rolling update functionality is used to update the instances in the instance group, not the containers in a deployment.
Option D: Updating the service yaml file with the new container image and using kubectl delete service/echo-service and kubectl
create –f <yaml-file> is not a valid option, as the service is not responsible for running the application containers and updating the
service will not update the application.
upvoted 
3 
times
CkWongCk
CkWongCk
 
1 year, 11 months ago
A is correct, update template spec image in deployment yml will trigger rollout deploy
upvoted 
1 
times
_kartik_raj
_kartik_raj
 
1 year, 2 months ago
Answer is A, you are wrong as hell, if you delete deployment its obvious app will face downtime
upvoted 
3 
times
jasenmornin
jasenmornin
 
2 years, 1 month agoSelected Answer: 
A
I think A is correct:
B. I don't understand the objective of this option.
C and D. These are eliminated because they involve suffering a downtime when the resources are eliminated, so they are not
fulfilling one of the requirements.
upvoted 
3 
times
markus_de
markus_de
 
2 years, 1 month ago
Selected Answer: 
A
Example from official Kubernetes docu (for NGINX):
kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
upvoted 
5 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A is right -- kubectl set image deployment/echo-deployment
upvoted 
1 
times
RitwickKumar
RitwickKumar
 
2 years, 4 months ago
Selected Answer: 
A
Source: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment
Deployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least
75% of the desired number of Pods are up (25% max unavailable).
Deployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures
that at most 125% of the desired number of Pods are up (25% max surge).
upvoted 
8 
times
Mikado211
Mikado211
 
2 years, 5 months ago
Selected Answer: 
A
Answer is A
It can't be C, if you delete and recreate the deployment you will have a downtime between the deletion and the recreation.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #57
Your company is using BigQuery as its enterprise data warehouse. Data is distributed over several Google Cloud projects. All
queries on BigQuery need to be billed on a single project. You want to make sure that no query costs are incurred on the
projects that contain the data. Users should be able to query the datasets, but not edit them. 
How should you configure users' access roles? 
A. 
Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery dataViewer on the
projects that contain the data.
B. 
Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery user on
the projects that contain the data.
C. 
Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer
on the projects that contain the data. 
Most Voted
D. 
Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser
on the projects that contain the data.
Correct Answer:
 
C 
Comments
RitwickKumar
RitwickKumar
 
Highly Voted
 
1 year, 10 months ago
Selected Answer: 
C
Both A & C are correct but using the principle of least privileges C is the most appropriate.
BigQuery User: (roles/bigquery.user)
When applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.
When applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this
role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. <b>Additionally, allows the
creation of new datasets within the project; the creator is granted the BigQuery Data Owner role(roles/bigquery.dataOwner) on
these new datasets.</b> 
Lowest-level resources where you can grant this role: Dataset
BigQuery Job User: (roles/bigquery.jobUser)
Provides permissions to run jobs, including queries, within the project.
Lowest-level resources where you can grant this role: Project
Community vote distribution
C (100%)Source: https://cloud.google.com/bigquery/docs/access-control
upvoted 
25 
times
kimharsh
kimharsh
 
Highly Voted
 
2 years, 5 months ago
Selected Answer: 
C
C is the correct Answer ,
A is wrong because bq User Permission will allow you to edit the dataset, which is something that we don't want in this scenario.
B and D is wrong because "You want to make sure that no query costs are incurred on the projects that contain the data" so you
don't want users to fire quires on the Project that contains the dataset , hence the "dataViewer" permission
https://cloud.google.com/bigquery/docs/access-control
upvoted 
21 
times
kratosmat
kratosmat
 
1 year, 3 months ago
It seems that User Permission doesn't allow to edit data, isn't it?
upvoted 
1 
times
plumbig11
plumbig11
 
Most Recent
 
2 days, 9 hours ago
Selected Answer: 
C
BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.
upvoted 
1 
times
JonathanSJ
JonathanSJ
 
5 days, 4 hours ago
Selected Answer: 
C
I will go for C.
upvoted 
1 
times
Edgo97
Edgo97
 
4 months, 2 weeks ago
The link to refer here: https://cloud.google.com/bigquery/docs/access-control
upvoted 
1 
times
SidsA
SidsA
 
1 year, 3 months ago
Selected Answer: 
C
The "roles/bigquery.jobUser" role provides the permission to run jobs, including querying, exporting and copying data, and creating
views and materialized views. This role does not provide permissions to create, update, or delete BigQuery resources, such as
datasets, tables, and models. Users with this role can only interact with BigQuery through jobs.
The "roles/bigquery.User" role, on the other hand, provides the permission to create, update, and delete BigQuery resources, as
well as run jobs. This role includes all the permissions of the "roles/bigquery.jobUser" role, and in addition allows users to manage
BigQuery resources, such as creating datasets, tables, and models, and modifying their schema and access controls.
upvoted 
3 
times
jlambdan
jlambdan
 
1 year, 3 months ago
Selected Answer: 
C
A is wrong because https://cloud.google.com/bigquery/docs/access-control#bigquery.user
C is correct because https://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser
upvoted 
1 
times
jay9114
jay9114
 
1 year, 6 months ago
Selected Answer: 
C
Important statements from the prompt
1. All queries need to be billed to a single project - one project that queries data stored on other projects. Let's call this our billing
project. 
a. jobUser is the best role to satisfy this need, because it provides permission to run jobs 
and queries within a project.
2. Other projects is where the data resides. These projects don't need much access besides the ability to be viewed (not edited).
a. The dataViewer role provide permission to read all datasets in the project.
upvoted 
7 
times
 
1 year, 6 months agoomermahgoub
omermahgoub
 
1 year, 6 months ago
The correct answer is A: Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery
dataViewer on the projects that contain the data.
To make sure that no query costs are incurred on the projects that contain the data and allow users to query the datasets but not
edit them, you should follow these steps:
Add all users to a group.
Grant the group the role of BigQuery user on the billing project. This will allow the group to run queries on BigQuery and incur
costs on the billing project.
Grant the group the role of BigQuery dataViewer on the projects that contain the data. This will allow the group to view the datasets
and run queries on them, but not edit them.
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The BigQuery Job User role (roles/bigquery.jobUser) and the BigQuery User role (roles/bigquery.user) have similar permissions,
but they differ in the scope of their permissions.
The BigQuery Job User role grants users the ability to create and modify query jobs, but it does not grant them the ability to run
queries or incur costs on the project. This role is intended for users who need to create and manage query jobs, but who should
not be able to run queries or incur costs.
The BigQuery User role grants users the ability to run queries and incur costs on the project, in addition to the ability to create and
modify query jobs. This role is intended for users who need to run queries and incur costs on the project, as well as create and
manage query jobs.
upvoted 
3 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Here is a summary of the differences between the BigQuery Job User role and the BigQuery User role:
BigQuery Job User role (roles/bigquery.jobUser):
Can create and modify query jobs
Cannot run queries or incur costs on the project
BigQuery User role (roles/bigquery.user):
Can create and modify query jobs
Can run queries and incur costs on the project
If you want to grant users the ability to create and modify query jobs, but not run queries or incur costs on the project, you should
use the BigQuery Job User role. If you want to grant users the ability to run queries and incur costs on the project, in addition to
the ability to create and modify query jobs, you should use the BigQuery User role.
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option B: Granting the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that
contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.
Option C: Granting the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that
contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.
Option D: Granting the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that
contain the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.
upvoted 
1 
times
Diwz
Diwz
 
2 months, 3 weeks ago
BigQuery User 
(roles/bigquery.user)
When applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.
When applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this
role can enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Additionally, allows the
creation of new datasets within the project; the creator is granted the BigQuery Data Owner role (roles/bigquery.dataOwner) on
these new datasets.
Bigquery.user has potential to create a dataset inside the project and creates becomes owner of the dataset. This is not the
requirement stated in the question scenario.
Answer is CAnswer is C
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
C is right 
Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the
projects that contain the data.
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
C
C. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the
projects that contain the data.
upvoted 
1 
times
Vedjha
Vedjha
 
1 year, 9 months ago
D is the answer:
Cloud BigQuery Roles
Cloud BigQuery IAM Roles
BigQuery Admin - bigquery.*
BigQuery Data Owner - bigquery.datasets.*, bigquery.models.*, bigquery.routines.*,
bigquery.tables.* (Does NOT have access to Jobs!)
BigQuery Data Editor - bigquery.tables.(create/delete/export/get/getData/getIamPolicy/
list/update/updateData/updateTag), bigquery.models.*, bigquery.routines.*,
bigquery.datasets.(create/get/getIamPolicy/updateTag)
BigQuery Data Viewer - get/list bigquery.(datasets/models/routines/tables)
BigQuery Job User - bigquery.jobs.create
BigQuery User - BigQuery Data Viewer + get/list (jobs, capacityCommitments, reservations
etc)
To see data, you need either BigQuery User or BigQuery Data Viewer roles
You CANNOT see data with BigQuery Job User roles
BigQuery Data Owner or Data Viewer roles do NOT have access to jobs!
upvoted 
1 
times
kimharsh
kimharsh
 
2 years, 5 months ago
C is the correct Answer , 
A is wrong because bq User Permission will allow you to edit the dataset, which is something that we don't want in this scenario.
B and D is wrong because "You want to make sure that no query costs are incurred on the projects that contain the data" so you
don't want users to fire quires on the Project that contains the dataset , hence the "dataViewer" permission
https://cloud.google.com/bigquery/docs/access-control
upvoted 
1 
times
victory108
victory108
 
2 years, 6 months ago
C. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the
projects that contain the data.
upvoted 
4 
times
LoveT
LoveT
 
2 years, 6 months ago
C looks to be the correct answer
upvoted 
2 
times
HenkH
HenkH
 
2 years, 6 months ago
Selected Answer: 
C
JobUser is the correct terminology for bq. Only read access to data sources is required.
upvoted 
1 
times
HenkH
HenkH
 
2 years, 6 months ago
bq is using jobs - so "user" isn't specific enough, jobuser is.
upvoted 
2 
times
elenamatay
elenamatay
 
2 years, 6 months agoHence C
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #58
You have developed an application using Cloud ML Engine that recognizes famous paintings from uploaded images. You want
to test the application and allow specific people to upload images for the next 24 hours. Not all users have a Google Account.
How should you have users upload images? 
A. 
Have users upload the images to Cloud Storage. Protect the bucket with a password that expires after 24 hours.
B. 
Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours. 
Most Voted
C. 
Create an App Engine web application where users can upload images. Configure App Engine to disable the application
after 24 hours. Authenticate users via Cloud Identity.
D. 
Create an App Engine web application where users can upload images for the next 24 hours. Authenticate users via
Cloud Identity.
Correct Answer:
 
B 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
Correct answer is B
upvoted 
44 
times
tartar
tartar
 
4 years, 5 months ago
B is ok
upvoted 
8 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Signed URL ... B is correct
upvoted 
3 
times
nitinz
nitinz
 
3 years, 10 months ago
B signed URL
upvoted 
3 
times
MyPractice
MyPractice
 
Highly Voted
 
5 years ago
Community vote distribution
B (100%)MyPractice
MyPractice
 
Highly Voted
 
5 years ago
Ans B
"When should you use a signed URL? In some scenarios, you might not want to require your users to have a Google account in
order to access Cloud Storage" 
"Signed URLs contain authentication information in their query string, allowing users without
credentials to perform specific actions on a resource"
https://cloud.google.com/storage/docs/access-control/signed-urls
upvoted 
28 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 4 hours ago
Selected Answer: 
B
I will go for B.
upvoted 
1 
times
sim7243
sim7243
 
1 month, 3 weeks ago
Selected Answer: 
B
Correct answer is B
upvoted 
1 
times
gun123
gun123
 
12 months ago
Selected Answer: 
B
Correct answer is B
upvoted 
1 
times
red_panda
red_panda
 
1 year, 5 months ago
Selected Answer: 
B
B is the answer
upvoted 
1 
times
fussili
fussili
 
1 year, 9 months ago
The correct answer is B.
A is not a good choice because it is not possible to set an expiration time for a password protected Cloud Storage bucket. This
means that if a user had the password, they would be able to upload images to the bucket even after the 24 hour period has
expired.
B is the correct answer because a signed URL can be generated to allow specific users to upload images to Cloud Storage
without requiring them to have a Google Account. The URL can be set to expire after 24 hours, which ensures that users can only
upload images during the allowed time period.
C is not the best choice because it involves creating an App Engine web application, which is more complex than using Cloud
Storage with a signed URL. Additionally, App Engine instances cannot be turned off programmatically, so it would not be possible
to disable the application after 24 hours.
D option is similar to option C, but it involves creating an App Engine web application. This would add unnecessary complexity to
the solution, and it would not provide any additional benefits compared to using Cloud Storage with a signed URL.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is B: Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.
To allow specific users to upload images to Cloud Storage for testing your Cloud ML Engine application, and to not require all
users to have a Google Account, you should use signed URLs. A signed URL is a URL that allows access to a specific resource in
Cloud Storage, and that is only valid for a specified period of time.
To create a signed URL that expires after 24 hours, you can use the gsutil signurl command. For example:
Copy code
gsutil signurl -d 24h service-account.json gs://bucket-name/object-name
This will generate a signed URL that allows users to upload an object to the specified bucket with the specified name, and that will
only be valid for 24 hours.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years agoomermahgoub
omermahgoub
 
2 years ago
Option A: Protecting the bucket with a password that expires after 24 hours would not be a secure or scalable solution, as it
would require you to distribute the password to all users and to update the password every 24 hours.
Option C: Creating an App Engine web application where users can upload images, and configuring App Engine to disable the
application after 24 hours, would not allow users to upload images after the application is disabled.
Option D: Creating an App Engine web application where users can upload images for the next 24 hours and authenticating
users via Cloud Identity would not allow users to upload images if they do not have a Google Account.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
B is right, Signed URL's will help in this scnerio.
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
B. Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.
upvoted 
1 
times
mv2000
mv2000
 
2 years, 6 months ago
On 06/30/2022 Exam.
upvoted 
2 
times
mygcpjourney2712
mygcpjourney2712
 
2 years, 9 months ago
Selected Answer: 
B
signed url
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
B is the correct answer
upvoted 
2 
times
haroldbenites
haroldbenites
 
3 years ago
Go for B.
upvoted 
1 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
B – Have users upload the images to Cloud Storage via signed URL which expires after 24 hours.
Signed URL is a preferable way to allow something with limited timeframe, doesn't require the account
upvoted 
1 
times
[Removed]
[Removed]
 
3 years, 2 months ago
B is right. Signed URL are best for users for short term access.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #59
Your web application must comply with the requirements of the European Union's General Data Protection Regulation (GDPR).
You are responsible for the technical architecture of your web application. What should you do? 
A. 
Ensure that your web application only uses native features and services of Google Cloud Platform, because Google
already has various certifications and provides 
ג
€pass-on
ג
 €compliance when you use native features.
B. 
Enable the relevant GDPR compliance setting within the GCPConsole for each of the services in use within your
application.
C. 
Ensure that Cloud Security Scanner is part of your test planning strategy in order to pick up any compliance gaps.
D. 
Define a design for the security of data in your web application that meets GDPR requirements. 
Most Voted
Correct Answer:
 
D 
Comments
AWS56
AWS56
 
Highly Voted
 
3 years, 5 months ago
Agree D
upvoted 
17 
times
AshokC
AshokC
 
Highly Voted
 
2 years, 9 months ago
D - https://cloud.google.com/security/gdpr 
The GDPR lays out specific requirements for businesses and organizations who are established in Europe or who serve users in
Europe. It:
Regulates how businesses can collect, use, and store personal data
Builds upon current documentation and reporting requirements to increase accountability
Authorizes fines on businesses who fail to meet its requirements
upvoted 
15 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 4 hours ago
Selected Answer: 
D
I will go for D.
upvoted 
1 
times
 
6 months, 2 weeks ago
Community vote distribution
D (100%)omermahgoub
omermahgoub
 
6 months, 2 weeks ago
The correct answer is option D: Define a design for the security of data in your web application that meets GDPR requirements.
The General Data Protection Regulation (GDPR) is a comprehensive data protection law that applies to any company that
processes the personal data of individuals in the European Union (EU). As the technical architect of your web application, it is your
responsibility to ensure that the application is compliant with GDPR requirements.
upvoted 
4 
times
omermahgoub
omermahgoub
 
6 months, 2 weeks ago
Option A: While it is true that Google has various certifications and provides pass-on compliance when you use native features,
simply using native features and services of Google Cloud Platform is not sufficient to ensure compliance with GDPR. You still
need to implement appropriate controls and safeguards to protect personal data and meet GDPR requirements.
Option B: Enabling the relevant GDPR compliance setting within the GCP console for each of the services in use within your
application may help ensure compliance with GDPR, but it is not sufficient on its own. You still need to implement appropriate
controls and safeguards to protect personal data and meet GDPR requirements.
Option C: Using Cloud Security Scanner as part of your test planning strategy can help identify potential security vulnerabilities
and compliance gaps in your web application, but it is not sufficient on its own to ensure compliance with GDPR. You still need to
implement appropriate controls and safeguards to protect personal data and meet GDPR requirements.
upvoted 
9 
times
megumin
megumin
 
8 months ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
Define a design for the security of data in your web application that meets GDPR requirements. 
D is right
upvoted 
1 
times
[Removed]
[Removed]
 
1 year, 4 months ago
Selected Answer: 
D
I got similar question on my exam.
upvoted 
7 
times
vincy2202
vincy2202
 
1 year, 6 months ago
D is the correct answer
upvoted 
1 
times
haroldbenites
haroldbenites
 
1 year, 7 months ago
Go for D
upvoted 
1 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
D
vote D
upvoted 
1 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
D – Define a design for the security of data in your web app that meets GDPR requirements.
upvoted 
1 
times
MikeB19
MikeB19
 
1 year, 10 months ago
A is wrong D is correct. The q refers is “Microsoft sql” not “MySQL”. App replication in MSsql is achieved with Availability Groups
within MSsql
https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-
server?view=sql-server-ver15
upvoted 
1 
timesMamthaSJ
MamthaSJ
 
1 year, 12 months ago
Answer is D
upvoted 
3 
times
victory108
victory108
 
2 years, 1 month ago
D. Define a design for the security of data in your web application that meets GDPR requirements.
upvoted 
2 
times
un
un
 
2 years, 1 month ago
D is correct
upvoted 
1 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is D
upvoted 
1 
times
lynx256
lynx256
 
2 years, 3 months ago
D is ok
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #60
You need to set up Microsoft SQL Server on GCP. Management requires that there's no downtime in case of a data center
outage in any of the zones within a 
GCP region. What should you do? 
A. 
Configure a Cloud SQL instance with high availability enabled. 
Most Voted
B. 
Configure a Cloud Spanner instance with a regional instance configuration.
C. 
Set up SQL Server on Compute Engine, using Always On Availability Groups using Windows Failover Clustering. Place
nodes in different subnets.
D. 
Set up SQL Server Always On Availability Groups using Windows Failover Clustering. Place nodes in different zones.
Correct Answer:
 
A 
Comments
learningpv
learningpv
 
Highly Voted
 
4 years, 11 months ago
A seems correct.
"... 
high availability (HA) configuration for Cloud SQL instances... A Cloud SQL instance configured for HA is also called a regional
instance and is located in a primary and secondary zone within the configured region.
In the event of an instance or zone failure, this configuration reduces downtime, and your data continues to be available to client
applications."
upvoted 
63 
times
mrealtor
mrealtor
 
2 years, 8 months ago
You need to set up a Microsoft SQL server. Why are we talking about Cloud SQL
upvoted 
5 
times
tycho
tycho
 
2 years, 4 months ago
and what is Cloud SQL -> a managed service for MySQL, Posrgers, and MS SQL server
upvoted 
10 
times
Ric350
Ric350
 
4 months, 2 weeks ago
Community vote distribution
D (49%)
A (49%)
Other
(2%)Actually, although Cloud SQL offers high availability configurations, it currently doesn't support Microsoft SQL Server as one of
its managed database engines. It primarily focuses on MySQL, PostgreSQL, and SQL Server (but not the full Microsoft SQL
Server). And the question clearly states "you need to set up Microsoft SQL." 
Very tricky question. The answer is D
upvoted 
3 
times
diluviouniv
diluviouniv
 
3 years, 5 months ago
but it says: you need to setup SQL Server
upvoted 
12 
times
learningpv
learningpv
 
4 years, 11 months ago
It applies for MySQL and HA is not available for MS SQL
upvoted 
5 
times
cetanx
cetanx
 
4 years, 6 months ago
It is available, please see;
https://cloud.google.com/sql/docs/sqlserver/high-availability?_ga=2.30855355.-503483612.1582800507
Also a video from Google;
https://youtu.be/vMUpNoukwnM
upvoted 
11 
times
Jos
Jos
 
4 years, 11 months ago
Yes it is available, its in beta, but when creating a "SQL Server 2017 Standard" in Cloud SQL menu you can chose single one or
HA (regional).
upvoted 
3 
times
tocsa
tocsa
 
7 months ago
The problem is that these questions are ancient (talking about StackDriver all the time for example, it was rebranded in
2020!!!). So unfortunately we need to think of "What did the professor think 2-4 years ago" when this question was created.
Otherwise I'd say A is the best all day!
upvoted 
1 
times
AmitAr
AmitAr
 
2 years, 7 months ago
D is correct.
Question is - "no downtime while installing MS SQL" , not on choosing or replacing with GCP product. I agree A is good solution
for this requirements.. however question is not on choosing database.. its for HA.. so I will choose D.
upvoted 
4 
times
SMS
SMS
 
Highly Voted
 
4 years, 9 months ago
Answer is A. Cloud SQL supports SQL Server and selecting high availability provides automatic failover within a region.
upvoted 
30 
times
gcloud007
gcloud007
 
Most Recent
 
1 day, 6 hours ago
Selected Answer: 
A
Not all regions have multiwriter disk, which is what you'll need incase of setting up SQL HA cluster on IaaS. Cloud SQL HA is the
right answer. https://cloud.google.com/sql/docs/sqlserver/high-availability?_ga=2.30855355.-503483612.1582800507#normal
upvoted 
1 
times
JonathanSJ
JonathanSJ
 
5 days, 3 hours ago
Selected Answer: 
D
I will go for D, because "no downtime" is required.
upvoted 
1 
times
plumbig11
plumbig11
 
3 weeks ago
Selected Answer: 
D
thinking about failover D is a better option
upvoted 
1 
times
Srrb20
Srrb20
 
3 weeks, 5 days ago
Selected Answer: 
DSelected Answer: 
D
Cloud SQL with HA is not sufficient because it does not provide the level of control and multi-zone redundancy that Always On
Availability Groups on Compute Engine can deliver. Always On is the recommended solution for enterprise-grade high availability
and disaster recovery in SQL Server deployments.
upvoted 
1 
times
klayytech
klayytech
 
1 month ago
Selected Answer: 
A
Cloud SQL offers high availability configurations, it currently support Microsoft SQL Server
please see;
https://cloud.google.com/sql/docs/sqlserver/high-availability?_ga=2.30855355.-503483612.1582800507
Also a video from Google;
https://youtu.be/vMUpNoukwnM
upvoted 
1 
times
valgorodetsky
valgorodetsky
 
1 month, 1 week ago
Selected Answer: 
D
- HA - has minimal downtime
- windows-server-failover-clustering has zero, but much harder to setup: https://cloud.google.com/compute/docs/tutorials/running-
windows-server-failover-clustering
upvoted 
2 
times
ngeorgiev2
ngeorgiev2
 
1 month, 3 weeks ago
A looks more relevant - Multiple zones (Highly available)
Automatic failover to another zone within your selected region. Recommended for production instances. Increases cost.
upvoted 
1 
times
SerGCP
SerGCP
 
1 month, 3 weeks ago
Selected Answer: 
D
Cloud SQL with HA is good and may reduce downtime. But the requirement is "no dowtime" so D.
upvoted 
2 
times
sim7243
sim7243
 
1 month, 3 weeks ago
A is the option,
upvoted 
1 
times
bd311b9
bd311b9
 
2 months ago
D is the right answer because high availability is created by putting a failover instance in a different zone.
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
The correct approach is: D
Set up SQL Server Always On Availability Groups using Windows Failover Clustering. Place nodes in different zones.
Here’s why this is the best option:
• SQL Server Always On Availability Groups: This solution provides high availability by automatically failing over to another node in
the event of a failure. It’s specifically designed for SQL Server and ensures minimal downtime in case of outages.
• Windows Failover Clustering: By configuring Windows Failover Clustering with Always On Availability Groups, you can achieve
high availability by ensuring that the SQL Server can failover to another node in case of a zone or node failure.
• Placing nodes in different zones: By deploying nodes in different zones within the same region, you ensure that your setup is
protected from any potential zone-level outages. If one zone experiences a failure, the other zone can take over without downtime.
upvoted 
1 
times
selected
selected
 
2 months, 1 week ago
use google docs instead of LLMs
upvoted 
1 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 4 weeks agoJohnJamesB1212
JohnJamesB1212
 
3 months, 4 weeks ago
Selected Answer: 
D
The correct answer is D. Set up SQL Server Always On Availability Groups using Windows Failover Clustering. Place nodes in
different zones.
Here’s why:
SQL Server Always On Availability Groups is a high-availability and disaster recovery solution for SQL Server that works across
multiple zones, ensuring minimal downtime in case of a data center outage within a region.
By placing the nodes in different zones, you ensure that the database remains accessible even if one zone goes down, meeting
the requirement of no downtime in case of a zone failure.
upvoted 
4 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 4 weeks ago
The other options are less suitable for this scenario:
A. Cloud SQL with high availability enabled would not work because Cloud SQL supports MySQL, PostgreSQL, and SQL Server
but has limitations for high-availability setups with SQL Server compared to Always On Availability Groups.
B. Cloud Spanner is a Google-native distributed database solution and not directly related to SQL Server.
C mentions using different subnets, but you specifically need to place the nodes in different zones to ensure availability across
multiple data centers within a region.
Thus, D is the most accurate solution for ensuring no downtime in the event of a zone failure while using Microsoft SQL Server
on GCP.
upvoted 
2 
times
desertlotus1211
desertlotus1211
 
5 months ago
Cloud SQL for SQL Server is a fully managed relational database service from Google Cloud that allows users to set up, maintain,
and manage Microsoft SQL Server databases in the cloud. Cloud SQL also supports MySQL and PostgreSQL
upvoted 
2 
times
desertlotus1211
desertlotus1211
 
5 months ago
Answer A
upvoted 
1 
times
nhatne
nhatne
 
6 months, 1 week ago
Selected Answer: 
A
go for A, Why not use the available option?
upvoted 
1 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
Selected Answer: 
A
It is A,
Who say D, better not to work in cloud, but yes in on premises windows servers LOL
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #61
The development team has provided you with a Kubernetes Deployment file. You have no infrastructure yet and need to deploy
the application. What should you do? 
A. 
Use gcloud to create a Kubernetes cluster. Use Deployment Manager to create the deployment.
B. 
Use gcloud to create a Kubernetes cluster. Use kubectl to create the deployment. 
Most Voted
C. 
Use kubectl to create a Kubernetes cluster. Use Deployment Manager to create the deployment.
D. 
Use kubectl to create a Kubernetes cluster. Use kubectl to create the deployment.
Correct Answer:
 
B 
Comments
MeasService
MeasService
 
Highly Voted
 
4 years, 8 months ago
It has to be B. gcloud for creating cluster and kubectl for creating deployment
upvoted 
54 
times
KouShikyou
KouShikyou
 
Highly Voted
 
4 years, 8 months ago
May I ask why C is correct?
I thought B was correct.
upvoted 
26 
times
nitinz
nitinz
 
3 years, 4 months ago
B, gcloud to manage GKE and to manage pods use kubctl.
upvoted 
2 
times
kumarp6
kumarp6
 
3 years, 8 months ago
B is correct, when you create a nodes in GKE you use gcloud rather than kubectl...
upvoted 
4 
times
res3
res3
 
4 years ago
yeap, gcloud command to create K8s cluster https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster
upvoted 
4 
times
Community vote distribution
B (100%)upvoted 
4 
times
tartar
tartar
 
3 years, 11 months ago
B is ok
upvoted 
9 
times
simiramis221
simiramis221
 
Most Recent
 
6 months, 2 weeks ago
Answer is B %100
upvoted 
1 
times
vamgcp
vamgcp
 
1 year, 5 months ago
Create a Google Kubernetes Engine (GKE) cluster: You can use the Google Cloud Console or the gcloud command-line tool to
create a GKE cluster, which will provide the underlying infrastructure for running your application.
Deploy the application to the cluster: You can use the kubectl command-line tool to apply the Kubernetes Deployment file provided
by the development team to the cluster.kubectl apply -f deployment.yaml
upvoted 
4 
times
megumin
megumin
 
1 year, 8 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
B
is the correct answer https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
B is the correct answer cluster https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
B is right
upvoted 
1 
times
SAMBIT
SAMBIT
 
2 years, 3 months ago
Kubctle comes live only when cluster has been created in the cloud console using cloud command
upvoted 
2 
times
ghadxx
ghadxx
 
2 years, 5 months ago
Selected Answer: 
B
Deployment Manager is used to automate the process of provisioning infrastructure. Therefore, gcloud and Deployment Manager
do the same thing. Meanwhile, kubectl is used to run commands against an already created cluster.
upvoted 
9 
times
haroldbenites
haroldbenites
 
2 years, 7 months ago
Go for B.
gcloud for create clusters.
kubectl is used when the cluster already has been created. For example to create deployments.
Kubectl has configured a config file where is specified the default cluster.
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 7 months ago
B is correct
upvoted 
1 
times
Zinhle
Zinhle
 
2 years, 8 months agoHi all may someone please share the link for the bank of questions because I cannot seem to locate them.
thank you
upvoted 
1 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
B – use gcloud to create cluster, use kubectl to create a deployment.
https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster
In fact, kubectl run creates a deployment.
https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app
upvoted 
2 
times
ale183
ale183
 
2 years, 9 months ago
Question for all , do we know if only new questions are part of the bank for new exam? 
Have any of the old questions appeared on
new exam?
upvoted 
3 
times
xaliq
xaliq
 
2 years, 9 months ago
B is corrent
upvoted 
1 
times
Raja101
Raja101
 
2 years, 9 months ago
Why not A ?
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #62
You need to evaluate your team readiness for a new GCP project. You must perform the evaluation and create a skills gap plan
which incorporates the business goal of cost optimization. Your team has deployed two GCP projects successfully to date.
What should you do? 
A. 
Allocate budget for team training. Set a deadline for the new GCP project.
B. 
Allocate budget for team training. Create a roadmap for your team to achieve Google Cloud certification based on job
role. 
Most Voted
C. 
Allocate budget to hire skilled external consultants. Set a deadline for the new GCP project.
D. 
Allocate budget to hire skilled external consultants. Create a roadmap for your team to achieve Google Cloud
certification based on job role.
Correct Answer:
 
B 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 1 month ago
B is correct.
upvoted 
46 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Yes it is
upvoted 
1 
times
passnow
passnow
 
5 years ago
I would agree with you because the question says create a skills gap plan
upvoted 
4 
times
nitinz
nitinz
 
3 years, 10 months ago
B, looks like cooked up question. Not gonna show up on actual test. Even if it does show up, its not market.
upvoted 
10 
times
 
3 years, 3 months ago
Community vote distribution
B (81%)
Other (19%)ACE_ASPIRE
ACE_ASPIRE
 
3 years, 3 months ago
exactly
upvoted 
1 
times
mawsman
mawsman
 
Highly Voted
 
4 years, 10 months ago
I think it's B. "You must perform the evaluation and create a skills gap plan incorporates the business goal of cost optimization."
The goal is to cost optimize - they might have deployed 2 projects but are they cost optimized? I think the only way to evaluate the
skills gap in cost optimization is to make them get certified and use the results to determine cost optimization skills gap. Quickly
pushing another project deadline would not help with cost optimization.
upvoted 
21 
times
Smart
Smart
 
4 years, 10 months ago
Agreed. How is setting up a GCP project deadline helping towards skill gap and cost optimization.
upvoted 
3 
times
maxdanny
maxdanny
 
Most Recent
 
4 months ago
Selected Answer: 
B
Explanation:
Training and certification based on specific job roles (e.g., Cloud Architect, Data Engineer) will ensure your team has the
necessary skills for the new GCP project and will help align their capabilities with cost optimization strategies.
Since your team has already successfully deployed two GCP projects, upskilling them with targeted training is a more cost-
effective and sustainable solution than relying on external consultants.
This option balances both team readiness and the business goal of cost optimization, while building long-term internal expertise.
upvoted 
3 
times
joecloud12
joecloud12
 
4 months, 4 weeks ago
Selected Answer: 
B
b is obvious..u need to address the skill gap
upvoted 
1 
times
geekywitcher
geekywitcher
 
6 months ago
Selected Answer: 
B
B is correct.
upvoted 
1 
times
Robert0
Robert0
 
7 months, 1 week ago
Selected Answer: 
B
B makes the most sense. If readiness is the goal, it makes sense to invest in formation
upvoted 
1 
times
GianpiGale
GianpiGale
 
8 months, 2 weeks ago
C and D excluded because the teams have successfully deployed 2 GCP project to date, seems best option is to train them
upvoted 
1 
times
Teckexam
Teckexam
 
11 months, 2 weeks ago
Selected Answer: 
B
B: this is logical answer. Setting project deadline or getting external consultants is not a good planning option.
upvoted 
1 
times
JPA210
JPA210
 
1 year, 2 months ago
Answer A is more realistic. Certification is important, but not to be in a question in the exam.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
B
You need to evaluate your team readiness for a new GCP project. You must perform the evaluation and create a skills gap plan
which incorporates the business goal of cost optimization.which incorporates the business goal of cost optimization.
The business Goal (Long Term Goal) is cost optimization, hence investment will be needed. Option A says setting up deadlines, it
will not help in cost optimization. But giving training and obtaining certification will do.
upvoted 
2 
times
piiizu
piiizu
 
1 year, 3 months ago
In the short run, A seems like the best option because this would enable a quicker transition to the cloud which is the business
Goal of cost optimization. In terms of cost, certification will cost the organisation more.
In the long run, B is the right option I would recommend as an architect. This would reduce the skill gap, increase proficiency and
ensure repeatability. The organisation will incur more but will reap the reward... Except they have a bad culture and the staff resign
right after getting the certification. Lol.
upvoted 
2 
times
Frusci
Frusci
 
1 year, 3 months ago
Selected Answer: 
B
B, better to train your team than hire, and setting a deadline would defeat the purpose of evaluating the team's readiness.
upvoted 
2 
times
PKookNN
PKookNN
 
1 year, 6 months ago
Selected Answer: 
B
B is a better answer
upvoted 
2 
times
kaaiden
kaaiden
 
1 year, 7 months ago
Selected Answer: 
A
GCP partnership need 3 project
upvoted 
1 
times
TheCloudGuruu
TheCloudGuruu
 
1 year, 7 months ago
Selected Answer: 
B
B is the right answer
upvoted 
2 
times
grejao
grejao
 
1 year, 9 months ago
It shows more like a trick than a really question.
Furthermore are this question relevant for certification?
upvoted 
1 
times
telp
telp
 
1 year, 9 months ago
Selected Answer: 
B
You are in a google exam. Always choose certification for your teams.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #63
You are designing an application for use only during business hours. For the minimum viable product release, you'd like to use
a managed product that automatically `scales to zero` so you don't incur costs when there is no activity. 
Which primary compute resource should you choose? 
A. 
Cloud Functions 
Most Voted
B. 
Compute Engine
C. 
Google Kubernetes Engine
D. 
AppEngine flexible environment
Correct Answer:
 
A 
Comments
abirroy
abirroy
 
Highly Voted
 
2 years, 3 months ago
Selected Answer: 
A
A. Cloud Functions - managed service scales down to 0
B. Compute Engine - not a managed service
C. Google Kubernetes Engine - not a managed service and wont scale down to 0
D. AppEngine flexible environment - managed service but wont scale down to 0
upvoted 
26 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Agree with A
upvoted 
3 
times
NiveusSol
NiveusSol
 
10 months, 1 week ago
GKE is a managed service.
upvoted 
4 
times
victory108
victory108
 
Highly Voted
 
3 years ago
A. Cloud Functions
upvoted 
11 
times
Community vote distribution
A (95%)
B (5%)upvoted 
11 
times
vpatiltech
vpatiltech
 
2 years, 10 months ago
Cloud function is more for event driven computing. We surely need k8s or app engine. Flex always have 1 instance running. So
GKE should be the option
upvoted 
5 
times
YAS007
YAS007
 
2 years, 8 months ago
from the doc :https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
Note: If you specify a minimum of zero nodes, an idle node pool can scale down completely. However, at least one node must
always be available in the cluster to run system Pods.
upvoted 
1 
times
6721sora
6721sora
 
2 years, 4 months ago
But no cost for System/Control nodes
upvoted 
1 
times
Toothpick
Toothpick
 
Most Recent
 
5 months, 1 week ago
The only correct answer here is cloud run which isn't listed as an option.
(A) can scale to zero, but is mean't for event driven workloads,not full sized applications, especially in the routing department
(B) Needs manual stoppage, managed groups also keep one instance alive always
(C) You're billed for the control plane even if the node pools are empty
(D) Flex can't scale down to zero currently
upvoted 
4 
times
MiguelMiguel
MiguelMiguel
 
1 year, 2 months ago
This question is confused. It's seems that the answer is cloud function because is the only platform that can scale 0 natively. If you
want to scale to zero k8s, you have to create a solution based in scheduler, function and pub sub. Compute Engine is not a
managed service, and app engine flex doesn't scale to 0. I would go to A but it's not clear.
upvoted 
1 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
agree A
upvoted 
1 
times
grejao
grejao
 
1 year, 9 months ago
Selected answer: D
I choosed D, it appears that we do not. have a right answer here. 
A. Cloud Functions - 
its more for event driven computing, not for full application
B. Compute Engine - not a managed service
C. Google Kubernetes Engine - not a managed service and wont scale down to 0
D. AppEngine flexible environment - Only Standard App Engine can scale to 0.
upvoted 
3 
times
Bedmed
Bedmed
 
1 year, 8 months ago
yes, but only Standard environment, not flexible environment
upvoted 
3 
times
GCPAnji
GCPAnji
 
1 year, 9 months ago
For an application that is only used during business hours and needs to scale to zero during periods of inactivity to minimize costs,
a good choice would be a Function-as-a-Service (FaaS) product like AWS Lambda or Google Cloud Functions.
upvoted 
1 
times
jlambdan
jlambdan
 
1 year, 9 months ago
Selected Answer: 
B
A a cloud function is not an application
B compute engine via MIG you can use an autoscaler with a schedule.
https://cloud.google.com/compute/docs/autoscaler/scaling-schedulesYou then can go from 0 to more instance when required
C K8s is two complex for this. 
You can have an autoscaler for the cluster in order to get the node number to 0, but it require the node to have no pods running. So
you have to configure your deployments and all your workload to scale to 0 too.
Other interference will be pod affinity, anti-affinity, disruption budget or unmanaged pod preventing pod eviction from node. But if
the pod is not evicted, the node cannot be deleted.
"autoscaler respects scheduling and eviction rules set on Pods. These restrictions can prevent a node from being deleted by the
autoscaler. " => https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#scheduling-and-disruption
D flexible cannot scale to 0
upvoted 
1 
times
jrisl1991
jrisl1991
 
1 year, 2 months ago
App Engine is not an application on its own either, nor B or C options. B and C are not managed products; "managed" means that
all of the infrastructure work (such as setting up an autoscheduler or autoscaler) will be managed by Google and not by the user.
Using an IaaS solution for a managed use case is already contradictory. 
App Engine Flex will always have at least 1 instance running. 
While Cloud Functions is not an application itself, it's the only resource that can scale to 0. Plus, the requirement states that they
will have an application running during business hours, but they never mentioned that the app will only be running in GCP or in
Cloud Functions. They could be running their app anywhere else, and only call Cloud Functions when needed.
upvoted 
1 
times
theBestStudent
theBestStudent
 
1 year ago
That is not the concept of managed: https://cloud.google.com/blog/topics/developers-practitioners/serverless-vs-fully-managed-
whats-difference
upvoted 
1 
times
CGS22
CGS22
 
1 year, 10 months ago
Selected Answer: 
A
A. Cloud Functions
upvoted 
2 
times
WFCheong
WFCheong
 
1 year, 12 months ago
Why not D? App Engines also can scale down to zero when there is no activity. https://cloud.google.com/appengine/docs/the-
appengine-environments#:~:text=Intended%20to%20run%20for%20free,when%20there%20is%20no%20traffic. Intended to run for
free or at very low cost, where you pay only for what you need and when you need it. For example, your application can scale to 0
instances when there is no traffic.
upvoted 
3 
times
CkWongCk
CkWongCk
 
1 year, 11 months ago
There are 2 mode for App engines, standard and flexible.
The standard environment can scale from zero instances up to thousands very quickly. In contrast, the flexible environment must
have at least one instance running for each active version and can take longer to scale out in response to traffic.
upvoted 
4 
times
thamaster
thamaster
 
2 years ago
Selected Answer: 
A
the only to scale to 0 is A
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is A. Cloud Functions.
Cloud Functions is a serverless compute service that lets you run code without provisioning or managing infrastructure. One of the
key benefits of using Cloud Functions is that it automatically scales to meet the demands of your workload and automatically
scales down to zero when there is no activity. This means that you only pay for the compute resources that you consume, which
can help to reduce costs when your application is not in use. Additionally, Cloud Functions is easy to use and allows you to deploy
your code with minimal effort, making it a good choice for a minimum viable product release.your code with minimal effort, making it a good choice for a minimum viable product release.
upvoted 
3 
times
backhand
backhand
 
2 years, 4 months ago
vote A
this is easy one. key word: managed product, scales to zero
scale to zero: app engine standard, cloud function
upvoted 
2 
times
vijbabu
vijbabu
 
2 years, 5 months ago
Selected Answer: 
B
Answer is B
upvoted 
1 
times
Dhiraj03
Dhiraj03
 
2 years, 6 months ago
Selected Answer: 
A
Cloud Functions can scale to zero when not in use
upvoted 
3 
times
Sskhan
Sskhan
 
2 years, 11 months ago
Selected Answer: 
A
Answer is A, As Option D App engine flexible can have minimum 1 instance active.
upvoted 
4 
times
Sskhan
Sskhan
 
2 years, 11 months ago
Answer is A, 
As Option D App engine flexible can have minimum 1 instance active.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #64
You are creating an App Engine application that uses Cloud Datastore as its persistence layer. You need to retrieve several
root entities for which you have the identifiers. You want to minimize the overhead in operations performed by Cloud
Datastore. What should you do? 
A. 
Create the Key object for each Entity and run a batch get operation 
Most Voted
B. 
Create the Key object for each Entity and run multiple get operations, one operation for each entity
C. 
Use the identifiers to create a query filter and run a batch query operation
D. 
Use the identifiers to create a query filter and run multiple query operations, one operation for each entity
Correct Answer:
 
A 
Comments
shashu07
shashu07
 
Highly Voted
 
4 years ago
Correct Answer: A
Create the Key object for each Entity and run a batch get operation
https://cloud.google.com/datastore/docs/best-practices 
Use batch operations for your reads, writes, and deletes instead of single operations. Batch operations are more efficient because
they perform multiple operations with the same overhead as a single operation.
Firestore in Datastore mode supports batch versions of the operations which allow it to operate on multiple objects in a single
Datastore mode call.
Such batch calls are faster than making separate calls for each individual entity because they incur the overhead for only one
service call. If multiple entity groups are involved, the work for all the groups is performed in parallel on the server side.
upvoted 
49 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
works fine .. A is right
upvoted 
2 
times
AWS56
AWS56
 
Highly Voted
 
4 years, 5 months ago
Agree A
upvoted 
7 
times
de1001c
de1001c
 
Most Recent
 
1 month ago
Community vote distribution
A (100%)de1001c
de1001c
 
Most Recent
 
1 month ago
Keep in mind that datastore is discontinued, Firestore being the recommended alternative.
upvoted 
4 
times
don_v
don_v
 
5 months, 3 weeks ago
According to 
"A. Create the Key object for each Entity and run a batch get operation"
which is wrong as the key is already created for each entity whenever it's persisted.
I believe the correct answer is C, -- to use a bulk query (a.k.a. "batch" in their terms).
You need a query with a criteria anyway to find a resultset, and not just a fetch by "get" operation to load by surrogate keys.
upvoted 
3 
times
vamgcp
vamgcp
 
1 year, 5 months ago
By using the "lookup by key" API of Cloud Datastore, you can minimize the overhead in operations performed by Cloud Datastore
and optimize the performance of your App Engine application.
from google.cloud import datastore
client = datastore.Client()
keys = [client.key('EntityKind', id) for id in entity_ids]
entities = client.get_multi(keys)
upvoted 
3 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
A. Create the Key object for each Entity and run a batch get operation
To minimize the overhead in operations performed by Cloud Datastore, you should use the batch get operation to retrieve multiple
entities in a single API call. To do this, you should create a Key object for each entity that you want to retrieve, then pass the Key
objects to the batch get operation. This will allow you to retrieve multiple entities in a single API call, reducing the number of
operations performed by Cloud Datastore and improving the efficiency of your application.
upvoted 
6 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option B, running multiple get operations, one operation for each entity, would not be an efficient way to retrieve the entities
because it would require multiple API calls to Cloud Datastore, which would increase the overhead and decrease the efficiency of
the application.
Option C, using the identifiers to create a query filter and running a batch query operation, would not be an efficient way to retrieve
the entities because it would require performing a query operation, which is generally more expensive than a get operation.
Option D, using the identifiers to create a query filter and running multiple query operations, one operation for each entity, would
not be an efficient way to retrieve the entities because it would require performing multiple query operations, which are generally
more expensive than get operations.
upvoted 
6 
times
megumin
megumin
 
1 year, 8 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is correct https://cloud.google.com/datastore/docs/best-practices#api_calls
upvoted 
2 
times
RitwickKumar
RitwickKumar
 
1 year, 10 months ago
Selected Answer: 
A
https://cloud.google.com/datastore/docs/concepts/entities#datastore-datastore-batch-lookup-python
upvoted 
2 
times
haroldbenites
haroldbenites
 
2 years, 7 months ago
go for A.go for A.
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 7 months ago
A is the right answer
upvoted 
2 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
2 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
A – create a key object for each entity, and run a batch get operations.
See Batch Operations section here: https://cloud.google.com/datastore/docs/concepts/entities
var keys = new Key[] { _keyFactory.CreateKey(1), _keyFactory.CreateKey(2) };
var tasks = _db.Lookup(keys[0], keys[1]);
1 and 2 are identifiers of the Key. Check Key / Identifier definition on the same link (top of that page)
Such batch calls are faster than making separate calls for each individual entity because they incur the overhead for only one
service call.
upvoted 
1 
times
victory108
victory108
 
3 years, 1 month ago
A. Create the Key object for each Entity and run a batch get operation
upvoted 
1 
times
un
un
 
3 years, 1 month ago
A is correct
upvoted 
1 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is A
upvoted 
1 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #65
You need to upload files from your on-premises environment to Cloud Storage. You want the files to be encrypted on Cloud
Storage using customer-supplied encryption keys. What should you do? 
A. 
Supply the encryption key in a .boto configuration file. Use gsutil to upload the files.
B. 
Supply the encryption key using gcloud config. Use gsutil to upload the files to that bucket.
C. 
Use gsutil to upload the files, and use the flag --encryption-key to supply the encryption key. 
Most Voted
D. 
Use gsutil to create a bucket, and use the flag --encryption-key to supply the encryption key. Use gsutil to upload the
files to that bucket.
Correct Answer:
 
C 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
In GCP document, key could be configured in .boto.
I didn't find information show gsutil suppots flag "--encryption-key".
https://cloud.google.com/storage/docs/encryption/customer-supplied-keys
upvoted 
46 
times
JaimeMS
JaimeMS
 
6 months, 3 weeks ago
The documentation is here:
https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt
Option C is correct. You can upload a file using customer-supplied encryption with the command:
gcloud storage cp SOURCE_DATA gs://BUCKET_NAME/OBJECT_NAME --encryption-key=YOUR_ENCRYPTION_KEY
upvoted 
13 
times
tartar
tartar
 
4 years, 5 months ago
A is ok
upvoted 
16 
times
nitinz
nitinz
 
3 years, 10 months ago
Community vote distribution
C (51%)
A (44%)
D (5%)A is correct
upvoted 
4 
times
kumarp6
kumarp6
 
4 years, 2 months ago
.boto file with encryption key, but it will works for individual users, every user should update their own .boto with same key. Also
while retrieving you should use the same key to decryption.
upvoted 
3 
times
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
I agree, A.(https://cloud.google.com/storage/docs/gsutil/addlhelp/UsingEncryptionKeys#generating-customer-supplied-encryption-
keys)
upvoted 
18 
times
klayytech
klayytech
 
Most Recent
 
6 days, 14 hours ago
Selected Answer: 
C
The correct answer is C. Here's why:
Customer-Supplied Encryption Keys (CSEK): When using CSEK, you provide the encryption key yourself. Google doesn't store
your key on their servers. You're responsible for managing and protecting it.
gsutil and --encryption-key: The gsutil command-line tool is the primary way to interact with Cloud Storage. To use CSEK with
gsutil, you use the --encryption-key flag directly with the upload command. This flag takes the base64 encoded encryption key as
its valu
upvoted 
1 
times
rrope
rrope
 
1 week, 1 day ago
Selected Answer: 
C
Customer-Supplied Encryption Keys (CSEK) are provided on a per-request basis. This means you provide the key during the
upload operation itself, not when creating the bucket or through persistent configuration files.
gsutil is the command-line tool for interacting with Cloud Storage. The --encryption-key flag specifically allows you to provide the
base64 encoded encryption key when uploading objects.
upvoted 
2 
times
rahuld19
rahuld19
 
1 week, 1 day ago
Selected Answer: 
A
right answer is A
upvoted 
1 
times
mahi_h
mahi_h
 
2 weeks, 3 days ago
Selected Answer: 
D
I see option D is not even discussed. The question said "upload files", meaning multiple object. Isn't the encrypted bucked creation
a secured way to store them in cloud storage?
upvoted 
2 
times
kip21
kip21
 
2 weeks, 4 days ago
Selected Answer: 
A
[GSUtil]
check_hashes
content_language
decryption_key1 ... 100
default_api_version
disable_analytics_prompt
encryption_key
upvoted 
1 
times
deep316
deep316
 
3 weeks, 2 days ago
Selected Answer: 
C
Option C: Use gsutil to upload the files and use the flag --encryption-key to supply the encryption key. This is the correct approach,
as it allows you to specify the CSEK directly at the time of upload, ensuring that your files are encrypted using your provided key.
upvoted 
2 
timesklayytech
klayytech
 
3 weeks, 5 days ago
Selected Answer: 
D
D. Use gsutil to create a bucket, and use the flag --encryption-key to supply the encryption key. Use gsutil to upload the files to that
bucket.
This option provides the most comprehensive and secure approach:
Create an encrypted bucket:
Use gsutil mb -b location gs://your-bucket-name --encryption-key=your_encryption_key
This ensures that all objects uploaded to this bucket will be encrypted with your provided key.
Upload files to the encrypted bucket:
Use gsutil cp your_local_file gs://your-bucket-name
By following this approach, you guarantee that your files are encrypted both at rest and in transit on Cloud Storage, providing a
robust security posture.
The other options either lack the encryption key specification or do not create an encrypted bucket, leaving your data vulnerable.
upvoted 
3 
times
desertlotus1211
desertlotus1211
 
1 month, 1 week ago
Selected Answer: 
A
The boto configuration file in Google Cloud Platform (GCP) controls how the gsutil command behaves:
Setting up gsutil
You can use the boto configuration file to set up gsutil to work through a proxy.
Using encryption keys
You can use the boto configuration file to use customer-managed or customer-supplied encryption keys.
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
1 month, 1 week ago
.boto is smoother to use consistently...
upvoted 
1 
times
icarogsm
icarogsm
 
1 month, 1 week ago
Selected Answer: 
A
A! I agree that the boto file sounds better
upvoted 
1 
times
46affda
46affda
 
1 month, 2 weeks ago
Option C is correct - 
please refer https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#command-line
upvoted 
1 
times
sim7243
sim7243
 
1 month, 3 weeks ago
Selected Answer: 
A
option A, 
Option A allows you to configure the .boto configuration file with the encryption key. This configuration file is used by gsutil to apply
settings, including encryption key management. By placing the encryption key in the .boto file, you ensure that every time gsutil is
used, it automatically supplies the correct key for encrypting files as they are uploaded to Cloud Storage.
Option C: The --encryption-key flag does not exist for gsutil. Instead, gsutil uses the .boto configuration file or the -o flag for
customer-supplied encryption keys.
upvoted 
2 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Answer: C
Use gsutil to upload the files, and use the flag -encryption-key to supply the encryption key.
Here’s why this is the best option:
1. Using gsutil: gsutil is the command-line tool for interacting with Google Cloud Storage, and it supports options for specifying
customer-supplied encryption keys directly during the upload process.customer-supplied encryption keys directly during the upload process.
2. Flag -encryption-key: The -encryption-key flag allows you to specify the encryption key at the time of uploading the files. This
ensures that the files are encrypted with the provided key as they are being uploaded to Cloud Storage.
upvoted 
3 
times
AlainBas
AlainBas
 
3 months ago
A is correct
upvoted 
1 
times
dfizban
dfizban
 
3 months ago
Selected Answer: 
C
Option C is correct.
upvoted 
3 
times
3fd692e
3fd692e
 
3 months ago
Selected Answer: 
C
Straight for the docs: https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt
upvoted 
5 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #66
Your customer wants to capture multiple GBs of aggregate real-time key performance indicators (KPIs) from their game servers
running on Google Cloud Platform and monitor the KPIs with low latency. How should they capture the KPIs? 
A. 
Store time-series data from the game servers in Google Bigtable, and view it using Google Data Studio.
B. 
Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console
to view them. 
Most Voted
C. 
Schedule BigQuery load jobs to ingest analytics files uploaded to Cloud Storage every ten minutes, and visualize the
results in Google Data Studio.
D. 
Insert the KPIs into Cloud Datastore entities, and run ad hoc analysis and visualizations of them in Cloud Datalab.
Correct Answer:
 
B 
Comments
suryalsp
suryalsp
 
Highly Voted
 
5 years ago
Ans is B. Data studio cannot be used with BigTable
https://datastudio.google.com/datahttps://datastudio.google.com/data
upvoted 
33 
times
Raja101
Raja101
 
3 years, 3 months ago
A is correct
upvoted 
4 
times
ErenYeager
ErenYeager
 
2 years, 1 month ago
As of today you can
upvoted 
7 
times
anshumankmr80
anshumankmr80
 
2 years ago
Source?
https://lookerstudio.google.com/data?search=big
upvoted 
2 
times
Community vote distribution
B (97%)
A
(3%)upvoted 
2 
times
HD2023
HD2023
 
1 year, 9 months ago
That’s BigQuery, not BigTable, no?
upvoted 
2 
times
jrisl1991
jrisl1991
 
1 year, 2 months ago
Looker is not the same as Data Studio. I know it came to replace it, but these questions are kind of old, so unless it clearly
says "looker", I wouldn't take both as the same.
upvoted 
2 
times
kolcsarzs
kolcsarzs
 
Highly Voted
 
5 years ago
correct is B
upvoted 
12 
times
nareshthumma
nareshthumma
 
Most Recent
 
2 months, 1 week ago
Answer is B
upvoted 
1 
times
belouh
belouh
 
2 months, 1 week ago
Selected Answer: 
B
Bigtable is not real time solution
upvoted 
1 
times
kip21
kip21
 
11 months, 3 weeks ago
B - Correct
A - It is not a real-time solution
upvoted 
1 
times
Saxena_Vibhor
Saxena_Vibhor
 
10 months, 2 weeks ago
Why is A not a real time solution? https://cloud.google.com/bigtable/docs/integrations#opentsdb 
it says: OpenTSDB is a time-series database that can use Bigtable for storage. Monitoring time-series data with OpenTSDB on
Bigtable and GKE shows how to use OpenTSDB to collect, record, and monitor time-series data on Google Cloud. The
OpenTSDB documentation provides additional information to help you get started.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
B
BigTable doesn't integrate with Data Studio
https://cloud.google.com/bigtable/docs/integrations
upvoted 
1 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
B is correct, you should create custom KPI in Stack Driver
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
B. Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to
view them.
To capture multiple GBs of aggregate real-time KPIs from game servers running on Google Cloud Platform and monitor them with
low latency, the customer should output custom metrics to Stackdriver from the game servers. Stackdriver allows you to collect
and store custom metrics, as well as view and analyze them in real-time using the Stackdriver Monitoring Console. The customer
can create a Dashboard in the Monitoring Console to view the KPIs and monitor them with low latency.
upvoted 
10 
times
omermahgoub
omermahgoub
 
2 years agoOption A, storing time-series data in Bigtable and viewing it using Data Studio, would not be suitable for capturing and monitoring
real-time KPIs with low latency. Bigtable is a scalable NoSQL database that is optimized for large-scale batch processing, and
Data Studio is a visualization tool that is not designed for real-time data analysis.
Option C, scheduling BigQuery load jobs to ingest analytics files uploaded to Cloud Storage every ten minutes and visualizing the
results in Data Studio, would not be suitable for capturing and monitoring real-time KPIs with low latency. BigQuery is a data
warehouse that is optimized for batch processing, and it is not designed for real-time data analysis.
Option D, inserting the KPIs into Cloud Datastore entities and running ad hoc analysis and visualizations of them in Cloud
Datalab, would not be suitable for capturing and monitoring real-time KPIs with low latency. Cloud Datastore is a NoSQL
document database, and Cloud Datalab is a data analysis and visualization tool that is not designed for real-time data analysis.
upvoted 
9 
times
jlambdan
jlambdan
 
1 year, 9 months ago
big table is not for batch. It is used in IOT...
https://cloud.google.com/bigtable
upvoted 
4 
times
Raja101
Raja101
 
2 years, 1 month ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
B
B is correct as Data studio does not support bigtable as a source
upvoted 
4 
times
zr79
zr79
 
2 years, 2 months ago
KPI, SLO,SLI all those work with observability which stackdriver
upvoted 
2 
times
jay9114
jay9114
 
2 years, 2 months ago
The reference provided seems irrelevant to this question.
upvoted 
1 
times
bolington
bolington
 
2 years, 7 months ago
A is the correct answer, the key word here, real time and low latency.
upvoted 
1 
times
Nirca
Nirca
 
2 years, 8 months ago
Selected Answer: 
B
BigTable has no connection to data studio. 
https://datastudio.google.com/data?search=Big
upvoted 
6 
times
BeetleJuice
BeetleJuice
 
2 years, 11 months ago
Selected Answer: 
B
B, it is
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
2 years, 12 months ago
I don't think there is a correct answer, but B looks correct in this.
If use Bigquery, then A is correct .If use Bigquery, then A is correct .
C is not for realtime.
D Datastore is for small usecase.
Keywords 'real time' ,'analytics' 
https://events.withgoogle.com/solution-design-pattern-gaming/analytics-pattern/
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #67
You have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to operate in
production. You want to monitor and maximize machine utilization. You also want to reliably deploy new versions of the
application. Which set of steps should you take? 
A. 
Perform the following: 1. Create a managed instance group with f1-micro type machines. 2. Use a startup script to clone
the repository, check out the production branch, install the dependencies, and start the Python app. 3. Restart the
instances to automatically deploy new production releases.
B. 
Perform the following: 1. Create a managed instance group with n1-standard-1 type machines. 2. Build a Compute
Engine image from the production branch that contains all of the dependencies and automatically starts the Python app. 3.
Rebuild the Compute Engine image, and update the instance template to deploy new production releases.
C. 
Perform the following: 1. Create a Google Kubernetes Engine (GKE) cluster with n1-standard-1 type machines. 2. Build a
Docker image from the production branch with all of the dependencies, and tag it with the version number. 3. Create a
Kubernetes Deployment with the imagePullPolicy set to 'IfNotPresent' in the staging namespace, and then promote it to
the production namespace after testing. 
Most Voted
D. 
Perform the following: 1. Create a GKE cluster with n1-standard-4 type machines. 2. Build a Docker image from the
master branch with all of the dependencies, and tag it with 'latest'. 3. Create a Kubernetes Deployment in the default
namespace with the imagePullPolicy set to 'Always'. Restart the pods to automatically deploy new production releases.
Correct Answer:
 
C 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
C is correct, need "ifnotpresent"when uploads to container registry
upvoted 
41 
times
medi01
medi01
 
1 year, 8 months ago
ifnotpresent won't pull new version.
upvoted 
4 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
Community vote distribution
C (55%)
A (36%)
Other (9%)heretolearnazure
heretolearnazure
 
1 year, 4 months ago
yes i agree
upvoted 
1 
times
TosO
TosO
 
Highly Voted
 
5 years, 1 month ago
C is the best choice. You can create a k8s cluster with just one node and use a different namespaces for staging and production.
In staging, you will test the changes
upvoted 
23 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Agreed
upvoted 
1 
times
Gayathri1608
Gayathri1608
 
Most Recent
 
1 month, 1 week ago
Selected Answer: 
C
i think suitable for the given scenario
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Answer is C
upvoted 
1 
times
44fa527
44fa527
 
4 months, 1 week ago
Selected Answer: 
C
should be option C because if you are working in real world, GKE is the best solution for such a case. Furthermore, its reliable,
scalable, flexible, at least the best option among the other three.
upvoted 
1 
times
cai_engineer
cai_engineer
 
4 months, 1 week ago
Selected Answer: 
A
Ngl it's A. Don't use GKE, it won't schedule the deployment as most of the resources already occupied by kube-system
upvoted 
1 
times
cai_engineer
cai_engineer
 
4 months, 1 week ago
Also you can deploy COS Containerd in a VM
upvoted 
1 
times
awsgcparch
awsgcparch
 
5 months, 1 week ago
Selected Answer: 
D
imagePullPolicy: Always ensures that the latest version of the image is always pulled, which guarantees that the most recent code
is deployed.
Restarting pods ensures that the new version is deployed without requiring manual intervention.
upvoted 
1 
times
krokskan
krokskan
 
10 months, 1 week ago
Selected Answer: 
B
B because Kubernetes will be overkill and A is not reliable
upvoted 
1 
times
Gall
Gall
 
11 months ago
Selected Answer: 
C
A is wrong as after the restart the script will be rerun and fetch the code directly from the repo (even if production). The load of the
massive number of dependencies will take a lot of timee, and the application version will be fuzzy.
upvoted 
1 
times
moumou
moumou
 
11 months, 1 week ago
C is correct, B (instance template cannote be updated once created.C is correct, B (instance template cannote be updated once created.
upvoted 
2 
times
kip21
kip21
 
11 months, 3 weeks ago
C - Correct
upvoted 
1 
times
AWS_Sam
AWS_Sam
 
12 months ago
The correct answer is C. Because it is the only option that RELIABLY tests the app in staging before it is applied to production.
Remember that one of the requirements in the question is to reliably deploy the app.
upvoted 
2 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
A
You don't need GKE for 0.1 CPU, only A meet hte needs
upvoted 
3 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
A
For 0.1 CPU I will never use GKE, considering the associated cost with control plane and not even one option in the question
mentioning micro instances for the node pool
upvoted 
4 
times
mastrrrr
mastrrrr
 
1 year, 1 month ago
Selected Answer: 
A
When we read the question - "0.1 CPU cores and 128 MB of memory" to operate in production. You want to monitor and
"maximize machine utilization"... Answer A should be a fit based on the question details. Would GKE for tiny application be
overkill?
upvoted 
4 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
Selected Answer: 
C
python app on compute engine is a disastrous architecture. C is the correct architecture which tests the app before putting to prod
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
C
You should use GKE, because your can scale up and down based on your demand. Also you can specifiy the resource size like
0.1 CPU and 128 MB of memory per Pod. 
Secondly, Kubernetes Deployment with the imagePullPolicy set to “IfNotPresent” in the staging namespace, and then promote it to
production namespace after testing. is best practice.
upvoted 
9 
times
A21325412
A21325412
 
1 year, 1 month ago
Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
Correct. Because we can spec the resources on our pods is why C is chosen over A (f1-micro). This is what allows us to
"maximize machine utilization"!
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #68
Your company wants to start using Google Cloud resources but wants to retain their on-premises Active Directory domain
controller for identity management. 
What should you do? 
A. 
Use the Admin Directory API to authenticate against the Active Directory domain controller.
B. 
Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML
SSO. 
Most Voted
C. 
Use Cloud Identity-Aware Proxy configured to use the on-premises Active Directory domain controller as an identity
provider.
D. 
Use Compute Engine to create an Active Directory (AD) domain controller that is a replica of the on-premises AD domain
controller using Google Cloud Directory Sync.
Correct Answer:
 
B 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
According to the reference, my understanding is B is correct.
And in the document(https://cloud.google.com/iap/docs/concepts-overview), it says:
If you need to create Google Accounts for your existing users, you can use Google Cloud Directory Sync to synchronize with your
Active Directory or LDAP server.
Is it possible to explain why correct answer is C?
upvoted 
44 
times
MikeB19
MikeB19
 
3 years, 4 months ago
It’s simple. Domain controllers are not meant authenticate saas or web applications. This includes iam. Domain controllers speak
ntlm and Kerberos. 
This why we use federation. Because web apps do not speak Kerberos or ntlm. They speak languages such oauth. Hence the
need for ad federation proxy
B is correct
upvoted 
5 
times
Community vote distribution
B (89%)
D (11%)Bill831231
Bill831231
 
3 years, 2 months ago
thanks for the explanation, may I ask if we go with SAML, why need sync the useraccount? seems we just need set up the
federation between cloud and on-premise
upvoted 
2 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
if not, you will not be able to access resources on GCP with same accounts as onprem.
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
2 years, 3 months ago
"...As a prerequisite for access to GCP resources, employees must have a Google identity set up..."
upvoted 
4 
times
tartar
tartar
 
4 years, 5 months ago
B is ok
upvoted 
9 
times
kumarp6
kumarp6
 
4 years, 2 months ago
B should be correct
upvoted 
5 
times
nitinz
nitinz
 
3 years, 10 months ago
B, use GCDS.
upvoted 
5 
times
MeasService
MeasService
 
Highly Voted
 
5 years, 2 months ago
B is the nearest answer I feel !
upvoted 
25 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 2 hours ago
Selected Answer: 
B
I will go for B
upvoted 
1 
times
eff12c1
eff12c1
 
7 months ago
Selected Answer: 
B
To integrate Google Cloud with your on-premises Active Directory (AD) domain controller for identity management while retaining
your on-premises AD, the best approach is:
B. Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO.
upvoted 
1 
times
svkds
svkds
 
7 months, 4 weeks ago
Selected Answer: 
D
The most suitable option for integrating Google Cloud resources with an on-premises Active Directory domain controller for identity
management is option D. This involves creating a replica of the on-premises Active Directory domain controller using Compute
Engine and Google Cloud Directory Sync for synchronization.
upvoted 
1 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
B is correct https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction
upvoted 
2 
times
vamgcp
vamgcp
 
1 year, 11 months ago
Connect your on-premises Active Directory to Google Cloud: You can use Google Cloud Directory Sync (GCDS) to synchronize
your on-premises Active Directory with Google Cloud. This allows you to use your existing Active Directory users and groups in
Google Cloud.Set up single sign-on (SSO): You can use Google Cloud Identity-Aware Proxy (IAP) to set up SSO for your Google Cloud
resources. IAP integrates with your on-premises Active Directory and allows users to log in to Google Cloud using their existing
Active Directory credentials.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
B. Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO.
To retain their on-premises Active Directory domain controller for identity management while using Google Cloud resources, the
company can use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure
SAML single sign-on (SSO). This will allow users to use their existing Active Directory credentials to access Google Cloud
resources, while still maintaining their on-premises Active Directory domain controller as the primary source of identity
management.
upvoted 
8 
times
omermahgoub
omermahgoub
 
2 years ago
Option A, using the Admin Directory API to authenticate against the Active Directory domain controller, would not be a suitable
solution because it would require implementing custom authentication logic in the application, which would be time-consuming
and error-prone.
Option C, using Cloud Identity-Aware Proxy configured to use the on-premises Active Directory domain controller as an identity
provider, would be a suitable solution, but it would not allow you to synchronize Active Directory usernames with cloud identities.
Option D, using Compute Engine to create an Active Directory (AD) domain controller that is a replica of the on-premises AD
domain controller using Google Cloud Directory Sync, would not be a suitable solution because it would require setting up and
maintaining an additional AD domain controller in Google Cloud, which would be unnecessary if the company wants to retain their
on-premises AD domain controller as the primary source of identity management.
upvoted 
3 
times
SureshbabuK
SureshbabuK
 
2 years, 1 month ago
Selected Answer: 
B
GCDS and Cloud Identity is provided exactly for this use case
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
B
B is correct https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction
upvoted 
2 
times
cbarg
cbarg
 
2 years, 6 months ago
Selected Answer: 
B
Answer is B
upvoted 
1 
times
SAMBIT
SAMBIT
 
2 years, 9 months ago
https://support.google.com/a/answer/106368?hl=en
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for B.
Cloud Directory Sync
https://cloud.google.com/blog/products/identity-security/using-your-existing-identity-management-system-with-google-cloud-
platform
upvoted 
4 
times
vincy2202
vincy2202
 
3 years, 1 month agovincy2202
vincy2202
 
3 years, 1 month ago
B is the correct answer
upvoted 
1 
times
pulkit0627
pulkit0627
 
3 years, 1 month ago
B as AD groups are directly mapped to Cloud Directory Sync
upvoted 
1 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
B – use Google Cloud Directory Sync to sync Active Directory user names with cloud identities and configure SAML SSO.
Check the flowchart here illustrating integration of your existing identity management system into GCP:
https://cloud.google.com/blog/products/identity-security/using-your-existing-identity-management-system-with-google-cloud-
platform
C – does not work, since Cloud IAP serves different purpose. It s a building block toward BeyondCorp, an enterprise security
model that enables every employee to work from untrusted networks without the use of a VPN.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #69
You are running a cluster on Kubernetes Engine (GKE) to serve a web application. Users are reporting that a specific part of the
application is not responding anymore. You notice that all pods of your deployment keep restarting after 2 seconds. The
application writes logs to standard output. You want to inspect the logs to find the cause of the issue. Which approach can you
take? 
A. 
Review the Stackdriver logs for each Compute Engine instance that is serving as a node in the cluster.
B. 
Review the Stackdriver logs for the specific GKE 
container that is serving the unresponsive part of the application.
Most Voted
C. 
Connect to the cluster using gcloud credentials and connect to a container in one of the pods to read the logs.
D. 
Review the Serial Port logs for each Compute Engine instance that is serving as a node in the cluster.
Correct Answer:
 
B 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
4 years, 2 months ago
think answer is B. C cannot be, you don't need to connect to the container to view logs, you connect to stackdriver for this
upvoted 
34 
times
crypt0
crypt0
 
4 years, 2 months ago
Stackdriver Logging seems to be enabled by default for GKE.
Looking here:
https://cloud.google.com/monitoring/kubernetes-engine/legacy-stackdriver/logging
For container and system logs, GKE deploys a per-node logging agent that reads container logs, adds helpful metadata, and then
stores them. The logging agent checks for container logs in the following sources:
Standard output and standard error logs from containerized processes
I would also go with B
upvoted 
12 
times
AzureDP900
AzureDP900
 
1 year, 2 months ago
Community vote distribution
B (100%)agreed with B
upvoted 
1 
times
nitinz
nitinz
 
2 years, 10 months ago
B, google wants you to use stackdriver.
upvoted 
6 
times
kumarp6
kumarp6
 
3 years, 2 months ago
Yes it is B
upvoted 
3 
times
crypt0
crypt0
 
4 years, 2 months ago
Is Stackdriver enabled by default?
Stackdriver Logging is independent and first needs to enable with GKE I guess?
upvoted 
1 
times
crypt0
crypt0
 
4 years, 2 months ago
Please forget this comment ^
Answer B should be correct.
upvoted 
8 
times
tartar
tartar
 
3 years, 5 months ago
B is ok
upvoted 
7 
times
Jack_in_Large
Jack_in_Large
 
3 years, 7 months ago
Yes for GKE
upvoted 
1 
times
JoeShmoe
JoeShmoe
 
Highly Voted
 
4 years, 1 month ago
B is correct. Serial console doesnt give you StdOut
upvoted 
9 
times
AdityaGupta
AdityaGupta
 
Most Recent
 
3 months ago
B. Review the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application.
GKE be default integrats with Google Operation Suit (Stackdriver) and you can filter the logs for more specific part of application i.e
container to view logs. Also it is most efficient way of investigation.
upvoted 
2 
times
jalberto
jalberto
 
4 months, 3 weeks ago
Selected Answer: 
B
B is the simples option and more effective
upvoted 
1 
times
MestreCholas
MestreCholas
 
10 months ago
Selected Answer: 
B
B. Review the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application.
Since the application writes logs to standard output, the logs should be available in the Stackdriver logs for the container running
the unresponsive part of the application. Kubernetes Engine automatically exports these logs to Stackdriver, so you can use the
Stackdriver Logging console to view the logs. Option A is not the best choice because reviewing the logs for each Compute
Engine instance would be time-consuming and may not provide the necessary information. Option C may work, but it involves
extra steps and may not be necessary if the logs are available in Stackdriver. Option D is not relevant in this case because Serial
Port logs are not likely to provide useful information for troubleshooting an unresponsive web application.
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year agoC. Connect to the cluster using gcloud credentials and connect to a container in one of the pods to read the logs.
To inspect the logs of a Kubernetes Engine (GKE) cluster to find the cause of an issue, you can connect to the cluster using
gcloud credentials and connect to a container in one of the pods to read the logs. This will allow you to access the logs of the
application as it is running in the cluster, which should help you identify the cause of the issue.
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year ago
Option A, reviewing the Stackdriver logs for each Compute Engine instance that is serving as a node in the cluster, would not be
suitable because the application writes logs to standard output, not to Stackdriver.
Option B, reviewing the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application,
would not be suitable because the application writes logs to standard output, not to Stackdriver.
Option D, reviewing the Serial Port logs for each Compute Engine instance that is serving as a node in the cluster, would not be
suitable because the application writes logs to standard output, not to the Serial Port.
upvoted 
3 
times
jaxclain
jaxclain
 
1 year, 1 month ago
Selected Answer: 
B
This should be easy, the answer is B. 
Just eliminate the wrong answers (A) is not correct because the question is about GKE and not CE.
C and D are totally lost
upvoted 
1 
times
habros
habros
 
1 year, 1 month ago
Selected Answer: 
B
A, C, D all sounds unfeasible (credentials, and compute engine)
upvoted 
1 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 2 months ago
Selected Answer: 
B
is correct answer
upvoted 
1 
times
Superr
Superr
 
1 year, 7 months ago
Selected Answer: 
B
correct
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
1 year, 12 months ago
B is correct ans.I agree.
https://cloud.google.com/blog/ja/products/management-tools/finding-your-gke-logs
upvoted 
2 
times
haroldbenites
haroldbenites
 
2 years ago
Go for B
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 1 month ago
B is the correct answer
upvoted 
1 
times
MaxNRG
MaxNRG
 
2 years, 2 months agoB – Review Stackdriver logs for specific GKE container that is serving the unresponsive part of the app.
This is a most directly matching answer for this Q, since it reviews GKE container logs, by that advertising this Stackdriver feature.
“For container and system logs, GKE deploys a per-node logging agent that reads container logs, adds helpful metadata, and then
stores them. The logging agent checks for container logs in the following sources:
• Standard output and standard error logs from containerized processes
• kubelet and container runtime logs
• Logs for system components, such as VM startup scripts”
Originally we thought, that D is a right answer, since were confused with 2 seconds restart. But, that’s restart for Pod, not for Node
(GCE)
D – Review Serial Port logs for each Compute Engince instance, that is serving as the in the cluster.
Serial Port output is standard feature of Compute Engine (which retains 1 MB most recent logs for analysis). But, it is irrelevant for
Pod’s restart, caused by malfunction of some container.
upvoted 
3 
times
MamthaSJ
MamthaSJ
 
2 years, 5 months ago
Answer is B
upvoted 
3 
times
lovingsmart2000
lovingsmart2000
 
2 years, 6 months ago
B is right answer. There is a catch here - Legacy logging of GKE with Stackdriver has deprecated. If this is used, you need to
migrate to Cloud Operations for GKE, a new enhanced offering by Google with same functionality. 
Future questions will have the answer choices with new tool "Cloud Operations for GKE" instead of Stackdriver.
https://cloud.google.com/monitoring/kubernetes-engine/legacy-stackdriver/logging
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #70
You are using a single Cloud SQL instance to serve your application from a specific zone. You want to introduce high
availability. What should you do? 
A. 
Create a read replica instance in a different region
B. 
Create a failover replica instance in a different region
C. 
Create a read replica instance in the same region, but in a different zone
D. 
Create a failover replica instance in the same region, but in a different zone 
Most Voted
Correct Answer:
 
D 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
Agree D
upvoted 
36 
times
tartar
tartar
 
4 years, 5 months ago
D is ok
upvoted 
7 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Yes D is right
upvoted 
4 
times
kimharsh
kimharsh
 
2 years, 7 months ago
this Question is very Old and should be deleted from the exam , there is no Failover replica now , to do an HA we just confer it for
the SQL instance that we have .
upvoted 
25 
times
nwk
nwk
 
2 years, 7 months ago
https://cloud.google.com/sql/docs/mysql/replication#:~:text=Read%20replicas%20neither%20provide%20high%20availability%20
nor%20offer%20it.&text=A%20primary%20instance%20cannot%20failover,any%20way%20during%20an%20outage.&text=Maint
Community vote distribution
D (59%)
C (34%)
B (7%)nor%20offer%20it.&text=A%20primary%20instance%20cannot%20failover,any%20way%20during%20an%20outage.&text=Maint
enance%20windows%20cannot%20be%20set,windows%20with%20the%20primary%20instance.
- Read replicas neither provide high availability nor offer it.
Agree D
upvoted 
7 
times
jay9114
jay9114
 
2 years, 3 months ago
That link is helpful! I navigated to the "quick reference for Cloud SQL read replicas" and read the "failover" and "high availability"
topics. They state: 
1. Failover - "A primary instance cannot failover to a read replica, and read replicas are unable to failover in any way during an
outage."
2. High Availability - "Read replicas neither provide high availability nor offer it."
upvoted 
2 
times
spuyol
spuyol
 
11 months, 1 week ago
Sorry for this, but in the same link you can read: 
High availability Read replicas allow you to enable high availability on the replicas.
(What I understand is that Read Replicas give you high availability on reads, of course, not in writes).
upvoted 
1 
times
GunjGupta
GunjGupta
 
Highly Voted
 
4 years, 7 months ago
Cloud SQL is regional. For high availability, we need to think fo a failover strategy. So Option D meets the requirement.
create failover replica in the same region but in different Zone
upvoted 
18 
times
plumbig11
plumbig11
 
Most Recent
 
1 day, 11 hours ago
Selected Answer: 
D
Read replica is not appropriated to failover.
Create a failover replica instance in the same region, but in a different zone
upvoted 
1 
times
Lenifia
Lenifia
 
2 months, 1 week ago
Selected Answer: 
C
C is the correct answer, failover is more like DR
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Answer is D: 
Create a failover replica instance in the same region, but in a different zone.
Here’s why this option is the best:
1. High Availability: A failover replica provides automatic failover capabilities, meaning that if the primary instance becomes
unavailable (due to a zone failure, for example), Cloud SQL can automatically promote the failover replica to be the new primary
instance, minimizing downtime.
2. Same Region, Different Zone: By creating the failover replica in the same region but a different zone, you ensure that the
instances are geographically close to each other, which helps maintain low latency and faster failover times while still protecting
against zone-specific outages.
3. Cost Efficiency: Using a failover replica in the same region is typically more cost-effective than setting up a replica in a different
region, as cross-region replication can introduce additional latency and costs.
upvoted 
2 
times
hehe_24
hehe_24
 
2 months, 2 weeks ago
C is incorrect. D is the right answer. Read-replica is just an instance for read operation, it cannot provide HA.
upvoted 
1 
times
0verK0alafied
0verK0alafied
 
8 months, 2 weeks ago
Selected Answer: 
DThe HA configuration provides data redundancy. A Cloud SQL instance configured for HA is also called a regional instance and
has a primary and secondary zone within the configured region. Within a regional instance, the configuration is made up of a
primary instance and a standby instance. Through synchronous replication to each zone's persistent disk, all writes made to the
primary instance are replicated to disks in both zones before a transaction is reported as committed. In the event of an instance or
zone failure, the standby instance becomes the new primary instance. Users are then rerouted to the new primary instance. This
process is called a failover.
https://cloud.google.com/sql/docs/mysql/high-availability#HA-configuration
upvoted 
3 
times
Gall
Gall
 
11 months ago
Selected Answer: 
D
"Note: Read replicas do not provide failover capability. To provide failover capability for an instance, see Configuring an instance for
high availability."
https://cloud.google.com/sql/docs/mysql/replication/
upvoted 
2 
times
parthkulkarni998
parthkulkarni998
 
1 year ago
Selected Answer: 
D
Correct answer would be D as a failover replica acts as a redundant copy incase of zone failure. However, option C causes
confusion because a read replica can provide availability for reads, in case of zone failure for primary, but they cant provide
support for writes. They would only work for reads.
upvoted 
1 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
D
D, its regional product and failover is required for HA
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
C
1. Failover replica is a legacy way and is not available in GCP now - B and D are not the options:
https://cloud.google.com/sql/docs/mysql/high-availability#legacy_mysql_high_availability_option
2. Cloud SQL is regional resource. However, cross-region read replicas are allowed now in Cloud SQL
(https://cloud.google.com/blog/products/databases/introducing-cross-region-replica-for-cloud-sql) - A and C are options. 
Chosen C, as there is no requirement or mention of cross-regional / global db.
upvoted 
1 
times
hogtrough
hogtrough
 
12 months ago
Read replica is not a valid choice for HA configurations. It does not provide automatic failover that is required for HA. It may be
called something different or this answer has changed, but D is still the best option.
upvoted 
1 
times
piiizu
piiizu
 
1 year, 3 months ago
The key is High Availability, not Resilience or Disaster Recovery. Therefore my answer is C
upvoted 
1 
times
someone2011
someone2011
 
1 year, 3 months ago
D.
In HA config, the second replica is caled stand by. The process of replacing the primary damaged node is called failover.
https://cloud.google.com/sql/docs/postgres/high-availability
upvoted 
2 
times
didek1986
didek1986
 
1 year, 4 months ago
Selected Answer: 
C
C for sure
upvoted 
1 
times
red_panda
red_panda
 
1 year, 7 months ago
Selected Answer: 
CC.
Failover is so old and deprecated
upvoted 
4 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
this Question is very Old and should be deleted from the exam , there is no Failover replica now , to do an HA we just confer it for
the SQL instance that we have 
.. agreed tested as well
upvoted 
1 
times
JC0926
JC0926
 
1 year, 8 months ago
Selected Answer: 
D
Option C is not the best choice because it suggests creating a read replica instance, which is designed to handle read traffic and
provide better performance in read-heavy workloads, but it is not intended for high availability.
On the other hand, Option D suggests creating a failover replica instance in the same region but in a different zone. Failover
replicas are designed specifically for high availability, as they maintain an up-to-date copy of the primary instance's data. If the
primary instance becomes unresponsive or fails, Cloud SQL automatically switches to the failover replica with minimal downtime.
In summary, to introduce high availability for your Cloud SQL instance, you should create a failover replica instance in the same
region but in a different zone (Option D) rather than creating a read replica instance (Option C), which doesn't provide high
availability in case of primary instance failures.
upvoted 
4 
times
DevOpsifier
DevOpsifier
 
1 year, 6 months ago
thanks!
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #71
Your company is running a stateless application on a Compute Engine instance. The application is used heavily during regular
business hours and lightly outside of business hours. Users are reporting that the application is slow during peak hours. You
need to optimize the application's performance. What should you do? 
A. 
Create a snapshot of the existing disk. Create an instance template from the snapshot. Create an autoscaled managed
instance group from the instance template.
B. 
Create a snapshot of the existing disk. Create a custom image from the snapshot. Create an autoscaled managed
instance group from the custom image.
C. 
Create a custom image from the existing disk. Create an instance template from the custom image. Create an
autoscaled managed instance group from the instance template. 
Most Voted
D. 
Create an instance template from the existing disk. Create a custom image from the instance template. Create an
autoscaled managed instance group from the custom image.
Correct Answer:
 
C 
Comments
sdsdfasdf4
sdsdfasdf4
 
Highly Voted
 
4 years ago
The easiest way would be to create template from --source-instance, and then create MIG, but it is not listed here, also you cannot
create a MIG from image directly, you need a template, so answer is C (image -> template -> mig).
upvoted 
31 
times
6721sora
6721sora
 
2 years, 4 months ago
C is correct. 
To sdsdfasd4's point - Not recommended to create template from --source-instance as If the existing instance contains a static
external IP address, that address is copied into the instance template and might limit the use of the template. 
Templates are best created from images or other templates. Creating the template from a running instance may require work to
clean it up before it can be used for a MIG
upvoted 
9 
times
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
C is the right answer
Community vote distribution
C (95%)
B (5%)C is the right answer
upvoted 
12 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
C is definitely the right answer
upvoted 
1 
times
nareshthumma
nareshthumma
 
Most Recent
 
2 months, 1 week ago
Answer A:
Create a snapshot of the existing disk. Create an instance template from the snapshot. Create an autoscaled managed instance
group from the instance template.
Here’s why this option is the best:
1. Autoscaling: By creating an autoscaled managed instance group, you can automatically adjust the number of instances based
on the load. This means that during peak business hours, additional instances will be created to handle the increased traffic,
improving application performance. During off-peak hours, the number of instances can scale down to reduce costs.
2. Snapshot Creation: Taking a snapshot of the existing disk ensures that you have a backup of the current state of your
application. This snapshot can be used to create the new instance template, ensuring that your autoscaled instances have the
same configuration as the original instance.
upvoted 
1 
times
46f094c
46f094c
 
6 months, 1 week ago
Selected Answer: 
B
I prefer B, is better because I don't need to stop the instance to create the disk image.
upvoted 
1 
times
pakilodi
pakilodi
 
1 year, 1 month ago
Selected Answer: 
C
C. The key here is stateless, so we don't need snapshot the actual instance, we can start from zero.
upvoted 
4 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
C
C
We can create an instance from an image or a custom image or a snapshot but an instance template can be created using either
image or custom image only. 
Also refer: https://cloud.google.com/compute/docs/instance-templates/create-instance-templates
upvoted 
4 
times
Deb2293
Deb2293
 
1 year, 10 months ago
Selected Answer: 
C
Option C is the correct choice because creating a custom image from the existing disk ensures that the application environment is
consistent and does not change between instances, which can reduce variability in performance. Creating an instance template
from the custom image allows you to easily create new instances that are based on the same image, which can save time and
effort. Finally, creating an autoscaled managed instance group allows you to automatically scale the number of instances based
on demand, which can ensure that there are enough instances to handle peak traffic while minimizing costs during periods of low
traffic
upvoted 
7 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is right.
Create a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled
managed instance group from the instance template.managed instance group from the instance template.
upvoted 
1 
times
abirroy
abirroy
 
2 years, 3 months ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
mv2000
mv2000
 
2 years, 6 months ago
06/30/2022 Exam
upvoted 
9 
times
SHalgatti
SHalgatti
 
2 years, 10 months ago
I think Snapshot option are not correct in this scenario as to take snapshot you need to stop the VM so C looks best option
upvoted 
2 
times
bargou
bargou
 
11 months, 1 week ago
it's possible to create a snapshot of running VM by reducing I/O disks
upvoted 
1 
times
PhuocT
PhuocT
 
3 years ago
Selected Answer: 
C
C is the answer
upvoted 
1 
times
Rajasa
Rajasa
 
3 years ago
Selected Answer: 
C
C is the answer
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for C.
Instance template can not be created from snapshot. Only from an image.
upvoted 
3 
times
vincy2202
vincy2202
 
3 years, 1 month ago
C is the right answer.
https://cloud.google.com/compute/docs/instance-templates/create-instance-
templates#using_custom_or_public_images_in_your_instance_templates
upvoted 
3 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
C
vote C
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #72
Your web application has several VM instances running within a VPC. You want to restrict communications between instances
to only the paths and ports you authorize, but you don't want to rely on static IP addresses or subnets because the app can
autoscale. How should you restrict communications? 
A. 
Use separate VPCs to restrict traffic
B. 
Use firewall rules based on network tags attached to the compute instances 
Most Voted
C. 
Use Cloud DNS and only allow connections from authorized hostnames
D. 
Use service accounts and configure the web application to authorize particular service accounts to have access
Correct Answer:
 
B 
Comments
AWS56
AWS56
 
Highly Voted
 
3 years, 5 months ago
Agree B
upvoted 
24 
times
kumarp6
kumarp6
 
2 years, 8 months ago
Yes B it is
upvoted 
2 
times
nitinz
nitinz
 
2 years, 4 months ago
B is correct
upvoted 
2 
times
omermahgoub
omermahgoub
 
Highly Voted
 
6 months, 2 weeks ago
B. Use firewall rules based on network tags attached to the compute instances
To restrict communications between VM instances within a VPC without relying on static IP addresses or subnets, you can use
firewall rules based on network tags attached to the compute instances. This will allow you to specify which instances are allowed
to communicate with each other and on which paths and ports. You can then attach the relevant network tags to the compute
instances when they are created, allowing you to control communication between the instances without relying on static IP
addresses or subnets.
Community vote distribution
B (100%)addresses or subnets.
upvoted 
12 
times
omermahgoub
omermahgoub
 
6 months, 2 weeks ago
Option A, using separate VPCs to restrict traffic, would not be a suitable solution because it would not allow the instances to
communicate with each other, which is likely necessary for the functioning of the web application.
Option C, using Cloud DNS and only allowing connections from authorized hostnames, would not be a suitable solution because
it would not allow you to control communication between the instances based on their IP addresses or other characteristics.
Option D, using service accounts and configuring the web application to authorize particular service accounts to have access,
would not be a suitable solution because it would not allow you to control communication between the instances based on their IP
addresses or other characteristics.
upvoted 
5 
times
SureshbabuK
SureshbabuK
 
Most Recent
 
7 months, 1 week ago
Selected Answer: 
B
Access to specific ports and protocol can be controlled only by firewall rule - Hence B is correct. D is not correct as service
account is to authenticate and Authorized a specific machine to resource or service not ports and protocols
upvoted 
4 
times
megumin
megumin
 
8 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
B is the best option.
upvoted 
1 
times
abirroy
abirroy
 
9 months, 3 weeks ago
Selected Answer: 
B
Use firewall rules based on network tags attached to the compute instances
upvoted 
2 
times
alexandercamachop
alexandercamachop
 
9 months, 4 weeks ago
The secret is "paths and ports".
Which tell us Firewall as our only option.
upvoted 
5 
times
medi01
medi01
 
2 months, 2 weeks ago
And how does firewall restrict "paths" pretty please?
upvoted 
3 
times
cbarg
cbarg
 
12 months ago
Selected Answer: 
B
B Firewall rules to restrict traffic
upvoted 
1 
times
haroldbenites
haroldbenites
 
1 year, 7 months ago
Go for B.
upvoted 
2 
times
vincy2202
vincy2202
 
1 year, 7 months ago
B is the right answer
upvoted 
2 
times
MaxNRG
MaxNRG
 
1 year, 8 months agoB – use firewall rules based on network tags attached to the compute instances 
This answer avoids using IP, which are replaced by tags.
upvoted 
3 
times
MamthaSJ
MamthaSJ
 
1 year, 12 months ago
Answer is B
upvoted 
4 
times
areza
areza
 
2 years ago
B is ok
upvoted 
1 
times
victory108
victory108
 
2 years, 1 month ago
B. Use firewall rules based on network tags attached to the compute instances
upvoted 
2 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is B
upvoted 
1 
times
lynx256
lynx256
 
2 years, 3 months ago
B is ok
upvoted 
1 
times
Vika
Vika
 
2 years, 4 months ago
Agree B
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #73
You are using Cloud SQL as the database backend for a large CRM deployment. You want to scale as usage increases and
ensure that you don't run out of storage, maintain 75% CPU usage cores, and keep replication lag below 60 seconds. What are
the correct steps to meet your requirements? 
A. 
1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and
change the instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to
reduce replication time. 
Most Voted
B. 
1. Enable automatic storage increase for the instance. 2. Change the instance type to a 32-core machine type to keep
CPU usage below 75%. 3. Create a Stackdriver alert for replication lag, and deploy memcache to reduce load on the master.
C. 
1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create
more space. 2. Deploy memcached to reduce CPU load. 3. Change the instance type to a 32-core machine type to reduce
replication lag.
D. 
1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create
more space. 2. Deploy memcached to reduce CPU load. 3. Create a Stackdriver alert for replication lag, and change the
instance type to a 32-core machine type to reduce replication lag.
Correct Answer:
 
A 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
Agree with A
upvoted 
26 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
Sharding database will reduce latency
upvoted 
4 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and change
the instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce
Community vote distribution
A (78%)
B (22%)the instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce
replication time.
upvoted 
2 
times
9xnine
9xnine
 
Highly Voted
 
2 years, 7 months ago
Has anyone who has taken the exam recently seen any lingering questions with the Stackdriver nomenclature or is it all cloud
logging, cloud monitoring, etc.?
upvoted 
16 
times
nareshthumma
nareshthumma
 
Most Recent
 
2 months, 1 week ago
Answer is: A
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
Selected Answer: 
B
While I understand the doubts of selecting a 32 core machine from the start, answer A might be wrong...
According to this article: 
https://cloud.google.com/sql/docs/mysql/instance-settings#impact
''For MySQL instances, changing either the machine type or the zone of the instance results in the instance going offline for
several minutes.''
And I understand that instance type == machine type.
If we switch to a PostgreSQL or SQL Server instance, similar warnings appear:
PostgreSQL: ''Changing the number of CPUs or the memory size results in the instance going offline for less than 60 seconds.
The total time for the changes to take effect can take several minutes.''
SQL Server: ''Changing the number of CPUs or the memory size results in the instance going offline for less than 60 seconds.''
upvoted 
2 
times
e5019c6
e5019c6
 
1 year ago
But I also understand that the question didn't mention anything about needing the DB to be online always, so maybe these offline
times are acceptable...
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
A
You are using Cloud SQL as the database backend for a large CRM deployment. You want to scale as usage increases and
ensure that you don't run out of storage, maintain 75% CPU usage cores, and keep replication lag below 60 seconds. What are
the correct steps to meet your requirements?
C & D is out of question as it is talking of 75% of storage, where in question it says 75% of CPU.
Option A says monitoring before before taking action and sharding will also help in reducing latency.
Option B specifies specific machine type, which is not correct and also memcache which is used to recude the round trip to fetch
data, it will help in reducing latency.
I would prefer to go with Option A, as it is correct sequence to solve the problem.
upvoted 
3 
times
red_panda
red_panda
 
1 year, 7 months ago
Selected Answer: 
A
For me is A.
upvoted 
1 
times
jfricker
jfricker
 
1 year, 10 months ago
The correct answer is D.
1. 
Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more
space.
2. 
Deploy memcached to reduce CPU load.
3. 
Create a Stackdriver alert for replication lag, and change the instance type to a 32-core machine type to reduce replication lag.
This approach ensures that you are able to address the three requirements specified in the question:
- 
Monitoring storage usage and increasing storage when it exceeds 75% to avoid running out of storage.
- 
Reducing CPU load by deploying memcached, which can be used to cache frequently-used data, offloading some of the load- 
Reducing CPU load by deploying memcached, which can be used to cache frequently-used data, offloading some of the load
from the database.
- 
Monitoring replication lag and increasing the number of cores to reduce lag.
upvoted 
4 
times
Charsoft
Charsoft
 
2 years ago
It may be A for the simple fact that all the other answers throw in a tiny detail about 32 cores. This seems like a red herring
(unnecessary details that are meant to distract), so for that reason, A is the answer.
upvoted 
5 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A is right
upvoted 
1 
times
6721sora
6721sora
 
2 years, 4 months ago
A is incorrect. because of the wording "Shard the database". How can you shard the database in Cloud SQL without causing
major disruptions? Sharding is not a core feature of RDBMS. 
B should be correct. inspite of the mention of a fixed 32 core
upvoted 
3 
times
jay9114
jay9114
 
2 years, 2 months ago
Ii
Kk
Kk
You can shard cloudsql. Review this article - https://cloud.google.com/community/tutorials/horizontally-scale-mysql-database-
backend-with-google-cloud-sql-and-
proxysql#:~:text=Common%20approaches%20for%20horizontally%20scaling,with%20Cloud%20SQL%20and%20ProxySQL.
upvoted 
4 
times
fiercedog
fiercedog
 
2 years ago
The article only mentions sharding as a concept, and not a solution for cloudsql.
upvoted 
1 
times
Deb2293
Deb2293
 
1 year, 10 months ago
https://cloud.google.com/community/tutorials/horizontally-scale-mysql-database-backend-with-google-cloud-sql-and-
proxysql#:~:text=SQL%20and%20ProxySQL.-,Sharding,logic%20or%20a%20query%20router.
You can shard MySQL. 
Answer should be A.
upvoted 
4 
times
mj20201
mj20201
 
2 years, 9 months ago
Selected Answer: 
A
vote for A
upvoted 
2 
times
haroldbenites
haroldbenites
 
3 years ago
Go for A
upvoted 
2 
times
vincy2202
vincy2202
 
3 years, 1 month ago
A is the correct answer
upvoted 
2 
timesupvoted 
2 
times
amxexam
amxexam
 
3 years, 4 months ago
We can directly eliminate C and D we are doing some work that is already automated.
Still, I cannot make a point why not B is better than A?
I believe adding memcash will give an additional boost
Can someone help me point out why A is better than B?
upvoted 
4 
times
[Removed]
[Removed]
 
3 years, 2 months ago
Just to back up what amxexam said, here is the link on automatically increasing storage based on trend analysis: 
https://cloud.google.com/sql/docs/mysql/instance-settings#storage-capacity-2ndgen
upvoted 
3 
times
HenkH
HenkH
 
3 years ago
That is correct - but doc only mentions auto storage increase for this specific product (cloud SQL).
upvoted 
1 
times
HenkH
HenkH
 
2 years, 2 months ago
Should read MySQl
upvoted 
2 
times
cotam
cotam
 
3 years, 2 months ago
I suppose B is not a better option, since it indicates 'add 32core cpu', with no info of the current usage that seems like a over-kill.
upvoted 
5 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 5 months ago
I would say only because of the below line|
"You want to scale as usage increases" Line 1
Creating a 32 core machine upfront where we do not know what was the source machine cores would not be ideal .
in that situation i would go with A
upvoted 
4 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is A
upvoted 
4 
times
areza
areza
 
3 years, 6 months ago
A it is
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #74
You are tasked with building an online analytical processing (OLAP) marketing analytics and reporting tool. This requires a
relational database that can operate on hundreds of terabytes of data. What is the Google-recommended tool for such
applications? 
A. 
Cloud Spanner, because it is globally distributed
B. 
Cloud SQL, because it is a fully managed relational database
C. 
Cloud Firestore, because it offers real-time synchronization across devices
D. 
BigQuery, because it is designed for large-scale processing of tabular data 
Most Voted
Correct Answer:
 
D 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
Agree D
upvoted 
20 
times
tartar
tartar
 
4 years, 5 months ago
D is ok
upvoted 
5 
times
Nastrand
Nastrand
 
3 years, 11 months ago
What about the relational part? BigQuery uses SQL but it's not relational... I'm not sure its D
upvoted 
4 
times
riflerrick
riflerrick
 
3 years, 6 months ago
BigQuery is relational!
upvoted 
5 
times
lovingsmart2000
lovingsmart2000
 
3 years, 5 months ago
Pls do not confuse - Cloud SQL and BigQuery are RDBMS. Cloud Datastore, Bigtable are NoSQL.
Community vote distribution
D (94%)
A (6%)Pls do not confuse - Cloud SQL and BigQuery are RDBMS. Cloud Datastore, Bigtable are NoSQL.
Right answer is D - BQ
upvoted 
14 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Yes it is D
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
D, OLAP=BQ
upvoted 
4 
times
Sur_Nikki
Sur_Nikki
 
1 year, 8 months ago
Well Said
upvoted 
1 
times
JasonL_GCP
JasonL_GCP
 
3 years, 2 months ago
The question asks "This requires a relational database that can operate on hundreds of terabytes of data", but bq doesn't meet
this condition?
upvoted 
2 
times
elaineshi
elaineshi
 
2 years, 7 months ago
BigQuery supports relational and query of join tables.
upvoted 
2 
times
gfhbox0083
gfhbox0083
 
Highly Voted
 
4 years, 6 months ago
D, for sure.
BigQuery for OLAP
Google Cloud Spanner for OLTP.
upvoted 
16 
times
gracjanborowiak
gracjanborowiak
 
Most Recent
 
5 months, 3 weeks ago
Selected Answer: 
D
sql is oltp 
olap is data cube. lots of data which we try to process somehow. biq query is for that. 
firestore is for mobile and web apps. not so fast nosql db.
upvoted 
1 
times
Anandmrk
Anandmrk
 
10 months, 2 weeks ago
Agreed
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
D
4 reasons to choose BQ (Supports Petabytes of data)
- OLAP Data 
- Relational DB (SQL)
- 100s of TB data
- Analystics and Reporting
upvoted 
1 
times
Ashish1995
Ashish1995
 
1 year, 9 months ago
Selected Answer: 
D
D is obvious
upvoted 
1 
times
CGS22
CGS22
 
1 year, 10 months ago
Selected Answer: 
DD is the right one
upvoted 
1 
times
SudhirAhirkar
SudhirAhirkar
 
2 years ago
Cloud SQL/Spanner is OLTP DB but not OLAP. BQ is a well-known OLAP for analytics and also supports RBMS feature too... so I
would got with D
upvoted 
1 
times
AniketD
AniketD
 
2 years, 1 month ago
Selected Answer: 
D
D is correct. BigQuery is relational. Cloud SQL is not OLAP; moreover it can not store/process hundreds of TB of data. Max size
is 64 TB only.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
SerGCP
SerGCP
 
2 years, 2 months ago
Selected Answer: 
D
https://cloud.google.com/products/databases.
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
The words you need to focus "You are tasked with building an online analytical processing (OLAP) marketing analytics and
reporting tool" which is BigQuery
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Big Query for large analytics , D is right
upvoted 
1 
times
Andre777
Andre777
 
2 years, 3 months ago
Selected Answer: 
D
The keyword in this context is OLAP. CloudSQL is Relational SQL for OLTP. Capacity wise, BQ supports for PB+ while
CloudSQL only have max capacity of up to ~10TB. Again the questions specifically mention "hundreds of TB of data". So D is the
answer.
upvoted 
2 
times
deepdowndave
deepdowndave
 
2 years, 3 months ago
Why is it not CloudSQL? It supports TB data storage and the question is about a relational database, not a data warehouse such
as BigQuery.
upvoted 
1 
times
Andre777
Andre777
 
2 years, 3 months ago
The keyword in this context is OLAP. CloudSQL is Relational SQL for OLTP. Capacity wise, BQ supports for PB+ while
CloudSQL only have max capacity of up to ~10TB. Again the questions specifically mention "hundreds of TB of data". So D is the
answer.
upvoted 
1 
times
alexandercamachop
alexandercamachop
 
2 years, 3 months ago
The answer is Big Query, D
Secret: Analytical, Hundreds of TBTs. Relational. 
All of this are strictly meet by Big Query, if it had not said Analytical but rather, other keywords like High Availability then Cloud
Spanner.
upvoted 
1 
times
Thornadoo
Thornadoo
 
2 years, 5 months agoThornadoo
Thornadoo
 
2 years, 5 months ago
Selected Answer: 
D
Guys, this is easy:
OLTP - Cloud Spanner & Cloud SQL
OLAP - Big Query
NoSQL - Filestore and Big Table
So answer is D.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #75
You have deployed an application to Google Kubernetes Engine (GKE), and are using the Cloud SQL proxy container to make
the Cloud SQL database available to the services running on Kubernetes. You are notified that the application is reporting
database connection issues. Your company policies require a post- mortem. What should you do? 
A. 
Use gcloud sql instances restart.
B. 
Validate that the Service Account used by the Cloud SQL proxy container still has the Cloud Build Editor role.
C. 
In the GCP Console, navigate to Stackdriver Logging. Consult logs for (GKE) and Cloud SQL. 
Most Voted
D. 
In the GCP Console, navigate to Cloud SQL. Restore the latest backup. Use kubectl to restart all pods.
Correct Answer:
 
C 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
4 years, 2 months ago
post mortem always includes log analysis, answer is C
upvoted 
66 
times
Sur_Nikki
Sur_Nikki
 
8 months ago
Thanks for the info
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 2 months ago
C is right for Root Cause Analysis.
upvoted 
1 
times
AWS56
AWS56
 
3 years, 11 months ago
AGREE C
upvoted 
3 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
2 years, 5 months ago
Answer is C
Community vote distribution
C (100%)upvoted 
5 
times
pakilodi
pakilodi
 
Most Recent
 
1 month ago
Selected Answer: 
C
C -> post-mortem = log analysis
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
3 months ago
Selected Answer: 
C
You can jump on to the conlusion hence answer is not B. Consulting logs is always a good way to start investigation. and A and D
is not a choice.
upvoted 
1 
times
ale_brd_111
ale_brd_111
 
1 year, 1 month ago
Stackdriver is deprecated, now you must navigate to Cloud Logging.
upvoted 
2 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 2 months ago
Selected Answer: 
C
C is the right answer
upvoted 
1 
times
Jay_Krish
Jay_Krish
 
1 year, 4 months ago
Selected Answer: 
C
Logical answer is C. But is Stackdriver Logging enabled by default? Appreciate if someone could answer this?
upvoted 
1 
times
haroldbenites
haroldbenites
 
2 years ago
Go for C
upvoted 
1 
times
pakilodi
pakilodi
 
2 years ago
Selected Answer: 
C
post mortem = logs
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 1 month ago
C is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 1 month ago
Selected Answer: 
C
vote C
upvoted 
1 
times
MaxNRG
MaxNRG
 
2 years, 2 months ago
C – in GCP Console navigate to Stackdriver Logging. Consult logs for Kubernetes Engine and Cloud SQL.
A/D – is an immediate attempt to fix an issue. No analysis.
B – is irrelevant at all. Cloud SQL proxy should not build anything in production.
upvoted 
4 
times
lovingsmart2000
lovingsmart2000
 
2 years, 5 months agoAnswer is C. I request all here - not to blindly follow the answers published at coursera or udemy as most of them are copy-pasted
answer and are not real. Examtopis provides the more accurate answers and also support with comments
upvoted 
4 
times
lovingsmart2000
lovingsmart2000
 
2 years, 5 months ago
Answer is B. I request all here - not to blindly follow the answers published at coursera or udemy as most of them are copy-pasted
answer and are not real. Examtopis provides the more accurate answers and also support with comments
upvoted 
2 
times
ashish_t
ashish_t
 
2 years, 2 months ago
Why Service Account needs Cloud Build Editor role for accessing Cloud SQL?
The role is misleading/wrong, so B is wrong.
upvoted 
4 
times
victory108
victory108
 
2 years, 7 months ago
C. In the GCP Console, navigate to Stackdriver Logging. Consult logs for Kubernetes Engine and Cloud SQL.
upvoted 
2 
times
un
un
 
2 years, 7 months ago
C is correct
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #76
Your company pushes batches of sensitive transaction data from its application server VMs to Cloud Pub/Sub for processing
and storage. What is the Google- recommended way for your application to authenticate to the required Google Cloud services?
A. 
Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles. 
Most Voted
B. 
Ensure that VM service accounts do not have access to Cloud Pub/Sub, and use VM access scopes to grant the
appropriate Cloud Pub/Sub IAM roles.
C. 
Generate an OAuth2 access token for accessing Cloud Pub/Sub, encrypt it, and store it in Cloud Storage for access from
each VM.
D. 
Create a gateway to Cloud Pub/Sub using a Cloud Function, and grant the Cloud Function service account the
appropriate Cloud Pub/Sub IAM roles.
Correct Answer:
 
A 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 5 months ago
Agree A
upvoted 
26 
times
nitinz
nitinz
 
3 years, 4 months ago
A is correct
upvoted 
2 
times
kumarp6
kumarp6
 
3 years, 8 months ago
Yes A it is
upvoted 
2 
times
JustJack21
JustJack21
 
Highly Voted
 
2 years, 10 months ago
It's because of questions like these that I do not feel guilty about using question banks :D In what world would you accept value
requirements like this from your user? Wouldn't you ask "Do you want to just authenticate? or the data to be encrypted on its way
to pub/sub?"
I'll ignore the first part of the question and assume all data is sensitive, and focus on "What is the Google- recommended way for
Community vote distribution
A (100%)I'll ignore the first part of the question and assume all data is sensitive, and focus on "What is the Google- recommended way for
your application to authenticate to the required Google Cloud services?" -- The answer then is A. 
Use encryption and defense-in-depth for the first part.
upvoted 
12 
times
bandegg
bandegg
 
6 months ago
> It's because of questions like these that I do not feel guilty about using question banks :D
Same. To me, it wasn't clear whether the servers were in google or not due to the question about accessing google cloud. It was
asked as if the VMs were outside of google
upvoted 
3 
times
AMEJack
AMEJack
 
1 year, 9 months ago
Service accounts use keys
upvoted 
1 
times
red_panda
red_panda
 
Most Recent
 
1 year ago
Selected Answer: 
A
A is correct for me. It's batch, so no cloud function
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
A. Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.
The Google-recommended way for your application to authenticate to Cloud Pub/Sub and other Google Cloud services when
running on Compute Engine VMs is to use VM service accounts. VM service accounts are automatically created when you create
a Compute Engine VM, and they are associated with the VM instance. To authenticate to Cloud Pub/Sub and other Google Cloud
services, you should ensure that the VM service accounts are granted the appropriate IAM roles.
upvoted 
8 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option B, ensuring that VM service accounts do not have access to Cloud Pub/Sub and using VM access scopes to grant the
appropriate Cloud Pub/Sub IAM roles, would not be a suitable solution because VM service accounts are required for
authentication to Google Cloud services.
Option C, generating an OAuth2 access token for accessing Cloud Pub/Sub, encrypting it, and storing it in Cloud Storage for
access from each VM, would not be a suitable solution because it would require manual management of access tokens, which
can be error-prone and insecure.
Option D, creating a gateway to Cloud Pub/Sub using a Cloud Function and granting the Cloud Function service account the
appropriate Cloud Pub/Sub IAM roles, would not be a suitable solution because it would not allow the application to directly
authenticate to Cloud Pub/Sub.
upvoted 
3 
times
Sur_Nikki
Sur_Nikki
 
1 year, 1 month ago
Great way of explanation..By removing/elimination approach
upvoted 
1 
times
megumin
megumin
 
1 year, 8 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Selected Answer: 
Ahttps://cloud.google.com/iam/docs/understanding-service-accounts
upvoted 
1 
times
Pazzooo
Pazzooo
 
2 years, 5 months ago
Selected Answer: 
A
The combination of Roles assigned to Service accounts granted to VMs is the way to go. :)
upvoted 
2 
times
elenamatay
elenamatay
 
2 years, 6 months ago
Service accounts are recommended for almost all cases in Pub/Sub (see
https://cloud.google.com/pubsub/docs/authentication#service-accounts)
upvoted 
3 
times
haroldbenites
haroldbenites
 
2 years, 7 months ago
Go for A.
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 7 months ago
A is the correct answer
upvoted 
1 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
A – ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.
Check Migrating Data to GCP section of this page:
https://cloud.google.com/iam/docs/understanding-service-accounts 
You will create a service account key and use it from an external process to call Cloud Platform APIs.
upvoted 
3 
times
Bakili
Bakili
 
2 years, 8 months ago
A is very correct
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A
upvoted 
2 
times
victory108
victory108
 
3 years, 1 month ago
A. Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.
upvoted 
3 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
Agreed with A
upvoted 
1 
times
un
un
 
3 years, 1 month ago
A is correct
upvoted 
1 
times
kartikjena31
kartikjena31
 
3 years, 3 months ago
Ans. A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #77
You want to establish a Compute Engine application in a single VPC across two regions. The application must communicate
over VPN to an on-premises network. 
How should you deploy the VPN? 
A. 
Use VPC Network Peering between the VPC and the on-premises network.
B. 
Expose the VPC to the on-premises network using IAM and VPC Sharing.
C. 
Create a global Cloud VPN Gateway with VPN tunnels from each region to the on-premises peer gateway.
D. 
Deploy Cloud VPN Gateway in each region. Ensure that each region has at least one VPN tunnel to the on-premises peer
gateway. 
Most Voted
Correct Answer:
 
D 
Comments
Googler2
Googler2
 
Highly Voted
 
4 years, 8 months ago
It can't be -A - VPC Network Peering only allows private RFC 1918 connectivity across two Virtual Private Cloud (VPC) networks.
In this example is one VPC with on-premise network
https://cloud.google.com/vpc/docs/vpc-peering
It is not definitely - B - Can't be
It is not C - Because Cloud VPN gateways and tunnels are regional objects, not global
So, it the answer is D - 
https://cloud.google.com/vpn/docs/how-to/creating-static-vpns
upvoted 
45 
times
amxexam
amxexam
 
3 years, 3 months ago
Why not A?
https://cloud.google.com/vpc/docs/vpc-peering#benefits_of_exchanging_custom_routes
The second use case is exactly what is in the question.
Don't get the argument about RFC 1918.
Will go with A
Community vote distribution
D (90%)
C (10%)Will go with A
upvoted 
1 
times
ochanz
ochanz
 
3 years ago
https://cloud.google.com/vpc/docs/vpc-peering allows internal IP address connectivity across two VPC so A is not the answer
as the on premise network need to use public IP. cmiiw
upvoted 
4 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
The question clearly asks us to use VPN.
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Agreed with D.
upvoted 
1 
times
TaherShaker
TaherShaker
 
Highly Voted
 
4 years, 1 month ago
Just Passed my exam and I answered (D) for this question
upvoted 
20 
times
M_Asep
M_Asep
 
3 years ago
sound promising dude
upvoted 
3 
times
Sur_Nikki
Sur_Nikki
 
1 year, 8 months ago
IS the Exam Idea questions enough dude, for passing this exam?
upvoted 
3 
times
ccpmad
ccpmad
 
Most Recent
 
6 months, 1 week ago
Selected Answer: 
D
Option C: Create a global VPN gateway and establish VPN tunnels from each region to the on-premises peer gateway. This
suggests that a single global VPN gateway manages the tunnels from both regions.
Option D: Deploy a VPN gateway in each region and ensure that each region has at least one VPN tunnel to the on-premises peer
gateway. This indicates that each region has its own VPN gateway.
>Option D ensures that there is a VPN gateway in each region, providing greater redundancy. If a gateway in one region fails, the
gateway in the other region remains operational.
upvoted 
2 
times
santoshchauhan
santoshchauhan
 
9 months, 1 week ago
Selected Answer: 
C
Global Cloud VPN Gateway: This feature allows for the creation of a single VPN gateway that can serve multiple regions within the
same VPC network. By creating a global VPN gateway, you can efficiently manage VPN connections from all regions of your VPC
to your on-premises network.
Simplicity and Efficiency: Using a global gateway simplifies the configuration and management of VPN connections as opposed to
maintaining separate regional VPN gateways. It centralizes the VPN endpoint on the Google Cloud side, reducing the complexity of
the network setup.
Reliable and Secure Communication: The global Cloud VPN Gateway allows for secure, encrypted tunnels between Google Cloud
and the on-premises network, ensuring that the application’s inter-regional and on-premises communications are secure.
upvoted 
2 
times
salvo007
salvo007
 
1 year ago
Selected Answer: 
D
C is wrong. A global vpn is a single region resource.
https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-ha-vpn?hl=it
gcloud compute vpn-gateways create GW_NAME \
--network=NETWORK \
--region=REGION \
--stack-type=IP_STACK--stack-type=IP_STACK
so D is the answer
upvoted 
2 
times
gcmrjbr
gcmrjbr
 
1 year ago
It´s option C! So, while the VPN Gateway itself is a regional resource, its scope can be effectively global as it can serve resources
across different regions within the same Virtual Private Cloud (VPC). This is why it’s sometimes referred to as a ‘global’ service in
the context of its functionality, even though strictly speaking, it’s a regional resource.
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
1 year, 3 months ago
Selected Answer: 
D
Each Cloud VPN gateway is a regional resource that uses one or more regional external IP addresses. A Cloud VPN gateway can
connect to a peer VPN gateway.
upvoted 
2 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
It can't be -A - VPC Network Peering only allows private RFC 1918 connectivity across two Virtual Private Cloud (VPC) networks.
In this example is one VPC with on-premise network https://cloud.google.com/vpc/docs/vpc-peering 
It is not definitely - B - Can't
be 
It is not C - Because Cloud VPN gateways and tunnels are regional objects, not global 
So, it the answer is D -
https://cloud.google.com/vpn/docs/how-to/creating-static-vpn
upvoted 
3 
times
vvkds
vvkds
 
1 year, 11 months ago
Selected Answer: 
D
D looks fine.
upvoted 
1 
times
oms_muc
oms_muc
 
2 years ago
Selected Answer: 
D
As HA isn't required, why do we need two VPN gateways?
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
D
D is the correct answer, 
in order to do A you will need VPN., or interconnect
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
there is two VPN:
1. classic VPN
2. HA VPN
upvoted 
1 
times
DrishaS4
DrishaS4
 
2 years, 5 months ago
Selected Answer: 
D
Cloud VPN Gateway is a regional service, not global.
upvoted 
4 
times
elaineshi
elaineshi
 
2 years, 7 months ago
Why not C? services across regions can communicate to each other, VPN only connects to the closet region, and all the VPC
shall be connected if firewall's set.
upvoted 
2 
timesharoldbenites
haroldbenites
 
3 years ago
Go for D.
Cloud VPN Gateway is regional. NOt Global
gcloud compute vpn-gateways create GW_NAME \
--network=NETWORK \
--region=REGION
upvoted 
2 
times
vincy2202
vincy2202
 
3 years, 1 month ago
D is the correct answer
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #78
Your applications will be writing their logs to BigQuery for analysis. Each application should have its own table. Any logs older
than 45 days should be removed. 
You want to optimize storage and follow Google-recommended practices. What should you do? 
A. 
Configure the expiration time for your tables at 45 days
B. 
Make the tables time-partitioned, and configure the partition expiration at 45 days 
Most Voted
C. 
Rely on BigQuery's default behavior to prune application logs older than 45 days
D. 
Create a script that uses the BigQuery command line tool (bq) to remove records older than 45 days
Correct Answer:
 
B 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
4 years, 8 months ago
Could you please help clarify? I think B is correct.
It looks like table will be deleted with option A.
https://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time
When you delete a table, any data in the table is also deleted. To automatically delete tables after a specified period of time, set
the default table expiration for the dataset or set the expiration time when you create the table.
upvoted 
39 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
Agreed and going with B
upvoted 
2 
times
kumarp6
kumarp6
 
3 years, 8 months ago
it is B, if you use option A, on 46th day there is no table/content in table for application :)
upvoted 
11 
times
nitinz
nitinz
 
3 years, 4 months ago
B partition table
upvoted 
4 
times
Community vote distribution
B (100%)upvoted 
4 
times
tartar
tartar
 
3 years, 11 months ago
B is ok
upvoted 
8 
times
aviv
aviv
 
Highly Voted
 
4 years, 6 months ago
Agreed with B.
upvoted 
10 
times
OSAMA911
OSAMA911
 
Most Recent
 
4 months, 2 weeks ago
Selected Answer: 
B
I think B is correct.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
9 months ago
Selected Answer: 
B
https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration
B is the correct answer.
upvoted 
3 
times
SSPPJi
SSPPJi
 
12 months ago
https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration
upvoted 
4 
times
FaizAhmed
FaizAhmed
 
1 year ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
Sur_Nikki
Sur_Nikki
 
1 year, 1 month ago
B seems correct as this will partitioning will create a filter criteria on the basis of which specified actions on logs will be taken
upvoted 
1 
times
examch
examch
 
1 year, 6 months ago
Selected Answer: 
B
B is the correct answer,
If your tables are partitioned by date, the dataset's default table expiration applies to the individual partitions. You can also control
partition expiration using the time_partitioning_expiration flag in the bq command-line tool or the expirationMs configuration setting
in the API. When a partition expires, data in the partition is deleted but the partitioned table is not dropped even if the table is empty.
https://cloud.google.com/bigquery/docs/best-practices-storage
upvoted 
7 
times
megumin
megumin
 
1 year, 8 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
MarcoEscanor
MarcoEscanor
 
1 year, 8 months ago
Selected Answer: 
B
B - You can control partition expiration using the time_partitioning_expiration flag in the bq command-line
https://cloud.google.com/bigquery/docs/best-practices-storage
upvoted 
2 
times
AhmedH7793
AhmedH7793
 
1 year, 9 months ago
Selected Answer: 
BB is okay
upvoted 
1 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Selected Answer: 
B
Using Table-Partitions.
upvoted 
1 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Using Table-Partitions.
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 4 months ago
Selected Answer: 
B
I got similar question on my exam.
upvoted 
1 
times
haroldbenites
haroldbenites
 
2 years, 7 months ago
Go for B.
https://cloud.google.com/bigquery/docs/creating-partitioned-tables#sql
CREATE TABLE
mydataset.newtable (transaction_id INT64, transaction_date DATE)
PARTITION BY
transaction_date
OPTIONS(
partition_expiration_days=3,
require_partition_filter=true
)
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 7 months ago
Selected Answer: 
B
B is the correct answer
upvoted 
2 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
B – Make the tables time-partitioned and configure the partition expiration at 45 days.
A – if you use table expiration time, then it will remove the whole table after 45 days.
D – requires extra work and is not automatic.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #79
You want your Google Kubernetes Engine cluster to automatically add or remove nodes based on CPU load. 
What should you do? 
A. 
Configure a HorizontalPodAutoscaler with a target CPU usage. Enable the Cluster Autoscaler from the GCP Console.
Most Voted
B. 
Configure a HorizontalPodAutoscaler with a target CPU usage. Enable autoscaling on the managed instance group for
the cluster using the gcloud command.
C. 
Create a deployment and set the maxUnavailable and maxSurge properties. Enable the Cluster Autoscaler using the
gcloud command.
D. 
Create a deployment and set the maxUnavailable and maxSurge properties. Enable autoscaling on the cluster managed
instance group from the GCP Console.
Correct Answer:
 
A 
Comments
Unfaithful
Unfaithful
 
Highly Voted
 
2 years, 5 months ago
Answer: A
Support: 
How does Horizontal Pod Autoscaler work with Cluster Autoscaler?
Horizontal Pod Autoscaler changes the deployment's or replicaset's number of replicas based on the current CPU load. If the load
increases, HPA will create new replicas, for which there may or may not be enough space in the cluster. If there are not enough
resources, CA will try to bring up some nodes, so that the HPA-created pods have a place to run. If the load decreases, HPA will
stop some of the replicas. As a result, some nodes may become underutilized or completely empty, and then CA will terminate
such unneeded nodes.
upvoted 
63 
times
heretolearnazure
heretolearnazure
 
4 months, 1 week ago
very well explained
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 2 months ago
Community vote distribution
A (100%)Nice and detailed explanation. I agree with A.
upvoted 
1 
times
LaxmanTiwari
LaxmanTiwari
 
7 months, 3 weeks ago
Nice and detailed explanation. I agree with A.
upvoted 
1 
times
Rajasa
Rajasa
 
2 years ago
Good Explaination
upvoted 
3 
times
natpilot
natpilot
 
Highly Voted
 
3 years, 11 months ago
i'm for A, but the question in ambiguous, because requires the autoscale of nodes (not pod) when the cpu overload, but in answer
use k8s pod autoscaler based on cpu load ( cpu load for pod, not nodes ). strange
upvoted 
25 
times
p4
p4
 
3 years, 1 month ago
Agreed, the question is not about pods, but answers are also talking about pods (not only)
A is correct because B is wrong according to 
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
"Caution: Do not enable Compute Engine autoscaling for managed instance groups for your cluster nodes. GKE's cluster
autoscaler is separate from Compute Engine autoscaling"
upvoted 
20 
times
skywalker
skywalker
 
3 years, 7 months ago
Confuse with the question like you mentioned. 
Autoscale is via nodes not pod.. and can only be configure using gcloud
command.
upvoted 
6 
times
LaxmanTiwari
LaxmanTiwari
 
Most Recent
 
7 months, 3 weeks ago
Nice and detailed explanation. I agree with A.
upvoted 
1 
times
Sur_Nikki
Sur_Nikki
 
8 months ago
A seems correct. y to create managed instance groups unnecessarily?
upvoted 
1 
times
Deb2293
Deb2293
 
10 months, 1 week ago
The answer is A.
More nodes mean it's horizontal scaling (increase VMs means vertical scaling of infrastructure). Cluster AutoScalar is used for
increasing number of nodes.
upvoted 
1 
times
examch
examch
 
1 year ago
Selected Answer: 
A
A is the Correct answer, Horizontal Pod Autoscaler and Cluster Autoscaler can be used together to provision new pods and new
nodes as per the CPU utilization.
https://www.youtube.com/watch?v=VNAWA6NkoBs
upvoted 
2 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
Rajeev26
Rajeev26
 
1 year, 2 months ago
Selected Answer: 
ASelected Answer: 
A
MIG not for GKE as option B and C, D are not relevant to question
upvoted 
1 
times
abirroy
abirroy
 
1 year, 3 months ago
Selected Answer: 
A
Configure a HorizontalPodAutoscaler with a target CPU usage. Enable the Cluster Autoscaler from the GCP Console.
upvoted 
1 
times
gee1979
gee1979
 
1 year, 3 months ago
Selected Answer: 
A
A...
The HPA and CA complement each other for truly efficient scaling. If the load increases, HPA will create new replicas. If there isn’t
enough space for these replicas, CA will provision some nodes, so that the HPA-created pods have a place to run.
The Horizontal Pod Autoscaler changes the shape of your Kubernetes workload by automatically increasing or decreasing the
number of Pods in response to the workload's CPU or memory consumption, or in response to custom metrics reported from
within Kubernetes or external metrics from sources outside of your cluster.
upvoted 
1 
times
6721sora
6721sora
 
1 year, 4 months ago
A is wrong.
Pod scaling only spins up additional pods. Not nodes.
Cluster Autoscaler does adding of nodes automatically.
I am surprised that so many people think that A is the correct answer.
Correct answer per me is C
upvoted 
2 
times
DrishaS4
DrishaS4
 
1 year, 5 months ago
Selected Answer: 
A
Horizontal Pod Autoscaler changes the deployment's or replicaset's number of replicas based on the current CPU load. If the load
increases, HPA will create new replicas, for which there may or may not be enough space in the cluster. If there are not enough
resources, CA will try to bring up some nodes, so that the HPA-created pods have a place to run. If the load decreases, HPA will
stop some of the replicas. As a result, some nodes may become underutilized or completely empty, and then CA will terminate
such unneeded nodes.
upvoted 
2 
times
[Removed]
[Removed]
 
1 year, 10 months ago
I got one question on my exam which showed autoscaling configuration and was asked to select correct configuration.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
1 year, 12 months ago
I agree A is correct.
I found quicklab.
Understanding and Combining GKE Autoscaling Strategies.
upvoted 
1 
times
ehgm
ehgm
 
2 years ago
Selected Answer: 
A
B and D: You must never change the GKE managed instance group.
C and D: maxUnavailable and maxSurge are used for rolling update
A. It is the correct.
upvoted 
5 
times
haroldbenites
haroldbenites
 
2 years ago
Go for A
upvoted 
1 
times
MaxNRG
MaxNRG
 
2 years, 2 months agoCreate Horizontal Autoscaler (min,max for pods):
kubectl autoscale deployment my-app --max 6 --min 4 --cpu-percent 50
Autoscaling cluster:
gcloud container clusters create example-cluster \
--zone us-central1-a \
--node-locations us-central1-a,us-central1-b,us-central1-f \
--num-nodes 2 --enable-autoscaling --min-nodes 1 --max-nodes 4
Check scaling an application and Horizontal Pod Autoscaler: 
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
Manual Cluster Resizing: https://cloud.google.com/kubernetes-engine/docs/how-to/resizing-a-cluster
https://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps
upvoted 
6 
times
MaxNRG
MaxNRG
 
2 years, 2 months ago
Correct answer A.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #80
You need to develop procedures to verify resilience of disaster recovery for remote recovery using GCP. Your production
environment is hosted on-premises. You need to establish a secure, redundant connection between your on-premises network
and the GCP network. 
What should you do? 
A. 
Verify that Dedicated Interconnect can replicate files to GCP. Verify that direct peering can establish a secure
connection between your networks if Dedicated Interconnect fails.
B. 
Verify that Dedicated Interconnect can replicate files to GCP. Verify that Cloud VPN can establish a secure connection
between your networks if Dedicated Interconnect fails. 
Most Voted
C. 
Verify that the Transfer Appliance can replicate files to GCP. Verify that direct peering can establish a secure connection
between your networks if the Transfer Appliance fails.
D. 
Verify that the Transfer Appliance can replicate files to GCP. Verify that Cloud VPN can establish a secure connection
between your networks if the Transfer Appliance fails.
Correct Answer:
 
B 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
I think B is correct answer.
upvoted 
44 
times
tartar
tartar
 
4 years, 5 months ago
B is ok
upvoted 
8 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Its quite a fun to use Transfer Appliance for DR, I think answer is B
upvoted 
6 
times
Sur_Nikki
Sur_Nikki
 
1 year, 8 months ago
Community vote distribution
B (86%)
A (14%)Sur_Nikki
Sur_Nikki
 
1 year, 8 months ago
Actually, how ca this be given as a option even?
upvoted 
1 
times
nitinz
nitinz
 
3 years, 10 months ago
only B works
upvoted 
1 
times
MeasService
MeasService
 
Highly Voted
 
5 years, 2 months ago
Agree B is correct. Transfer appliance is a physical appliance for transferring huge bulk of data. does not fit into disaster recovery
testing. out of A and B, B seems to be more nearest answer. One would not have direct peering and Dedicated interconnect in a
solution
upvoted 
27 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
B
Dedicated Interconnect as Primary , and Cloud VPN as Backup
upvoted 
2 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
answer is B
upvoted 
1 
times
hitmax87
hitmax87
 
7 months, 3 weeks ago
Selected Answer: 
B
I go to B, because direct peering anyway requires VPN connection if you want to get access to VPC.
upvoted 
3 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
A
Why you guys are choosing VPN? the reason to use Dedicated Interconnect is to have the max bandwidth available, does VPN
give you that option in the first place? why not thinking about separate direct peering connection which might give a better
performance than VPN?
upvoted 
2 
times
MahAli
MahAli
 
1 year ago
BTW with direct peering you are going through the service provider network which makes more sense to get different connectivity
option
upvoted 
1 
times
Diwz
Diwz
 
8 months, 4 weeks ago
Direct peering allows only on premises to connect to Google services in GCP . If needed to connect with Google workspace
where all projects hosted they dedicated or partner interconnect is required. 
https://cloud.google.com/network-connectivity/docs/direct-peering
B is the best answer
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
You need to develop procedures to verify resilience of disaster recovery for remote recovery using GCP. Your production
environment is hosted on-premises. You need to establish a secure, redundant connection between your on-premises network
and the GCP network.
What should you do?
A. Verify that Dedicated Interconnect can replicate files to GCP. Verify that direct peering can establish a secure connection
between your networks if Dedicated Interconnect fails.
B. Verify that Dedicated Interconnect can replicate files to GCP. Verify that Cloud VPN can establish a secure connection between
your networks if Dedicated Interconnect fails.Why Not A, as question asks "to establish a secure, redundant connection between your on-premises network and the GCP
network."
Is VPN considered more reliable than Direct Peering?? Both VPN and Direct Peering will provide redundant connection.
I am not concerned about cost Direct Interconnect is already there.
upvoted 
1 
times
FaizAhmed
FaizAhmed
 
1 year, 6 months ago
Selected Answer: 
B
B is right,
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is B. Verify that Dedicated Interconnect can replicate files to GCP. Verify that Cloud VPN can establish a
secure connection between your networks if Dedicated Interconnect fails.
Dedicated Interconnect is a connection that provides a private, dedicated connection between your on-premises network and GCP
over a Google-owned network. It is a secure and reliable option for connecting your on-premises network to GCP. You can use it to
replicate files to GCP as a part of your disaster recovery plan.
If Dedicated Interconnect fails for any reason, it is a good idea to have a backup solution in place to establish a secure connection
between your networks. Cloud VPN is a secure and reliable solution for establishing a connection between your on-premises
network and GCP. It uses a virtual private network (VPN) tunnel to securely connect the networks, and it is a good backup option if
Dedicated Interconnect fails.
upvoted 
8 
times
omermahgoub
omermahgoub
 
2 years ago
The Transfer Appliance is a physical storage device that you can use to transfer large amounts of data from your on-premises
storage to GCP. It is not a connection option and cannot be used to establish a secure connection between your on-premises
network and GCP. Therefore, the options C and D are not correct.
upvoted 
1 
times
stefanop
stefanop
 
1 year, 1 month ago
Why not A? Is Cloud VPN better than Direct Peering in this scenario?
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
For DR with Google Cloud and on-prem use Dedicated Interconnect with HA VPN
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
B is right without any second thought. Question is straight forward.
upvoted 
1 
times
Rajeev26
Rajeev26
 
2 years, 2 months ago
Selected Answer: 
B
Transfer appliance you need to carry to GCP center like water bottle :)
upvoted 
1 
times
abirroy
abirroy
 
2 years, 3 months ago
Selected Answer: 
B
Verify that Dedicated Interconnect can replicate files to GCP. Verify that direct peering can establish a secure connection between
your networks if Dedicated Interconnect fails.
upvoted 
1 
times
alexandercamachop
alexandercamachop
 
2 years, 3 months agoalexandercamachop
alexandercamachop
 
2 years, 3 months ago
It is definitely B
1. Interconnect is the first option so that is right.
2. Eliminates A, since Direct Peering is not supported in GCP, the option is Google Cloud VPN connection to onpremises site.
upvoted 
3 
times
BeCalm
BeCalm
 
1 year, 10 months ago
GCP supports direct peering in 100 locations
upvoted 
2 
times
DrishaS4
DrishaS4
 
2 years, 5 months ago
Selected Answer: 
B
Transfer appliance is a physical appliance for transferring huge bulk of data. does not fit into disaster recovery testing
upvoted 
1 
times
Matalf
Matalf
 
2 years, 5 months ago
Selected Answer: 
B
only B have redundacy
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #81
Your company operates nationally and plans to use GCP for multiple batch workloads, including some that are not time-critical.
You also need to use GCP services that are HIPAA-certified and manage service costs. 
How should you design to meet Google best practices? 
A. 
Provision preemptible VMs to reduce cost. Discontinue use of all GCP services and APIs that are not HIPAA-compliant.
B. 
Provision preemptible VMs to reduce cost. Disable and then discontinue use of all GCP services and APIs that are not
HIPAA-compliant. 
Most Voted
C. 
Provision standard VMs in the same region to reduce cost. Discontinue use of all GCP services and APIs that are not
HIPAA-compliant.
D. 
Provision standard VMs to the same region to reduce cost. Disable and then discontinue use of all GCP services and
APIs that are not HIPAA-compliant.
Correct Answer:
 
B 
Comments
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
Disabling and then discontinuing allows you to see the effects of not using the APIs, so you can gauge (check) alternatives. So that
leaves B and D as viable answers. The question says only some are not time-critical which implies others are... this means
preemptible VMs are good because they will secure a spot for scaling when needed. So I'm also going to choose B.
upvoted 
42 
times
Musk
Musk
 
4 years, 5 months ago
If others are time-critical, preemtible does not fit. Answer is D.
upvoted 
11 
times
army234
army234
 
3 years, 9 months ago
No mention of others in the question. In an exam it's important to not being in individual assumptions and focus on the
information in question. Key word here is "not time-critical"
upvoted 
6 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Community vote distribution
B (74%)
D (22%)
A
(4%)AzureDP900
AzureDP900
 
2 years, 2 months ago
agree otherwise answer goes to non-preemtible VM's
upvoted 
2 
times
Sur_Nikki
Sur_Nikki
 
1 year, 8 months ago
Ver well said..."In an exam it's important to not being in individual assumptions and focus on the information in question. Key
word here is "not time-critical""
upvoted 
2 
times
Darahaas
Darahaas
 
4 years, 3 months ago
And the others are not spoken about. By taking the question just by the context that it sets, preemptible 
is what I choose. So it's
B according to me.
upvoted 
3 
times
Sur_Nikki
Sur_Nikki
 
1 year, 7 months ago
Correctly explained
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
correct
upvoted 
1 
times
netizens
netizens
 
10 months, 3 weeks ago
Why you decided to emphasize on key word "not time-critical" but not" "operates nationally"?
upvoted 
1 
times
Karna
Karna
 
Highly Voted
 
4 years, 5 months ago
They say that some (not all) of the Batch workloads are not time critical which implies that there are time critical Batch workloads
for which Preemptible VMs are not appropriate, so going with D as the answer
upvoted 
20 
times
[Removed]
[Removed]
 
4 years, 5 months ago
I dont think it means use premtible vms for everything. It says to use preemtible vms to reduce cost
upvoted 
10 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 1 hour ago
Selected Answer: 
B
I will go for B.
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
1 month ago
Selected Answer: 
A
The answer is A. 
Though some workloads are critical, the question is really asking about saving costs on the non-critical
workloads. it's already understood that some workloads will incur a cost. So 'how' do we save in general?? Use preemptible VMs.
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Answer is B
upvoted 
1 
times
Ponchi14
Ponchi14
 
6 months ago
D is the correct answer and for the following regions:
1. Deploying a read replica and then manually failing over by stopping the current cloud SQL instance will only impact the
authentication layer, which is what we're looking to test.
2. Stopping all VM's won't have an impact on the authentication layer testing becasuse Cloud SQL is a PaaS service, you cannot
just turn off the VM's
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 2 weeks agoSephethus
Sephethus
 
6 months, 2 weeks ago
What is the difference between A and B other than adding the obvious that you'd disable them if you discontinue using them. This
is the most obnoxiously confusing question I've ever read and someone else pointed out that *some* are not time critical which
implies others are time critical.
upvoted 
1 
times
JaimeMS
JaimeMS
 
7 months ago
Selected Answer: 
D
If the question would have said: "The batch are not time-critical", then Option B with preentible VMs.
BUT, it clearly points that only SOME are not time-critical". Option D is the only one that satisfies all conditions
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
D
D
As per the requirement, some of the batches are not time critical - which means some are critical. Choosing preemptible VMs may
mean time critical batches may be affected. Cost effective solutions should not come at cost of requirements. 
Disabling and then discontinuing allows you to see the effects of not using the APIs, so you can gauge (check) alternatives.
upvoted 
1 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
Selected Answer: 
B
When it is "some" batches are non-critical - when we want to focus on price reduction and if it is batch apps, then we can definitely
choose "Pre-emptible VMs". Also non-compliant APIs needs to be disabled for sure (APIs can be enabled if there is a need).
Putting altogether, B is the right answer
upvoted 
2 
times
Net50
Net50
 
1 year, 5 months ago
Selected Answer: 
D
I think the answer should be D. some of the batches are not time critical which means some are. Choosing premtible VMs may
mean time critical batches may be affected in some cases. Even though the solution needs to be cost effective, it should not
come at cost of requirements. Hence D
upvoted 
3 
times
Diwz
Diwz
 
8 months, 4 weeks ago
Premptible VMs is better for batch workloads which are critical 
and not critical since it takes bit longer to complete the load when
some VMs are preempted . It greatly reduces cost with using preempted vms for batch workloads. 
B is the better answer
upvoted 
1 
times
Sur_Nikki
Sur_Nikki
 
1 year, 8 months ago
B looks good to me
upvoted 
1 
times
8d31d36
8d31d36
 
1 year, 10 months ago
To design a GCP solution that meets Google's best practices for operating nationally with multiple batch workloads, including
some that are not time-critical, and using HIPAA-compliant services while managing service costs, you should provision standard
VMs in the same region to reduce cost, and use GCP services that are HIPAA-compliant as needed. Therefore, the correct option
is C.
Preemptible VMs can provide cost savings, but they are not recommended for workloads that are not time-critical, as they may be
interrupted at any time. Provisioning standard VMs in the same region will provide better performance and stability, and can still be
cost-effective by using features such as sustained-use discounts and committed use discounts.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
BB is ok
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
Assumption here is that cost is more important that the time critical batches, therefore use preemptible instances. Disable and
discontinue is a better option as it gives you opportunity to see the impact before blasting any APIs or services that are not
certified.
upvoted 
4 
times
Prashant2022
Prashant2022
 
2 years, 3 months ago
ans is B - HAHAHA
upvoted 
2 
times
DrishaS4
DrishaS4
 
2 years, 5 months ago
Selected Answer: 
B
https://cloud.google.com/security/compliance/hipaa#unique_features
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #82
Your customer wants to do resilience testing of their authentication layer. This consists of a regional managed instance group
serving a public REST API that reads from and writes to a Cloud SQL instance. 
What should you do? 
A. 
Engage with a security company to run web scrapers that look your for users' authentication data om malicious websites
and notify you if any is found.
B. 
Deploy intrusion detection software to your virtual machines to detect and log unauthorized access.
C. 
Schedule a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application
behaves. 
Most Voted
D. 
Configure a read replica for your Cloud SQL instance in a different zone than the master, and then manually trigger a
failover while monitoring KPIs for our REST API.
Correct Answer:
 
C 
Comments
Kri_2525
Kri_2525
 
Highly Voted
 
5 years, 1 month ago
As per google documentation(https://cloud.google.com/solutions/scalable-and-resilient-apps) answer is C.
C: A well-designed application should scale seamlessly as demand increases and decreases, and be resilient enough to
withstand the loss of one or more compute resources.
Resilience: designed to withstand the unexpected
A highly-available, or resilient, application is one that continues to function despite expected or unexpected failures of components
in the system. If a single instance fails or an entire zone experiences a problem, a resilient application remains fault tolerant—
continuing to function and repairing itself automatically if necessary. Because stateful information isn’t stored on any single
instance, the loss of an instance—or even an entire zone—should not impact the application’s performance.
upvoted 
62 
times
Jack_in_Large
Jack_in_Large
 
4 years, 7 months ago
Shutting off all VMs in a zone is not good approach for testing of authentication
upvoted 
6 
times
vartiklis
vartiklis
 
3 years ago
Community vote distribution
C (65%)
D (35%)vartiklis
vartiklis
 
3 years ago
You're not testing *authentication*, you're testing *the resilience of the authentication layer*. "A resilient app is one that continues
to function despite failures of system components" (https://cloud.google.com/architecture/scalable-and-resilient-
apps#resilience_designing_to_withstand_failures) - such as shutting down all VMs in a zone.
upvoted 
19 
times
elaineshi
elaineshi
 
2 years, 7 months ago
Agree, Chaos testing is to shutdown random instances.
upvoted 
4 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
yes chaos testing is industry standard
upvoted 
2 
times
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 1 month ago
Since the question is asking to do a resilience testing, I prefer C.
upvoted 
16 
times
Darahaas
Darahaas
 
4 years, 3 months ago
Resilience testing of the "Authentication Layer", not the "Application". So the answer is B.
upvoted 
4 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 1 hour ago
Selected Answer: 
C
I will go for C.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 2 weeks ago
Selected Answer: 
C
D. is not correct as this tests the resilience of the database (Cloud SQL) but not necessarily the authentication layer. The
authentication layer might have separate components or dependencies that need to be tested under failure conditions.
upvoted 
2 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Agree with C
upvoted 
1 
times
wooyourdaddy
wooyourdaddy
 
3 months ago
Selected Answer: 
D
Option C, which involves scheduling a disaster simulation exercise to shut off all VMs in a zone, is indeed a strong choice for
resilience testing. This approach helps you understand how your application behaves under failure conditions and ensures that
your system can handle unexpected disruptions.
However, Option D is also highly relevant. Configuring a read replica for your Cloud SQL instance in a different zone and manually
triggering a failover while monitoring KPIs for your REST API directly tests the resilience of your database layer. This can provide
valuable insights into the failover process and the impact on your application’s performance and availability.
Both options have their merits, but if the primary goal is to test the resilience of the authentication layer specifically, Option D might
be more targeted and effective.
upvoted 
2 
times
hitmax87
hitmax87
 
7 months, 3 weeks ago
Selected Answer: 
C
C is correct. It is not D because you are not designing system, your goal is testing existed system
upvoted 
2 
times
666Amitava666
666Amitava666
 
8 months, 2 weeks ago
Selected Answer: 
CChaos testing
upvoted 
3 
times
activist
activist
 
9 months ago
I choose Answer C
https://cloud.google.com/sql/docs/mysql/replication 
This URL states "Read replicas are read-only; you cannot write to them. The read replica processes queries, read requests, and
analytics traffic, thus reducing the load on the primary instance." 
"Note: Read replicas do not provide failover capability. To provide failover capability for an instance, see Configuring an instance for
high availability."
"As a best practice, put read replicas in a different zone than the primary instance when you use HA on your primary instance. This
practice ensures that read replicas continue to operate when the zone that contains the primary instance has an outage. See the
Overview of high availability for more information."
upvoted 
3 
times
santoshchauhan
santoshchauhan
 
9 months, 1 week ago
Selected Answer: 
D
Testing Database Resilience: By setting up a read replica in a different zone and triggering a manual failover, you simulate a failure
of the primary database. This allows you to assess how well your authentication layer and the overall application cope with the loss
of the primary database.
Monitoring Performance and Availability: During the failover, monitoring key performance indicators (KPIs) for your REST API will
give you insights into how the application's performance and availability are impacted. This helps in identifying potential bottlenecks
and areas for improvement in your resilience strategy.
Ensuring Data Continuity: A read replica ensures data continuity and minimizes downtime, which is critical for an authentication
system. The replica will take over as the primary database during the failover, ensuring that the authentication service remains
functional.
upvoted 
1 
times
Rehamss
Rehamss
 
9 months, 2 weeks ago
Selected Answer: 
D
D is okay
upvoted 
1 
times
Teckexam
Teckexam
 
10 months, 3 weeks ago
Selected Answer: 
C
Authentication layer resiliency can be covered as part of overall application resiliency testing. Option C is asking to use read replica
which is not useful in case of testing resiliency in case of failure
upvoted 
2 
times
practice_sample
practice_sample
 
10 months, 3 weeks ago
Selected Answer: 
C
Read replicas do not provide failover capability. 
https://cloud.google.com/sql/docs/mysql/replication#read-replicas
upvoted 
3 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Selected Answer: 
C
It is c
upvoted 
2 
times
Tamim321
Tamim321
 
1 year ago
Selected Answer: 
C
Read replica do not provide failover capability 
https://cloud.google.com/sql/docs/mysql/replication#:~:text=Note%3A%20Read%20replicas%20do%20not,HA%20on%20your%20
primary%20instance.
upvoted 
6 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
CC is the good choice
upvoted 
2 
times
juliansierra
juliansierra
 
1 year, 2 months ago
I choose C.
I don't say D because the REST API read and WRITE in the database, if you create a READ replica in Cloud SQL, the REST API
will not have the possibility of write in the database. The answer D doesn't mention anything about promote the read replica to
master.
upvoted 
6 
times
parthkulkarni998
parthkulkarni998
 
1 year ago
Exactly. Because in GCP a read replica cant be auto upgraded to become a master in case of failover. So basically the database
will allow only READ operations and not WRITE operations. Basically leaving it non-functional
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #83
Your BigQuery project has several users. For audit purposes, you need to see how many queries each user ran in the last
month. What should you do? 
A. 
Connect Google Data Studio to BigQuery. Create a dimension for the users and a metric for the amount of queries per
user.
B. 
In the BigQuery interface, execute a query on the JOBS table to get the required information.
C. 
Use 'bq show' to list all jobs. Per job, use 'bq ls' to list job information and get the required information.
D. 
Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required
information. 
Most Voted
Correct Answer:
 
D 
Comments
Googler2
Googler2
 
Highly Voted
 
4 years, 8 months ago
D- reasons:
1.-Cloud Audit Logs maintains audit logs for admin activity, data access and system events. BIGQUERY is automatically send to
cloud audit log functionality.
2.- In the filter you can filter relevant BigQuery Audit messages, you can express filters as part of the export
https://cloud.google.com/logging/docs/audit
https://cloud.google.com/bigquery/docs/reference/auditlogs#ids
https://cloud.google.com/bigquery/docs/reference/auditlogs#auditdata_examples
upvoted 
49 
times
GooglecloudArchitect
GooglecloudArchitect
 
4 years, 5 months ago
D is the right as you can get the monthly view of the query usage across all the users and projects for auditing purpose. C does
need appropriate permission to see the detail level data. Monthly view is tough to get directly from the bq ls or bq show
commands.
upvoted 
9 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
Answer is D
Community vote distribution
D (49%)
B (45%)
Other
(6%)Answer is D
upvoted 
1 
times
Zarmi
Zarmi
 
Highly Voted
 
4 years, 8 months ago
Answer is D:
https://cloud.google.com/bigquery/docs/reference/auditlogs#example_query_cost_breakdown_by_identity
upvoted 
27 
times
BobbyFlash
BobbyFlash
 
3 years ago
Nailed it
upvoted 
2 
times
ErenYeager
ErenYeager
 
2 years, 1 month ago
No mention about exporting to bq
upvoted 
1 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days, 1 hour ago
Selected Answer: 
B
I will go for B because it is more efficient and easy.
upvoted 
1 
times
alpay
alpay
 
1 month, 1 week ago
Selected Answer: 
D
"Audit logs versus INFORMATION_SCHEMA views
Audit logs help you answer the question "Who did what, where, and when?" within your Google Cloud resources. Audit logs are the
definitive source of information for system activity by user and access patterns and should be your primary source for audit or
security questions."
https://cloud.google.com/bigquery/docs/introduction-audit-workloads
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Answer is B
In the BigQuery interface, execute a query on the JOBS table to get the required 
information.
Explanation:
JOBS Table:BigQuery automatically logs job information, including queries, in a special table called JOBS.
By querying this table, you can retrieve details about each job, including the user who ran it, the query text, and the timestamp.
Why the Other Options Are Less Suitable:
Connect Google Data Studio to BigQuery: While this can visualize data, you still need to execute a query to pull the data first. This
option is not directly querying for the information you need.
Use ‘bq show’ and ‘bq ls’: These commands provide metadata about jobs but do not efficiently retrieve the count of queries per
user, especially for a large number of jobs over a month.
Use Cloud Audit Logging: This approach could work but would be more complex and less efficient for simply counting queries. The
JOBS table is specifically designed for this purpose, making it easier to extract the necessary data.
upvoted 
2 
times
awsgcparch
awsgcparch
 
5 months, 1 week ago
Selected Answer: 
B
Using the INFORMATION_SCHEMA.JOBS_BY_USER table within BigQuery is the most efficient and straightforward method to
get the required audit information about the number of queries each user ran in the last month. Therefore, option B is the best
choice.. D.While Cloud Audit Logs can provide detailed logs of activities, querying them directly for this purpose is less efficient
than using the JOBS table in BigQuery. Additionally, setting up and querying audit logs involves more steps and may require
exporting logs to BigQuery for complex queries.
upvoted 
5 
times
awsgcparch
awsgcparch
 
5 months, 1 week ago
Selected Answer: 
B
Why B is the Best Answer:
Direct Access to Job Metadata: BigQuery maintains metadata about jobs (including query jobs) in the INFORMATION_SCHEMADirect Access to Job Metadata: BigQuery maintains metadata about jobs (including query jobs) in the INFORMATION_SCHEMA
views, specifically in the INFORMATION_SCHEMA.JOBS table.
Detailed Information: This table contains information about all jobs, including who ran them, when they were run, and the type of
job. This makes it easy to filter and count queries by user.
Querying JOBS Table: You can write a SQL query to count the number of queries executed by each user over the specified
period.
upvoted 
4 
times
eff12c1
eff12c1
 
7 months ago
Selected Answer: 
B
Querying the INFORMATION_SCHEMA.JOBS_BY_USER view in BigQuery is the most efficient and straightforward way to obtain
the number of queries each user ran in the last month. This method leverages built-in BigQuery capabilities designed specifically
for auditing and monitoring query jobs.
Cloud Audit Logs provide detailed logging information but are more complex to query for specific metrics like the number of queries
run by each user. BigQuery’s INFORMATION_SCHEMA.JOBS_BY_USER is designed for this purpose and is easier to use for
querying job data.
upvoted 
4 
times
JaimeMS
JaimeMS
 
7 months ago
Selected Answer: 
D
Audit logs, Option D
upvoted 
1 
times
AhmedSami
AhmedSami
 
10 months, 4 weeks ago
Selected Answer: 
C
reason:
https://cloud.google.com/logging/docs/audit#data-access
Data Access audit logs—except for BigQuery Data Access audit logs—are disabled by default because audit logs can be quite
large. If you want Data Access audit logs to be written for Google Cloud services other than BigQuery, you must explicitly enable
them
upvoted 
1 
times
SSS987
SSS987
 
11 months, 3 weeks ago
I finally decide to go with Option D over B 
because we or the auditor might not have access to the metadata. In fact, in our project,
not all of us had access to query this view.
"To get the permission that you need to query the INFORMATION_SCHEMA.JOBS view, ask your administrator to grant you the
BigQuery Resource Viewer"
https://cloud.google.com/bigquery/docs/information-schema-jobs#required_role.
(And not because of the wordings "Table" instead of "view" - don't think an architect exam will try to assess your memory of
whether it is a table or a view or your understanding of the difference between a table and a view).
upvoted 
2 
times
PhatLau
PhatLau
 
11 months, 3 weeks ago
Selected Answer: 
D
C - bq show: To view job details (https://cloud.google.com/bigquery/docs/managing-jobs#view_job_details_2)
bq ls: To list jobs (https://cloud.google.com/bigquery/docs/managing-jobs#list_jobs)
So D is the correct one.
upvoted 
1 
times
zaxxon
zaxxon
 
1 year ago
Selected Answer: 
C
https://cloud.google.com/bigquery/docs/managing-jobs#list_jobs_in_a_project
upvoted 
1 
times
muh21
muh21
 
1 year, 3 months ago
I think B is the correct answer
upvoted 
1 
times
TheCloudGuruu
TheCloudGuruu
 
1 year, 7 months agoSelected Answer: 
D
Cloud Logging
upvoted 
1 
times
VarunGo
VarunGo
 
1 year, 8 months ago
Selected Answer: 
B
B is correct. here's the link - https://cloud.google.com/bigquery/docs/information-schema-jobs
upvoted 
5 
times
medi01
medi01
 
1 year, 8 months ago
Selected Answer: 
B
JOBS system table does exist and it contains exactly the info we need: one record for each job executed by users (query is one of
the type of the jobs)
upvoted 
2 
times
Ric350
Ric350
 
5 months, 2 weeks ago
Yes, but this is assuming you have the required role of BigQuery Resource Viewer which is needed and does not clarify in the
question! So does that make D the right answer? 
And with D, you need the logs viewer role. 
The question is a bad one as it
doesn't clarify any roles in this scenario.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #84
You want to automate the creation of a managed instance group. The VMs have many OS package dependencies. You want to
minimize the startup time for new 
VMs in the instance group. 
What should you do? 
A. 
Use Terraform to create the managed instance group and a startup script to install the OS package dependencies.
B. 
Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed
instance group with the VM image. 
Most Voted
C. 
Use Puppet to create the managed instance group and install the OS package dependencies.
D. 
Use Deployment Manager to create the managed instance group and Ansible to install the OS package dependencies.
Correct Answer:
 
B 
Comments
crypt0
crypt0
 
Highly Voted
 
3 years, 8 months ago
Why is it not answer B?
upvoted 
42 
times
kumarp6
kumarp6
 
2 years, 8 months ago
B is the answer,
upvoted 
6 
times
Jos
Jos
 
3 years, 5 months ago
It is.
upvoted 
10 
times
tartar
tartar
 
2 years, 11 months ago
B is ok
upvoted 
11 
times
 
2 years, 4 months ago
Community vote distribution
B (100%)nitinz
nitinz
 
2 years, 4 months ago
It is B
upvoted 
4 
times
JoeShmoe
JoeShmoe
 
Highly Voted
 
3 years, 7 months ago
B- minimal start time means a pre-baked golden image
upvoted 
21 
times
omermahgoub
omermahgoub
 
Most Recent
 
6 months, 2 weeks ago
The correct answer is B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the
managed instance group with the VM image.
Managed instance groups are a way to manage a group of Compute Engine instances as a single entity. If you want to automate
the creation of a managed instance group, you can use tools such as Terraform, Deployment Manager, or Puppet to automate the
process.
To minimize the startup time for new VMs in the instance group, you should create a custom VM image with all of the OS package
dependencies pre-installed. This will allow you to create new VMs from the custom image, which will significantly reduce the
startup time compared to installing the dependencies on each VM individually. You can then use Deployment Manager to create
the managed instance group with the custom VM image.
upvoted 
12 
times
omermahgoub
omermahgoub
 
6 months, 2 weeks ago
Option A, using Terraform to create the managed instance group and a startup script to install the OS package dependencies,
would not minimize the startup time for new VMs in the instance group. 
Option C, using Puppet to create the managed instance group and install the OS package dependencies, would not minimize the
startup time for new VMs in the instance group. 
Option D, using Deployment Manager to create the managed instance group and Ansible to install the OS package
dependencies, would not minimize the startup time for new VMs in the instance group.
upvoted 
5 
times
megumin
megumin
 
7 months, 4 weeks ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance
group with the VM image.
upvoted 
1 
times
minmin2020
minmin2020
 
8 months, 3 weeks ago
Selected Answer: 
B
B will reduce the startup time
upvoted 
1 
times
DrishaS4
DrishaS4
 
11 months ago
Selected Answer: 
B
B- minimal start time means a pre-baked golden image
upvoted 
4 
times
BigSteveO
BigSteveO
 
11 months, 3 weeks ago
As someone who works on Terraform. It may not be Googles best practice, even though it's built in just need to be initialized. But it
is the easiest way to build and restructure infrastructure with a simple line of code change and a quick shell command to apply
terraform. I mean B would work. But it doesn't include the start-up script for the OS dependencies to be loaded. ?>?>? Any
feedback?
upvoted 
1 
times
Ric350
Ric350
 
11 months, 2 weeks agoRic350
Ric350
 
11 months, 2 weeks ago
Start up scripts aren't need here as you're making a custom OS image with all OS package dependencies. 
Question is not
asking for the easiest way, it's asking how to minimize VM startup times. Not having to run the startup scripts because it's baked
into the image is how I understand and interpret this, therefore B.
upvoted 
2 
times
mv2000
mv2000
 
12 months ago
06/30/2022 Exam
upvoted 
5 
times
rogerlovato
rogerlovato
 
1 year, 5 months ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
haroldbenites
haroldbenites
 
1 year, 7 months ago
Go for B
upvoted 
1 
times
Godlike
Godlike
 
1 year, 7 months ago
yes B is right
upvoted 
2 
times
vincy2202
vincy2202
 
1 year, 7 months ago
B is the right answer
upvoted 
2 
times
exam_war
exam_war
 
1 year, 8 months ago
go with B. D: it involves so many other third software to configure/manage which makes build more complicated.
upvoted 
1 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
B – create a custom VM instance image with all OS dependencies. Use Deployment Manager to create a MIG with the VM image.
Read more about Public and Custom VM Images: https://cloud.google.com/compute/docs/images
Custom images are available in your project only, they don’t add cost to your VM instances, incur image storage cost (0.085$
GB/month)
D – could be also an alternative (if to consider requirement to install dependencies in start up script). But, last sentence stresses
on “minimize VM’s start up time”. So, B is fastest solution. Also, what is a point to use Ansible if you can complete same task via
startup script of Deployment Manager. Ansible won’t make this faster, but just will add 3rd party dependency.
upvoted 
3 
times
victory108
victory108
 
2 years, 1 month ago
B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance
group with the VM image.
upvoted 
2 
times
un
un
 
2 years, 1 month ago
B is correct
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #85
Your company captures all web traffic data in Google Analytics 360 and stores it in BigQuery. Each country has its own dataset.
Each dataset has multiple tables. 
You want analysts from each country to be able to see and query only the data for their respective countries. 
How should you configure the access rights? 
A. 
Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add
all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate
dataset with view access with each respective analyst country-group. 
Most Voted
B. 
Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add
all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate
tables with view access with each respective analyst country-group.
C. 
Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add
all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery dataViewer. Share the appropriate
dataset with view access with each respective analyst country- group.
D. 
Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add
all country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery dataViewer. Share the appropriate
table with view access with each respective analyst country-group.
Correct Answer:
 
A 
Comments
Sebatian
Sebatian
 
Highly Voted
 
5 years, 1 month ago
It should be A. The question requires that user from each country can only view a specific data set, so BQ dataViewer cannot be
assigned at project level. Only A could limit the user to query and view the data that they are supposed to be allowed to.
upvoted 
61 
times
jits1984
jits1984
 
1 year, 8 months ago
Should be C.
https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer
Community vote distribution
A (75%)
C (25%)https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer
Data viewer role can be applied to a Table and a View. 
JobUser can be applied only at a Project level not at a Dataset level
https://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser
upvoted 
11 
times
jits1984
jits1984
 
1 year, 4 months ago
incorrect, should be A, BigQuery Job User 
(roles/bigquery.jobUser)
Provides permissions to run jobs, including queries, within the project.
upvoted 
3 
times
RKS_2021
RKS_2021
 
1 year, 3 months ago
A is wrong
upvoted 
1 
times
wk
wk
 
Highly Voted
 
5 years, 2 months ago
Should be C
https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer
When applied to a dataset, dataViewer provides permissions to:
Read the dataset's metadata and to list tables in the dataset.
Read data and metadata from the dataset's tables.
When applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however,
are necessary to allow the running of jobs.
upvoted 
32 
times
Jack_in_Large
Jack_in_Large
 
4 years, 7 months ago
Option C grant read permission to all datasets globally, which violated the request "You want analysts from each country
to be able to see and query only the data for their respective countries"
So the correct answer is A.
upvoted 
31 
times
BrunoTostes
BrunoTostes
 
3 years, 2 months ago
https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer
"When applied to a dataset.." you can apply dataViewer role to a specific dataset.
upvoted 
9 
times
Cloud_Architect_05
Cloud_Architect_05
 
Most Recent
 
1 month, 1 week ago
Should be A. DataViewer: "When applied at the project or organization level, this role can also enumerate all datasets in the
project. Additional roles, however, are necessary to allow the running of jobs."
https://cloud.google.com/bigquery/docs/access-control
upvoted 
1 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
C.
Explanation:
Each country should have its own group to manage access efficiently. This allows you to easily add or remove analysts from their
respective groups.
By adding analysts to their specific country groups, you can manage permissions in a way that aligns with their data access
needs.
This group will include all country groups. It simplifies the management of roles for all analysts collectively.
The dataViewer role provides permission to view datasets and tables. This role allows analysts to read data without the ability to
modify it, which is appropriate for your use case.
Granting view access to the respective datasets for each country group ensures that analysts can only access data relevant to
their country. This is crucial for maintaining data privacy and compliance.their country. This is crucial for maintaining data privacy and compliance.
Why Other Options Are Less Suitable:
Using BigQuery jobUser Role: The BigQuery jobUser role allows users to run jobs (like queries) but does not inherently grant
access to view datasets or tables. This option would not effectively limit visibility to data by country.
upvoted 
2 
times
Diwz
Diwz
 
9 months, 3 weeks ago
Selected Answer: 
C
It is C.
Question says analyst should be 
able to see and query only the data for their respective countries. BigQueryDta viewer permission
will allow only to read and query the table/view data
upvoted 
2 
times
awsgcparch
awsgcparch
 
5 months, 1 week ago
You cant query with dataviewer. user with the roles/bigquery.dataViewer role has read-only access to datasets and tables but
does not inherently have the permissions to run queries (which are considered jobs in BigQuery). The dataViewer role allows
users to view dataset metadata and table contents but does not include the ability to create or execute jobs.The dataViewer role
alone does not allow users to run queries. Analysts need the ability to run queries, which requires the jobUser role.
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
11 months ago
Selected Answer: 
A
Go with a.
upvoted 
1 
times
islamfouda
islamfouda
 
11 months, 2 weeks ago
Selected Answer: 
C
C is right, even if DataViwer is granted on Project level but Dataset is shared with view access to only the country group.
upvoted 
1 
times
JohnDohertyDoe
JohnDohertyDoe
 
11 months, 4 weeks ago
Selected Answer: 
A
A is the correct answer. Tested the two scenarios, with `jobUser` permissions it does not allow the user to see a dataset.
Whereas with `dataViewer` it has permissions for all the datasets. Note the difference is in the initial permission across the project
and not per dataset.
upvoted 
7 
times
bandegg
bandegg
 
1 year ago
Selected Answer: 
A
It's A because in order to query, on needs the jobUser role. dataViewer doesn't grant the ability to actually query the datasets one
has been given access to.
https://cloud.google.com/bigquery/docs/running-queries#required_permissions
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
I'm siding with C on this one.
jobUser role has the bigquery.jobs.create permission, which allow it to load data into BQ, which analyst shouldn't do.
Data Viewer has no permissions to add or edit data (It can create a snapshot of the data, extract it or replicate it at most)
upvoted 
1 
times
whoosh007
whoosh007
 
1 year ago
Selected Answer: 
C
BigQuery Data Viewer 
(roles/bigquery.dataViewer)
When applied to a table or view, this role provides permissions to:
Read data and metadata from the table or view.
This role cannot be applied to individual models or routines.
When applied to a dataset, this role provides permissions to:
Read the dataset's metadata and list tables in the dataset.
Read data and metadata from the dataset's tables.Read data and metadata from the dataset's tables.
When applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however,
are necessary to allow the running of jobs.
Lowest-level resources where you can grant this role:
Table and view
BigQuery Job User 
(roles/bigquery.jobUser)
Provides permissions to run jobs, including queries, within the project.
Lowest-level resources where you can grant this role:
Project
Analyst must query data --> BigQuery Data Viewer
upvoted 
2 
times
steghe
steghe
 
1 year, 2 months ago
Selected Answer: 
A
A: JobUser 
to execute queries in general. Data viewer for viewing the country dataset.
upvoted 
1 
times
TopTalk
TopTalk
 
1 year, 2 months ago
Selected Answer: 
C
Lowest-level resources where you can grant this role: 
dataViewer: Table, View
jobUser: Project
You don't want to grant access to the entire project, only the dataset which is divided per country. Definitely C. 
https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer
upvoted 
1 
times
don_v
don_v
 
11 months, 3 weeks ago
Correct answer is A.
Note this: "Share the appropriate dataset with *view access* with each respective analyst country-group".
"view access" is the key.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
Selected Answer: 
A
A. Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all
country-groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate dataset with
view access with each respective analyst country-group.
As all analysts need to execute query, they need JobUser role.
They should be restricted to view all datasets (not tables) of respective country.
upvoted 
1 
times
RKS_2021
RKS_2021
 
1 year, 3 months ago
It is C for Sure, A give Project level permissions, which defied requirement to have access to the Data set level.
upvoted 
1 
times
jits1984
jits1984
 
1 year, 4 months ago
Selected Answer: 
A
JobUser required to run queries
upvoted 
1 
times
jits1984
jits1984
 
1 year, 4 months ago
Selected Answer: 
C
https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer
Data viewer role can be applied to a Table and a View.JobUser can be applied only at a Project level not at a Dataset level
https://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser
upvoted 
2 
times
jits1984
jits1984
 
1 year, 4 months ago
Incorrect - should be A
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #86
You have been engaged by your client to lead the migration of their application infrastructure to GCP. One of their current
problems is that the on-premises high performance SAN is requiring frequent and expensive upgrades to keep up with the
variety of workloads that are identified as follows: 20 TB of log archives retained for legal reasons; 500 GB of VM boot/data
volumes and templates; 500 GB of image thumbnails; 200 GB of customer session state data that allows customers to restart
sessions even if off-line for several days. 
Which of the following best reflects your recommendations for a cost-effective storage allocation? 
A. 
Local SSD for customer session state data. Lifecycle-managed Cloud Storage for log archives, thumbnails, and VM
boot/data volumes.
B. 
Memcache backed by Cloud Datastore for the customer session state data. Lifecycle-managed Cloud Storage for log
archives, thumbnails, and VM boot/data volumes. 
Most Voted
C. 
Memcache backed by Cloud SQL for customer session state data. Assorted local SSD-backed instances for VM boot/data
volumes. Cloud Storage for log archives and thumbnails.
D. 
Memcache backed by Persistent Disk SSD storage for customer session state data. Assorted local SSD-backed instances
for VM boot/data volumes. Cloud Storage for log archives and thumbnails.
Correct Answer:
 
B 
Comments
OSNG
OSNG
 
Highly Voted
 
4 years ago
B is correct.
WHY NOT OTHERS.
A: is wrong Local SSD in non-persistent therefore cannot be used for session state (as questions also need to save data for users
who are offline for several days).
C: Again Local SSD cannot be used for boot volume (because its Non-persistent again) and always used for temporary data
storage.
D: Same reason as C.
WHY B?
Left with B that's why, but the question is how to store Boot/Data volume on Cloud Storage?
- Storing other type of data is easy but most comments were about boot volume.
- Boot volume can be stored to Cloud Storage by creating an Custom Image.
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#selecting_image_storage_location
Community vote distribution
B (67%)
D (33%)https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#selecting_image_storage_location
---- Upvote if agree for the clarification of others ----
upvoted 
109 
times
rsamant
rsamant
 
3 years, 6 months ago
Cloud Storage can be used to store image but it can't be used for boot.
upvoted 
8 
times
Davidik79
Davidik79
 
2 years, 9 months ago
"If you need to move your Compute Engine boot disk data outside of your Compute Engine project, you can export a boot disk
image to Cloud Storage as a tar.gz file"
Doc ref: https://cloud.google.com/compute/docs/images/export-image
upvoted 
1 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 4 months ago
Customer is migrating their apps , not only data.
So B is wrong.
App wont work with data volumes in compresses format on cloud storage ( obvious)
upvoted 
2 
times
Chute5118
Chute5118
 
2 years, 4 months ago
Cloud Volumes Service has the ability to send volumes of the CVS service type to Google Cloud Object Storage for long-term
backup and archive. This data-management feature complements volume snapshots, which provide access for development or
test use cases that require short-term recovery
https://cloud.google.com/architecture/partners/netapp-cloud-volumes/back-up
upvoted 
1 
times
Manh
Manh
 
3 years, 4 months ago
it's B. the question is all about storing data. B is right answer
upvoted 
2 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 4 months ago
How can u use cloud storage for VM boot/data volumnes ?
B is wrong
upvoted 
1 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 4 months ago
All the options are debatable and have some flaw. 
But closes answer is B
although it has a flaw mentioned above but is still better than other options
upvoted 
3 
times
neversaynever
neversaynever
 
2 years, 11 months ago
Answer is D - boot volumes (not boot images) cannot come from Cloud Storage - so B is not the answer.
upvoted 
5 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 4 months ago
Guys I think its a english error.
The last line need to be read carefully
Decouple the line after , and Vm boot/data volumes.
I think they mean to use vm persistent disks for boot and data volumes.
B is the answer
upvoted 
2 
times
siumk
siumk
 
Highly Voted
 
4 years, 10 months ago
IMHO Answer is B:
Memcache backed by Cloud Datastore
https://cloud.google.com/appengine/docs/standard/python/memcacheCompute Engine image can be stored in Cloud Storage
https://cloud.google.com/solutions/image-management-best-practices
After the complete sequence of bytes from the disk are written to the file, the file is archived using the tar format and then
compressed using the GZIP format. You can then upload the resulting *.tar.gz file to Cloud Storage and register it as an image in
Compute Engine.
upvoted 
17 
times
Ayzen
Ayzen
 
4 years, 8 months ago
The problem with B is that they are using SAN for data volumes of working VMs, not just to store templates/images. All answers
here are quite bad. But I would go with D, as they are talking about several days of saving users' stale session data, which is
something that can be accomplished with SSD.
upvoted 
10 
times
Bijesh
Bijesh
 
4 years, 1 month ago
@ayzen yes. 
IS cloud datastore optimized to handle such a data (200GB)
upvoted 
1 
times
Toothpick
Toothpick
 
Most Recent
 
5 months, 1 week ago
None of these provide an effective method of storing boot/data volumes
The correct approach would be to create persistent disk Drives for boot/data volumes directly and go with B or D for the remainder
of requirements.
upvoted 
1 
times
otts
otts
 
1 year ago
B is correct. the question lays emphasis on 
a cost-effective storage allocation, and persistent disks are costly than B (that rules
out D)
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
Selected Answer: 
D
This is a troublesome question...
IMHO, it's D. I was between C & D, but seeing how they stored the customer session data as files, storing it in SQL would require
refactoring, and maybe higher latencies.
I'm against B because of the latency added by booting & loading data off Cloud Storage, since it adds network latency to the
equation.
BUT this is assuming the method of using cloud storage is via gcsfuse, that is, using the bucket as a HD.
Now if the way of using it is via image of the disk that is loaded when the instance starts, that would be ok. And that's what I
expected of option D, that it would load an image of the boot/data volume in it's non-persistent disks. No 'persistent' data would be
stored in it, so anything lost when it's shutdown can be ignored.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
Selected Answer: 
D
D is correct answer because
- Memcache backed by Persistent Disk SSD storage for customer session state data. - Persistent disk will ensure that Session
data is preserved evern if not used for long time.
- Assorted local SSD-backed instances for VM boot/data volumes. Provides faster boot up for VMs. (There is no requirement for
persistent storage)
- Cloud Storage for log archives and thumbnails. - For low cost and scalable solution.
upvoted 
1 
times
ductrinh
ductrinh
 
1 year, 3 months ago
d wrong because of local ssd cannot use for boot image. its temporary and will be cleared if vm was suspended >> so chose b
upvoted 
2 
times
A21325412
A21325412
 
1 year, 1 month ago
A lot of folks keep saying D is wrong because of "local SSD". It never mentioned "local" in option D. It said "Persistent Disk SSD".
https://cloud.google.com/compute/docs/diskshttps://cloud.google.com/compute/docs/disks
I would definitely choose D.
upvoted 
2 
times
A21325412
A21325412
 
1 year, 1 month ago
My bad, I was focusing on the memcache. I would choose C. The "assorted local ssd" for VM boot/data volumes can't work, as
Local SSDs are ephemeral, meaning it's lost if the VM instance is stopped or deleted.
upvoted 
1 
times
A21325412
A21325412
 
1 year, 1 month ago
I think I'm tired. I meant to write Option B as the correct answer.
Memcache backed by Cloud Datastore, etc.
upvoted 
1 
times
Murtuza
Murtuza
 
1 year, 3 months ago
Local SSD means Ephemeral ( Temp Disk ) 
so do not confused with PERSISTENT disk ( Permanent Disk ). The data that you
store on a local SSD persists only until the instance is stopped or deleted.
upvoted 
1 
times
medi01
medi01
 
1 year, 8 months ago
Selected Answer: 
B
Local SSD cannot be used for neither boot nor data!!! This rules out B&C. Oh, and A too.
upvoted 
1 
times
Kamaly
Kamaly
 
1 year, 8 months ago
Selected Answer: 
B
Cloud Datastore is the right solution to store the session data
upvoted 
1 
times
feholen210
feholen210
 
1 year, 9 months ago
Selected Answer: 
B
B Seems correct.
upvoted 
1 
times
ile02
ile02
 
1 year, 10 months ago
D. makes more sense
upvoted 
1 
times
WAENANY
WAENANY
 
1 year, 10 months ago
Selected Answer: 
D
d makes more sense
upvoted 
1 
times
Deb2293
Deb2293
 
1 year, 10 months ago
Selected Answer: 
D
ChatGPT says option D.
upvoted 
1 
times
Deb2293
Deb2293
 
1 year, 10 months ago
chatgpt has knowledge till Sept 2021. Don't rely on it bro
upvoted 
3 
times
kaleemahmad75
kaleemahmad75
 
1 year, 11 months ago
Selected Answer: 
D
My answer is D
upvoted 
1 
timesupvoted 
1 
times
Santanu_01
Santanu_01
 
2 years ago
I will go with Option D as it is best practice to keep similar data together and 
seprate OS, Volatile and permanent
upvoted 
2 
times
thamaster
thamaster
 
2 years ago
Selected Answer: 
D
i'll go D because i don't think Cloud storage can be used for booting a VM.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #87
Your web application uses Google Kubernetes Engine to manage several workloads. One workload requires a consistent set of
hostnames even after pod scaling and relaunches. 
Which feature of Kubernetes should you use to accomplish this? 
A. 
StatefulSets 
Most Voted
B. 
Role-based access control
C. 
Container environment variables
D. 
Persistent Volumes
Correct Answer:
 
A 
Comments
Eroc
Eroc
 
Highly Voted
 
3 years, 8 months ago
StatefulSets is a feature of Kubernetes, which the question asks about. Yes, Persistent volumes are required by StatefulSets
(https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/). See the Google documentations for mentioning of
hostnames (https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset)... Answer A
upvoted 
58 
times
tartar
tartar
 
2 years, 11 months ago
A is ok
upvoted 
6 
times
OrangeTiger
OrangeTiger
 
1 year, 6 months ago
thank you!
upvoted 
1 
times
kumarp6
kumarp6
 
2 years, 8 months ago
A is correct, statefulset
upvoted 
2 
times
nitinz
nitinz
 
2 years, 4 months ago
Community vote distribution
A (100%)It is A
upvoted 
2 
times
omermahgoub
omermahgoub
 
Highly Voted
 
6 months, 2 weeks ago
A. StatefulSets
To ensure that a workload in Kubernetes has a consistent set of hostnames even after pod scaling and relaunches, you should
use StatefulSets. StatefulSets are a type of controller in Kubernetes that is used to manage stateful applications. They provide a
number of features that are specifically designed to support stateful applications, including:
Stable, unique network identifiers for each pod in the set
Persistent storage that is automatically attached to pods
Ordered, graceful deployment and scaling of pods
Ordered, graceful deletion and termination of pods
By using StatefulSets, you can ensure that your workload has a consistent set of hostnames even if pods are scaled or
relaunched, which can be important for applications that rely on stable network identifiers.
upvoted 
18 
times
Tamirm
Tamirm
 
4 months, 4 weeks ago
You are the best.. thanks for all the hard work to explain
upvoted 
2 
times
Wangyu60
Wangyu60
 
3 months ago
obviously from chatGPT, but still good to share.
upvoted 
1 
times
kaleemahmad75
kaleemahmad75
 
Most Recent
 
5 months, 3 weeks ago
Selected Answer: 
A
A is the answer
upvoted 
1 
times
megumin
megumin
 
7 months, 4 weeks ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Deepak31
Deepak31
 
8 months ago
A StatefulSet is the Kubernetes controller used to run the stateful application as containers (Pods) in the Kubernetes cluster.
StatefulSets assign a sticky identity—an ordinal number starting from zero—to each Pod instead of assigning random IDs for each
replica Pod. A new Pod is created by cloning the previous Pod’s data.
upvoted 
2 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
this is straight forward question if you know kubernetes concepts. A is right
upvoted 
1 
times
zr79
zr79
 
8 months, 3 weeks ago
I do not know Kubernetes
upvoted 
2 
times
DrishaS4
DrishaS4
 
11 months ago
Selected Answer: 
A
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
upvoted 
3 
times
mv2000
mv2000
 
12 months ago
06/30/2022 Exam
upvoted 
3 
timesupvoted 
3 
times
haroldbenites
haroldbenites
 
1 year, 7 months ago
Go for A.
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
upvoted 
2 
times
vincy2202
vincy2202
 
1 year, 7 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
2 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
A – StatefulSets
StatefulSets are suitable for deploying Kafka, MySQL, Redis, ZooKeeper, and other applications needing unique, persistent
identities and stable hostnames. Read more about StatefulSets. https://cloud.google.com/kubernetes-
engine/docs/concepts/statefulset
C – Container Env Variable, are good if you need to init containers with some static content. E.g. Pod passes to containers its
HOSTNAME (where containers are running), namespace and user defined vars. So, env vars is a way for Pod to init containers at
start up. But, stable hostnames must be preserved at Pod level via StatefulSets.
Defining Env Vars for Container: https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/
upvoted 
6 
times
Arjun1983
Arjun1983
 
1 year, 8 months ago
StatefulSets are designed to deploy stateful applications and clustered applications that save data to persistent storage, such as
Compute Engine persistent disks. StatefulSets are suitable for deploying Kafka, MySQL, Redis, ZooKeeper, and other applications
needing unique, persistent identities and "stable hostnames". Answer is A
upvoted 
2 
times
victory108
victory108
 
2 years, 1 month ago
A. StatefulSets
upvoted 
3 
times
un
un
 
2 years, 1 month ago
A is correct
upvoted 
1 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is A
upvoted 
1 
times
BhupalS
BhupalS
 
2 years, 6 months ago
A is the Ans 
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
upvoted 
1 
times
Chulbul_Pandey
Chulbul_Pandey
 
2 years, 7 months ago
StatefulSets for sequencing.. 
A is correct
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #88
You are using Cloud CDN to deliver static HTTP(S) website content hosted on a Compute Engine instance group. You want to
improve the cache hit ratio. 
What should you do? 
A. 
Customize the cache keys to omit the protocol from the key. 
Most Voted
B. 
Shorten the expiration time of the cached objects.
C. 
Make sure the HTTP(S) header 
ג
€Cache-Region
ג
 €points to the closest region of your users.
D. 
Replicate the static content in a Cloud Storage bucket. Point CloudCDN toward a load balancer on that bucket.
Correct Answer:
 
A 
Comments
shandy
shandy
 
Highly Voted
 
5 years, 1 month ago
Option A is Correct.
https://cloud.google.com/cdn/docs/caching#cache-keys
upvoted 
23 
times
tartar
tartar
 
4 years, 4 months ago
A is ok
upvoted 
7 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Yes, A is correct
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
A, both http and https will use same key.
upvoted 
3 
times
MestreCholas
MestreCholas
 
1 year, 9 months ago
Community vote distribution
A (67%)
D (33%)https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio
upvoted 
6 
times
gfhbox0083
gfhbox0083
 
Highly Voted
 
4 years, 7 months ago
A, for sure.
By default, Cloud CDN uses the complete request URL to build the cache key. For performance and scalability, it’s important to
optimize cache hit ratio. To help optimize your cache hit ratio, you can use custom cache keys .....
upvoted 
10 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
A
Cache Keys and Protocols: Cloud CDN uses cache keys to identify and store content in its cache. By default, the protocol (HTTP
or HTTPS) is included in the cache key. This means that the same content served over HTTP and HTTPS will be cached
separately, reducing the cache hit ratio.
Omitting the Protocol: Customizing the cache keys to omit the protocol allows Cloud CDN to treat HTTP and HTTPS requests for
the same content as identical. This increases the chance of a cache hit, as the CDN can serve the cached content regardless of
the protocol used in the request.
Improved Cache Hit Ratio: By consolidating the cache entries for HTTP and HTTPS versions of the content, you effectively
increase the cache hit ratio. This leads to better performance, reduced latency, and lower costs.
upvoted 
2 
times
nareshthumma
nareshthumma
 
2 months, 1 week ago
Answer is D
upvoted 
1 
times
Prajjwal199831
Prajjwal199831
 
10 months, 2 weeks ago
Selected Answer: 
A
A is right
upvoted 
2 
times
JB28
JB28
 
10 months, 3 weeks ago
option D
upvoted 
2 
times
Pime13
Pime13
 
11 months ago
Selected Answer: 
A
https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio
upvoted 
1 
times
bargou
bargou
 
11 months, 1 week ago
Selected Answer: 
D
i'm with D Option
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
Selected Answer: 
A
https://cloud.google.com/cdn/docs/best-practices#using_custom_cache_keys_to_improve_cache_hit_ratio
A is correct option, use custom keys.
upvoted 
2 
times
Gregwaw
Gregwaw
 
1 year, 3 months ago
Selected Answer: 
D
A and D could be OK but there is no information in the question that both http and https protocols are used (it is in fact not probable
to use both protocols). Anwer A would be only valid when both http and https are in use.
upvoted 
1 
times
MikeH20
MikeH20
 
1 year agoMikeH20
MikeH20
 
1 year ago
Incorrect, respectfully. "HTTP(S)" (the parenthesis) implies that HTTP requests can be made using *either* HTTP or HTTPS,
especially if TLS is not mandatory. 
If a site returns a 200 OK for http://www.example.com/picture/of/a/cat.jpg and
httpS://www.example.come/picture/of/a/cat.jpg, then you should exclude the protocol in the cache key to increase the cache-hit
ratio. If you don't, the CDN will treat both URLs as different objects.
upvoted 
2 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
A sounds correct
upvoted 
1 
times
daidaidai
daidaidai
 
1 year, 4 months ago
Replicating static content in a Cloud Storage bucket and pointing CloudCDN toward a load balancer on that bucket may improve
the distribution of content but is not directly related to improving the cache hit ratio based on the customizing of cache keys.
upvoted 
1 
times
VarunGo
VarunGo
 
1 year, 8 months ago
Selected Answer: 
D
D
This option is the best because Cloud Storage has built-in caching and can serve content faster than Compute Engine instances.
It also allows for better scalability and availability. By pointing Cloud CDN towards a load balancer on the Cloud Storage bucket, the
cache hit ratio can be improved as the content will be served directly from the cache without needing to access the Compute
Engine instances. 
Option A (Customize the cache keys to omit the protocol from the key) may not be effective in improving the cache hit ratio as it
only removes the protocol from the cache key and does not address the underlying issue of slow content delivery.
upvoted 
2 
times
Deb2293
Deb2293
 
1 year, 10 months ago
Selected Answer: 
D
Customizing the cache keys by omitting the protocol from the key (option A) can be a valid approach to improve the cache hit ratio
for CDN delivered Compute Engine, but it may not be the most effective solution for all cases.
Customizing the cache keys can improve the cache hit ratio by reducing the number of cache misses caused by variations in the
request URL, headers, and parameters. However, customizing the cache keys requires careful consideration of the caching
policies, traffic patterns, and content types
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
A. Customize the cache keys to omit the protocol from the key.
To improve the cache hit ratio with Cloud CDN, you should customize the cache keys to omit the protocol (e.g. HTTP or HTTPS)
from the key. This will allow Cloud CDN to cache the same content under both HTTP and HTTPS, which can help to improve the
hit ratio by allowing Cloud CDN to serve content from cache more frequently.
To customize the cache keys, you can use the --key-include-protocol flag when enabling Cloud CDN for your Compute Engine
instance group or load balancer. Setting this flag to false will cause Cloud CDN to omit the protocol from the cache key.
Other options, such as shortening the expiration time of cached objects or replicating content in Cloud Storage, may also help to
improve the cache hit ratio, but customizing the cache keys to omit the protocol is likely to have the greatest impact.
upvoted 
8 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio. A is right
upvoted 
3 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Each cache entry in a Cloud CDN cache is identified by a cache key. When a request comes into the cache, the cache converts
the URI of the request into a cache key, and then compares it with keys of cached entries. If it finds a match, the cache returns
the object associated with that key.
upvoted 
2 
times
aut0pil0t
aut0pil0t
 
2 years, 4 months agoSelected Answer: 
A
Use case:
"A logo needs to be cached whether displayed through HTTP or HTTPS. When you customize the cache keys for the backend
service that holds the logo, clear the Protocol checkbox so that requests through HTTP and HTTPS count as matches for the
logo's cache entry."
https://cloud.google.com/cdn/docs/best-practices#using_custom_cache_keys_to_improve_cache_hit_ratio
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #89
Your architecture calls for the centralized collection of all admin activity and VM system logs within your project. 
How should you collect these logs from both VMs and services? 
A. 
All admin and VM system logs are automatically collected by Stackdriver.
B. 
Stackdriver automatically collects admin activity logs for most services. The Stackdriver Logging agent must be installed
on each instance to collect system logs. 
Most Voted
C. 
Launch a custom syslogd compute instance and configure your GCP project and VMs to forward all logs to it.
D. 
Install the Stackdriver Logging agent on a single compute instance and let it collect all audit and access logs for your
environment.
Correct Answer:
 
B 
Comments
MeasService
MeasService
 
Highly Voted
 
4 years, 8 months ago
Does not agree with D. B is the nearest answer I feel !
upvoted 
43 
times
KouShikyou
KouShikyou
 
4 years, 8 months ago
Agree.
upvoted 
9 
times
tartar
tartar
 
3 years, 11 months ago
B is ok
upvoted 
12 
times
nitinz
nitinz
 
3 years, 4 months ago
It is B, all rest are BS
upvoted 
2 
times
kumarp6
kumarp6
 
3 years, 8 months ago
Community vote distribution
B (81%)
A (19%)B is correct, D is SPOF ...
upvoted 
2 
times
shandy
shandy
 
Highly Voted
 
4 years, 7 months ago
Admin and event logs are configured by default. VM System logs require a logging agent to be configured. So A is not valid.
Answer is B
upvoted 
20 
times
JaimeMS
JaimeMS
 
Most Recent
 
1 month ago
Selected Answer: 
B
Admin and event logs are configured by default. VM System logs require a logging agent to be configured. Answer is B
upvoted 
1 
times
hitmax87
hitmax87
 
1 month, 2 weeks ago
Selected Answer: 
A
Stackdriver Logging agent doesn't need for audit logs
upvoted 
1 
times
convers39
convers39
 
6 months ago
Selected Answer: 
B
B is correct
Now it is recommended to use OpsAgent as a replacement. Although you can create a VM instance with OpsAgent
automatically enabled, which makes it look like 'the logging is automatically enabled', under the hood you need to install the
agent on the instance.
https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/install-agent-vm-creation
upvoted 
3 
times
stefanop
stefanop
 
6 months, 3 weeks ago
Selected Answer: 
A
A: Stackdriver already collects admin logs and GCE logs.
upvoted 
2 
times
spuyol
spuyol
 
6 months, 3 weeks ago
Answer is A
you don't need to install the Stackdriver Logging agent or any other agents in order to collect admin or system logs from
compute engine instances (VM):
https://cloud.google.com/compute/docs/logging/activity-logs
https://cloud.google.com/compute/docs/logging/audit-logging
upvoted 
2 
times
squishy_fishy
squishy_fishy
 
7 months, 3 weeks ago
Answer is B, but it would be more accurate if the answer mentioned install the Ops Agent on the VMs.
https://cloud.google.com/logging/docs/agent/ops-agent
upvoted 
1 
times
jlambdan
jlambdan
 
1 year, 3 months ago
Selected Answer: 
B
answer is B as per this tutorial step: https://cloud.google.com/logging/docs/logging-gce-quickstart#install-agent
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Stackdriver does not require the Stackdriver Logging agent to be installed in order to collect system logs.
Stackdriver is a cloud monitoring and logging platform that is integrated with Google Cloud Platform (GCP) and is designed to
collect, monitor, and troubleshoot logs from your GCP resources. By default, Stackdriver automatically collects admin activity
logs for most GCP services, as well as VM system logs. This means that you don't need to install the Stackdriver Logging agent
or any other agents in order to collect these logs - they are automatically collected and centralized by Stackdriver.
However, if you want to collect logs from other sources that are not automatically collected by Stackdriver (e.g. logs from
applications running on your VMs, logs from on-premises systems, etc.), you can use the Stackdriver Logging agent to forward
these logs to Stackdriver. The agent is a lightweight daemon that runs on your VMs or other hosts, and it can be used to collectthese logs to Stackdriver. The agent is a lightweight daemon that runs on your VMs or other hosts, and it can be used to collect
logs from various sources and forward them to Stackdriver for centralized storage and analysis.
upvoted 
6 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Answer is A
In Google Cloud Platform (GCP), you can use Stackdriver to collect and centralize all admin activity and VM system logs within
your project. Stackdriver is a powerful cloud monitoring and logging platform that is integrated with GCP, and it provides a
number of features that are specifically designed to help you collect, monitor, and troubleshoot logs from your GCP resources.
One of the key features of Stackdriver is that it automatically collects admin activity logs for most GCP services, as well as VM
system logs. This means that you don't need to install any agents or configure any additional components to collect these logs
- they are automatically collected and centralized by Stackdriver.
To view and analyze your logs in Stackdriver, you can use the Stackdriver Logs Viewer, which provides a powerful interface for
searching, filtering, and aggregating your logs. You can also use the Stackdriver Logs API to programmatically access your
logs, or use the Stackdriver Logging agent to forward your logs to other log management or analysis tools.
upvoted 
6 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Selected Answer: 
B
https://cloud.google.com/logging/docs/agent/logging/installation#before_you_begin
upvoted 
5 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
Thank you for sharing the link, B is right
upvoted 
1 
times
panqueca
panqueca
 
2 years, 1 month ago
Selected Answer: 
B
it's B
upvoted 
2 
times
haroldbenites
haroldbenites
 
2 years, 7 months ago
Go for B.
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 7 months ago
B is the correct answer.
upvoted 
2 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
B – stackdriver automatically collects admin activity logs for most services. Stackdriver Logging Agenct must be installed on
each instance to collect system logs.
Read more about Logging Agent. https://cloud.google.com/logging/docs/agent/
Logging agent streams logs from 3rd party apps and systems SW (syslog on Linux) to Logging. It is best practice to run the
Logging agent on all your VM instances. It runs on Linux and Windows.
Cloud Audit Logs says that Admin Activity audit logs are always enabled.
upvoted 
3 
times
unnikrisb
unnikrisb
 
2 years, 8 months ago
Agree with B
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #90
You have an App Engine application that needs to be updated. You want to test the update with production traffic before
replacing the current application version. 
What should you do? 
A. 
Deploy the update using the Instance Group Updater to create a partial rollout, which allows for canary testing.
B. 
Deploy the update as a new version in the App Engine application, and split traffic between the new and current
versions. 
Most Voted
C. 
Deploy the update in a new VPC, and use Google's global HTTP load balancing to split traffic between the update and
current applications.
D. 
Deploy the update as a new App Engine application, and use Google's global HTTP load balancing to split traffic between
the new and current applications.
Correct Answer:
 
B 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
3 years, 8 months ago
I think B is correct. Because GAE supports service version control and A/B test.
Is my understanding correct?
upvoted 
60 
times
kumarp6
kumarp6
 
2 years, 8 months ago
Yes, B is correct
upvoted 
5 
times
nitinz
nitinz
 
2 years, 4 months ago
Only B works.
upvoted 
4 
times
ADVIT
ADVIT
 
Highly Voted
 
3 years, 4 months ago
Only one 
App Engine application can be created per Project.
Community vote distribution
B (100%)Only one 
App Engine application can be created per Project.
So it's B.
upvoted 
15 
times
omermahgoub
omermahgoub
 
Most Recent
 
6 months, 2 weeks ago
B. Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions.
To test an update to an App Engine application with production traffic before replacing the current version, you can deploy the
update as a new version in the App Engine application and split traffic between the new and current versions. This is known as a
"blue-green" deployment, and it allows you to test the new version with a portion of production traffic while the current version
is still serving the remainder of traffic.
To split traffic between the new and current versions, you can use the App Engine traffic splitting feature. This feature allows
you to specify the percentage of traffic that should be sent to each version, and it can be used to gradually ramp up traffic to
the new version over time. This allows you to test the new version with a small portion of traffic initially, and gradually increase
the traffic as you become more confident in the update.
upvoted 
11 
times
omermahgoub
omermahgoub
 
6 months, 2 weeks ago
Other options, such as deploying the update in a new VPC or as a new App Engine application, are not recommended for
testing updates with production traffic, as they can be more complex and may require additional steps to set up.
upvoted 
3 
times
TonytheTiger
TonytheTiger
 
7 months ago
Answer B : 
You can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a
service. Splitting traffic allows you to conduct A/B testing between your versions and provides control over the pace when
rolling out features.
Traffic splitting is applied to URLs that do not explicitly target a version. For example, the following URLs split traffic because
they target all the available versions within the specified service:
https://cloud.google.com/appengine/docs/standard/splitting-traffic
upvoted 
3 
times
megumin
megumin
 
7 months, 4 weeks ago
Selected Answer: 
B
B is ok
upvoted 
2 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
B is right , Option D is just to confuse you.
Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions.
upvoted 
2 
times
DrishaS4
DrishaS4
 
11 months ago
Selected Answer: 
B
Versioning is supported in App Engine.
upvoted 
2 
times
ghadxx
ghadxx
 
1 year, 4 months ago
Selected Answer: 
B
Versioning is supported in App Engine.
upvoted 
2 
times
haroldbenites
haroldbenites
 
1 year, 7 months ago
Go for D,
The option B don´t say with wich service will split the traffic.
The option D gives more datail and makes sense.
upvoted 
1 
times
ale_brd_111
ale_brd_111
 
7 months, 2 weeks ago
No mate, only one app engine per project can be deployed, you can have multiple version on the same app tho. D is to
confuse you. B is the only feasible answer in here.
upvoted 
4 
timesupvoted 
4 
times
vincy2202
vincy2202
 
1 year, 7 months ago
B is the correct answer
upvoted 
1 
times
robotgeek
robotgeek
 
1 year, 7 months ago
A is not because "Instance Group Updater " is only for Computer Engine MIG
upvoted 
1 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
B – Deploy the update as a new version in AppEngine app, and split traffic between the new and current versions.
Traffic Splitting is feature of AppEngine for A/B testing. 
https://cloud.google.com/appengine/docs/standard/python/splitting-traffic
upvoted 
6 
times
[Removed]
[Removed]
 
1 year, 8 months ago
B is correct. App Engine supports versioning.
upvoted 
1 
times
unnikrisb
unnikrisb
 
1 year, 8 months ago
B is correct... Canary Testing -> Traffic Splitting
upvoted 
1 
times
victory108
victory108
 
2 years, 1 month ago
B. Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions
upvoted 
3 
times
un
un
 
2 years, 1 month ago
B is the answer
upvoted 
1 
times
getzsagar
getzsagar
 
2 years, 2 months ago
Answer - B 
Configure how much traffic the version that you just deployed should receive.
By default, the initial version that you deploy to your App Engine application is automatically configured to receive 100% of
traffic. However, all subsequent versions that you deploy to that same App Engine application must be manually configured,
otherwise they receive no traffic.
For details about how to configure traffic for your versions, see Migrating and Splitting Traffic.
https://cloud.google.com/appengine/docs/admin-api/migrating-splitting-traffic
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #91
All Compute Engine instances in your VPC should be able to connect to an Active Directory server on specific ports. Any other
traffic emerging from your instances is not allowed. You want to enforce this using VPC firewall rules. 
How should you configure the firewall rules? 
A. 
Create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100
to allow the Active Directory traffic for all instances. 
Most Voted
B. 
Create an egress rule with priority 100 to deny all traffic for all instances. Create another egress rule with priority 1000
to allow the Active Directory traffic for all instances.
C. 
Create an egress rule with priority 1000 to allow the Active Directory traffic. Rely on the implied deny egress rule with
priority 100 to block all traffic for all instances.
D. 
Create an egress rule with priority 100 to allow the Active Directory traffic. Rely on the implied deny egress rule with
priority 1000 to block all traffic for all instances.
Correct Answer:
 
A 
Comments
wk
wk
 
Highly Voted
 
4 years, 8 months ago
Should be A, there is no implied deny egress but only implied allow egress
https://cloud.google.com/vpc/docs/firewalls#default_firewall_rules
Every VPC network has two implied firewall rules. These rules exist, but are not shown in the Cloud Console:
The implied allow egress rule: An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible
(65535) lets any instance send traffic to any destination, except for traffic blocked by GCP. Outbound access may be restricted
by a higher priority firewall rule. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance
has an external IP address or uses a NAT instance. Refer to Internet access requirements for more details.
The implied deny ingress rule: An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible
(65535) protects all instances by blocking incoming traffic to them. Incoming access may be allowed by a higher priority rule.
Note that the default network includes some additional rules that override this one, allowing certain types of incoming traffic.
upvoted 
93 
times
Community vote distribution
A (100%)nitinz
nitinz
 
3 years, 4 months ago
It is A, rest all do not make sense. If you think of any other option then go back and read about firewalls. Seriously you are not
ready for this exam.
upvoted 
2 
times
zr79
zr79
 
1 year, 8 months ago
thank you
upvoted 
1 
times
kumarp6
kumarp6
 
3 years, 8 months ago
B is correct...
upvoted 
2 
times
p4
p4
 
3 years, 7 months ago
from a book:
"Firewall rules control network traffic by blocking or allowing traffic into (ingress) or out of (egress) a network. Two implied
firewall rules are defined with VPCs: one blocks all incoming traffic, and the other allows all outgoing traffic. You can change
this behavior
Virtual Private Clouds 115
116 Chapter 6 ■ Designing Networks
by defining firewall rules with higher priority. Firewall rules have a priority specified by an integer from 0 to 65535, with 0
being the highest priority and 65535 being the lowest."
so this confirms A
upvoted 
13 
times
SSS987
SSS987
 
5 months, 3 weeks ago
Good summary. To the point!
upvoted 
1 
times
MeasService
MeasService
 
Highly Voted
 
4 years, 8 months ago
Agree Correct is A. There is no implied deny egress only deny ingress rule
upvoted 
10 
times
MyPractice
MyPractice
 
4 years, 6 months ago
Agree with A . only Implied allow egress rule (or) Implied deny ingress rule. 
There is No "Implied deny egress rule" which rules out C & D
upvoted 
3 
times
ManishKS
ManishKS
 
Most Recent
 
11 months, 1 week ago
B. Create an egress rule with priority 100 to deny all traffic for all instances. Create another egress rule with priority 1000 to
allow the Active Directory traffic for all instances.
This option creates a deny all rule with a lower priority and an allow rule with a higher priority. 
This option will work as intended, as the Active Directory traffic will be allowed and all other outbound traffic will be blocked.
upvoted 
2 
times
Emmarof
Emmarof
 
1 year, 3 months ago
The answer to this question is A.
Explanation:
To enforce the requirement that all Compute Engine instances in your VPC should be able to connect to an Active Directory
server on specific ports while blocking any other traffic emerging from instances, the following two egress rules should be
created:
Create an egress rule with priority 1000 to deny all traffic for all instances.
Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.
In this configuration, the rule that allows the AD traffic has a lower priority number than the rule that denies all other traffic.
Therefore, this rule should be evaluated first.
upvoted 
2 
timesupvoted 
2 
times
Deb2293
Deb2293
 
1 year, 4 months ago
Selected Answer: 
A
It should be A.
It cannot be D as The Implied allow egress rule, with its action of “allow”, allows all traffic out to the 0.0. 0.0/0 destination,
which basically means everywhere. The priority of the implied allow egress rule is the lowest possible, 65535. The implied deny
ingress rule, with an action of “deny”, blocks all incoming connections.
upvoted 
1 
times
8d31d36
8d31d36
 
1 year, 4 months ago
The correct answer is B.
To enforce that all Compute Engine instances in a VPC can connect to an Active Directory server on specific ports while
blocking any other traffic, you should create an egress rule with a high priority (lower numerical value) to deny all traffic from
all instances, and another egress rule with a lower priority (higher numerical value) to allow traffic to the Active Directory server
on the specific ports.
Option B creates the necessary egress rules in the correct order: a deny-all rule with a high priority (100), followed by an allow
rule for the Active Directory traffic with a lower priority (1000). This way, traffic to the Active Directory server is allowed, but all
other traffic is denied.
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
It is pretty straight forward question, It this case priority low should be allow and high priority rules deny all requests. A is right
upvoted 
2 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Selected Answer: 
A
https://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules
upvoted 
1 
times
mv2000
mv2000
 
1 year, 12 months ago
06/30/2022 Exam question.
upvoted 
6 
times
moiradavis
moiradavis
 
1 year, 11 months ago
Oh, really? I got this question on my exam 2 years ago, I did not expect to repeat this kind of questions in the current exam.
upvoted 
1 
times
Baumster
Baumster
 
2 years, 4 months ago
OT: why is there no way to mark questions for review/repeat later on?
upvoted 
1 
times
haroldbenites
haroldbenites
 
2 years, 7 months ago
Go for A.
While the priority is higher, the egress rule is more restricted.
While the priority is higher, the ingress rule is more free.
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 7 months ago
Selected Answer: 
A
A is correct answer
upvoted 
1 
times
vchrist
vchrist
 
2 years, 7 months agovchrist
vchrist
 
2 years, 7 months ago
Selected Answer: 
A
to understand rules priority:
https://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules
upvoted 
1 
times
nqthien041292
nqthien041292
 
2 years, 7 months ago
Selected Answer: 
A
Vote A
upvoted 
1 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
A – create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to
allow the Active Directory traffic for all instances.
Default Firewall rules (aka implied rules) are following:
1) Egress traffic is allowed to all IP/ports.
2) Ingress traffic is disabled completely.
Both these rules have lowest priority (65535) and cannot be removed.
https://cloud.google.com/vpc/docs/firewalls#default_firewall_rules
upvoted 
2 
times
victory108
victory108
 
3 years, 1 month ago
A. Create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to
allow the Active Directory traffic for all instances.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #92
Your customer runs a web service used by e-commerce sites to offer product recommendations to users. The company has
begun experimenting with a machine learning model on Google Cloud Platform to improve the quality of results. 
What should the customer do to improve their model's results over time? 
A. 
Export Cloud Machine Learning Engine performance metrics from Stackdriver to BigQuery, to be used to analyze the
efficiency of the model.
B. 
Build a roadmap to move the machine learning model training from Cloud GPUs to Cloud TPUs, which offer better
results.
C. 
Monitor Compute Engine announcements for availability of newer CPU architectures, and deploy the model to them as
soon as they are available for additional performance.
D. 
Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data. 
Most
Voted
Correct Answer:
 
D 
Comments
ghadxx
ghadxx
Highly Voted 
1 year, 4 months ago
Selected Answer: 
D
Model performance is generally based on the volume of its training data input. The more the data, the better the model.
upvoted 
20 
times
AzureDP900
AzureDP900
8 months, 3 weeks ago
I agree with you, D is right
upvoted 
1 
times
Sur_Nikki
Sur_Nikki
1 month, 3 weeks ago
Yes, correctly said..This is actually a question for Data Engineer role
upvoted 
1 
times
sgofficial
sgofficial
Highly Voted 
11 months, 1 week ago
Selected Answer: 
D
Community vote distribution
D (100%)A,B,C is defining about the performance of ML but not the result....only the training data will give good ML result/predictions
upvoted 
6 
times
Deb2293
Deb2293
Most Recent 
4 months ago
Selected Answer: 
D
Best answer is D. Other 3 makes no sense
upvoted 
1 
times
PST21
PST21
4 months, 2 weeks ago
Need to improve the model results and not performance .. hence D
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
6 months, 3 weeks ago
Selected Answer: 
D
Answer is D
upvoted 
1 
times
DrishaS4
DrishaS4
11 months ago
Selected Answer: 
D
Model performance is generally based on the volume of its training data input. The more the data, the better the model.
upvoted 
3 
times
sivre
sivre
1 year, 3 months ago
The following insights and recommendations can be exported (to bigquery):
IAM recommender
VM machine type recommender
Managed instance group machine type recommender 
Idle PD recommender
Idle VM recommender
Cloud SQL overprovisioned instance recommender 
Cloud SQL idle instance recommender
Unattended project recommender
Cloud Run Service Identity recommender
https://cloud.google.com/recommender/docs/bq-export/export-recommendations-to-bq
None of this is correlated with Machine Learning, how can be D? looks more A the answer
upvoted 
2 
times
kimharsh
kimharsh
1 year, 2 months ago
what we will do with metrics , it won't improve our Machine learning model , D is the closest answer , also it didn't say export it
said Save , which could be manually moving the data to BQ
upvoted 
2 
times
Pime13
Pime13
1 year, 5 months ago
Selected Answer: 
D
i vote D
upvoted 
3 
times
victory108
victory108
1 year, 6 months ago
D. Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data.
upvoted 
2 
times
LoveT
LoveT
1 year, 6 months agoMost Voted
A voting comment increases the vote count for the chosen answer by one. 
Upvoting a comment with a selected answer will also increase the vote count towards that answer by one. 
So if you see a comment that you already agree with,
you can upvote it instead of posting a new comment.
Save
 
Cancel
Loading 
...
Commenting
"training data" is the key in option "D" and that's the answer
upvoted 
4 
times
vincy2202
vincy2202
1 year, 6 months ago
Selected Answer: 
D
D seems to be the correct answer
upvoted 
1 
timesCommenting
×
In order to participate in the comments you need to be logged-in. 
You can 
sign-up 
or 
login 
(it's free).
Ok 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #93
A development team at your company has created a dockerized HTTPS web application. You need to deploy the application on
Google Kubernetes Engine (GKE) and make sure that the application scales automatically. 
How should you deploy to GKE? 
A. 
Use the Horizontal Pod Autoscaler and enable cluster autoscaling. Use an Ingress resource to load-balance the HTTPS
traffic. 
Most Voted
B. 
Use the Horizontal Pod Autoscaler and enable cluster autoscaling on the Kubernetes cluster. Use a Service resource of
type LoadBalancer to load-balance the HTTPS traffic.
C. 
Enable autoscaling on the Compute Engine instance group. Use an Ingress resource to load-balance the HTTPS traffic.
D. 
Enable autoscaling on the Compute Engine instance group. Use a Service resource of type LoadBalancer to load-balance
the HTTPS traffic.
Correct Answer:
 
A 
Comments
crypt0
crypt0
 
Highly Voted
 
5 years, 2 months ago
Why not using Ingress? (A)
upvoted 
29 
times
techalik
techalik
 
4 years, 1 month ago
I think A is OK:
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
It is A, K8s best way to LB is Ingress.
upvoted 
5 
times
Smart
Smart
 
4 years, 10 months ago
"Ingress is a Kubernetes resource that encapsulates a collection of rules and configuration for routing external HTTP(S) traffic
to internal services.
Community vote distribution
A (71%)
B (29%)to internal services.
On GKE, Ingress is implemented using Cloud Load Balancing. When you create an Ingress in your cluster, GKE creates an
HTTP(S) load balancer and configures it to route traffic to your application."
Are you exposing multiple services through single IP address? Hence, do you need routing your traffic?
Correct answer is B.
upvoted 
42 
times
Smart
Smart
 
4 years, 10 months ago
My bad, as stated by other, Service doesn't support L7 load balancing. Hence, need to setup ingress resource. Correct answer
is A.
upvoted 
46 
times
tartar
tartar
 
4 years, 4 months ago
B is ok.
https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app
upvoted 
9 
times
GopiSivanathan
GopiSivanathan
 
4 years, 2 months ago
service resource does a NLB using IP address, however, Ingress does HTTP(S) Load balancer. A should be an answer.
upvoted 
8 
times
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
Name is service resource, it's B:
https://cloud.google.com/kubernetes-engine/docs/concepts/service?hl=es-419
upvoted 
13 
times
nareshthumma
nareshthumma
 
Most Recent
 
2 months, 1 week ago
Answer A
upvoted 
1 
times
mstaicu
mstaicu
 
6 months ago
Selected Answer: 
A
A and B both create under the hood a Service of type LoadBalancer with external IP address. However, when it comes to http(s)
traffic an ingress is the way to go because of ssl termination and for the routing options.
upvoted 
2 
times
huuthanhdlv
huuthanhdlv
 
7 months, 2 weeks ago
Selected Answer: 
A
C & D is clearly incorrect.
B is incorrect because of this:
"service of type LoadBalancer to load-balance the HTTPS traffic." 
GKE Service Load Balancer is L4 Network or Internal Load Balancer, does not support HTTPS traffic.
Thus only A is correct.
upvoted 
4 
times
hitmax87
hitmax87
 
7 months, 3 weeks ago
Selected Answer: 
A
The clue is HTTPS traffic. You need L7 stack. It can be achieved only through ingress controller.
upvoted 
3 
times
nanasenishino
nanasenishino
 
7 months, 3 weeks ago
B
A. Ingress resource: While Ingress can be used for external load balancing, it often requires additional configuration for HTTPS
termination (offloading SSL from your application containers). Additionally, LoadBalancer services typically offer a simplertermination (offloading SSL from your application containers). Additionally, LoadBalancer services typically offer a simpler
setup for basic external load balancing without HTTPS termination concerns.
C & D. Compute Engine Instance Group Autoscaling: GKE manages its own nodes separate from Compute Engine instances.
Autoscaling on a Compute Engine instance group wouldn't manage the Kubernetes pods or nodes effectively in this scenario.
upvoted 
1 
times
Pime13
Pime13
 
11 months ago
Selected Answer: 
A
service loadBalancer: https://cloud.google.com/kubernetes-engine/docs/concepts/service-load-balancer
This page provides a general overview of how Google Kubernetes Engine (GKE) creates and manages Google Cloud load
balancers when you apply a Kubernetes LoadBalancer Services manifest. It describes the different types of load balancers and
how settings like the externalTrafficPolicy and GKE subsetting for L4 internal load balancers determine how the load balancers
are configured. -> l4 tcp/udp not https
Ingress: https://cloud.google.com/kubernetes-engine/docs/concepts/ingress This page provides a general overview of what
Ingress for external Application Load Balancers is and how it works. Google Kubernetes Engine (GKE) provides a built-in and
managed Ingress controller called GKE Ingress. This controller implements Ingress resources as Google Cloud load balancers
for HTTP(S) workloads in GKE. -S http(s)
upvoted 
3 
times
gun123
gun123
 
12 months ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
bandegg
bandegg
 
1 year ago
Selected Answer: 
A
I'm assuming B is the suggested answer because a the question doesn't state that the application should be available externally.
Services allow exposing resources internally and to load balancers.
However, it should be A, as the assumption would be a an external web application.
https://cloud.google.com/kubernetes-engine/docs/concepts/service
upvoted 
2 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
B
Most if the labs in Google boost skills discuss how to expose the deployment using a load balancer.
upvoted 
2 
times
AwsSuperTrooper
AwsSuperTrooper
 
1 year, 1 month ago
Selected Answer: 
A
https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
"This page provides a general overview of what Ingress for external Application Load Balancers is and how it works. Google
Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. This controller implements
Ingress resources as Google Cloud load balancers for HTTP(S) workloads in GKE."
upvoted 
2 
times
thewalker
thewalker
 
1 year, 1 month ago
https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
As there is no mention about the type of the traffic, Internal or external - Going with A - Ingress.
upvoted 
1 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
Selected Answer: 
B
Option-C and D are straightforwardly wrong
Between A and B : B is the correct answer, because it makes use of loadbalancing the ingress in K8S native style. That is the
reason why cluster scaling is also done. 
This is how it should 
External Load Balancing Ingress --> K8S Service of type LoadBalancer --> pods that can autoscale
Directly allowing external loadbalcing ingress to autoscaled Pod, doesn't makes sense to use GKEDirectly allowing external loadbalcing ingress to autoscaled Pod, doesn't makes sense to use GKE
upvoted 
1 
times
someone2011
someone2011
 
1 year, 3 months ago
Ingress is Https while Service is TCP/UDP. 
https://cloud.google.com/load-balancing/docs/choosing-load-balancer
https://cloud.google.com/kubernetes-engine/docs/concepts/service-networking
upvoted 
2 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
B is correct
upvoted 
2 
times
willyf1
willyf1
 
1 year, 4 months ago
Selected Answer: 
A
A Is the best choice
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #94
You need to design a solution for global load balancing based on the URL path being requested. You need to ensure operations
reliability and end-to-end in- transit encryption based on Google best practices. 
What should you do? 
A. 
Create a cross-region load balancer with URL Maps.
B. 
Create an HTTPS load balancer with URL Maps. 
Most Voted
C. 
Create appropriate instance groups and instances. Configure SSL proxy load balancing.
D. 
Create a global forwarding rule. Configure SSL proxy load balancing.
Correct Answer:
 
B 
Comments
victory108
victory108
 
Highly Voted
 
2 years, 1 month ago
B. Create an HTTPS load balancer with URL maps.
upvoted 
14 
times
betiy
betiy
 
Highly Voted
 
3 years, 6 months ago
URL paths supported only in HTTP(S) Load balancing 
https://cloud.google.com/load-balancing/docs/ssl/#FAQ
upvoted 
10 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days ago
Selected Answer: 
B
I will go for B.
upvoted 
1 
times
examch
examch
 
6 months, 1 week ago
Selected Answer: 
B
B is the correct Answer,
Google Cloud HTTP(S) load balancers and Traffic Director use a Google Cloud configuration resource called a URL map to
Community vote distribution
B (100%)Google Cloud HTTP(S) load balancers and Traffic Director use a Google Cloud configuration resource called a URL map to
route HTTP(S) requests to backend services or backend buckets.
For example, with an external HTTP(S) load balancer, you can use a single URL map to route requests to different destinations
based on the rules configured in the URL map:
Requests for https://example.com/video go to one backend service.
Requests for https://example.com/audio go to a different backend service.
Requests for https://example.com/images go to a Cloud Storage backend bucket.
Requests for any other host and path combination go to a default backend service.
URL maps are used with the following Google Cloud products:
External HTTP(S) Load Balancing (global, regional, and classic modes)
Internal HTTP(S) Load Balancing
Traffic Director
https://cloud.google.com/load-balancing/docs/url-map-concepts
upvoted 
6 
times
omermahgoub
omermahgoub
 
6 months, 2 weeks ago
B. Create an HTTPS load balancer with URL Maps.
An HTTPS load balancer is a type of load balancer that can distribute incoming HTTPS traffic to one or more back-end services,
such as Compute Engine instances or Google Kubernetes Engine clusters. It can also provide SSL/TLS termination, enabling you
to use your own SSL/TLS certificates and keys.
You can use URL Maps to configure the HTTPS load balancer to route traffic based on the URL path being requested. This
allows you to set up different URL paths to be served by different back-end services, providing a high level of flexibility in your
load balancing configuration.
upvoted 
1 
times
omermahgoub
omermahgoub
 
6 months, 2 weeks ago
Option A, creating a cross-region load balancer with URL Maps, is also a valid solution, but it is not specifically designed for
end-to-end in-transit encryption.
Option C, creating appropriate instance groups and instances and configuring SSL proxy load balancing, is not a complete
solution for global load balancing. SSL proxy load balancing is a feature that enables you to terminate SSL/TLS connections at
the load balancer and establish a new SSL/TLS connection between the load balancer and the back-end service. It is not a
global load balancing solution in and of itself.
Option D, creating a global forwarding rule and configuring SSL proxy load balancing, is not a complete solution for global
load balancing based on the URL path being requested. A global forwarding rule is a type of load balancing configuration
that directs traffic to a specific back-end service based on the IP address and port of the incoming request. It does not allow
for routing based on the URL path.
Regenerate
upvoted 
3 
times
TonytheTiger
TonytheTiger
 
7 months ago
Answer B: 
URL maps used with global external HTTP(S) load balancers and regional external HTTP(S) load balancer support
several advanced traffic management features such as header-based traffic steering, weight-based traffic splitting, and request
mirroring.
https://cloud.google.com/load-balancing/docs/https#url-maps
upvoted 
3 
times
megumin
megumin
 
7 months, 4 weeks ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Sbgani
Sbgani
 
10 months ago
Selected Answer: 
B
UrlMaps are used to route requests to a backend service based on rules that you define for the host and path of an incomingUrlMaps are used to route requests to a backend service based on rules that you define for the host and path of an incoming
URL.
upvoted 
5 
times
Sbgani
Sbgani
 
10 months ago
https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_url_map ANS B
upvoted 
2 
times
DrishaS4
DrishaS4
 
11 months ago
Selected Answer: 
B
https://cloud.google.com/load-balancing/docs/https/url-map
upvoted 
4 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
thank you for pointing the link
upvoted 
1 
times
haroldbenites
haroldbenites
 
1 year, 6 months ago
Go for B
upvoted 
2 
times
vincy2202
vincy2202
 
1 year, 7 months ago
Selected Answer: 
B
B is correct answer
upvoted 
2 
times
nqthien041292
nqthien041292
 
1 year, 7 months ago
Selected Answer: 
B
Vote B
upvoted 
1 
times
un
un
 
2 years, 1 month ago
B is correct
upvoted 
1 
times
ccmcwolf
ccmcwolf
 
2 years, 2 months ago
there are interl https load balancers they are regional https://cloud.google.com/load-balancing/docs/l7-internal
upvoted 
1 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is B
upvoted 
2 
times
bnlcnd
bnlcnd
 
2 years, 5 months ago
confused with A vs B. A has the word "cross region" but finally find out HTTP/S Load Balancing is naturally global.
- B
upvoted 
5 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #95
You have an application that makes HTTP requests to Cloud Storage. Occasionally the requests fail with HTTP status codes of
5xx and 429. 
How should you handle these types of errors? 
A. 
Use gRPC instead of HTTP for better performance.
B. 
Implement retry logic using a truncated exponential backoff strategy. 
Most Voted
C. 
Make sure the Cloud Storage bucket is multi-regional for geo-redundancy.
D. 
Monitor https://status.cloud.google.com/feed.atom and only make requests if Cloud Storage is not reporting an incident.
Correct Answer:
 
B 
Comments
bigob4ek
bigob4ek
 
Highly Voted
 
3 years, 7 months ago
Answer is B
You should use exponential backoff to retry your requests when receiving errors with 5xx or 429 response codes from Cloud
Storage.
https://cloud.google.com/storage/docs/request-rate
upvoted 
42 
times
nitinz
nitinz
 
2 years, 4 months ago
It is B
upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
I agree with you, B should be right
upvoted 
1 
times
Sbgani
Sbgani
 
Highly Voted
 
10 months ago
HTTP 408, 429, and 5xx response codes.
Exponential backoff algorithm
For requests that meet both the response and idempotency criteria, you should generally use truncated exponential backoff.
Community vote distribution
B (100%)For requests that meet both the response and idempotency criteria, you should generally use truncated exponential backoff.
Truncated exponential backoff is a standard error handling strategy for network applications in which a client periodically
retries a failed request with increasing delays between requests.
An exponential backoff algorithm retries requests exponentially, increasing the waiting time between retries up to a maximum
backoff time. See the following workflow example to learn how exponential backoff works:
You make a request to Cloud Storage.
If the request fails, wait 1 + random_number_milliseconds seconds and retry the request.
If the request fails, wait 2 + random_number_milliseconds seconds and retry the request.
If the request fails, wait 4 + random_number_milliseconds seconds and retry the request.
And so on, up to a maximum_backoff time.
Continue waiting and retrying up to a maximum amount of time (deadline), but do not increase the maximum_backoff wait
period between retries
upvoted 
13 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days ago
Selected Answer: 
B
I will go for B.
upvoted 
1 
times
omermahgoub
omermahgoub
 
6 months, 2 weeks ago
. Implement retry logic using a truncated exponential backoff strategy.
HTTP status codes of 5xx and 429 typically indicate that there is a temporary issue with the service or that the rate of requests is
too high. To handle these types of errors, it is generally recommended to implement retry logic in your application using a
truncated exponential backoff strategy.
Truncated exponential backoff involves retrying the request after an initial delay, and then increasing the delay exponentially
for each subsequent retry up to a maximum delay. This approach helps to reduce the number of failed requests and can
improve the reliability of your application.
upvoted 
3 
times
omermahgoub
omermahgoub
 
6 months, 2 weeks ago
Option A, using gRPC instead of HTTP for better performance, is not directly related to handling HTTP status codes of 5xx and
429. gRPC is a high-performance RPC framework that can be used in place of HTTP, but it is not a solution for handling errors.
Option C, making sure the Cloud Storage bucket is multi-regional for geo-redundancy, may help improve the reliability of the
service, but it is not a solution for handling errors.
Option D, monitoring https://status.cloud.google.com/feed.atom and only making requests if Cloud Storage is not reporting
an incident, is not a practical solution for handling errors. This approach would require constantly monitoring the status page
and could result in significant delays in processing requests. Instead, it is generally recommended to implement retry logic in
your application to handle errors.
upvoted 
2 
times
megumin
megumin
 
7 months, 4 weeks ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Sbgani
Sbgani
 
10 months ago
Selected Answer: 
B
https://cloud.google.com/storage/docs/retry-strategy
upvoted 
2 
times
DrishaS4
DrishaS4
 
11 months ago
Selected Answer: 
B2xx – successful requests;
4xx, 5xx – failed requests;
3xx – requests that require redirect.
https://cloud.google.com/storage/docs/json_api/v1/status-codes
upvoted 
2 
times
haroldbenites
haroldbenites
 
1 year, 6 months ago
Go for B
upvoted 
2 
times
vincy2202
vincy2202
 
1 year, 7 months ago
B is the correct answer
upvoted 
2 
times
nqthien041292
nqthien041292
 
1 year, 7 months ago
Selected Answer: 
B
Vote B
upvoted 
2 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
B
vote B
upvoted 
1 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
B – Implement retry logic using a truncated exponential backoff strategy.
Per HTTP status and error codes for JSON the status codes are:
2xx – successful requests;
4xx, 5xx – failed requests;
3xx – requests that require redirect.
https://cloud.google.com/storage/docs/json_api/v1/status-codes
429 – Too many requests: your app tries to use more that its limit, additional requests will fail. Decrease your client’s requests
and/or use truncated exponential backoff (used for all requests with 5xx and 429 errors).
https://cloud.google.com/storage/docs/retry-strategy
upvoted 
2 
times
victory108
victory108
 
2 years, 1 month ago
B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.
upvoted 
2 
times
victory108
victory108
 
2 years, 1 month ago
This B. Implement retry logic using a truncated exponential backoff strategy.
upvoted 
1 
times
un
un
 
2 years, 1 month ago
Answer is B.
Link provided by 
bigob4ek has details
upvoted 
1 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is B
upvoted 
1 
times
CloudGenious
CloudGenious
 
2 years, 4 months ago
As per google, if you run into any issue as increase latency or erroe rate ,pause your ramp up this give cloudstorage more time
to scale your bucket . Best is backoff when 5xx ,429,408 response code
upvoted 
2 
times
bnlcnd
bnlcnd
 
2 years, 5 months ago
https://cloud.google.com/storage/docs/exponential-backoffhttps://cloud.google.com/storage/docs/exponential-backoff
- B
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #96
You need to develop procedures to test a disaster plan for a mission-critical application. You want to use Google-recommended
practices and native capabilities within GCP. 
What should you do? 
A. 
Use Deployment Manager to automate service provisioning. Use Activity Logs to monitor and debug your tests.
B. 
Use Deployment Manager to automate service 
provisioning. Use Stackdriver to monitor and debug your tests. 
Most Voted
C. 
Use gcloud scripts to automate service provisioning. Use Activity Logs to monitor and debug your tests.
D. 
Use gcloud scripts to automate service provisioning. Use Stackdriver to monitor and debug your tests.
Correct Answer:
 
B 
Comments
crypt0
crypt0
 
Highly Voted
 
4 years, 2 months ago
I think answer B is correct:
https://cloud.google.com/solutions/dr-scenarios-planning-guide
upvoted 
54 
times
nitinz
nitinz
 
2 years, 10 months ago
It is B, Google Best practice ---> never use scripts. They do not trust anyone else's code it seems.
upvoted 
12 
times
fraloca
fraloca
 
2 years, 11 months ago
https://cloud.google.com/solutions/dr-scenarios-planning-guide#test_your_plan_regularly
upvoted 
2 
times
kumarp6
kumarp6
 
3 years, 2 months ago
B is correct
upvoted 
2 
times
tartar
tartar
 
3 years, 4 months ago
B is ok
Community vote distribution
B (89%)
D (11%)B is ok
upvoted 
11 
times
passnow
passnow
 
Highly Voted
 
4 years ago
Boom, everyone studied and did their labs, stackdriver is google's recommended tool for monitoring and debbuging. I agree
with u all that B is the correct answer
upvoted 
23 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days ago
Selected Answer: 
B
I will go for B.
upvoted 
1 
times
duzapo
duzapo
 
3 months, 3 weeks ago
Selected Answer: 
D
D its correct cause are 3 multiregions-availables and one bucket only can deploy in one multi region
https://cloud.google.com/storage/docs/locations?hl=es-419#location-mr
upvoted 
1 
times
jalberto
jalberto
 
4 months, 2 weeks ago
Selected Answer: 
B
I think B is the correct answer
upvoted 
1 
times
faridomu
faridomu
 
6 months, 3 weeks ago
Why not A?
upvoted 
2 
times
Jlharidon
Jlharidon
 
1 year ago
Selected Answer: 
B
Deploy managment + Stackdriver trained ig GCSB
upvoted 
1 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 2 months ago
B is right
upvoted 
1 
times
tycho
tycho
 
1 year, 4 months ago
in practice, D could work as well..
upvoted 
1 
times
adacek1
adacek1
 
11 months ago
yeah, but only native solutions should be taken into consideration (as stated in requirements), so scripts are basically ruled out
upvoted 
1 
times
gaojun
gaojun
 
1 year, 9 months ago
Answer B is correct
upvoted 
1 
times
Skr6266
Skr6266
 
1 year, 10 months ago
Selected Answer: 
B
Deployment Manager + Cloud Monitoring and Logging solution.Deployment Manager + Cloud Monitoring and Logging solution.
upvoted 
1 
times
haroldbenites
haroldbenites
 
2 years ago
Go for B
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 1 month ago
Selected Answer: 
B
B is the correct answer
upvoted 
1 
times
nqthien041292
nqthien041292
 
2 years, 1 month ago
Selected Answer: 
B
Vote B
upvoted 
1 
times
ganeshrev
ganeshrev
 
2 years, 1 month ago
Selected Answer: 
B
Google recommended Practice
upvoted 
1 
times
victory108
victory108
 
2 years, 7 months ago
B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #97
Your company creates rendering software which users can download from the company website. Your company has customers
all over the world. You want to minimize latency for all your customers. You want to follow Google-recommended practices. 
How should you store the files? 
A. 
Save the files in a Multi-Regional Cloud Storage bucket.
B. 
Save the files in a Regional Cloud Storage bucket, one bucket per zone of the region.
C. 
Save the files in multiple Regional Cloud Storage buckets, one bucket per zone per region.
D. 
Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region. 
Most Voted
Correct Answer:
 
D 
Comments
JoeShmoe
JoeShmoe
 
Highly Voted
 
5 years, 1 month ago
Its D, create multi region buckets in Americas, Europe and Asia
upvoted 
65 
times
AmitAr
AmitAr
 
2 years, 7 months ago
What is point of Multi-Regional bucket, if this need to saved multiple times. I believe option (D) is for creating confusion only.
It should be (A)..
upvoted 
16 
times
JaimeMS
JaimeMS
 
7 months ago
Let's try option A: you select a single multi-region bucket (e.g. Americas). Are you improving the latency of your clients in
Asia? You do not.
Thus, Option A is not complete.
upvoted 
2 
times
AmitAr
AmitAr
 
2 years, 7 months ago
Read the question again.. I think (d) is correct.. eg. 1 bucket in US-multi-region, 2nd in AS-multi-region, 3rd in EU-multi-
region
Community vote distribution
D (64%)
A (24%)
C (12%)region
upvoted 
10 
times
giovanicascaes
giovanicascaes
 
1 year, 9 months ago
Yes, D seems correct. There are 3 multi-regions: ASIA, EU and US. In order to be global, there must be multi-region buckets
in this 3 locations.
Reference: https://cloud.google.com/storage/docs/locations#location-mr
upvoted 
6 
times
turbo8p
turbo8p
 
2 years, 1 month ago
Check the current create bucket UI. You cannot select Asia multi-region and US multi-region at the same go. So to support
global customer, you need to create multiple Multi-region buckets.
upvoted 
13 
times
Urban_Life
Urban_Life
 
3 years ago
This can't be D. It should be A.
upvoted 
6 
times
kilo10x
kilo10x
 
1 year, 5 months ago
wrong its A
upvoted 
1 
times
MyPractice
MyPractice
 
5 years ago
why " multiple Multi-Regional"? 
- A should be the right ans & addressing the global users - "More importantly, is that
multiregional heavily leverages Edge caching and CDNs to provide the content to the end user"
https://medium.com/google-cloud/google-cloud-storage-what-bucket-class-for-the-best-performance-5c847ac8f9f2
upvoted 
14 
times
xavi1
xavi1
 
3 years, 4 months ago
because a multi-regional includes all the locations of ONE region, not the others.
upvoted 
13 
times
MeasService
MeasService
 
Highly Voted
 
5 years, 2 months ago
I would go with A 
(https://cloud.google.com/storage/docs/locations)
upvoted 
32 
times
JonathanSJ
JonathanSJ
 
Most Recent
 
5 days ago
Selected Answer: 
D
I will go for D because the clients are global, and the primary objective is minimizing latency for ALL.
upvoted 
1 
times
Amrx
Amrx
 
2 months, 1 week ago
Selected Answer: 
D
D is correct. Multi-region buckets are still specific to their own regional area, Americas, Europe and Asia. It's not A, doesn't
cover the whole world.
upvoted 
1 
times
hehe_24
hehe_24
 
2 months, 2 weeks ago
I go for A. I never came across"multiple multi-region"
upvoted 
1 
times
JaimeMS
JaimeMS
 
7 months ago
Selected Answer: 
D
Its D, create multi region buckets in Americas, Europe and Asia
upvoted 
2 
times
tlopsm
tlopsm
 
7 months, 1 week agoI think it is D. 
Keyword: Your company has customers all over the world.
Lists Multi Regional cloud
Multi-Region Name Multi-Region Description
ASIA Data centers in Asia, excluding Hong Kong and Indonesia
EU Data centers within member states of the European Union*
US Data centers in the United States
Ans A. suggests "A" multi-regional Cloud. that means one of the above multi-regional cloud
Ans D: suggests 
"multiple" Multi-Regional so 2 or (preferably) all of the multi-regional cloud with one bucket per multi-region
(less task)
upvoted 
1 
times
dija123
dija123
 
9 months ago
Selected Answer: 
A
Agree with A
upvoted 
1 
times
5091a99
5091a99
 
9 months, 4 weeks ago
Answer is A.
As for D: 
This would lead to data duplication and increased storage costs, as well as potential data consistency issues across
different multi-regional buckets.
upvoted 
1 
times
Amrita2012
Amrita2012
 
10 months, 2 weeks ago
Selected Answer: 
A
Behind Multi-Regional Cloud Storage bucket, CDN is used hence good option to use for software download service.
upvoted 
2 
times
YasserM
YasserM
 
10 months, 3 weeks ago
Selected Answer: 
A
Buckets are not with zones, so B,C,D should be wrong. I go with A
upvoted 
2 
times
xaqanik
xaqanik
 
11 months ago
I vote for A. Creating multiple multiregional Bucket is not seems practical and efficient. Besides that you can use CDN for
minimizing latency also.
upvoted 
1 
times
SSS987
SSS987
 
11 months, 3 weeks ago
One doubt - if we are going with option D, how can we handle this in the application logic - pointing to different buckets
depending on region? Pls suggest.
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
Selected Answer: 
C
I'm going with C. Totally agree with 'theBestStudent' comment.
Multi-regional offers more availability for files, but worse latency, which was the requirement.
This link says it all:
https://cloud.google.com/storage/docs/locations#considerations
If you look at the table you can see the performance of the regional bucket (200Gbps) is much higher than the Multi-regional
(50Gbps).
Also, Multi-regional buckets are 'only' available in US, Europe and East Asia. You would be letting behind places like South
America, Africa, Canada, India, Indonesia, Middle East and Australia.
upvoted 
2 
times
theBestStudent
theBestStudent
 
1 year ago
Selected Answer: 
C
It is C. Certainly is C, the other ones make no sense.
Multi regional is mostly for HA, performance is lower than regional. Regional gives better latency, so what you need to do is toMulti regional is mostly for HA, performance is lower than regional. Regional gives better latency, so what you need to do is to
have multiple regional buckets, in different regions of course https://cloud.google.com/storage/docs/locations
upvoted 
4 
times
yilexar
yilexar
 
1 year, 2 months ago
A is correct. Regional and Dual-Regional buckets are optimized for latency. 200Gps vs. 50Gps of multi-region bucket. 
https://cloud.google.com/storage/docs/locations
upvoted 
2 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
Selected Answer: 
A
A is the right answer- Multi-region buckets should be used for high availability / content delivery.
D is the wrong answer - There is nothing called "multiple multi region". This coinage itself is wrong
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #98
Your company acquired a healthcare startup and must retain its customers' medical information for up to 4 more years,
depending on when it was created. Your corporate policy is to securely retain this data, and then delete it as soon as
regulations allow. 
Which approach should you take? 
A. 
Store the data in Google Drive and manually delete records as they expire.
B. 
Anonymize the data using the Cloud Data Loss Prevention API and store it indefinitely.
C. 
Store the data in Cloud Storage and use lifecycle management to delete files when they expire. 
Most Voted
D. 
Store the data in Cloud Storage and run a nightly batch script that deletes all expired data.
Correct Answer:
 
C 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
Agree C
upvoted 
23 
times
desertlotus1211
desertlotus1211
 
Most Recent
 
1 month, 1 week ago
Selected Answer: 
B
Why not B? It's patients' health records...
upvoted 
1 
times
Peto12
Peto12
 
1 week, 6 days ago
Because the requirement says 4 more years retention, not indefinitely.
upvoted 
1 
times
i_maddog_i
i_maddog_i
 
1 year, 10 months ago
Selected Answer: 
C
It's C
upvoted 
1 
times
Community vote distribution
C (86%)
B (14%)upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
I agree with C
upvoted 
1 
times
ACE_ASPIRE
ACE_ASPIRE
 
2 years, 4 months ago
I got this question in exam.
upvoted 
4 
times
DrishaS4
DrishaS4
 
2 years, 5 months ago
Selected Answer: 
C
go for C
upvoted 
1 
times
Dhiraj03
Dhiraj03
 
2 years, 6 months ago
Options C undoubtedly
upvoted 
1 
times
gaojun
gaojun
 
2 years, 9 months ago
Go for C
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 10 months ago
I got similar question on my exam which involved life cycle management and bucket lock.
upvoted 
3 
times
Rajasa
Rajasa
 
3 years ago
Selected Answer: 
C
Go for C
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for C
upvoted 
1 
times
vincy2202
vincy2202
 
3 years, 1 month ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
gabrielzeven
gabrielzeven
 
3 years, 1 month ago
D sounds like i would do it, but C sound like a lab or exam
upvoted 
1 
times
nqthien041292
nqthien041292
 
3 years, 1 month ago
Selected Answer: 
C
Vote C
upvoted 
1 
times
victory108
victory108
 
3 years, 7 months ago
C. Store the data in Cloud Storage and use lifecycle management to delete files when they expire.C. Store the data in Cloud Storage and use lifecycle management to delete files when they expire.
upvoted 
4 
times
un
un
 
3 years, 7 months ago
C is correct
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #99
You are deploying a PHP App Engine Standard service with Cloud SQL as the backend. You want to minimize the number of
queries to the database. 
What should you do? 
A. 
Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from
memcache before issuing a query to Cloud SQL. 
Most Voted
B. 
Set the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys
containing query results.
C. 
Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key
called 
ג
€cached_queries
ג
.€
D. 
Set the memcache service level to shared. Create a key called 
ג
€cached_queries
ג
 ,€and return database values from the key
before using a query to Cloud SQL.
Correct Answer:
 
A 
Comments
hiteshrup
 
Highly Voted
 
3 years ago
A dedicated memset is always better than shared until cost-effectiveness specify in the exam as objective. So Option C and D
are ruled out. 
From A and B, Option B is sending and updating query every minutes which is over killing. So reasonable option
left with A which balance performance and cost. 
My answer will be A
upvoted 
30 
times
ArtistS
 
1 month, 2 weeks ago
Good job bro
upvoted 
1 
times
...
...
Eroc
 
Highly Voted
 
4 years, 2 months ago
https://cloud.google.com/appengine/docs/standard/php/memcache/using
upvoted 
23 
times
nitinz
 
2 years, 10 months ago
Community vote distribution
A (100%)A is correct
upvoted 
6 
times
...
dlzhang
 
2 years, 6 months ago
https://cloud.google.com/memorystore/docs/redis/redis-overview
upvoted 
2 
times
...
tartar
 
3 years, 4 months ago
A is ok
upvoted 
11 
times
...
...
Sur_Nikki
 
Most Recent
 
7 months, 4 weeks ago
Best is A
upvoted 
2 
times
...
megumin
 
1 year, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
...
AzureDP900
 
1 year, 2 months ago
A is fine.. dedicated mem cache
upvoted 
1 
times
...
ACE_ASPIRE
 
1 year, 4 months ago
I got this question in exam.
upvoted 
5 
times
...
DrishaS4
 
1 year, 5 months ago
Selected Answer: 
A
https://cloud.google.com/appengine/docs/standard/php/memcache/using
upvoted 
1 
times
...
gaojun
 
1 year, 9 months ago
Obviously, the answer is A
upvoted 
1 
times
...
ehgm
 
2 years ago
Selected Answer: 
A
Dedicated and shared will resolve the problem, the key is: store all queries in only one key "cached_queries" is not good, we have
limits: https://cloud.google.com/appengine/docs/standard/python/memcache Create a key of each query is better.
upvoted 
3 
times
...
vincy2202
 
2 years, 1 month ago
A is the correct answerupvoted 
2 
times
...
nqthien041292
 
2 years, 1 month ago
Selected Answer: 
A
Vote A
upvoted 
1 
times
...
joe2211
 
2 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
1 
times
...
victory108
 
2 years, 7 months ago
A. Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from
memcache before issuing a query to Cloud SQL.
upvoted 
3 
times
...
un
 
2 years, 7 months ago
A is correct
upvoted 
1 
times
...
Ausias18
 
2 years, 9 months ago
Answer is A
upvoted 
1 
times
...
ga
 
2 years, 10 months ago
A is correct
upvoted 
1 
times
...
BobBui
 
2 years, 10 months ago
My answer is A
upvoted 
1 
times
... 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #100
You need to ensure reliability for your application and operations by supporting reliable task scheduling for compute on GCP.
Leveraging Google best practices, what should you do? 
A. 
Using the Cron service provided by App Engine, publish messages directly to a message-processing utility service
running on Compute Engine instances.
B. 
Using the Cron service provided by App Engine, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic
using a message-processing utility service running on Compute Engine instances. 
Most Voted
C. 
Using the Cron service provided by Google Kubernetes Engine (GKE), publish messages directly to a message-processing
utility service running on Compute Engine instances.
D. 
Using the Cron service provided by GKE, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a
message-processing utility service running on Compute Engine instances.
Correct Answer:
 
B 
Comments
JoeShmoe
JoeShmoe
 
Highly Voted
 
4 years, 7 months ago
Answer is B
upvoted 
32 
times
Smart
Smart
 
Highly Voted
 
4 years, 4 months ago
B is correct. More appropriately: https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine
upvoted 
30 
times
fraloca
fraloca
 
3 years, 5 months ago
https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine#schedule-compute-engine
upvoted 
4 
times
xaqanik
xaqanik
 
Most Recent
 
5 months ago
Selected Answer: 
B
You can create Cron job using HTTP endpoint, Pub/Sub and App engine.
Community vote distribution
B (96%)
D
(4%)You can create Cron job using HTTP endpoint, Pub/Sub and App engine.
upvoted 
1 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
B
Answer is B, but this question is outdated, Today the best practices for cron is Cloud Scheduler: fully managed enterprise-grade
cron job scheduler
https://cloud.google.com/scheduler/?gad_source=1&gclsrc=ds&gclsrc=ds
upvoted 
16 
times
JaimeMS
JaimeMS
 
1 month ago
Thanks... I was a little confused by this options
upvoted 
2 
times
JPA210
JPA210
 
8 months, 3 weeks ago
This seems to be an old question, despite B could be the more correct answer, it is not exactly a good one. 'Using the Cron
service provided by App Engine', the cron service is provided by Cloud Scheduler, not App Engine. App Engine HTTP endpoint
can be a target for the cron task.
upvoted 
9 
times
salim_
salim_
 
1 year, 2 months ago
Selected Answer: 
B
https://cloud.google.com/blog/products/gcp/reliable-task-scheduling-on-google-compute-engine
upvoted 
3 
times
rr4444
rr4444
 
1 year, 3 months ago
Something feels missing/broken about this question
Even before comments in discussion that correctly mentioned Cloud Scheduler, which is not mentioned in the question
upvoted 
6 
times
parthkulkarni998
parthkulkarni998
 
6 months, 3 weeks ago
This is because cloud scheduler is a newly released service which is a replacement to cloud app engine cron service.
upvoted 
1 
times
dataqueen_3110
dataqueen_3110
 
1 year, 5 months ago
"By using Cloud Scheduler for scheduling and Pub/Sub for distributed messaging, you can build an application to reliably
schedule tasks across a fleet of Compute Engine instances." 
https://cloud.google.com/architecture/reliable-task-scheduling-
compute-engine
Answer is B. 
(Note: It was down to B or D but containerization was not mentioned)
upvoted 
2 
times
beehive
beehive
 
1 year, 6 months ago
Answer is B.
Cloud Scheduler provides a fully managed, enterprise-grade service that lets you schedule events. After you have scheduled a
job, Cloud Scheduler will call the configured event handlers, which can be App Engine services, HTTP endpoints, or Pub/Sub
subscriptions.
To run tasks on your Compute Engine instance in response to Cloud Scheduler events, you need to relay the events to those
instances. One way to do this is by calling an HTTP endpoint that runs on your Compute Engine instances. Another option is to
pass messages from Cloud Scheduler to your Compute Engine instances using Pub/Sub.
upvoted 
8 
times
habros
habros
 
1 year, 7 months ago
Selected Answer: 
B
A and C are out… messages are to be sent to pub sub and processed using a client. D is overkill for this purpose
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
BSelected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
B is right
upvoted 
1 
times
Nirca
Nirca
 
1 year, 9 months ago
Selected Answer: 
B
Ans is B https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine
upvoted 
1 
times
Sbgani
Sbgani
 
1 year, 10 months ago
Ans is B https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine
refer the examples with diagram
upvoted 
1 
times
zellck
zellck
 
1 year, 9 months ago
the link points to use Cloud Scheduler, and not Cron service provided by App Engine.
upvoted 
3 
times
zr79
zr79
 
1 year, 8 months ago
This is the new way to run schedule
upvoted 
2 
times
FAD04
FAD04
 
1 year, 10 months ago
I got this question in exam 01/09/2022
upvoted 
5 
times
pp0709
pp0709
 
1 year, 10 months ago
Selected Answer: 
D
This solution can be implemented using both A and D
1) With App Engine - https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml
2) With GKE - https://cloud.google.com/kubernetes-engine/docs/how-to/cronjobs
They ask for best practices and it's well known that GKE (aka containers) is the best practice for building modern infra solution. 
Yet another confusing PCA question on the card. Honestly, think the quality of the questions can be mightily improved.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 8 months ago
GKE is too expensive if all you are after is cron scheduling.
upvoted 
1 
times
pp0709
pp0709
 
1 year, 10 months ago
Sorry, can be implemented using both B and D
upvoted 
2 
times
kapara
kapara
 
11 months, 1 week ago
You right, but D is overkill. 
So B if the best practices for this task.
upvoted 
1 
times
medi01
medi01
 
1 year, 2 months ago
A is a bad solution as "send message directly to the utility" is not really reliable, you'd want pub/sub in between.A is a bad solution as "send message directly to the utility" is not really reliable, you'd want pub/sub in between.
upvoted 
1 
times
6721sora
6721sora
 
1 year, 10 months ago
B says Appengine.
But Cloud Scheduler is itself a managed service.
To schedule jobs via AppEngine, the cron.yaml has to be used.
It can be done similarly via GKE as well.
This question is confusing
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #101
Your company is building a new architecture to support its data-centric business focus. You are responsible for setting up the
network. Your company's mobile and web-facing applications will be deployed on-premises, and all data analysis will be
conducted in GCP. The plan is to process and load 7 years of archived .csv files totaling 900 TB of data and then continue
loading 10 TB of data daily. You currently have an existing 100-MB internet connection. 
What actions will meet your company's needs? 
A. 
Compress and upload both archived 
files and files uploaded daily using the gsutil 
ג
"€m option.
B. 
Lease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage.
Establish a connection with Google using a Dedicated Interconnect or Direct Peering connection and use it to upload files
daily. 
Most Voted
C. 
Lease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage.
Establish one Cloud VPN Tunnel to VPC networks over the public internet, and compress and upload files daily using the
gsutil 
ג
"€m option.
D. 
Lease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage.
Establish a Cloud VPN Tunnel to VPC networks over the public internet, and compress and upload files daily.
Correct Answer:
 
B 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
With option A, daily data would take 27 hours.
My answer is B.
How do you think?
upvoted 
52 
times
xaqanik
xaqanik
 
11 months ago
Also with option A you need months to download archive files(900TB)
upvoted 
1 
times
kumarp6
kumarp6
 
4 years, 2 months ago
Community vote distribution
B (90%)
Other (10%)B is correct
upvoted 
5 
times
Jay_82
Jay_82
 
4 years ago
ok but dedicated connection is available from 10 GBPS right where as in question it says internet connection is 100 MB, to me D
is correct.
upvoted 
5 
times
9xnine
9xnine
 
2 years, 7 months ago
Dedicated Interconnect will be a new connection and will not run over the existing internet connection. With dedicated
interconnect the existing ISP becomes irrelevant. If you were trying to use VPN the existing internet connection would be
relevant. Answer is B.
upvoted 
5 
times
nitinz
nitinz
 
3 years, 10 months ago
it is B
upvoted 
7 
times
malequardos
malequardos
 
3 years, 7 months ago
Direct peering is meant only to connect to G Suite Services. Its reference may invalidate the whole answer.
upvoted 
2 
times
NG123
NG123
 
2 years, 6 months ago
True. B is the most apt answer with just this extra bit "direct peering" raising some confusion.
upvoted 
1 
times
wk
wk
 
Highly Voted
 
5 years, 2 months ago
Agree B. 100Mbps connections for 10TB data transfer is takes too long
https://cloud.google.com/solutions/transferring-big-data-sets-to-gcp#close
upvoted 
21 
times
JJu
JJu
 
5 years, 1 month ago
not 100Mbps. 100MB
upvoted 
3 
times
misho
misho
 
4 years, 7 months ago
even with 100MB internet it's slow. It's 800 Mbps and transfer for 10 TB will take 2 days
upvoted 
5 
times
bogd
bogd
 
3 years, 10 months ago
There is no such thing as a "100MB" internet connection :) . That must be a speed (per second), and I would guess that the "B"
is just a typo (it is highly atypical to measure bandwidth in Bps).
upvoted 
5 
times
plumbig11
plumbig11
 
Most Recent
 
3 days, 16 hours ago
Selected Answer: 
B
Lease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. (In this
case is the more appropriated solution)
Establish a connection with Google using a Dedicated Interconnect or Direct Peering connection and use it to upload files daily.
(Because of the size Dedicated interconnect or peering performs better and offer a better latency)
upvoted 
1 
times
RickMorais
RickMorais
 
5 months, 3 weeks ago
Selected Answer: 
B
There is no option than B.
upvoted 
1 
timesupvoted 
1 
times
ManojNegi
ManojNegi
 
7 months, 1 week ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
Devx198912233
Devx198912233
 
11 months, 2 weeks ago
Selected Answer: 
B
B would be correct as pubsub b service might redeliver messages. When you receive messages in order and the Pub/Sub
service redelivers a message with an ordering key, Pub/Sub maintains order by also redelivering the subsequent messages with
the same ordering key. The Pub/Sub service redelivers these messages in the order that it originally received them.
upvoted 
1 
times
gun123
gun123
 
12 months ago
Selected Answer: 
D
B has an option of direct peering too which is not a recommended practise
upvoted 
1 
times
BisoWafik
BisoWafik
 
1 year ago
Selected Answer: 
B
B makes sense
upvoted 
1 
times
odacir
odacir
 
1 year, 1 month ago
Selected Answer: 
B
B. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time
upvoted 
1 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
Selected Answer: 
C
I don't think the company needs "dedicated intereconnect" - because it is clearly said that the company wants to do "data-centric"
business. Dedicated interconnect is more for having private-access to google cloud. 
C seems like a correct option to me
upvoted 
1 
times
odacir
odacir
 
1 year, 1 month ago
It's B, transferring over 100mbps 10TB daily is not possible, because could take up to 12 days.. You need a better connection.
upvoted 
1 
times
telp
telp
 
1 year, 10 months ago
Selected Answer: 
B
Answer B => Dedicated interconnect will provide a private network with 10gbs. The internet limited to 100 mb is not possible to
use cloud VPN ( it will use public internet so be limited for the daily)
upvoted 
2 
times
sunny2421
sunny2421
 
2 years ago
B is correct.
upvoted 
1 
times
habros
habros
 
2 years, 1 month ago
Selected Answer: 
B
B. Since it is a new network just sign up for a dedicated line…
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
BSelected Answer: 
B
B is ok
upvoted 
1 
times
Balaji_Sakthi
Balaji_Sakthi
 
2 years, 2 months ago
its option B. i think
upvoted 
1 
times
zr79
zr79
 
2 years, 2 months ago
you can not use gsutil to load 10TB daily >>>and then continue loading 10 TB of data daily<<< it will take longer than 24hrs to
upload using gsutil
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
B is the best, VPN doesn't scale very well for huge data
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #102
You are developing a globally scaled frontend for a legacy streaming backend data API. This API expects events in strict
chronological order with no repeat data for proper processing. 
Which products should you deploy to ensure guaranteed-once FIFO (first-in, first-out) delivery of data? 
A. 
Cloud Pub/Sub alone
B. 
Cloud Pub/Sub to Cloud Dataflow 
Most Voted
C. 
Cloud Pub/Sub to Stackdriver
D. 
Cloud Pub/Sub to Cloud SQL
Correct Answer:
 
B 
Comments
exampanic
exampanic
 
Highly Voted
 
5 years ago
I believe the answer is B. 
"Pub/Sub doesn't provide guarantees about the order of message delivery. Strict message ordering
can be achieved with buffering, often using Dataflow." 
https://cloud.google.com/solutions/data-lifecycle-cloud-platform
upvoted 
68 
times
TiagoM
TiagoM
 
3 years, 8 months ago
Now Pub/Sub guarantees message order. Until the exam does not change I would pick B.
upvoted 
9 
times
jask
jask
 
3 years, 3 months ago
Answer is B. The question is talking about guaranteed-once FIFO delivery of data. Although Pub/sub provides data in order
(FIFO) but it does 'at-least' once delivery of data. So, we need Dataflow for deduplication of data.
upvoted 
14 
times
emirhosseini
emirhosseini
 
2 years, 2 months ago
I believe Pub/Sub now also supports exactly once delivery (in preview):
https://cloud.google.com/pubsub/docs/exactly-once-delivery
upvoted 
9 
times
Community vote distribution
B (57%)
A (43%)melono
melono
 
2 years, 2 months ago
https://cloud.google.com/pubsub/docs/exactly-once-delivery
reference
upvoted 
1 
times
melono
melono
 
2 years, 2 months ago
Pub/Sub supports exactly-once delivery, within a cloud region.
The question states ¨global¨, so needs Dataflow
upvoted 
9 
times
CosminCiuc
CosminCiuc
 
1 year, 11 months ago
I believe that only the frontend is scaled globally. The backend API is the one that requires ordered delivery of the messages
and guaranteed-once delivery of data. Currently, Pub/Sub supports ordered delivery within the same region
(https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order) and exactly-once delivery within the same
region (https://cloud.google.com/pubsub/docs/exactly-once-delivery#exactly-once_delivery_guarantees).
The right answer could be A, Pub/Sub alone.
upvoted 
3 
times
zanfo
zanfo
 
2 years, 9 months ago
the correct is B https://cloud.google.com/pubsub/docs/stream-messages-dataflow
upvoted 
1 
times
xhova
xhova
 
Highly Voted
 
4 years, 8 months ago
B is the answer. CloudSQL is only for storage, to get the messages in order you need timestamp processed in dataflow to
arrange them before putting it in any storage volume. The system described is not querying a db it is expecting a stream of
messages only dataflow can correct the order. ACID has no value here because the db is not being queried. You'll not find any
documentation on pub/sub order being corrected with a db. See notes below on pub/sub and dataflow using timestamps and
windows to ensure order
https://cloud.google.com/pubsub/docs/pubsub-dataflow
upvoted 
28 
times
plumbig11
plumbig11
 
Most Recent
 
3 days, 16 hours ago
Selected Answer: 
B
Cloud Pub/Sub to Dataflow because the data is streaming.
upvoted 
1 
times
selected
selected
 
1 month, 3 weeks ago
Selected Answer: 
B
https://cloud.google.com/pubsub/docs/exactly-once-delivery#regional_considerations
upvoted 
1 
times
awsgcparch
awsgcparch
 
5 months, 1 week ago
Selected Answer: 
A
Google Cloud Pub/Sub now supports message ordering, which ensures that messages with the same ordering key are delivered
in the exact order they were published. This feature addresses the requirement for strict chronological order without the need
for additional services.
Key Features of Cloud Pub/Sub with Message Ordering:
Message Ordering: By using ordering keys, Pub/Sub can guarantee that messages are delivered in the order they are published.
Exactly-once Delivery: Pub/Sub supports at-least-once delivery and can be configured to handle duplicate messages.
Scalability and Reliability: Pub/Sub is a fully managed service that scales automatically and ensures high availability.
upvoted 
4 
times
19040e5
19040e5
 
7 months, 2 weeks ago
Selected Answer: 
A
A. Cloud Pub/Sub alone
Cloud Pub/Sub Ordered Delivery: Cloud Pub/Sub natively supports ordered delivery when using the same ordering key. This
guarantees that messages with the same key are delivered to subscribers in the order they were published, preventing out-of-guarantees that messages with the same key are delivered to subscribers in the order they were published, preventing out-of-
order events.
Exactly-once Delivery: Cloud Pub/Sub also offers exactly-once delivery within a region, ensuring that each message is delivered
to a subscriber only once.
upvoted 
6 
times
hitmax87
hitmax87
 
7 months, 3 weeks ago
Selected Answer: 
B
Pub/Sub provide ordered delivery but doesn't ensure deduplication. Cloud SQL is regional resource and it cant perform custom
logic. I go with B
upvoted 
1 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
A
A is correct. While Cloud Dataflow can be used for data processing, it doesn't guarantee FIFO order on its own. Additionally,
introducing another processing layer adds complexity and might not be necessary for this specific requirement.
upvoted 
2 
times
yas_cloud
yas_cloud
 
10 months, 3 weeks ago
Selected Answer: 
B
Dont see a need for SQL here as the answer suggests. Option B is more appropriate
upvoted 
1 
times
Devx198912233
Devx198912233
 
11 months, 2 weeks ago
Selected Answer: 
B
B would be correct as pubsub b service might redeliver messages. When you receive messages in order and the Pub/Sub service
redelivers a message with an ordering key, Pub/Sub maintains order by also redelivering the subsequent messages with the
same ordering key. The Pub/Sub service redelivers these messages in the order that it originally received them.
upvoted 
1 
times
Pime13
Pime13
 
11 months, 2 weeks ago
Selected Answer: 
B
b, even if pubsub now have exactly-once-delivery this is within a region. question is for a global app.
https://cloud.google.com/pubsub/docs/exactly-once-delivery#exactly-once_delivery_2
upvoted 
3 
times
theBestStudent
theBestStudent
 
1 year, 2 months ago
Answer is B.
Key words: globally scaled 
and ensure delivery only once: Pubsub + Dataflow.
If it were only one region, it would be fine to say just pubsub, but it is globally scaled.
upvoted 
1 
times
Frusci
Frusci
 
1 year, 3 months ago
Selected Answer: 
B
B, you need dataflow to deduplicate. Pub/Sub does "at-least" once delivery.
upvoted 
2 
times
AL_everyday
AL_everyday
 
1 year, 4 months ago
Selected Answer: 
A
It should be A
upvoted 
1 
times
jlambdan
jlambdan
 
1 year, 6 months ago
Selected Answer: 
A
https://cloud.google.com/pubsub/docs/exactly-once-delivery
upvoted 
2 
times
AmarReddy
AmarReddy
 
1 year, 7 months agoAnswer: B
upvoted 
1 
times
TheCloudGuruu
TheCloudGuruu
 
1 year, 7 months ago
Selected Answer: 
B
Pub/Sub and Dataflow
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #103
Your company is planning to perform a lift and shift migration of their Linux RHEL 6.5+ virtual machines. The virtual machines
are running in an on-premises 
VMware environment. You want to migrate them to Compute Engine following Google-recommended practices. What should
you do? 
A. 
1. Define a migration plan based on the list of the applications and their dependencies. 2. Migrate all virtual machines
into Compute Engine individually with Migrate for Compute Engine.
B. 
1. Perform an assessment of virtual machines running in the current VMware environment. 2. Create images of all disks.
Import disks on Compute Engine. 3. Create standard virtual machines where the boot disks are the ones you have imported.
C. 
1. Perform an assessment of virtual machines running in the current VMware environment. 2. Define a migration plan,
prepare a Migrate for Compute Engine migration RunBook, and execute the migration. 
Most Voted
D. 
1. Perform an assessment of virtual machines running in the current VMware environment. 2. Install a third-party agent
on all selected virtual machines. 3. Migrate all virtual machines into Compute Engine.
Correct Answer:
 
C 
Comments
kopper2019
kopper2019
 
Highly Voted
 
3 years, 6 months ago
Ans ) C ,
Migrate for Compute Engine organizes groups of VMs into Waves. After understanding the dependencies of your applications,
create runbooks that contain groups of VMs and begin your migration!
https://cloud.google.com/migrate/compute-engine/docs/4.5/how-to/migrate-on-premises-to-gcp/overview
upvoted 
36 
times
technodev
technodev
 
Highly Voted
 
2 years, 11 months ago
Selected Answer: 
C
I got this question in my exam.
upvoted 
15 
times
Sur_Nikki
Sur_Nikki
 
1 year, 7 months ago
Community vote distribution
C (92%)
B (8%)Did u passed...? If yes, then Congratulations and let me know the correct answer
upvoted 
1 
times
plumbig11
plumbig11
 
Most Recent
 
3 days, 16 hours ago
Selected Answer: 
C
1. Perform an assessment of virtual machines running in the current VMware environment. (You should as best pratice perform
an assessment of virtual machines and define a migration plan) 2. Define a migration plan, prepare a Migrate for Compute
Engine migration RunBook, and execute the migration. (compute Engine migration Runbook is the best pratice)
upvoted 
1 
times
3fd692e
3fd692e
 
3 months ago
Selected Answer: 
C
Assess, Plan, Migrate. Textbook perfect
upvoted 
1 
times
salim_
salim_
 
1 year, 7 months ago
Selected Answer: 
C
https://cloud.google.com/migrate/compute-engine/docs/4.11/how-to/migrate-on-premises-to-gcp/overview
upvoted 
1 
times
8d31d36
8d31d36
 
1 year, 10 months ago
Selected Answer: 
B
The reason why Option B is preferable over Option C is that it involves creating images of all disks and importing them into
Compute Engine, which can significantly reduce the amount of time required for the migration. Additionally, creating standard
virtual machines from the imported disks is a straightforward process, and it ensures that the migrated virtual machines are
identical to the on-premises virtual machines, which can simplify the migration process and minimize the risk of compatibility
issues.
upvoted 
2 
times
examch
examch
 
2 years ago
Selected Answer: 
C
C is the correct answer,
Runbooks are created from the Migrate for Compute Engine Manager. The system queries VMware or AWS for VMs and
generates a CSV for you to edit.
By editing the CSV, you define:
The VMs in a wave.
The order in which those VMs are migrated.
The type and disk space of VMs that are launched on Google Cloud.
Other characteristics that are defined in the Runbook reference.
https://cloud.google.com/migrate/compute-engine/docs/4.8/how-to/organizing-migrations/creating-and-modifying-
runbooks#generating_runbook_templates
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is most suitable for this use
upvoted 
1 
times
ACE_ASPIRE
ACE_ASPIRE
 
2 years, 4 months ago
I got this question in exam.
upvoted 
5 
times
AzureDP900
AzureDP900
 
2 years, 6 months agoAzureDP900
AzureDP900
 
2 years, 6 months ago
C is right, It defines all logical steps to migrate on-premise to google cloud.
upvoted 
2 
times
meokey
meokey
 
2 years, 8 months ago
Does Ans. C) still valid as of latest GCE 5.0? 
in the doc "Migrating VM groups" with version GCE 5.0, I do not see "runbook" anymore which is explained up to version GCE
4.8.
https://cloud.google.com/migrate/compute-engine/docs/5.0/how-to/migrating-vm-groups
upvoted 
2 
times
gaojun
gaojun
 
2 years, 9 months ago
Go for C
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 10 months ago
Selected Answer: 
C
I got this question on my exam. Answered C.
upvoted 
2 
times
Sur_Nikki
Sur_Nikki
 
1 year, 7 months ago
Thanks
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for C.
upvoted 
1 
times
nikiwi
nikiwi
 
3 years ago
why not A?
seems pretty obvious if you look at the google doc: https://cloud.google.com/migrate/compute-
engine/docs/5.0/concepts/lifecycle
upvoted 
1 
times
atlasga
atlasga
 
3 years ago
When you are doing cloud migrations, you do migrations in "waves" which are groupings of one or more
applications/workloads. Moving machines individually would break things, such as dependencies. This is standard industry
practice.
upvoted 
3 
times
vincy2202
vincy2202
 
3 years, 1 month ago
C is the correct answer.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #104
You need to deploy an application to Google Cloud. The application receives traffic via TCP and reads and writes data to the
filesystem. The application does not support horizontal scaling. The application process requires full control over the data on
the file system because concurrent access causes corruption. The business is willing to accept a downtime when an incident
occurs, but the application must be available 24/7 to support their business operations. You need to design the architecture of
this application on Google Cloud. What should you do? 
A. 
Use a managed instance group with instances in multiple zones, use Cloud Filestore, and use an HTTP load balancer in
front of the instances.
B. 
Use a managed instance group with instances in multiple zones, use Cloud Filestore, and use a network load balancer in
front of the instances.
C. 
Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk,
and use an HTTP load balancer in front of the instances.
D. 
Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk,
and use a network load balancer in front of the instances. 
Most Voted
Correct Answer:
 
D 
Comments
VishalB
VishalB
 
Highly Voted
 
2 years, 12 months ago
Correct Ans : D 
Since the Traffic is TCP, Ans A & C gets eliminated as HTTPS load balance is not supported.
B - File storage system is Cloud Firestore which do not give full control, hence eliminated.
D - Unmanaged instance group with network load balance with regional persistent disk for storage gives full control which is
required for the migration.
upvoted 
63 
times
kimharsh
kimharsh
 
2 years ago
what about the fact that is the unmanaged instance group is not regional , so you can't create it in more than 1 zone ?
upvoted 
7 
times
Jerryzzyy
Jerryzzyy
 
11 months ago
Community vote distribution
D (81%)
B (19%)Jerryzzyy
Jerryzzyy
 
11 months ago
Can we group to running instances in different zones to an unmanaged instance group?
upvoted 
1 
times
poseidon24
poseidon24
 
2 years, 11 months ago
almost all good, except for File Storage, is not Cloud Firestore, it is a new service for sharing filesystems across VMs (like a NAS
in a traditional infrastructure).
upvoted 
10 
times
kopper2019
kopper2019
 
Highly Voted
 
3 years ago
Ans ) D , unmanaged instance group as application does not support horizontal scaling and network load balancer as no mention
of http traffic .
upvoted 
28 
times
Polosaty
Polosaty
 
Most Recent
 
3 months, 3 weeks ago
In Unmanaged Instance Group instances cannot be in different zones. I think that correct is D but maybe a mistake in the question.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
5 months, 2 weeks ago
Selected Answer: 
D
Why not a B? 
Because the application doesn't support for horizonal scale.
I chose D.
upvoted 
1 
times
duzapo
duzapo
 
10 months ago
Selected Answer: 
D
D is corrrect TCP load balancer plus UNMANAGED
upvoted 
2 
times
TheCloudGuruu
TheCloudGuruu
 
1 year, 1 month ago
Selected Answer: 
D
must be unmanaged
upvoted 
1 
times
JC0926
JC0926
 
1 year, 3 months ago
Selected Answer: 
D
Since the application does not support horizontal scaling, a managed instance group is not required. Instead, an unmanaged
instance group can be used to ensure that the application runs on multiple instances in different zones for high availability.
The network load balancer is designed to handle TCP and UDP traffic
The HTTP(S) load balancer is designed specifically for HTTP and HTTPS traffic.
upvoted 
15 
times
Sur_Nikki
Sur_Nikki
 
1 year, 1 month ago
Thanks for the apt explanation
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
An unmanaged instance group allows you to create and manage a group of Compute Engine instances manually, rather than
using an autoscaling solution like a managed instance group. This is appropriate for an application that does not support horizontal
scaling, as you can manually create and manage the number of instances needed to meet the traffic demands.
To ensure high availability and minimize downtime, you should deploy the instances in different zones and use a regional
persistent disk to store the application's data. This will ensure that the application is still available even if one of the instances or a
zone experiences an outage.
A network load balancer should be used in front of the instances to distribute traffic to the instances. A network load balancer is a
highly available and scalable load balancing solution that operates at the network layer and can handle high volumes of traffic. It
can also balance traffic across multiple zones to ensure that the application is always available to users.
upvoted 
21 
timesupvoted 
21 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Therefore, the correct answer is option D: Use an unmanaged instance group with an active and standby instance in different
zones, use a regional persistent disk, and use a network load balancer in front of the instances.
upvoted 
10 
times
oms_muc
oms_muc
 
1 year, 6 months ago
Selected Answer: 
D
Regional Persistent Disk, as App requires full control of filesystem data without concurrent access (block storage vs. file storage
(NAS).
https://cloud.google.com/compute/docs/instance-groups
https://cloud.google.com/load-balancing/docs/choosing-load-balancer
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
D
Option A & B eliminated because we cannot use managed instance group since the app does not support Horizontal scaling 
Option C > HTTP load balancer is Layer 7 & application is receiving traffic via TCP
Option D > is best answer because we are using Network load balancer Layer 4 which meets the condition "application receives
traffic via TCP"
upvoted 
5 
times
fiercedog
fiercedog
 
1 year, 6 months ago
Selected Answer: 
D
Checking the comparative analysis of storage options, we can see that Filestore is not suitable for the workload, hence A and B
are out. C is out because it restricts to HTTP traffic. Answer is D
upvoted 
2 
times
fiercedog
fiercedog
 
1 year, 6 months ago
https://cloud.google.com/architecture/storage-advisor#comparative_analysis
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
SerGCP
SerGCP
 
1 year, 8 months ago
Selected Answer: 
B
D is not possibile you cannot create a regional unmanaged instance group. https://cloud.google.com/compute/docs/instance-
groups/creating-groups-of-unmanaged-instances
upvoted 
3 
times
KyubiBlaze
KyubiBlaze
 
1 year, 6 months ago
Downtime is acceptable, Disk is regional, in case of issue the unmanaged instance group can be moved to other zone, disk has
data. A&B are not at all an option
upvoted 
3 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
D
D. Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and
use a network load balancer in front of the instances.
D is the only option as the application does not support horizontal scaling (no MIG), it needs full control (no filestore) and has TCP
traffic (no HTTP LB).
upvoted 
1 
times
Santanu_01
Santanu_01
 
1 year, 8 months ago
D is more appropriate solution ---In the ques. it is mentioned done not support horizontal scaling --> hence Unmanaged Instance ,D is more appropriate solution ---In the ques. it is mentioned done not support horizontal scaling --> hence Unmanaged Instance ,
and traffic is TCP --- 
hence N/W 
Load balancer
upvoted 
1 
times
Rajeev26
Rajeev26
 
1 year, 9 months ago
Selected Answer: 
D
does not support horizontal scaling so MIG not needed. TCP traffic so NW LB is ok
upvoted 
1 
times
Nirca
Nirca
 
1 year, 9 months ago
Selected Answer: 
D
concurrent access causes corruption - it can not 
be a managed group
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #105
Your company has an application running on multiple Compute Engine instances. You need to ensure that the application can
communicate with an on-premises service that requires high throughput via internal IPs, while minimizing latency. What should
you do? 
A. 
Use OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud.
B. 
Configure a direct peering connection between the on-premises environment and Google Cloud.
C. 
Use Cloud VPN to configure a VPN tunnel between the on-premises environment and Google Cloud.
D. 
Configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud. 
Most Voted
Correct Answer:
 
D 
Comments
kopper2019
 
Highly Voted
 
3 years, 6 months ago
Ans ) D , Reason : high throughput via internal IPs
upvoted 
67 
times
ShadowLord
 
2 years, 4 months ago
This is tricky questions , it can be achieved by C and D ... Multiple Computes and Costs .. they are trying to test knowledge on
VPN and Tunnels ....
upvoted 
2 
times
...
...
XDevX
 
Highly Voted
 
3 years, 6 months ago
IMHO the correct answer is D. Reason: 
"requires high throughput via internal IPs, while minimizing latency" - both are aspects
you cannot guarantee with using VPN traversing the internet.
upvoted 
23 
times
...
JonathanSJ
 
Most Recent
 
4 days, 23 hours ago
Selected Answer: 
D
I will go for D.
upvoted 
1 
times
Community vote distribution
D (98%)
C
(2%)...
[Removed]
 
4 months, 1 week ago
Selected Answer: 
D
D seems to be correct
upvoted 
1 
times
...
the1dv
 
11 months, 3 weeks ago
Selected Answer: 
D
Should be D as high throughput
upvoted 
1 
times
...
Gilbaliano
 
1 year, 1 month ago
Selected Answer: 
D
Justo be D
upvoted 
1 
times
Gilbaliano
 
1 year, 1 month ago
Should be D
upvoted 
1 
times
...
...
kamradamir
 
1 year, 1 month ago
Selected Answer: 
D
It should be D, since need to communicate via Internal IP
upvoted 
1 
times
...
thewalker
 
1 year, 1 month ago
Selected Answer: 
C
Communication through internal IPs - VPN. So, C
upvoted 
1 
times
ccpmad
 
6 months, 4 weeks ago
yes, you are very smart but question says high throughput and minimizing latency > Interconnect
upvoted 
1 
times
...
[Removed]
 
1 year ago
https://cloud.google.com/network-
connectivity/docs/interconnect/concepts/overview#:~:text=Also%2C%20Cloud%20Interconnect%20connections%20provide,directly%20accessible%20from%20both%20networks.
upvoted 
2 
times
...
...
duzapo
 
1 year, 3 months ago
Selected Answer: 
D
D cause you need high throughput
upvoted 
2 
times
...
marcjimz
 
1 year, 5 months ago
D - high throughputupvoted 
1 
times
...
wooloo
 
1 year, 5 months ago
How is C marked a correct one?
upvoted 
2 
times
...
Danomine416
 
1 year, 8 months ago
Initially thought 'D' but the question says 'high throughput via internal IPs' so go with VPN answer 'C'
upvoted 
2 
times
...
grejao
 
1 year, 9 months ago
It can´t be VPN, Only interconnect can minimizing latency. 
D is the right answer.
upvoted 
1 
times
...
zerg0
 
1 year, 10 months ago
Selected Answer: 
D
Internal IP + 
high throughput
upvoted 
5 
times
...
WFCheong
 
1 year, 11 months ago
Selected Answer: 
D
Ans ) D , Reason : high throughput via internal IPs
upvoted 
3 
times
...
omermahgoub
 
2 years ago
D. Configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud. 
A Cloud
Dedicated Interconnect is a high-bandwidth, low-latency network connection that allows you to connect your on-premises
environment to Google Cloud Platform (GCP) using a dedicated network connection. It provides a direct physical connection
between your on-premises network and GCP, which can help to reduce latency and increase the throughput of your application.
upvoted 
4 
times
omermahgoub
 
2 years ago
Option A, using OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud, is not a
recommended approach. OpenVPN is a free and open-source software application that implements virtual private network
(VPN) techniques to create secure point-to-point connections. While OpenVPN can be used to establish a VPN tunnel between
an on-premises environment and GCP, it may not provide the level of performance and reliability required for high-throughput
applications. 
Option B, configuring a direct peering connection between the on-premises environment and Google Cloud, is not
a recommended approach. Direct Peering is a high-bandwidth, low-latency network connection that allows you to connect your
on-premises network directly to Google's network. While Direct Peering can be used to connect your on-premises environment
to GCP, it is typically used for high-bandwidth workloads such as video streaming and may not be suitable for applications that
require low latency. 
Option C, using Cloud VPN to configure a VPN tunnel between the on-premises
upvoted 
2 
times
omermahgoub
 
2 years ago
Cloud VPN, would also involve routing traffic over the public internet and would not provide the low latency and high
throughput that you need.
upvoted 
1 
times
...
medi01
 
1 year, 8 months agomedi01
 
1 year, 8 months ago
Yeah, direct peering is high bandwidth and low latency, but may not be suitable for applications that require low latency.
Good job, ChatGPT...
upvoted 
2 
times
...
...
...
SureshbabuK
 
2 years, 1 month ago
Selected Answer: 
D
Highthoughput connection VPN - No Interconnect - Y 
via Internal IP addresses. VPN - yes Interconnect - Y 
Interconnect 
yes for
both so D
upvoted 
3 
times
... 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #106
You are managing an application deployed on Cloud Run for Anthos, and you need to define a strategy for deploying new
versions of the application. You want to evaluate the new code with a subset of production traffic to decide whether to proceed
with the rollout. What should you do? 
A. 
Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions. 
Most Voted
B. 
Deploy a new service to Cloud Run with the new version. Add a Cloud Load Balancing instance in front of both services.
C. 
In the Google Cloud Console page for Cloud Run, set up continuous deployment using Cloud Build for the development
branch. As part of the Cloud Build trigger, configure the substitution variable TRAFFIC_PERCENTAGE with the percentage of
traffic you want directed to a new version.
D. 
In the Google Cloud Console, configure Traffic Director with a new Service that points to the new version of the
application on Cloud Run. Configure Traffic Director to send a small percentage of traffic to the new version of the
application.
Correct Answer:
 
A 
Comments
VishalB
VishalB
 
Highly Voted
 
2 years, 12 months ago
  Correct Answer: A
o Each deployment to a service creates a revision. A revision consists of a specific container image, along with environment
settings such as environment variables, memory limits, or concurrency value.
o Once the new revision is deployed to a Service you can manage the traffic using MANAGE TRAFFIC option inside the revision
tab
  https://cloud.google.com/run/docs/resource-model
upvoted 
54 
times
omermahgoub
omermahgoub
 
Highly Voted
 
1 year, 6 months ago
The correct answer is A. Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between
revisions.
Cloud Run for Anthos allows you to deploy new revisions of your application with a specific percentage of traffic, which allows
you to perform a gradual rollout of the new version. To do this, you can follow these steps:
Deploy a new revision of your application to Cloud Run with the new version.
Community vote distribution
A (74%)
C (26%)Deploy a new revision of your application to Cloud Run with the new version.
In the Cloud Run for Anthos console, navigate to the service that you want to roll out the new version for.
In the "Revisions" tab, you should see the new revision listed alongside the current revision.
Use the traffic percentage slider to specify the percentage of traffic that you want to send to the new revision. You can set the
percentage to a small value initially, such as 5%, and gradually increase it over time as you evaluate the new version.
Once you have set the traffic percentage, Cloud Run for Anthos will start directing a portion of the traffic to the new revision,
allowing you to evaluate the new version with a subset of production traffic.
upvoted 
19 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option B, deploying a new service and adding a Cloud Load Balancer instance in front of both services, is not recommended
because it would require you to create and manage a separate service for the new version, which would be more complex and
less efficient than deploying a new revision.
Option C, using continuous deployment with Cloud Build and substitution variables, is not relevant to this scenario because it
involves deploying new versions automatically based on changes to a development branch, rather than manually deploying
new revisions with a specific percentage of traffic. 
Option D, using Traffic Director, is also not relevant because Traffic Director is used for managing traffic between different
services or clusters, rather than between revisions of the same service.
upvoted 
11 
times
Sur_Nikki
Sur_Nikki
 
1 year, 1 month ago
Extremely convinced by your explanations..Have u given this exam?
upvoted 
1 
times
baertierchen
baertierchen
 
1 year ago
Those Answers are surely generated by ChatgGPT
upvoted 
1 
times
ccpmad
ccpmad
 
3 weeks, 5 days ago
Yes, chatgpt did the exam, LOL
upvoted 
2 
times
Pime13
Pime13
 
Most Recent
 
5 months ago
Selected Answer: 
A
https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration#gradual
upvoted 
2 
times
02fc23a
02fc23a
 
7 months, 1 week ago
Selected Answer: 
A
A:
https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration#gradual
upvoted 
1 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
A
La Opción A - El uso de dashboards predefinidos proporciona una visión inmediata y eficiente del estado del sistema, y la
capacidad de agregar métricas personalizadas y crear políticas de alertas permite una respuesta rápida y efectiva a los
incidentes. Generar un Dashboard por incidente introduce complejidad innecesaria.
upvoted 
1 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
A
https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration
upvoted 
1 
timesupvoted 
1 
times
JPA210
JPA210
 
8 months, 3 weeks ago
You all forget that in the exam they expect you to choose the most complete answer, and C is the most efficient and complete
one, where you show knowledge of Cloud Build also, and in the real life that is how you should implement this solution.
upvoted 
1 
times
SSS987
SSS987
 
5 months, 2 weeks ago
C is saying "from development branch"!
upvoted 
2 
times
HRS1954
HRS1954
 
10 months ago
A is correct - https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration#gradual
upvoted 
2 
times
BigfootPanda
BigfootPanda
 
12 months ago
Selected Answer: 
C
C is ok as we want to DEFINE A STRATEGY
upvoted 
3 
times
TheCloudGuruu
TheCloudGuruu
 
1 year, 1 month ago
Selected Answer: 
A
Answer is A
upvoted 
1 
times
zerg0
zerg0
 
1 year, 4 months ago
Selected Answer: 
A
https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration
upvoted 
4 
times
KM0107
KM0107
 
1 year, 6 months ago
Selected Answer: 
A
Selected A
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
sfsdeniso
sfsdeniso
 
1 year, 8 months ago
A is correct
currently there is possibility to use tags to test in production without receiving real traffic:
https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration#tags
upvoted 
2 
times
sfsdeniso
sfsdeniso
 
1 year, 8 months ago
whats wrong with C is configuring deployment from 'development branch'
this is supper ugly
upvoted 
2 
times
zr79
zr79
 
1 year, 8 months ago
you can do a lab on this >>>Deploy Your Website on Cloud Run<<< Manage traffic is there
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
A
A. Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions.A. Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #107
You are monitoring Google Kubernetes Engine (GKE) clusters in a Cloud Monitoring workspace. As a Site Reliability Engineer
(SRE), you need to triage incidents quickly. What should you do? 
A. 
Navigate the predefined dashboards in the Cloud Monitoring workspace, and then add metrics and create alert policies.
Most Voted
B. 
Navigate the predefined dashboards in the Cloud Monitoring workspace, create custom metrics, and install alerting software
on a Compute Engine instance.
C. 
Write a shell script that gathers metrics from GKE nodes, publish these metrics to a Pub/Sub topic, export the data to
BigQuery, and make a Data Studio dashboard.
D. 
Create a custom dashboard in the Cloud Monitoring workspace for each incident, and then add metrics and create alert
policies.
Correct Answer:
 
A 
Comments
kopper2019
 
Highly Voted
 
3 years ago
Ans ) A .
upvoted 
57 
times
...
DiegoMDZ
 
Highly Voted
 
2 years, 12 months ago
It's A for me... Create a dashboard for each incident?? I think D isn't a good choice...
upvoted 
31 
times
bandegg
 
6 months ago
Yeah, creating a new dashboard for each incident doesn't seem like the quickest option.
upvoted 
3 
times
...
...
6a8c7ad
 
Most Recent
 
1 month ago
quickly would mean custom, and you can call it what you want. 
D
upvoted 
1 
times
Community vote distribution
A (58%)
D (42%)...
hitmax87
 
1 month, 2 weeks ago
Selected Answer: 
D
If you need extended functionality you should create your dashboards, metrics and alerts. Dont mess existing ones.
upvoted 
1 
times
...
coolie1234
 
2 months, 1 week ago
Selected Answer: 
D
Key is custom dashboard
upvoted 
1 
times
...
Gino17m
 
2 months, 2 weeks ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
...
mesodan
 
4 months ago
Selected Answer: 
A
A is correct. Option D is highly inefficient and time-consuming. Creating individual dashboards for every incident is impractical
and slows down the triage process.
upvoted 
2 
times
...
OrangeTiger
 
5 months, 2 weeks ago
I will go with A. uhhm, opinions are divided. In such cases, Q is often not good.
upvoted 
1 
times
...
hzaoui
 
5 months, 2 weeks ago
Selected Answer: 
A
Explanation: Cloud Monitoring provides predefined dashboards for monitoring GKE clusters, which facilitate an immediate and
comprehensive view of cluster performance and health. As an SRE, utilizing these dashboards helps triage incidents quickly. You
can also add additional metrics that are pertinent to the incident and create alert policies that will notify you when specific
conditions indicative of an incident are met. This strategy allows for the proactive monitoring of incidents and rapid response
when necessary.
upvoted 
4 
times
...
SSS987
 
5 months, 2 weeks ago
Ans: D. Although creating dashboard per incident sounds confusing and inefficient, it is still better than the impossible option A
as we can't edit or add metrics to a predefined dashboard. Inefficient option vs Impossible Option - Inefficient one is ok!
upvoted 
3 
times
Gino17m
 
2 months, 2 weeks ago
Optipn A is possible. You can't add widgets to predefined dashboard but you can create alert policies based on metrics.
Although the structure of the question is actually misleading
upvoted 
2 
times
...
...
ade7cae
 
6 months, 3 weeks ago
Selected Answer: 
ASelected Answer: 
A
A is correct. For D, creating dashboards for each incident isn't practical
upvoted 
1 
times
...
spuyol
 
6 months, 3 weeks ago
Currently you can not modify pre-defined dashboards. Has no sense to create a dashboard for each incident. Conclusion: no
answer is good enough
upvoted 
1 
times
...
brentc
 
7 months, 3 weeks ago
Selected Answer: 
D
We do have dashboards for each incident
upvoted 
2 
times
...
TopTalk
 
9 months, 1 week ago
Selected Answer: 
D
"You can't delete or modify the automatically-created dashboards; however, when support for copying the dashboard exists, you
can modify the copy. In general, you can also copy charts on a predefined dashboard to a dashboard that you create. Dashboards
that you create are custom dashboards. Custom dashboards let you display information that is of interest to you, organized in a
way that's useful to you. " https://cloud.google.com/monitoring/charts/predefined-dashboards
upvoted 
3 
times
ArtistS
 
7 months, 3 weeks ago
Create a dashboard for each inc ????? serious
upvoted 
1 
times
...
...
TopTalk
 
9 months, 3 weeks ago
Selected Answer: 
D
"To view the chart associated with an alerting policy and information about incidents in the same context as your metric data,
add alert charts and incident widgets to your CUSTOM dashboard." https://cloud.google.com/monitoring/dashboards/alerts-and-
incidents
upvoted 
3 
times
...
duzapo
 
10 months ago
Selected Answer: 
A
Ans A any sense a dashboar per incident
upvoted 
1 
times
...
heretolearnazure
 
10 months, 1 week ago
A is correct
upvoted 
1 
times
... 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #108
You are implementing a single Cloud SQL MySQL second-generation database that contains business-critical transaction data.
You want to ensure that the minimum amount of data is lost in case of catastrophic failure. Which two features should you
implement? (Choose two.) 
A. 
Sharding
B. 
Read replicas
C. 
Binary logging 
Most Voted
D. 
Automated backups 
Most Voted
E. 
Semisynchronous replication
Correct Answer:
 
CD 
Comments
kopper2019
kopper2019
 
Highly Voted
 
3 years ago
Ans) C and D
Cloud SQL. If you use Cloud SQL, the fully managed Google Cloud MySQL database, you should enable automated backups and
binary logging for your Cloud SQL instances. This allows you to perform a point-in-time recovery, which restores your database
from a backup and recovers it to a fresh Cloud SQL instance
upvoted 
38 
times
HenkH
HenkH
 
1 year, 8 months ago
And: a read-replica won't help against "catastrophic failures" like accidental deletions
upvoted 
5 
times
RVivek
RVivek
 
1 year, 4 months ago
catastrophic 
failure means disaster like a zonal datacenter level failure or regional failure
upvoted 
4 
times
victory108
victory108
 
Highly Voted
 
2 years, 11 months ago
C. Binary logging
Community vote distribution
CD (75%)
BD (25%)C. Binary logging
D. Automated backups
upvoted 
11 
times
[Removed]
[Removed]
 
Most Recent
 
6 months, 1 week ago
CD
Binary Logging: Binary logging in MySQL records changes to the database. It can be used for backup and replication, and it's
essential for point-in-time recovery. With binary logging, you can roll your database forward to any point in time, minimizing
data loss.
Automated Backups: Automated backups periodically take a snapshot of your database. In the event of a catastrophic failure,
you can restore your database to the state it was in at the time of the last backup. This can also help minimize data loss.
While read replicas and semisynchronous replication can enhance availability and performance, they do not directly minimize
data loss.
Also, you cannot create a read replica without enabling Automated backups and Enable binary logging
Sharding can improve performance but it's not directly aimed at data loss prevention.
https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
upvoted 
4 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
CD
Prerequisites for creating a read replica
Before you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:
Automated backups must be enabled.
Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these
logs.
At least one backup must have been created after binary logging was enabled.
https://cloud.google.com/sql/docs/mysql/replication#requirements
upvoted 
5 
times
someone2011
someone2011
 
9 months, 3 weeks ago
CD
Before being able to create a read replica, you have to make sure "binary logging and automated backup" are enabled. So
picking only D or C without the other one makes no sense.
https://cloud.google.com/sql/docs/mysql/replication/create-replica
upvoted 
1 
times
HRS1954
HRS1954
 
10 months ago
The correct answers are C. & D.
Binary logging is a feature of MySQL that records all changes made to the database. This log can be used to restore the
database to a previous state in case of a failure.
Automated backups are regularly scheduled backups of the database. They are the most reliable way to ensure that data is not
lost in case of a catastrophic failure.
upvoted 
4 
times
heretolearnazure
heretolearnazure
 
10 months, 1 week ago
C and D. Read replicas wont work in this case.
upvoted 
1 
times
FaizAhmed
FaizAhmed
 
1 year ago
C. Binary logging
D. Automated backups
upvoted 
1 
times
taer
taer
 
1 year, 3 months ago
Selected Answer: 
CDBinary logging records changes to the data, which can help you recover data and minimize data loss during an unexpected
failure. Automated backups create regular backups of your database, allowing you to restore the database to a specific point in
time in case of a catastrophic failure.
upvoted 
3 
times
abbottWang
abbottWang
 
1 year, 4 months ago
Selected Answer: 
CD
backup data automatically
upvoted 
1 
times
telp
telp
 
1 year, 4 months ago
CD => the answer B is for performance issue. The question focus on data loss prevention.
upvoted 
1 
times
medi01
medi01
 
1 year, 2 months ago
So you are going with a SINGLE instance of MySQL for a critical business application.
upvoted 
1 
times
okixavi
okixavi
 
1 year, 5 months ago
Selected Answer: 
BD
B and D:
No need to explain D, but B... here is why
When you set up a read replica, automaticaly binary logging is activated. Then, in case of desaster, you can promote manually a
read replica and it will have all data before the desaster occurs.
upvoted 
3 
times
r1ck
r1ck
 
1 year, 4 months ago
sure, binary logging starts Automatically upon configuring read-replica?? 
- Don't think so,
https://cloud.google.com/sql/docs/mysql/replication/create-replica
upvoted 
2 
times
Jeena345
Jeena345
 
1 year, 5 months ago
B and D are correct answers as per below reference,
1. Before you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:
Automated backups must be enabled.
2. Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these
logs.
3. At least one backup must have been created after binary logging was enabled.
It means creating read replica already covers binary logging.
Please read the following references for more information
https://cloud.google.com/solutions/cloud-sql-mysql-disaster-recovery-complete-failover-fallback
https://medium.com/google-cloud/cloud-sql-recovering-from-regional-failure-in-10-minutes-or-less-mysql-fc055540a8f0
Replication in Cloud SQL | Cloud SQL for MySQL | Google Cloud
upvoted 
1 
times
mmathiou
mmathiou
 
1 year, 1 month ago
Yes, you are correct that creating a read replica requires binary logging to be enabled on the primary instance. However, the
purpose of a read replica is to scale read traffic and offload it from the primary instance, not to prevent data loss in case of
catastrophic failure. While enabling binary logging is a requirement for creating a read replica, it is not the primary purpose of
a read replica. IMO the two features that should be implemented to ensure minimum data loss in case of catastrophic failurea read replica. IMO the two features that should be implemented to ensure minimum data loss in case of catastrophic failure
are Binary logging and Automated backups.
upvoted 
3 
times
examch
examch
 
1 year, 6 months ago
Selected Answer: 
CD
C and D are correct answers,
Backups help you restore lost data to your Cloud SQL instance. Additionally, if an instance is having a problem, you can restore
it to a previous state by using the backup to overwrite it. Enable automated backups for any instance that contains necessary
data. Backups protect your data from loss or damage.
Enabling automated backups, along with binary logging, is also required for some operations, such as clone and replica
creation.
https://cloud.google.com/sql/docs/mysql/backup-recovery/backups#what_backups_provide
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
C. Binary logging
D. Automated backups
Binary logging is a feature of MySQL that records all changes made to the database in a binary log file. By enabling binary
logging on your Cloud SQL instance, you can use the log file to recover your database in case of catastrophic failure.
Automated backups are a feature of Cloud SQL that allows you to automatically create and retain backups of your database. By
enabling automated backups, you can restore your database in case of catastrophic failure or other data loss events.
Option A, sharding, is not a recommended approach. Sharding is a technique for distributing data across multiple servers to
improve performance and scalability. While sharding can help to improve the performance of a database, it is not specifically
designed to protect against data loss in case of catastrophic failure.
upvoted 
4 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option B, read replicas, is not a recommended approach. Read replicas are copies of a database that can be used to offload
read traffic from the primary database. While read replicas can improve the performance of a database, they are not
specifically designed to protect against data loss in case of catastrophic failure.
Option E, semisynchronous replication, is not a recommended approach. Semisynchronous replication is a method of
replicating data between a primary database and one or more secondary databases. While semisynchronous replication can
help to ensure that data is replicated quickly and accurately, it is not specifically designed to protect against data loss in case
of catastrophic failure.
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
CD
cd is ok
upvoted 
1 
times
diasporabro
diasporabro
 
1 year, 8 months ago
Selected Answer: 
CD
I see Read Replicas as more of a performance thing, than DR thing
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #109
You are working at a sports association whose members range in age from 8 to 30. The association collects a large amount of
health data, such as sustained injuries. You are storing this data in BigQuery. Current legislation requires you to delete such
information upon request of the subject. You want to design a solution that can accommodate such a request. What should you
do? 
A. 
Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.
Most Voted
B. 
When ingesting new data in BigQuery, run the data through the Data Loss Prevention (DLP) API to identify any personal
information. As part of the DLP scan, save the result to Data Catalog. Upon a deletion request, query Data Catalog to find
the column with personal information.
C. 
Create a BigQuery view over the table that contains all data. Upon a deletion request, exclude the rows that affect the
subject's data from this view. Use this view instead of the source table for all analysis tasks.
D. 
Use a unique identifier for each individual. Upon a deletion request, overwrite the column with the unique identifier with
a salted SHA256 of its value.
Correct Answer:
 
A 
Comments
milan74
milan74
 
Highly Voted
 
3 years, 5 months ago
According to me, the question states "The association collects a large amount of health data, such as sustained injuries." and
the nuance on the word such => " Current legislation requires you to delete "SUCH" information upon request of the subject. "
So from that point of view the question is not to delete the entire user records but specific data related to personal health data.
With DLP you can use InfoTypes and InfoType detectors to specifically scan for those entries and how to act upon them (link
https://cloud.google.com/dlp/docs/concepts-infotypes)
I would say B.
upvoted 
84 
times
AmitAr
AmitAr
 
2 years, 7 months ago
(A) - Primary task is "legislation requires you to delete" .. and B is not deleting.
only A is deleting
upvoted 
11 
times
Community vote distribution
A (62%)
B (38%)upvoted 
11 
times
BeCalm
BeCalm
 
1 year, 10 months ago
Deletion is implied in "Upon a deletion request, query Data Catalog to find the column with personal information."
upvoted 
1 
times
zanfo
zanfo
 
2 years, 9 months ago
I want to delete all the informations about the user, not only those individuate by DLP. ALL THE INFORMATIONS of the
users...B is not correct! the correct is A
upvoted 
8 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 4 months ago
There is no need of DLP.
All the data is sensitive but only upon user request it needs deletion.
So A should be the correct answer.
upvoted 
12 
times
Arad
Arad
 
3 years, 1 month ago
as PhilipKoku mentioned below:
A) is the correct answer. B) is only masking the data and then when a request is received, it identified the record but it doesn’t
delete it. D) Is masking the ID.
upvoted 
12 
times
mgm7
mgm7
 
3 years ago
B is not masking the data but identifying where it is to take action on at later date if required
upvoted 
6 
times
XDevX
XDevX
 
Highly Voted
 
3 years, 6 months ago
IMHO a) is the correct answer because it is easier to operate. The question is not how to mask data and so on but just to delete
data on request, so I don't think that we have to use for just the deletion of specific data DLP.
upvoted 
36 
times
rrope
rrope
 
Most Recent
 
1 week, 1 day ago
Selected Answer: 
A
A. Use a unique identifier for each individual.
upvoted 
1 
times
andyk87
andyk87
 
2 weeks, 3 days ago
Selected Answer: 
B
Option B is better when the requirement is to delete only the PII health data, not all data related to the individual.
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
It had better be A, if not then you're not a good organization
upvoted 
2 
times
hitmax87
hitmax87
 
7 months, 3 weeks ago
Selected Answer: 
B
Data Loss Prevention must have!
upvoted 
1 
times
Gino17m
Gino17m
 
8 months ago
Selected Answer: 
B
I vote for B. 
I had some doubts whether A was correct, but:
- I'm not convinced by the argument "only A talks about deleting" (it would be too easy if it was about choosing an answer
containing the word "delete" ;)
- the question says "design a solution that can accommodate such a request" - I'm not very fluent in english, but
"accommodate" imho means more "facilitate" than "accomplish" here"accommodate" imho means more "facilitate" than "accomplish" here
- I think that the task is about deleting health data not everything related with unique identifier
- Data Catalog allows you to manage data, knowing in which datasets and in which tables what data is stored. Answer "A"
somehow imposes the data model - each table with data related to a given individual must contain the ID of this individual (in
a real data model this does not have to be the case).
upvoted 
3 
times
Djenko
Djenko
 
9 months, 2 weeks ago
Selected Answer: 
A
Should be A)
upvoted 
2 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
A
A is correct. As for option B: While DLP is valuable for identifying sensitive data, it might not be sufficient for this specific case.
DLP cannot necessarily determine an individual's right to deletion based solely on data classification. Additionally, relying on
Data Catalog to store the results adds unnecessary complexity and potential inconsistencies.
upvoted 
3 
times
Gall
Gall
 
11 months ago
Selected Answer: 
B
B. The A removes all data, not SUCH only.
upvoted 
2 
times
NoCrapEva
NoCrapEva
 
11 months, 1 week ago
Selected Answer: 
A
Ans. B assumes you will delete the Personal Information found in the Catalog... 
Some people are reading GDPR into this
question (we are not told what country and what legislation). The question states you must delete all information (not just
personal informarion) on request. 
Ans B is a red herring !
Answer must = 
A
upvoted 
2 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
A
(A) - Primary task is "legislation requires you to delete" .. and B is not deleting.
only A is deleting
upvoted 
5 
times
Jconnor
Jconnor
 
1 year, 1 month ago
Well, A would delete all rows with the identifier, I guess including the ones that are not confidential, also what does it mean
unique identifier? each user is unique already. Ridiculous. 
B would identify the columns that contain personal data, but B is
prone to errors as changes in legislation of what is consider injury would be excluded and all data would need to be re-
ingested. 
Unfortunately B is closer and less damaging than A.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
B
Either A or B is the answer. 
A - will delete all the info about the subject, which is not the intension. Only the sensitive data to be deleted. Hence, B.
upvoted 
2 
times
Anubhav451
Anubhav451
 
1 year, 2 months ago
B is correct.. Check in chatGPT also.
upvoted 
1 
times
DA95
DA95
 
1 year ago
chatgpt select A
The most appropriate solution for accommodating the deletion request of personal health data stored in BigQuery, as per
current legislation, would be:
A. Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.A. Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.
Here's why this approach is suitable:
Unique Identifier: Assigning a unique identifier to each individual is a standard practice in managing and querying datasets. It
helps in precisely identifying and isolating records associated with a specific individual.
Direct Deletion of Rows: Upon receiving a deletion request, you can directly delete all rows associated with the individual's
unique identifier. This approach ensures that the data is completely removed from your dataset, complying with the legislative
requirement to delete personal information upon request.
upvoted 
1 
times
JPA210
JPA210
 
1 year, 2 months ago
From one side A is an easy way, low effort, to implement this solution, but if we think like an architect and like an exam
question, B is more complete and a better solution, since it can mask all the sensitive information, not only for the users that
request it, but for all, which is a best practice.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
Selected Answer: 
B
We do not need to delete entire recrod of sports person but some health information collected by association. B would be
correct answer.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #110
Your company has announced that they will be outsourcing operations functions. You want to allow developers to easily stage
new versions of a cloud-based application in the production environment and allow the outsourced operations team to
autonomously promote staged versions to production. You want to minimize the operational overhead of the solution. Which
Google Cloud product should you migrate to? 
A. 
App Engine 
Most Voted
B. 
GKE On-Prem
C. 
Compute Engine
D. 
Google Kubernetes Engine
Correct Answer:
 
A 
Comments
kopper2019
kopper2019
 
Highly Voted
 
3 years, 6 months ago
A. App Engine
upvoted 
36 
times
arsav
arsav
 
Highly Voted
 
3 years, 5 months ago
Answer should be A as only with App Engine we have a default service account which allows the user to deploy the changes per
project. for GKE we may have to configure additional permission for both DEV and Operations team to deploy the changes.
https://cloud.google.com/appengine/docs/standard/php/service-account
upvoted 
24 
times
Sephethus
Sephethus
 
Most Recent
 
6 months, 2 weeks ago
A. App Engine.
Explanation:
Why A is correct:
App Engine: Google App Engine is a fully managed platform-as-a-service (PaaS) that allows developers to build and deploy
applications quickly and easily without worrying about managing the underlying infrastructure. It supports continuous
integration and continuous deployment (CI/CD) processes, enabling developers to stage new versions of applications easily.
Community vote distribution
A (85%)
Other (15%)integration and continuous deployment (CI/CD) processes, enabling developers to stage new versions of applications easily.
Staging and Promotion: App Engine has built-in support for traffic splitting and versioning, which allows you to stage new
versions of your application and gradually promote them to production. This can be done with minimal operational overhead,
making it ideal for scenarios where operational functions are outsourced.
Minimal Operational Overhead: Since App Engine is fully managed, it reduces the operational burden significantly, making it
easier for the outsourced operations team to handle promotions and manage the application.
upvoted 
1 
times
JaimeMS
JaimeMS
 
7 months ago
Selected Answer: 
A
A. App Engine
upvoted 
1 
times
jaisonPathiyil
jaisonPathiyil
 
8 months, 2 weeks ago
Is this answers are really correct or misleading to us..?
upvoted 
3 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
A
While both GKE and App Engine offer functionalities for deploying cloud-based applications, App Engine is more managed
service compared to GKE, resulting in lower operational overhead.
upvoted 
2 
times
Anandmrk
Anandmrk
 
10 months, 2 weeks ago
Selected Answer: 
A
I did my exam today and saw this question. But I am sure A was the answer due to the operational overhead phrase
upvoted 
3 
times
[Removed]
[Removed]
 
1 year ago
A
why not D?
It requires more operational overhead compared to App Engine, as it involves managing the Kubernetes infrastructure.
upvoted 
2 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
D
In the question, we see "cloud-based application" - I assume, it means cloud native -> dockers/containers -> K8s -> GKE.
Hence, D is my option.
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
Selected Answer: 
D
I agreed with "omermahgoub" the answer should be D.
As you will bundle the application and its dependencies into container image and deploy. All environments will have same
image deployed from Dev, TEST, Staging to PROD. There will be less operational overheard for operations team.
upvoted 
1 
times
nocrush
nocrush
 
1 year, 3 months ago
Selected Answer: 
A
A
App Engine reduces ops overhead
upvoted 
2 
times
Frusci
Frusci
 
1 year, 3 months ago
Selected Answer: 
AA. You deploy your new version to App Engine without setting it as the default version. The ops team then just has to make it
the default version when they want to promote it. Simplest answer.
upvoted 
2 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
App Engine because of less ovehead.
upvoted 
1 
times
JohnWick2020
JohnWick2020
 
1 year, 7 months ago
Answer is A.
By process of elimination you arrive at App Engine or GKE. Now the requirement is to "to minimize the operational overhead of
the solution". 
On the IaaS to PaaS spectrum, this can only be App Engine!
IaaS = Compute Engine.
Hybrid = GKE (engineering heavy).
PaaS = App Engine.
upvoted 
3 
times
Atanu
Atanu
 
1 year, 7 months ago
Selected Answer: 
A
"You want to minimize the operational overhead of the solution" ..This sentence is they key to go with Option A. GKE carries
overhead as it's not purely PaaS.
upvoted 
3 
times
Sur_Nikki
Sur_Nikki
 
1 year, 7 months ago
It should be D. As GKE is considered to be the master product/service for creating a deployment and managing and keeping all
the environments in SYNC
upvoted 
1 
times
sithin_nair
sithin_nair
 
1 year, 9 months ago
Selected Answer: 
A
A is right as the requirement is to deploy new changes and manage the application with no operational overhead.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #111
Your company is running its application workloads on Compute Engine. The applications have been deployed in production,
acceptance, and development environments. The production environment is business-critical and is used 24/7, while the
acceptance and development environments are only critical during office hours. Your CFO has asked you to optimize these
environments to achieve cost savings during idle times. What should you do? 
A. 
Create a shell script that uses the gcloud command to change the machine type of the development and acceptance
instances to a smaller machine type outside of office hours. Schedule the shell script on one of the production instances to
automate the task.
B. 
Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after
office hours and start them just before office hours. 
Most Voted
C. 
Deploy the development and acceptance applications on a managed instance group and enable autoscaling.
D. 
Use regular Compute Engine instances for the production environment, and use preemptible VMs for the acceptance and
development environments.
Correct Answer:
 
B 
Comments
pamepadero
pamepadero
 
Highly Voted
 
3 years, 6 months ago
B is the answer.
https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs 
Schedule VMs to auto start and stop: The benefit of a platform like Compute Engine is that you only pay for the compute resources
that you use. Production systems tend to run 24/7; however, VMs in development, test or personal environments tend to only be
used during business hours, and turning them off can save you a lot of money! 
https://cloud.google.com/blog/products/storage-data-transfer/save-money-by-stopping-and-starting-compute-engine-instances-on-
schedule
Cloud Scheduler, GCP’s fully managed cron job scheduler, provides a straightforward solution for automatically stopping and
starting VMs. By employing Cloud Scheduler with Cloud Pub/Sub to trigger Cloud Functions on schedule, you can stop and start
groups of VMs identified with labels of your choice (created in Compute Engine). Here you can see an example schedule that
stops all VMs labeled "dev" at 5pm and restarts them at 9am, while leaving VMs labeled "prod" untouched
upvoted 
38 
times
Community vote distribution
B (71%)
C (26%)
D
(3%)upvoted 
38 
times
sgofficial
sgofficial
 
2 years, 5 months ago
Excellent ......even the good CFO is telling leave the office after 5.oo and come next day to work :)
upvoted 
16 
times
Ric350
Ric350
 
2 years, 5 months ago
Great answer and documentation. Def B
upvoted 
2 
times
rzygor
rzygor
 
2 years, 5 months ago
Question says that dev/test are "not critical", it doesn't mean that they are not needed at all ...
upvoted 
17 
times
kopper2019
kopper2019
 
Highly Voted
 
3 years, 6 months ago
Ans ) B , assuming VM doesn't need to be up after office hours .
upvoted 
25 
times
rrope
rrope
 
Most Recent
 
6 days, 9 hours ago
Selected Answer: 
D
By utilizing preemptible VMs for the acceptance and development environments, you can significantly reduce costs without
compromising the availability and performance of your critical production environment.
upvoted 
1 
times
25lion52
25lion52
 
3 months, 1 week ago
Selected Answer: 
C
Stop the dev and acceptance envs is super weird. Any critical problems or overtimes will be an issue with this approach. Simple
auto scaling environment is a good solution IMHO
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 2 weeks ago
B is right answer
upvoted 
1 
times
dija123
dija123
 
9 months ago
Selected Answer: 
B
Agree with B
upvoted 
1 
times
spuyol
spuyol
 
11 months ago
Answer D
A: too complex and maybe small or zero saving if you can't find a valid smaller machine type
B: Not valid. Question says that PRE environments are not critical after office hours. But it doesn't say no service at all
C: Some risk is introduced if you have different architecture on PRE than PRO envs
D: It's the only valid and realiable option. Simple and effective. It's my choice. In a real scenario I will first start with this and then
review if the savings are enough before more complicated choices
upvoted 
3 
times
Gino17m
Gino17m
 
8 months, 2 weeks ago
Ad. "B: Not valid. Question says that PRE environments are not critical after office hours. But it doesn't say no service at all"
But the Question says that PRE environments are critical during office hours, so you can't use preemptible VMs - 
"Compute
Engine might stop (preempt) these instances if it needs to reclaim the compute capacity for allocation to other VMs"
upvoted 
1 
times
the1dv
the1dv
 
11 months, 3 weeks ago
Selected Answer: 
C
MIG's with autoscaling will scale to Zero if not neededMIG's with autoscaling will scale to Zero if not needed
upvoted 
3 
times
spuyol
spuyol
 
11 months ago
some risks are added if you have different architecture on PRO and PRE envs
upvoted 
1 
times
AWS_Sam
AWS_Sam
 
1 year ago
B for sure
upvoted 
1 
times
parthkulkarni998
parthkulkarni998
 
1 year ago
Selected Answer: 
C
Also managed instance group reduces instances in case of low/no-traffic incurring lesser charges. Ideally, its a cleaner approach
considering the ask is to optimize during "idle time". Incase people are working in different time zones, late shifts it doesnt make
sense to trigger shutdown at a predefined times.
upvoted 
3 
times
odacir
odacir
 
1 year, 1 month ago
Selected Answer: 
B
B is the answer. But today, you don't need complicated CRON + CF. Auto shutdown by cron expression it's a feature built in:
https://cloud.google.com/compute/docs/instances/schedule-instance-start-stop
upvoted 
1 
times
werdy92
werdy92
 
1 year, 2 months ago
really wondering why not C...Not critical is not equivalent with not running at all....
upvoted 
5 
times
parthkulkarni998
parthkulkarni998
 
1 year ago
Also managed instance group reduces instances in case of low/no-traffic incurring lesser charges. Ideally, its a cleaner approach
considering the ask is to optimize during "idle time". Incase people are working in different time zones, late shifts it doesnt make
sense to trigger shutdown at a predefined times.
upvoted 
2 
times
wooloo
wooloo
 
1 year, 5 months ago
"are only critical during office hours" does not mean it could be completely stopped. So may the option C correct?
upvoted 
6 
times
mifrah
mifrah
 
1 year, 9 months ago
In my opinion B is over-engineered:
Why not just add an "instance schedule" for start/stop the Compute Engines?
Why creating a scheduler and writing a Cloud Function...
upvoted 
3 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
Just exactly what I have thought. It is enough with instance schedule". But GCP wants you to spend money and use cloud
functions LOL
upvoted 
1 
times
MaryMei
MaryMei
 
1 year, 10 months ago
Selected Answer: 
B
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations
B seems close to this Google provided service option, the extra step should be using idle VM recommendations to find and stop
idle VM instances to reduce waste of resources
upvoted 
1 
times
PAUGURU
PAUGURU
 
1 year, 10 months ago
Since the price of preemptibles is 1/4 the price of a standard machine D costs far less than B since office hours are 1/3 of whole
day. It costs less to keep them running 24h as preemptibles.day. It costs less to keep them running 24h as preemptibles.
upvoted 
3 
times
DevOpsifier
DevOpsifier
 
1 year, 6 months ago
Yes, but preemptibles use GCP excess resources so you will achieve the opposite of the desired effect, during office hours, they
will underperform in the best case (worst case will stop altogether) and, during non-office hours, preemptibles will work well...
upvoted 
1 
times
windsor_43
windsor_43
 
2 years ago
The Answer is B.
Just had my exam today with a pass, this question was in the exam. Dated 31/12/22
Thanks to this site it was by far my most valuable
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #112
You are moving an application that uses MySQL from on-premises to Google Cloud. The application will run on Compute Engine
and will use Cloud SQL. You want to cut over to the Compute Engine deployment of the application with minimal downtime and
no data loss to your customers. You want to migrate the application with minimal modification. You also need to determine the
cutover strategy. What should you do? 
A. 
1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-
premises MySQL server. 2. Stop the on-premises application. 3. Create a mysqldump of the on-premises MySQL server. 4.
Upload the dump to a Cloud Storage bucket. 5. Import the dump into Cloud SQL. 6. Modify the source code of the
application to write queries to both databases and read from its local database. 7. Start the Compute Engine application. 8.
Stop the on-premises application.
B. 
1. Set up Cloud SQL proxy and MySQL proxy. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the
dump to a Cloud Storage bucket. 4. Import the dump into Cloud SQL. 5. Stop the on-premises application. 6. Start the
Compute Engine application.
C. 
1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-
premises MySQL server. 2. Stop the on-premises application. 3. Start the Compute Engine application, configured to read
and write to the on-premises MySQL server. 4. Create the replication configuration in Cloud SQL. 5. Configure the source
database server to accept connections from the Cloud SQL replica. 6. Finalize the Cloud SQL replica configuration. 7. When
replication has been completed, stop the Compute Engine application. 8. Promote the Cloud SQL replica to a standalone
instance. 9. Restart the Compute Engine application, configured to read and write to the Cloud SQL standalone instance.
Most Voted
D. 
1. Stop the on-premises application. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a
Cloud Storage bucket. 4. Import the dump into Cloud SQL. 5. Start the application on Compute Engine.
Correct Answer:
 
C 
Comments
victory108
victory108
 
Highly Voted
 
2 years, 11 months ago
C. 1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises
MySQL server. 2. Stop the on-premises application. 3. Start the Compute Engine application, configured to read and write to
Community vote distribution
C (100%)MySQL server. 2. Stop the on-premises application. 3. Start the Compute Engine application, configured to read and write to
the on-premises MySQL server. 4. Create the replication configuration in Cloud SQL. 5. Configure the source database server to
accept connections from the Cloud SQL replica. 6. Finalize the Cloud SQL replica configuration. 7. When replication has been
completed, stop the Compute Engine application. 8. Promote the Cloud SQL replica to a standalone instance. 9. Restart the
Compute Engine application, configured to read and write to the Cloud SQL standalone instance.
upvoted 
33 
times
don_v
don_v
 
5 months, 3 weeks ago
Agree with C.
The only confusing is step "5. Configure the source database server to accept connections from the Cloud SQL replica."
Is that not replication should go in the opposite direction from the on-premise (a.k.a. "source") database to Cloud SQL replica
(presuming the latter is configured with a public IP address)?
upvoted 
1 
times
kopper2019
kopper2019
 
Highly Voted
 
3 years ago
Ans C, from this guy muhasinem 
External replica promotion migration
In the migration strategy of external replica promotion, you create an external database replica and synchronize the existing
data to that replica. This can happen with minimal downtime to the existing database.
When you have a replica database, the two databases have different roles that are referred to in this document as primary and
replica.
After the data is synchronized, you promote the replica to be the primary in order to move the management layer with minimal
impact to database uptime.
In Cloud SQL, an easy way to accomplish the external replica promotion is to use the automated migration workflow. This
process automates many of the steps that are needed for this type of migration.
upvoted 
22 
times
de1001c
de1001c
 
Most Recent
 
3 weeks, 6 days ago
Selected Answer: 
C
Minimal downtime. Downtime in A is time to take mysql dump + fix potential failures, not good. Downtime in C is just the time
from restart the service.
upvoted 
1 
times
Gino17m
Gino17m
 
2 months, 2 weeks ago
Selected Answer: 
C
I wonder how Examtopics determines the so-called "Correct Answer" ..... C is correct
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
10 months, 1 week ago
Answer is C
upvoted 
1 
times
aliounegdiop
aliounegdiop
 
1 year, 1 month ago
B is the correct answ
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 2 months ago
Option A, writing to two databases form the app :(
Option C all the way, it also aligns to GCP Data Migration Service.
upvoted 
2 
times
DRK8109
DRK8109
 
1 year, 2 months ago
mysql dump always causes long downtime.
upvoted 
4 
times
musumusu
musumusu
 
1 year, 3 months ago
Correct Answer A
C is unnecceory expensive and loss of data at after step 3
upvoted 
2 
timesupvoted 
2 
times
BeCalm
BeCalm
 
1 year, 3 months ago
C seems to be the best answer but it is still a bit confusing.
So basically there's a bi-directional sync between the 2 databases? Cloud instance is the primary and is writing into the on-
prem and on-prem is being replicated into the Cloud.
upvoted 
2 
times
NodummyIQ
NodummyIQ
 
1 year, 6 months ago
Option C is not the correct answer because it involves modifying the application to read and write to both the on-premises
MySQL server and Cloud SQL, which would involve significant modification to the application and could introduce potential
complications or errors. It is generally better to minimize modification to the application when performing a migration. Option
D, on the other hand, involves simply importing a mysqldump of the on-premises MySQL server into Cloud SQL and starting the
application on Compute Engine, which is a simpler and more straightforward approach that involves minimal modification to
the application.
upvoted 
2 
times
SureshbabuK
SureshbabuK
 
1 year, 6 months ago
Selected Answer: 
C
Examtopic providing A as correct answer is causing confusion,
upvoted 
6 
times
Jose56
Jose56
 
1 year, 7 months ago
Selected Answer: 
C
C for minimal downtime
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
zr79
zr79
 
1 year, 8 months ago
Answer is C 
we have a new service https://cloud.google.com/database-migration
upvoted 
7 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
C
C because it has minimal modification to the application or database. Also it's easier to fail back to the original solution if the
cloud implementation has issues (assuming that there will be a "post-go-live" monitoring period).
upvoted 
2 
times
minmin2020
minmin2020
 
1 year, 8 months ago
C because it has minimal modification to the application or database. Also it's easier to fail back to the original solution if the
cloud implementation has issues (assuming that there will be a "post-go-live" monitoring period).
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #113
Your organization has decided to restrict the use of external IP addresses on instances to only approved instances. You want to
enforce this requirement across all of your Virtual Private Clouds (VPCs). What should you do? 
A. 
Remove the default route on all VPCs. Move all approved instances into a new subnet that has a default route to an
internet gateway.
B. 
Create a new VPC in custom mode. Create a new subnet for the approved instances, and set a default route to the
internet gateway on this new subnet.
C. 
Implement a Cloud NAT solution to remove the need for external IP addresses entirely.
D. 
Set an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in
the allowedValues list. 
Most Voted
Correct Answer:
 
D 
Comments
victory108
victory108
 
Highly Voted
 
2 years, 11 months ago
D. Set an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in the
allowedValues list.
upvoted 
24 
times
AnilKr
AnilKr
 
Highly Voted
 
2 years, 10 months ago
Ans - D, 
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip
you might want to restrict external IP address so that only specific VM instances can use them. This option can help to prevent
data exfiltration or maintain network isolation. Using an Organization Policy, you can restrict external IP addresses to specific
VM instances with constraints to control use of external IP addresses for your VM instances within an organization or a project.
upvoted 
20 
times
james2033
james2033
 
Most Recent
 
1 month ago
Selected Answer: 
D
https://cloud.google.com/compute/docs/ip-addresses/configure-static-external-ip-address#disableexternalip
upvoted 
1 
times
Community vote distribution
D (100%)odacir
odacir
 
4 months, 1 week ago
"You cannot apply the constraint retroactively. All VMs that have external IP addresses before you enable the policy retain their
external IP addresses." 
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip
It shouldn't be option D then
upvoted 
1 
times
beehive
beehive
 
1 year, 6 months ago
D is correct one. 
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip
upvoted 
3 
times
rascalbrick
rascalbrick
 
1 year, 6 months ago
Show on my Exam,unfortunatekt im failed..:(
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
D is correct
upvoted 
1 
times
2M
2M
 
1 year, 9 months ago
Selected Answer: 
D
option D
upvoted 
2 
times
ACE_ASPIRE
ACE_ASPIRE
 
1 year, 10 months ago
I got this question in exam.
upvoted 
2 
times
Sur_Nikki
Sur_Nikki
 
1 year, 1 month ago
Answer pleaase
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years ago
D is right. constraints/compute.vmExternalIpAccess
upvoted 
1 
times
JoeyCASD
JoeyCASD
 
2 years, 1 month ago
vote for D
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip
upvoted 
2 
times
ss909098
ss909098
 
2 years, 4 months ago
Selected Answer: 
D
D it is
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 4 months ago
Selected Answer: 
D
I got similar question on my exam. Answered D.
upvoted 
3 
timesupvoted 
3 
times
technodev
technodev
 
2 years, 5 months ago
Got this question in my exam, answered D
upvoted 
2 
times
haroldbenites
haroldbenites
 
2 years, 6 months ago
Go for D.
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 7 months ago
D is the correct answer
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #114
Your company uses the Firewall Insights feature in the Google Network Intelligence Center. You have several firewall rules
applied to Compute Engine instances. 
You need to evaluate the efficiency of the applied firewall ruleset. When you bring up the Firewall Insights page in the Google
Cloud Console, you notice that there are no log rows to display. What should you do to troubleshoot the issue? 
A. 
Enable Virtual Private Cloud (VPC) flow logging.
B. 
Enable Firewall Rules Logging for the firewall rules you want to monitor. 
Most Voted
C. 
Verify that your user account is assigned the compute.networkAdmin Identity and Access Management (IAM) role.
D. 
Install the Google Cloud SDK, and verify that there are no Firewall logs in the command line output.
Correct Answer:
 
B 
Comments
nohel
nohel
 
Highly Voted
 
3 years, 6 months ago
Answer is B
when you create a firewall rule there is an option for firewall rule logging on/off. It is set to off by default.
To get firewall insights or view the logs for a specific firewall rule you need to enable logging while creating the rule or you can
enable it by editing that rule.
https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights#enabling-fw-
rules-logging
upvoted 
36 
times
victory108
victory108
 
Highly Voted
 
3 years, 5 months ago
B. Enable Firewall Rules Logging for the firewall rules you want to monitor.
upvoted 
15 
times
GlebG
GlebG
 
Most Recent
 
5 months, 2 weeks ago
First D, then B
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 2 weeks ago
Selected Answer: 
B
Community vote distribution
B (100%)Selected Answer: 
B
Corrent answer is B
upvoted 
2 
times
RVivek
RVivek
 
1 year, 11 months ago
Selected Answer: 
B
https://cloud.google.com/vpc/docs/firewall-rules-logging
upvoted 
1 
times
windsor_43
windsor_43
 
2 years ago
The Answer is B
Just had my exam today with a pass, this question was in the exam. Dated 31/12/22
Thanks to this site it was by far my most valuable
upvoted 
5 
times
jay9114
jay9114
 
2 years ago
Selected Answer: 
B
You have to enable logging for a firewall rule in order to see the rows. 
"When you enable logging for a firewall rule, Google Cloud creates an entry called a connection record each time the rule
allows or denies traffic."
https://cloud.google.com/vpc/docs/firewall-rules-logging
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
B. Enable Firewall Rules Logging for the firewall rules you want to monitor.
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
Enable firewall rules logging , B is right
upvoted 
1 
times
DrishaS4
DrishaS4
 
2 years, 5 months ago
Selected Answer: 
B
https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights#enabling-fw-
rules-logging
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
B is most appropriate answer, I will choose B.
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
https://cloud.google.com/vpc/docs/firewall-rules-logging
upvoted 
1 
times
tannV
tannV
 
2 years, 8 months ago
Answered B. Got this question!
upvoted 
2 
times
azureaspirant
azureaspirant
 
2 years, 10 months agoazureaspirant
azureaspirant
 
2 years, 10 months ago
02/15/21 exam
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for B
upvoted 
1 
times
vincy2202
vincy2202
 
3 years, 1 month ago
B is the correct answer
https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights
upvoted 
1 
times
pakilodi
pakilodi
 
3 years, 1 month ago
Selected Answer: 
B
B is the answer here
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #115
Your company has sensitive data in Cloud Storage buckets. Data analysts have Identity Access Management (IAM) permissions
to read the buckets. You want to prevent data analysts from retrieving the data in the buckets from outside the office network.
What should you do? 
A. 
1. Create a VPC Service Controls perimeter that includes the projects with the buckets. 2. Create an access level with
the CIDR of the office network. 
Most Voted
B. 
1. Create a firewall rule for all instances in the Virtual Private Cloud (VPC) network for source range. 2. Use the
Classless Inter-domain Routing (CIDR) of the office network.
C. 
1. Create a Cloud Function to remove IAM permissions from the buckets, and another Cloud Function to add IAM
permissions to the buckets. 2. Schedule the Cloud Functions with Cloud Scheduler to add permissions at the start of
business and remove permissions at the end of business.
D. 
1. Create a Cloud VPN to the office network. 2. Configure Private Google Access for on-premises hosts.
Correct Answer:
 
A 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years ago
Should be A.
For all Google Cloud services secured with VPC Service Controls, you can ensure that:
Resources within a perimeter are accessed only from clients within authorized VPC networks using Private Google Access with
either Google Cloud or on-premises.
https://cloud.google.com/vpc-service-controls/docs/overview
upvoted 
72 
times
ArtistS
ArtistS
 
7 months, 2 weeks ago
Enforce a security perimeter with VPC Service Controls to isolate resources of multi-tenant Google Cloud services—reducing the
risk of data exfiltration or data breach.
upvoted 
1 
times
poseidon24
poseidon24
 
2 years, 11 months ago
Correct, this is about data exfiltration.
Community vote distribution
A (100%)Correct, this is about data exfiltration.
See: https://youtu.be/EXwJFL24QzY
upvoted 
14 
times
Sivanaga
Sivanaga
 
1 year, 9 months ago
nice one, thank you man
upvoted 
2 
times
mv2000
mv2000
 
2 years ago
Thanks for including the youtube video it was very helpful
upvoted 
1 
times
XDevX
XDevX
 
Highly Voted
 
3 years ago
IMHO c is wrong - the question is not to restrict access only for business hours but to restrict access to office network.
In my opinion the only realistic approach seems to be a)
https://cloud.google.com/vpc-service-controls/docs/supported-products#table_storage
upvoted 
17 
times
19040e5
19040e5
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
A
It's obviously A.
C mentions office hours which has nothing to do with the question!
upvoted 
1 
times
Gino17m
Gino17m
 
2 months, 2 weeks ago
Selected Answer: 
A
A is correct answer. Examtopics should change so-called "Correct Answer" from C to A to stop confusin users.
upvoted 
2 
times
kalyan_krishna742020
kalyan_krishna742020
 
2 months, 3 weeks ago
I'm preparing for a test and see that questions from 115 onwards are considered valid. Can anyone who's taken the test offer any
insights or advice? Thank you!
upvoted 
1 
times
discuss24
discuss24
 
6 months ago
A is the correct answer. The question is specific to accessing the data outside of the office network. If the question talked about
outside of work business hours then, we can consider C
upvoted 
1 
times
JPA210
JPA210
 
8 months, 3 weeks ago
How can be possible that Examtopics say that the correct answer is C?! It doesn't make any sense! A is the correct one.
upvoted 
2 
times
heretolearnazure
heretolearnazure
 
10 months, 1 week ago
A is correct answer
upvoted 
1 
times
RVivek
RVivek
 
1 year, 5 months ago
Selected Answer: 
A
https://cloud.google.com/vpc-service-controls/docs/overview
upvoted 
1 
times
vamgcp
vamgcp
 
1 year, 5 months ago
A is correct because, For all Google Cloud services secured with VPC Service
Controls, you can ensure that resources within a perimeter are accessed only
from clients within authorized VPC networks using Private Google Access with
either Google Cloud or on-premises.upvoted 
1 
times
examch
examch
 
1 year, 6 months ago
Selected Answer: 
A
A is the correct answer,
https://cloud.google.com/vpc-service-controls/docs/overview#isolate
* A VM within a Virtual Private Cloud (VPC) network that is part of a service perimeter can read from or write to a Cloud Storage
bucket in the same perimeter. However, VPC Service Controls doesn't allow VMs within VPC networks that are outside the
perimeter to access Cloud Storage buckets that are inside the perimeter.
* A copy operation between two Cloud Storage buckets succeeds if both buckets are in the same service perimeter, but if one of
the buckets is outside the perimeter, the copy operation fails.
* VPC Service Controls doesn't allow a VM within a VPC network that is inside a service perimeter to access Cloud Storage
buckets that are outside the perimeter.
upvoted 
4 
times
thamaster
thamaster
 
1 year, 6 months ago
Selected Answer: 
A
answer C will not prevent connection from outside of office network
upvoted 
1 
times
cshubham173
cshubham173
 
1 year, 6 months ago
Selected Answer: 
A
For all Google Cloud services secured with VPC Service Controls, you can ensure that:
Resources within a perimeter are accessed only from clients within authorized VPC networks using Private Google Access with
either Google Cloud or on-premises.
https://cloud.google.com/vpc-service-controls/docs/overview
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
A
A. Best option
B. Not all instances need this restriction
C. You are not restricting remote access. The users can still access remotely using their credentials during the business day. The
ask is to restrict data retrieval from outside the office network (what if they are working from home...?)
D. VPN - too much overhead
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
A. Best option
B. Not all instances need this restriction
C. You are not restricting remote access. The users can still access remotely using their credentials during the business day. The
ask is to restrict data retrieval from outside the office network (what if they are working from home...?)
D. 
VPN - too much overhead
upvoted 
2 
times
kazob
kazob
 
1 year, 8 months ago
Selected Answer: 
A
A for obvious reasons
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #116
You have developed a non-critical update to your application that is running in a managed instance group, and have created a
new instance template with the update that you want to release. To prevent any possible impact to the application, you don't
want to update any running instances. You want any new instances that are created by the managed instance group to contain
the new update. What should you do? 
A. 
Start a new rolling restart operation.
B. 
Start a new rolling replace operation.
C. 
Start a new rolling update. Select the Proactive update mode.
D. 
Start a new rolling update. Select the Opportunistic update mode. 
Most Voted
Correct Answer:
 
D 
Comments
XDevX
XDevX
 
Highly Voted
 
3 years, 6 months ago
IMHO the correct answer is d) opportunistic mode, not c) proactive mode.
The requirement is not to update any running instances. 
see: https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups
For automated rolling updates, you must set the mode to proactive.
Alternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG
applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are
created. New instances can be created when you or another service, such as an autoscaler, resizes the MIG.
upvoted 
58 
times
victory108
victory108
 
Highly Voted
 
3 years, 5 months ago
D. Start a new rolling update. Select the Opportunistic update mode.
upvoted 
12 
times
Chang401
Chang401
 
Most Recent
 
3 months, 1 week ago
Selected Answer: 
D
Community vote distribution
D (93%)
C (7%)In Google Cloud, the main difference between proactive and opportunistic updates in a managed instance group (MIG) is when
they are applied:
Proactive updates: Automatically apply updates to existing VMs.
Opportunistic updates: Only apply updates when you manually select a VM to update or when new instances are created. 
for all those who thinks its C please google and check its a straight forward question and you guyz are confusing people
upvoted 
1 
times
shashii82
shashii82
 
9 months, 4 weeks ago
Option C. To release a non-critical update to your application without updating any running instances and ensuring that new
instances created by the managed instance group contain the new update, you should choose option C: Start a new rolling
update and select the Proactive update mode.
In the Proactive update mode, the managed instance group creates new instances with the updated template while keeping the
existing instances running until they are eventually replaced. This allows you to roll out the update gradually without affecting
the currently running instances.
upvoted 
4 
times
kip21
kip21
 
11 months, 3 weeks ago
D - Correct
Managed instance groups support two types of update:
Automatic, or proactive, updates
Selective, or opportunistic, updates
If you want to apply updates automatically, set the type to proactive.
Alternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG
applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are
created.
upvoted 
3 
times
discuss24
discuss24
 
12 months ago
D is correct, per Google's documentation (The MIG applies an opportunistic update only when you manually initiate the update
on selected instances or when new instances are created. New instances can be created when you or another service, such as an
autoscaler, resizes the MIG.)
upvoted 
1 
times
[Removed]
[Removed]
 
1 year ago
D
Because the question says: "you don't want to update any running instances. You want any new instances that are created by the
managed instance group to contain the new update."
For the above case, we chose opportunistic
Proactive vs Opportunistic:
Proactive: If you want to apply updates automatically, set the type to proactive.
Opportunistic: Alternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic
update. The MIG applies an opportunistic update only when you manually initiate the update on selected instances or when
new instances are created.
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#type
upvoted 
1 
times
spuyol
spuyol
 
1 year ago
Selected answer: D
"To prevent any possible impact to the application, you don't want to update any running instances"
this is automatic achievable only by using opportunistic applying it during autoscale actions as documentation says: if you want
to selectively apply a new configuration only to new or to specific instances in a MIG, see Selectively apply VM configuration
updates in a MIG
upvoted 
2 
times
6b13108
6b13108
 
1 year, 1 month ago
In my opinion C is correct for Proactive update mode. Considering the following doc; "....Automated updates support up to two
instance template versions in your MIG. This means that you can specify two different instance template versions for your group,instance template versions in your MIG. This means that you can specify two different instance template versions for your group,
which is useful for performing canary updates.
To start a basic rolling update where the update is applied to all instances in the group, follow the instructions below....."
See Also:
"Update type
Managed instance groups support two types of update:
Automatic, or proactive, updates
Selective, or opportunistic, updates
If you want to apply updates automatically, set the type to proactive."
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-
groups#starting_a_basic_rolling_update
upvoted 
2 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
C
https://cloud.google.com/compute/docs/instance-groups/updating-
migs#choosing_between_automated_and_selective_updates
Automatic (proactive): Use this method if you want the MIG to automatically apply new configurations to all or to a subset of
existing VMs in the group. The level of disruption to running VMs depends on the update policy that you configure. You can use
this method to canary update new instance templates. To use this method, set the MIG's update type to "proactive".
Selective (opportunistic): Use this method if you want to apply the update manually or if you want to update all existing VMs in
the group at once. You target any or all VMs to be updated to the latest configuration. To use this method, set the MIG's update
type to "opportunistic".
Hence, C
upvoted 
3 
times
JPA210
JPA210
 
1 year, 2 months ago
It is option C, with proactive update, you are not updating the running instances, you start new ones with the new configuration
template. And stop the old ones, so there is not disruption to the service.
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
Selected Answer: 
D
To apply an updated configuration to existing VMs, you can set up an automatic update–also known as a proactive update
type. The MIG automatically rolls out configuration updates to all or to a subset of the group's VMs.
Alternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG
applies an opportunistic update only when you manually initiate the update on selected instances or when new instances are
created. New instances can be created when you or another service, such as an autoscaler, resizes the MIG.
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups
upvoted 
2 
times
duzapo
duzapo
 
1 year, 3 months ago
Selected Answer: 
D
https://cloud.google.com/compute/docs/instance-groups/updating-migs#selective_updates
no more explications neded
upvoted 
1 
times
jawulya
jawulya
 
1 year, 4 months ago
Selected Answer: 
D
The key here is "you don't want to update any running instances". Only opportunistic support this.
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
1 year, 4 months ago
D is correct
upvoted 
1 
times
abhi52
abhi52
 
1 year, 4 months agoSelected Answer: 
D
Are the people creating these tests retarded? C is correct? how? Correct answer is D
upvoted 
1 
times
bhinar
bhinar
 
1 year, 4 months ago
Selected Answer: 
C
If you want to apply updates automatically, set the type to proactive.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #117
Your company is designing its application landscape on Compute Engine. Whenever a zonal outage occurs, the application
should be restored in another zone as quickly as possible with the latest application data. You need to design the solution to
meet this requirement. What should you do? 
A. 
Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest
snapshot to restore the disk in the same zone.
B. 
Configure the Compute Engine instances with an instance template for the application, and use a regional persistent
disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in
another zone in the same region. Use the regional persistent disk for the application data. 
Most Voted
C. 
Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest
snapshot to restore the disk in another zone within the same region.
D. 
Configure the Compute Engine instances with an instance template for the application, and use a regional persistent
disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in
another region. Use the regional persistent disk for the application data.
Correct Answer:
 
B 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years ago
Answer is B, it only request zonal resiliency.
Regional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional
persistent disks can be a good building block to use when you implement HA services in Compute Engine.
https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk
upvoted 
52 
times
AmitRBS
AmitRBS
 
2 years, 1 month ago
B. Agree, clearly it’s B. Focus on keyword “zone”
upvoted 
4 
times
Ssoumya
Ssoumya
 
Highly Voted
 
3 years ago
Community vote distribution
B (97%)
D
(3%)Ssoumya
Ssoumya
 
 
3 years ago
Answer is B
upvoted 
14 
times
oscarcampos
oscarcampos
 
Most Recent
 
2 months, 1 week ago
why D ?
upvoted 
1 
times
mesodan
mesodan
 
4 months ago
Selected Answer: 
B
It is B. As for D: Spinning up the application in another region might be too geographically distant, leading to higher latency and
potential issues.
upvoted 
1 
times
hzaoui
hzaoui
 
5 months, 2 weeks ago
Selected Answer: 
B
A regional persistent disk is designed to provide synchronous replication of data between two zones in the same region, ensuring
that data remains available even if one zone is affected by an outage. By using an instance template along with a regional disk, you
can quickly create new instances in an available zone during a zonal outage and attach the regional persistent disk to continue
operations with the latest application data.
upvoted 
1 
times
SSS987
SSS987
 
5 months, 2 weeks ago
Can someone explain why not C - snapshot?
upvoted 
1 
times
xaqanik
xaqanik
 
5 months ago
B - uses instance template which is ready for deploy. Option C requires manual configuration and this may take more time . But
we need as quickly as possible .
upvoted 
1 
times
thewalker
thewalker
 
7 months, 3 weeks ago
Selected Answer: 
D
Option D suggests using the same approach as Option B but restoring the application in another region instead of the same
region. This approach provides high availability and disaster recovery across regions, making it suitable for applications that
require high RTOs and minimal data loss.
upvoted 
1 
times
convers39
convers39
 
6 months ago
can you use the regional persistent disk in a different region?
upvoted 
3 
times
don_v
don_v
 
5 months, 3 weeks ago
apparently, nope.
upvoted 
1 
times
JPA210
JPA210
 
8 months, 3 weeks ago
Of course it is answer B. I would like to understand who chooses the right answers in examtopics! Choosing here option D is
completely wrong. This takes people less instructed to be mistaken.
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
9 months ago
Selected Answer: 
B
B. Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the
application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the
same region. Use the regional persistent disk for the application data. Most Voted
Why to change the region as mentioned in Option D, when the ask is different zone only.upvoted 
1 
times
duzapo
duzapo
 
9 months, 4 weeks ago
Selected Answer: 
B
Answer is B, it only request zonal resiliency
upvoted 
1 
times
heretolearnazure
heretolearnazure
 
10 months, 1 week ago
It says restore in a zonal. Hence answer is B.
upvoted 
1 
times
mifrah
mifrah
 
1 year, 3 months ago
B. In my opinion, I cannot use a regional disk in a different region!!! So, it can only be another zone in the same region. Therefore
D must be wrong!
upvoted 
2 
times
SambuSoni
SambuSoni
 
1 year, 5 months ago
Answer B is Correct - since it talks about spin up application in different zone but same region. 
Whereas,D is incoorect , since its talking about spin up application in different region which is not our requirement.
upvoted 
1 
times
CosminCiuc
CosminCiuc
 
1 year, 5 months ago
If it is a regional persistent disk created in region A for example. If I start the compute engine instance in the region B, how am I
going to use a regional disk from region A (another region)? I do not think it is possible. So answer D should be excluded. 
I do believe that the correct answer is B.
upvoted 
2 
times
windsor_43
windsor_43
 
1 year, 6 months ago
The Answer is B
Just had my exam today with a pass, this question was in the exam. Dated 31/12/22
Thanks to this site it was by far my most valuable
upvoted 
5 
times
rascalbrick
rascalbrick
 
1 year, 6 months ago
14/12/22 Exam,but IM failed :(
upvoted 
1 
times
Praveen_G
Praveen_G
 
1 year, 6 months ago
Tomorrow 12/27/22 is my exam :)
upvoted 
2 
times
Sur_Nikki
Sur_Nikki
 
1 year, 1 month ago
How was it?
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #118
Your company has just acquired another company, and you have been asked to integrate their existing Google Cloud
environment into your company's data center. Upon investigation, you discover that some of the RFC 1918 IP ranges being
used in the new company's Virtual Private Cloud (VPC) overlap with your data center IP space. What should you do to enable
connectivity and make sure that there are no routing conflicts when connectivity is established? 
A. 
Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses
so there is no overlapping IP space. 
Most Voted
B. 
Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT
on the overlapping IP space.
C. 
Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route
advertisement to block the overlapping IP space.
D. 
Create a Cloud VPN connection from the new VPC to the data center, and apply a firewall rule that blocks the
overlapping IP space.
Correct Answer:
 
A 
Comments
VishalB
VishalB
 
Highly Voted
 
3 years, 5 months ago
Correct Answer: A
- IP Should not overlap so applying new IP address is the solution
upvoted 
42 
times
zanfo
zanfo
 
2 years, 9 months ago
A is not correct. "What should you do to enable connectivity and make sure that there are no routing conflicts when connectivity is
established?" if you apply VPN con BGP, the actual IP address will be propagated to on prem environment with overlapping
RFC1918 as result. B is correct with custom route
upvoted 
7 
times
TotoroChina
TotoroChina
 
Highly Voted
 
3 years, 6 months ago
Answer is C.
Community vote distribution
A (43%)
B (41%)
C (16%)https://cloud.google.com/network-connectivity/docs/router/how-to/advertising-custom-ip
upvoted 
36 
times
meh009
meh009
 
3 years, 2 months ago
The Q states to establish connectivity. This would merely prevent that. Ans is A
upvoted 
5 
times
don_v
don_v
 
11 months, 3 weeks ago
I would also agree with C.
Still, this part is confusing: "C. Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and
apply a custom route advertisement to *block* the overlapping IP space."
To *block*? Not to block. just to alias with advertised IP addresses.
upvoted 
2 
times
RKS_2021
RKS_2021
 
3 years, 5 months ago
ANS is B
https://cloud.google.com/architecture/best-practices-vpc-design
upvoted 
8 
times
imgcp
imgcp
 
3 years, 5 months ago
B is NOT correct. Cloud NAT is specifically used for translating the IP address of the outbound packets destined to the Internet.
But this question is about using VPN communication between two private IP address spaces (RFC1918). Cloud NAT cannot
achieve the purpose here, you can't use Cloud NAT to translate from one private IP to another private ip. I would vote for C.
upvoted 
13 
times
dija123
dija123
 
8 months, 2 weeks ago
You can use private or hybrid NAT
https://cloud.google.com/nat/docs/overview#private-nat
upvoted 
2 
times
Bill831231
Bill831231
 
3 years, 2 months ago
Thanks for the clarification, just one question, without a solution like NAT or reip, the service on the devices with overlapping IP
subnet will be unavailable for on-premise devices, not sure if the question also about this
upvoted 
1 
times
RKS_2021
RKS_2021
 
1 year, 3 months ago
It will be a NAT Router instance, which will route the traffic. I have practically applied the configuration.
upvoted 
2 
times
elenamatay
elenamatay
 
2 years, 12 months ago
You can't use Cloud NAT according to this documentation: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-
addresses
"Can I use Cloud NAT to connect a VPC network to another network to work around overlapping IP addresses? No, Cloud NAT
cannot apply to any custom route whose next hop is not the default internet gateway. For example, Cloud NAT cannot apply to
traffic sent to a next hop Cloud VPN tunnel, even if the destination is a publicly routable IP address."
upvoted 
16 
times
Peto12
Peto12
 
Most Recent
 
1 week, 4 days ago
Selected Answer: 
B
With A you need to apply new IP addresses, with B you can use private NAT.
upvoted 
1 
times
andreacola
andreacola
 
2 months ago
Selected Answer: 
B
Assume that the resources in your VPC network need to communicate with the resources in a VPC network or an on-premises or
other cloud provider network that is owned by a different business unit. However, that network contains subnets whose IP
addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway that translatesaddresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway that translates
traffic between the subnets in your VPC network to the non-overlapping subnets of the other network.
upvoted 
5 
times
Abhinavchawlac2d
Abhinavchawlac2d
 
2 months, 4 weeks ago
Correct Option:
B. Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the
overlapping IP space.
This option effectively allows you to connect the two environments while addressing the overlapping IP space issue through NAT,
ensuring that the VMs can communicate without conflicts.
upvoted 
2 
times
3fd692e
3fd692e
 
3 months ago
Selected Answer: 
B
There is a Private NAT you can use and is specifically designed to resolve overlapping private IP issues:
https://medium.com/niveus-solutions/private-cloud-nat-and-why-we-need-it-on-gcp-
f6ad0c96facb#:~:text=Private%20Cloud%20NAT%20with%20NCC,helps%20connect%20onprem%20to%20gcp.
upvoted 
5 
times
lucaluca1982
lucaluca1982
 
5 months ago
Selected Answer: 
B
Given that you are not going out to the internet and you need to use a Cloud Router for your VPC, you need to ensure that there is
no overlap in the IP ranges between your data center and the newly acquired company's VPC. The best approach to manage this
without renumbering the entire network is to use Network Address Translation (NAT) to handle the overlapping IP addresses.
upvoted 
1 
times
nhatne
nhatne
 
6 months ago
Selected Answer: 
B
would go for B
upvoted 
2 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
The answer is B. Cloud VPN and Cloud NAT help you get around this problem easily without all the work of creating a new subnet
and reassigning IPs to everything.
Cloud NAT: Network Address Translation (NAT) allows you to translate IP addresses in your VPC to a different IP range, avoiding
conflicts with overlapping IP ranges in your data center. This ensures that traffic can flow between the environments without
routing conflicts.
Cloud VPN: Establishing a Cloud VPN connection provides secure connectivity between the new VPC and your data center. By
combining this with Cloud NAT, you can effectively manage and resolve the IP address overlap.
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
Cloud NAT does not directly resolve IP address conflicts due to overlapping ranges. Cloud NAT is typically used for instances
without external IP addresses to access the internet while preserving their internal IPs for internal communications.
upvoted 
1 
times
eff12c1
eff12c1
 
7 months ago
Selected Answer: 
B
Using Cloud NAT to translate overlapping IP addresses is the most effective solution to ensure seamless connectivity between the
new company's VPC and your company's data center without routing conflicts. This approach avoids the complexity of
reconfiguring IP addresses and ensures that both networks can communicate effectively.
https://cloud.google.com/nat/docs/overview#private-nat
upvoted 
4 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
It is not NAT, we are not going out to internet. We need cloud router
upvoted 
1 
times
VegasDegenerate
VegasDegenerate
 
3 days, 5 hours agoNAT doesn’t necessarily need to be used only for internet communication
upvoted 
1 
times
sandyrao
sandyrao
 
7 months ago
Selected Answer: 
B
Ans is B
upvoted 
1 
times
pico
pico
 
7 months, 3 weeks ago
Selected Answer: 
B
https://cloud.google.com/nat/docs/overview#private-nat
Assume that the resources in your VPC network need to communicate with the resources in a VPC network or an on-premises or
other cloud provider network that is owned by a different business entity. However, the VPC network of that business entity
contains subnets whose IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private
NAT gateway that routes traffic between the subnets in your VPC network to the non-overlapping subnets of that business entity.
upvoted 
2 
times
Polosaty
Polosaty
 
9 months, 3 weeks ago
Selected Answer: 
B
I was absolutely sure that B was obviously wrong until I found that 
https://cloud.google.com/nat/docs/overview#private-nat
So it seems like the answer is B...
upvoted 
7 
times
JaimeMS
JaimeMS
 
8 months ago
B. THIS should be the accepted answer, the link you provide is 100% certain. It's a Private Hybrid NAT:
" ...private-to-private translations... traffic between VPC networks and on-premises networks..."
"...IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway..."
B, 100%
upvoted 
2 
times
shashii82
shashii82
 
9 months, 4 weeks ago
The challenge with Option A is that changing IP addresses can be complex and might impact existing applications, configurations,
and dependencies within the new company's VPC. It might introduce additional complexity and potential risks during the integration
process.
Option C, on the other hand, allows you to maintain the existing IP addressing in the new company's VPC while selectively
blocking the overlapping IP space during the routing process. This can be a more flexible and less disruptive approach, especially
in scenarios where readdressing is not practical.
In summary, both options might have their use cases, but Option C provides a solution that doesn't require changing IP addresses
and can help avoid potential disruptions caused by such changes.
upvoted 
1 
times
bargou
bargou
 
11 months ago
Selected Answer: 
A
with C option we would not able to connect to VM with those overlapping IP.
we need to add a middle VPC between them. it will be more complicated.
we have not choice here except reassigning IP adresses so i choose option A
upvoted 
1 
times
stefanop
stefanop
 
1 year ago
I think now the answer should change since Private NAT is publicly available: https://cloud.google.com/nat/docs/private-nat
upvoted 
7 
times
Jconnor
Jconnor
 
1 year, 1 month ago
Apply new IP addresses? You do not apply new IP, you replace them. Either poorly written or deceiving. To enable connectivity and
avoid routing conflicts, C is perfect. Long term of course we need to replace IP, but not to enable connectivity. C.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #119
You need to migrate Hadoop jobs for your company's Data Science team without modifying the underlying infrastructure. You
want to minimize costs and infrastructure management effort. What should you do? 
A. 
Create a Dataproc cluster using standard worker instances.
B. 
Create a Dataproc cluster using preemptible worker instances. 
Most Voted
C. 
Manually deploy a Hadoop cluster on Compute Engine using standard instances.
D. 
Manually deploy a Hadoop cluster on Compute Engine using preemptible instances.
Correct Answer:
 
B 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years, 6 months ago
Should be B, 
you want to minimize costs.
https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-
preemptible_secondary_workers
upvoted 
68 
times
J19G
J19G
 
3 years, 2 months ago
Agree, the migration guide also recommends to think about preemptible worker nodes:
https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#using_preemptible_worker_nodes
upvoted 
4 
times
ale_brd_111
ale_brd_111
 
2 years, 1 month ago
I think it's A.
The question does not mention anything about minimize the costs, all the questions in GCP exams that require minimize the
costs as requirement literally mention that in the question.
Also in order to minimize the costs you need to build jobs that are fault tolerant, as workers instances are preemptible. This also
requires some kind of Dev investment of work. So if not mentioned in the question fault tolerant and minimize costs then is not
required/needed.
Doc states below:
Only use preemptible nodes for jobs that are fault-tolerant or that are low enough priority that occasional job failure won't disrupt
your business.
Community vote distribution
B (58%)
A (42%)your business.
upvoted 
2 
times
Amrx
Amrx
 
2 months, 1 week ago
Question literally says "You want to minimize costs and infrastructure management effort."
upvoted 
1 
times
grejao
grejao
 
1 year, 9 months ago
OMG, you again?
zetalexg says:
It's dissapointing that you waste your time writting on this topic instead of paying attention at the questions.
upvoted 
5 
times
XDevX
XDevX
 
3 years, 6 months ago
Hi TotoroChina, 
I had the same thought when I first read the question - the problem I see is, in real business I think you would try to mix preemtible
instances and on-demand instances... Here you have to choose between only preemtible instances and on-demand instances...
Preemptible instances have some downsides - so we would need more details and ideally a mixed approach. That's why both
answers might be correcy, a) and b)...
Do you see that different?
Thanks!
Cheers,
D.
upvoted 
5 
times
kopper2019
kopper2019
 
3 years, 6 months ago
but you need to reduce management overhead so B
if you create a cluster manually and create and maintain GCE is not the way to go
upvoted 
5 
times
HenkH
HenkH
 
2 years, 1 month ago
B requires to create new instances at least every 24h.
upvoted 
2 
times
Sukon_Desknot
Sukon_Desknot
 
2 years, 2 months ago
"without modifying the underlying infrastructure" is the watch word. Most likely did not utilize preemptible on-premises
upvoted 
7 
times
Yogi42
Yogi42
 
1 year, 11 months ago
A cost-savings consideration: Using preemptible VMs does not always save costs since preemptions can cause longer job
execution with resulting higher job costs. This is mentioned in above link So I think Ans should be A
upvoted 
3 
times
firecloud
firecloud
 
Highly Voted
 
3 years, 5 months ago
It's A, the primary workers can only be standard, where secondary workers can be preemtible.------In addition to using standard
Compute Engine VMs as Dataproc workers (called "primary" workers), Dataproc clusters can use "secondary" workers.
There are two types of secondary workers: preemptible and non-preemptible. All secondary workers in your cluster must be of the
same type, either preemptible or non-preemptible. The default is preemptible.
upvoted 
35 
times
Manh
Manh
 
3 years, 3 months ago
agreed
upvoted 
2 
times
plumbig11
plumbig11
 
Most Recent
 
9 hours, 34 minutes ago
Selected Answer: 
B
Dataproc, as they want to save costs, preemptible is the best option, for sure.
upvoted 
1 
timesLestrang
Lestrang
 
3 months ago
Probably A
- Primary workers must be standard
- Preemptible doesn't always save cost
- Infrastructure on prem doesn't have spot machines
- You cannot choose spot/preemptible when creating the cluster, only when provisioning secondary nodes, which are actually
preemptible by default.
- They do not store data, only do data processing
upvoted 
1 
times
pcamaster
pcamaster
 
3 months, 1 week ago
Selected Answer: 
B
B: being a multiple-choice question, we need to focus on explicit keywords here. 
Management effort => Managed service -> Dataproc.
Cost-optimization => Preemptible. 
For ones who say "but that also requires fault toleration": well, there is no explicit keyword in the question says "we have critical
jobs" or "out data scientists team has not takes into account toleration". So we must not assume that's needed.
upvoted 
1 
times
afsarkhan
afsarkhan
 
5 months, 3 weeks ago
Selected Answer: 
A
I will go with A , reason preemptible instances are unpredictable and there is no mention of work criticallity. So my answer is A
against B
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 2 weeks ago
Selected Answer: 
A
A - only secondary workers can be preemptible and "Using preemptible VMs does not always save costs since preemptions can
cause longer job execution with resulting higher job costs" according to:
https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-
preemptible_secondary_workers
upvoted 
1 
times
dija123
dija123
 
8 months, 2 weeks ago
Selected Answer: 
B
Agree with B
upvoted 
1 
times
Diwz
Diwz
 
9 months, 1 week ago
Answer is B. 
The 
secondary worker type instance for default 
Dataproc cluster is preemptible VMs.
https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms
upvoted 
1 
times
shashii82
shashii82
 
9 months, 4 weeks ago
Dataproc: Dataproc is a fully managed Apache Spark and Hadoop service on Google Cloud Platform. It allows you to run clusters
without the need to manually deploy and manage Hadoop clusters on Compute Engine.
Preemptible Worker Instances: Preemptible instances are short-lived, cost-effective virtual machine instances that are suitable for
fault-tolerant and batch processing workloads. Since Hadoop jobs can often tolerate interruptions, using preemptible instances can
significantly reduce costs.
Option B leverages the benefits of Dataproc for managing Hadoop clusters without the need for manual deployment and takes
advantage of preemptible instances to minimize costs. This aligns well with the goal of minimizing both costs and infrastructure
management efforts.
upvoted 
1 
times
VidhyaBupesh
VidhyaBupesh
 
10 months, 1 week ago
Using preemptible VMs does not always save costs since preemptions can cause longer job execution with resulting higher job
costs
upvoted 
1 
timesupvoted 
1 
times
Amrita2012
Amrita2012
 
10 months, 2 weeks ago
Selected Answer: 
A
Using standard Compute Engine VMs as Dataproc workers (called "primary" workers), Preemptible can be only used for
secondary workers hence A is valid answer
upvoted 
1 
times
Pime13
Pime13
 
11 months ago
Selected Answer: 
B
minimize costs -> preemtipble
upvoted 
2 
times
d0094d6
d0094d6
 
11 months ago
Selected Answer: 
B
You want to minimize costs and infrastructure management effort > B
upvoted 
2 
times
d0094d6
d0094d6
 
11 months ago
"You want to minimize costs and infrastructure management effort" -> B
upvoted 
1 
times
didek1986
didek1986
 
11 months, 3 weeks ago
Selected Answer: 
A
It is A
upvoted 
1 
times
Romio2023
Romio2023
 
11 months, 4 weeks ago
Selected Answer: 
B
Answer should be B, because minizing the costs is wanted.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #120
Your company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute Engine instance on
each VPC. Network subnets do not overlap and must remain separated. The network configuration is shown below. 
 
Instance #1 is an exception and must communicate directly with both Instance #2 and Instance #3 via internal IPs. How should
you accomplish this? 
A. 
Create a cloud router to advertise subnet #2 and subnet #3 to subnet #1.
B. 
Add two additional NICs to Instance #1 with the following configuration: 
ג
€¢ NIC1 
ג
 ‹—VPC: VPC #2 
ג
 ‹—SUBNETWORK:
subnet #2 
ג
€¢ NIC2 
ג
 ‹—VPC: VPC #3 
ג
 ‹—SUBNETWORK: subnet #3 Update firewall rules to enable traffic between
instances. 
Most Voted
C. 
Create two VPN tunnels via CloudVPN: 
1
€¢ 
ג
 between VPC #1 and VPC #2. 
1
€¢ 
ג
 between VPC #2 and VPC #3. Update
firewall rules to enable traffic between the instances.
D. 
Peer all three VPCs: 
ג
€¢ Peer VPC #1 with VPC #2. 
ג
€¢ Peer VPC #2 with VPC #3. Update firewall rules to enable traffic
between the instances.Correct Answer:
 
B 
Comments
XDevX
XDevX
 
Highly Voted
 
3 years, 6 months ago
According to my understanding the requirement is that only VM1 shall be able to communicate with VM2 and VM3, but not
VM2 with VM3.
We can exclude d) as d) would enable VM2 to communicate with VM3 as well - my assumption is, that if the quizzer wanted
that d) is the correct answer, he would make just 2 peerings - 1x between VM1 and VM2 and 1x between VM1 and VM3
repectively the VPCs.
We can exclude c) as well - there is no connection between VPC1 and VPC3.
IMHO a) will not work.
So the only correct answer seems to be b) - what I don't understand is why we have to update the firewall rules as IMHO the
default firewall rules enable such communication (maybe some restrictive rules are implemented - not enough details in the
question to clarify that part). Please correct me if I am wrong.
upvoted 
25 
times
lazybeanbag
lazybeanbag
 
3 years, 5 months ago
I think it is because the instances are in separate VPCs.
"Google Cloud Virtual Private Cloud (VPC) networks are by default isolated private networking domains. Networks have a
global scope and contain regional subnets. VM instances within a VPC network can communicate among themselves using
internal IP addresses as long as firewall rules permit. However, NO INTERNAL IP ADDRESS COMMUNICATION IS ALLOWED
BETWEEN networks, unless you set up mechanisms such as VPC Network Peering or Cloud VPN."
The instructions for setting up multiple interfaces tells you to check your firewall rules as as the firewall rules of the VPC apply
to the network interface that it is attached to.
https://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces
upvoted 
8 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 4 months ago
The answer is "B". 
The following link has this - "Use multiple network interfaces when an individual instance needs access to
more than one VPC network, but you don't want to connect both networks directly."
https://cloud.google.com/vpc/docs/multiple-interfaces-concepts
upvoted 
4 
times
b6f53d8
b6f53d8
 
1 year, 2 months ago
you can not add additional network interface to existing VM's
upvoted 
1 
times
JeffClarke111
JeffClarke111
 
3 years, 6 months ago
Correct, maybe fw on the VM
upvoted 
2 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 4 months ago
The answer is "B". 
The following link has this - "Use multiple network interfaces when an individual instance needs access to
more than one VPC network, but you don't want to connect both networks directly."
https://cloud.google.com/vpc/docs/multiple-interfaces-concepts
upvoted 
13 
times
Pankaj_007
Pankaj_007
 
2 years, 1 month ago
B will not work.
VM instances within a VPC network can communicate among themselves using internal IP addresses as long as firewall rules
permit. However, no internal IP address communication is allowed between networks, unless you set up mechanisms such as
VPC Network Peering or Cloud VPN.
Community vote distribution
B (71%)
C (15%)
D (15%)VPC Network Peering or Cloud VPN.
upvoted 
1 
times
sameer2803
sameer2803
 
2 years ago
this link says VM can have multiple NICs and attached to different VPCs.
https://cloud.google.com/vpc/docs/create-use-multiple-interfaces
so B is the answer
upvoted 
6 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
3 years, 6 months ago
Answer is B
upvoted 
11 
times
coutcin
coutcin
 
2 years, 7 months ago
Instances are exist. You can not add or remove additional NICs to a VM
upvoted 
6 
times
plumbig11
plumbig11
 
Most Recent
 
9 hours, 33 minutes ago
Selected Answer: 
B
Add two additional NICs to Instance
upvoted 
1 
times
awsgcparch
awsgcparch
 
5 months, 1 week ago
Selected Answer: 
B
Direct Connectivity:
Adding multiple NICs to Instance #1 allows it to be part of multiple VPCs directly. This configuration enables direct
communication with Instance #2 and Instance #3 via internal IPs without requiring additional routing configurations.
Simplicity:
This approach is straightforward and avoids the complexity of setting up VPC peering or VPN tunnels. It ensures that only
Instance #1 has access to both VPC #2 and VPC #3, maintaining the separation of the other VPCs.
upvoted 
2 
times
afsarkhan
afsarkhan
 
5 months, 3 weeks ago
Selected Answer: 
D
VPC peering will allow access to instance 2 & 3 from 1 with internal IP, with necessary firewall rules added.
upvoted 
1 
times
dija123
dija123
 
8 months, 2 weeks ago
Selected Answer: 
B
B for sure
upvoted 
1 
times
shashii82
shashii82
 
9 months, 4 weeks ago
Option B allows you to add additional NICs to Instance #1, each connected to a different VPC, facilitating direct
communication between Instance #1 and the other instances while maintaining separate subnets.
upvoted 
1 
times
kshlgpt
kshlgpt
 
1 year ago
B is wrong. NIC can only be configured while creating the instance. Here the instance is already created. 
C is correct answer. 
Refer limitation in this link:
https://cloud.google.com/vpc/docs/create-use-multiple-interfaces
upvoted 
2 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
Selected Answer: 
B
Router, VPN and VPC Peering for all 3 network is not required.Router, VPN and VPC Peering for all 3 network is not required.
Only option B solves the given scenario.
upvoted 
1 
times
rusll
rusll
 
1 year, 4 months ago
All answers are incorrect: subnets do not overlap and must remain separated. => can't choose A or C or D. 
Which leaves us with A: you can't attach nics to a compute engine instance after creation : see:
https://cloud.google.com/vpc/docs/create-use-multiple-interfaces
upvoted 
3 
times
natpilot
natpilot
 
1 year, 8 months ago
Is D the correct, peering with adeguate forewall rule for only communication of Instance 1 with Instance 2 and 3
upvoted 
1 
times
mifrah
mifrah
 
1 year, 9 months ago
I vote for B:
VPC peering does not support "cascading". Peer VPC 1 with VPC 2, and VPC 2 with VPC 3 does not allow traffic from VPC 1 to
VPC 3.
upvoted 
1 
times
razabpn
razabpn
 
1 year, 10 months ago
Selected Answer: 
B
B: NIC usecase 
when an individual instance needs access to more than one VPC network, but you don't want to connect both
networks directly
https://cloud.google.com/vpc/docs/multiple-interfaces-concepts
upvoted 
1 
times
examch
examch
 
1 year, 12 months ago
Selected Answer: 
B
B is the correct answer,
Connect the VPC1 instance to VPC2 instance with NIC1 and Connect VPC1 instance to VPC3 instance with NIC2. And update
firewall rules to enable traffic between them.
https://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces
upvoted 
1 
times
thamaster
thamaster
 
2 years ago
Selected Answer: 
B
best practice is to add NIC to first instance
upvoted 
1 
times
ANKITMANDLA
ANKITMANDLA
 
2 years ago
Only solution is peering. N1 peering to 
n3 and n3 to n1 makes all network peered. So answer should be D
upvoted 
1 
times
Pankaj_007
Pankaj_007
 
2 years, 1 month ago
B would be incorrect --> As without VPC peering or VPN it will not come into Play.
D --> This is good as once VPN is established from 1 --> 2 and from 2 --> 3 ... data can flow from 1 to 3 via 2 ...
upvoted 
1 
times
Pankaj_007
Pankaj_007
 
2 years, 1 month ago
I mean C should be correct ..
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #121
You need to deploy an application on Google Cloud that must run on a Debian Linux environment. The application requires
extensive configuration in order to operate correctly. You want to ensure that you can install Debian distribution updates with
minimal manual intervention whenever they become available. What should you do? 
A. 
Create a Compute Engine instance template using the most recent Debian image. Create an instance from this template,
and install and configure the application as part of the startup script. Repeat this process whenever a new Google-
managed Debian image becomes available.
B. 
Create a Debian-based Compute Engine instance, install and configure the application, and use OS patch management
to install available updates. 
Most Voted
C. 
Create an instance with the latest available Debian image. Connect to the instance via SSH, and install and configure
the application on the instance. Repeat this process whenever a new Google-managed Debian image becomes available.
D. 
Create a Docker container with Debian as the base image. Install and configure the application as part of the Docker
image creation process. Host the container on Google Kubernetes Engine and restart the container whenever a new update
is available.
Correct Answer:
 
B 
Comments
victory108
victory108
 
Highly Voted
 
3 years, 5 months ago
B. Create a Debian-based Compute Engine instance, install and configure the application, and use OS patch management to
install available updates.
upvoted 
24 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
3 years, 6 months ago
Answer is B
upvoted 
9 
times
ccpmad
ccpmad
 
Most Recent
 
6 months, 4 weeks ago
Selected Answer: 
B
It is B, using Os path. 
Community vote distribution
B (86%)
Other (14%)It is B, using Os path. 
It is interesting the OS policy also, to run commands inside the VMs.
upvoted 
1 
times
NoCrapEva
NoCrapEva
 
11 months, 1 week ago
Selected Answer: 
D
The question requires MINIMAL manual intervention for patching Debian Linux. 
Therefore it makes most sense to create a Docker
container using the Debian Marketplace Distro 
(as long as the FROM field in your Dockerfile points to `$distro:latest` from Cloud
Marketplace)
Ref: 
https://cloud.google.com/blog/products/containers-kubernetes/exploring-container-security-let-google-do-the-patching-with-new-
managed-base-images
and
https://cloud.google.com/blog/products/containers-kubernetes/exploring-container-security-how-containers-enable-passive-
patching-and-a-better-model-for-supply-chain-security
upvoted 
2 
times
gcmrjbr
gcmrjbr
 
11 months, 4 weeks ago
You want to ensure that you can install Debian distribution updates with minimal manual intervention whenever they become
available... | if it is in the cloud then option A | if it is on the internet then option b... It is a trick question!
upvoted 
1 
times
gcmrjbr
gcmrjbr
 
11 months, 4 weeks ago
I vote for A
upvoted 
1 
times
biswa_b
biswa_b
 
1 year, 6 months ago
Option A, "Create a Compute Engine instance template using the most recent Debian image. Create an instance from this
template, and install and configure the application as part of the startup script. Repeat this process whenever a new Google-
managed Debian image becomes available," is the correct choice.
This approach allows you to automate the process of creating a new instance with the latest Debian image and configuring the
application. By using an instance template and a startup script, you can ensure that the application is correctly configured each
time a new instance is created. When a new Debian image becomes available, you can simply create a new instance template
and repeat the process.
Option B, using OS patch management, would allow you to install updates, but it wouldn't necessarily ensure that the application is
correctly configured after an update.
upvoted 
3 
times
freecloud
freecloud
 
1 year, 6 months ago
Can someone explain why the answer is not D ? Isn't the best practice to use containers ?
upvoted 
3 
times
afsarkhan
afsarkhan
 
5 months, 3 weeks ago
because restarting pod does not change the base image version of the application image. Hence D is a wrong answer
upvoted 
1 
times
tes1298t
tes1298t
 
1 year, 4 months ago
Because it mentions "restart container whenever update is available". Restarting doesn't just update the OS. You need to build
docker image with new version
upvoted 
3 
times
JPA210
JPA210
 
1 year, 2 months ago
When you restart the container, will it not download the new image and reconfigure the application? I believe so, there is an
option to configure the pods to download always the last image.
upvoted 
1 
times
natpilot
natpilot
 
1 year, 8 months ago
A is correct, with template and startup script you can create multiple instance with minimal manual intervention; when the new
debian release will be available, you need update only the template with new image of debian distribution.debian release will be available, you need update only the template with new image of debian distribution.
upvoted 
2 
times
examch
examch
 
1 year, 12 months ago
Selected Answer: 
B
B is the correct answer,
Use OS patch management to apply operating system patches across a set of Compute Engine VM instances (VMs). Long
running VMs require periodic system updates to protect against defects and vulnerabilities.
The OS patch management service has two main components:
Patch compliance reporting, which provides insights on the patch status of your VM instances across Windows and Linux
distributions. Along with the insights, you can also view recommendations for your VM instances.
Patch deployment, which automates the operating system and software patch update process. A patch deployment schedules
patch jobs. A patch job runs across VM instances and applies patches.
https://cloud.google.com/compute/docs/os-patch-management
upvoted 
5 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
2 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
B
B is the simplest option and with minimal intervention. Other answers may be technically possible but the question does not ask
for anything else (e.g. containers, templates, etc.)
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 3 months ago
I will go with B , there is no need to application configuration each time
upvoted 
1 
times
zellck
zellck
 
2 years, 3 months ago
Selected Answer: 
B
B is the answer.
https://cloud.google.com/compute/docs/os-patch-management
Use OS patch management to apply operating system patches across a set of Compute Engine VM instances (VMs).
upvoted 
2 
times
Jay_Krish
Jay_Krish
 
2 years, 3 months ago
Selected Answer: 
B
Key words : "with minimal manual intervention whenever they become available"
With OS Patch Management the OS will have all the latest available updates at that point automatically which wouldn't be the case
with A.
upvoted 
2 
times
backhand
backhand
 
2 years, 4 months ago
vote B
about A using latest template for update patch, next time you have to make template again.
ans B, make once setting for good basically with no intervention.
upvoted 
2 
times
patashish
patashish
 
2 years, 4 months ago
A is correct answer.
I am not able to understand B option and ppl are simply adding patch management URL as reference. How this will relate with
answer ? A is correct answer.
upvoted 
1 
times
enter_co
enter_co
 
2 years, 2 months ago
A only answers to 'minimal management overhead', but not to 'install patches as soon as possible'.
upvoted 
1 
times
Ric350
Ric350
 
2 years, 5 months ago
The ask in this question is "you want to ensure that you can install Debian distribution updates (which is OS updates specifically)
with minimal manual intervention whenever they become available." 
That is accomplished by an OS patch management.
https://cloud.google.com/compute/docs/os-patch-management
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #122
You have an application that runs in Google Kubernetes Engine (GKE). Over the last 2 weeks, customers have reported that a
specific part of the application returns errors very frequently. You currently have no logging or monitoring solution enabled on
your GKE cluster. You want to diagnose the problem, but you have not been able to replicate the issue. You want to cause
minimal disruption to the application. What should you do? 
A. 
1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs
from affected Pods. 
Most Voted
B. 
1. Create a new GKE cluster with Cloud Operations for GKE enabled. 2. Migrate the affected Pods to the new cluster, and
redirect traffic for those Pods to the new cluster. 3. Use the GKE Monitoring dashboard to investigate logs from affected
Pods.
C. 
1. Update your GKE cluster to use Cloud Operations for GKE, and deploy Prometheus. 2. Set an alert to trigger whenever
the application returns an error.
D. 
1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the affected
Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Set an alert to trigger whenever the
application returns an error.
Correct Answer:
 
A 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years, 6 months ago
According to the reference, answer should be A.
https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine
upvoted 
48 
times
MF2C
MF2C
 
3 years ago
But updating cluster requires downtime, isn't it?
upvoted 
5 
times
Nick89GR
Nick89GR
 
2 years, 8 months ago
Community vote distribution
A (64%)
C (34%)
D
(2%)No it actually does not require to shut down the cluster:
https://cloud.google.com/stackdriver/docs/solutions/gke/installing#console_1
upvoted 
5 
times
enter_co
enter_co
 
2 years, 2 months ago
The problem in A) answer is that it is not alert-based. All recent trainings recommend use of alerts for troubleshooting, not
dashboards.
upvoted 
2 
times
don_v
don_v
 
11 months, 3 weeks ago
What about "2. Use the GKE Monitoring dashboard to investigate logs from affected Pods" then?
I'd really like to learn how anyone can use "Monitoring dashboard to investigate logs".
It's just absurd.
upvoted 
1 
times
shashii82
shashii82
 
9 months, 3 weeks ago
Monitoring console – In the Kubernetes Engine section of the Monitoring console, select the appropriate cluster, nodes, pod or
containers to view the associated logs.
upvoted 
1 
times
don_v
don_v
 
11 months, 3 weeks ago
Also, an "alert" is the keyword here.
No need to make anyone pressing a button every 5 minutes 
or so (like in 
In the TV show "Lost," where not pushing the button in
the Swan Station results in a catastrophic electromagnetic event. o-;
upvoted 
1 
times
poseidon24
poseidon24
 
3 years, 5 months ago
correct, from GCP best practices for GKE we should rely on native logging capabilities. No need for additional solutions like
Prometheus. Also it is about reviewing logs, monitoring the service, not receiving alerts each time its happens, that will not
provide any insight on the issue.
upvoted 
19 
times
victorlie
victorlie
 
3 years, 4 months ago
Also, as long you know there is a problem, i think you should investigate immediately the issue, not wait for new errors
upvoted 
6 
times
XDevX
XDevX
 
Highly Voted
 
3 years, 6 months ago
IMHO a) is the correct answer, not c)
The point is, that we have a scenario in that often errors in GKE happen - within 2 week a lot of people complained about a lot of
errors. For the past we have no data at all as we have not monitored anything. That means we will collect data from now on to find
out what the problem is. The additional value of an alert is not clear - and it for me not clear why we need additionally to install
Prometheus considering that until now we had no GKE monitoring at all. Please correct me if I am wrong.
upvoted 
18 
times
e5019c6
e5019c6
 
1 year ago
Right, if we enable Cloud Operations we should be able to see the logs from this point onwards. Data of past errors would not be
visible. It's not rational to expect developers to check every hour for appearances of the error in the logs, and that's where an alert
comes in handy. It'll notify you when the conditions that led to the error appear again so that developers can analyze the logs and
understand the problem.
I agree that installing Prometheus is not needed today, but it seems that it was the only option at the time to set up alerts and, in
my opinion, the alerts are vital to diagnose the problem.
upvoted 
1 
times
afsarkhan
afsarkhan
 
Most Recent
 
5 months, 3 weeks ago
Selected Answer: 
A
B & D are asking to create new GKE cluster so we can ignore them
From A and C , we can achieve the requirement with the help of A itself. Monitoring & Logging should help investigate the issue.
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 2 weeks agoGino17m
Gino17m
 
8 months, 2 weeks ago
Selected Answer: 
C
C
1. "You currently have no logging or monitoring solution enabled on your GKE cluster" and "you have not been able to replicate the
issue"- nothing interesting in GKE monitoring dashboard
2. No alerting in answer A
upvoted 
1 
times
yas_cloud
yas_cloud
 
10 months ago
Options A and C are less disruptive. Option C adds Prometheus on top which looks like overkills for this simple/initial level of
troubleshooting. I would go with option A
upvoted 
1 
times
[Removed]
[Removed]
 
1 year ago
A
A provides native solution to GCLoud
Why not C?
from GCP best practices for GKE we should rely on native logging capabilities. No need for additional solutions like Prometheus.
Also it is about reviewing logs, monitoring the service, not receiving alerts each time its happens, that will not provide any insight on
the issue. 
Prometheus could potentially help identify when the issue occurs, it doesn't directly help with diagnosing the root cause of the
problem.
B & D rejected because migration will cause distruption. 
https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
Selected Answer: 
C
If we enable Cloud Operations we should be able to see the logs from this point onwards. Data of past errors would not be visible.
It's not rational to expect developers to check every hour for appearances of the error in the logs, and that's where an alert comes
in handy. It'll notify you when the conditions that led to the error appear again so that developers can analyze the logs and
understand the problem.
I agree that installing Prometheus is not needed today, but it seems that it was the only option at the time they created this
question to set up alerts and, in my opinion, the alerts are vital to diagnose the problem.
upvoted 
2 
times
rohen21
rohen21
 
1 year, 1 month ago
Marked in Green is the real exam ans, or the community most voted one? I'm confused now hehe
upvoted 
1 
times
[Removed]
[Removed]
 
1 year ago
Look for answers in discussion, read related documents as well. 95% times, most voted answer in discussions is corrrect, but
reading google's doc is also necessary
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
As far as I know, it's the answer the uploader of the question set as correct. But it's not necesarily the correct one. The
community voted one is more probably right.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
C
As per https://cloud.google.com/stackdriver/docs/managed-prometheus - Correct option, I feel is C.
upvoted 
1 
times
CyanideX
CyanideX
 
1 year, 2 months ago
Selected Answer: 
A
Answer is Aupvoted 
1 
times
JPA210
JPA210
 
1 year, 2 months ago
I think answer A is enough, but if you want a more complete solution C could be a good option:
https://cloud.google.com/stackdriver/docs/managed-prometheus
upvoted 
1 
times
midori_jn
midori_jn
 
1 year, 3 months ago
Could anyone kindly explain why B is incorrect? Thank you.
upvoted 
2 
times
JC0926
JC0926
 
1 year, 8 months ago
Selected Answer: 
A
A. 1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs from
affected Pods.
By updating your GKE cluster to use Cloud Operations for GKE (formerly known as Stackdriver), you enable monitoring and
logging without disrupting the application. The GKE Monitoring dashboard allows you to investigate logs from affected Pods, which
helps you diagnose the problem that customers have reported. This approach minimizes disruption to the application while
providing the necessary information to identify and resolve the issue
upvoted 
3 
times
kratosmat
kratosmat
 
1 year, 8 months ago
Selected Answer: 
A
As described here
https://cloud.google.com/stackdriver/docs/solutions/gke
is it possible to install prometheus as part of cloud operation suite.
upvoted 
1 
times
thamaster
thamaster
 
2 years ago
Selected Answer: 
A
you want to minimize change so A is the best answer also you don't know what is the issue by reviewing the logs you can find
something
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
The best option is D. 1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the
affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Set an alert to trigger whenever the
application returns an error.
Here's why:
Option A does not involve creating a new GKE cluster, which means you will not be able to isolate the affected Pods from the rest
of the application. This can make it difficult to diagnose the issue without disrupting the entire application.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
Option B involves deploying a new version of the application, which may or may not fix the issue. Additionally, this option does not
address the root cause of the issue or provide a way to monitor the application for future errors.
Option C involves deploying a new version of the application and setting an alert to trigger whenever the application returns an
error. However, this option does not involve creating a new GKE cluster or migrating the affected Pods to a separate cluster,
which means that the issue could continue to affect the entire application.
upvoted 
1 
times
CosminCiuc
CosminCiuc
 
1 year, 11 months ago
You cannot automatically migrate pods from one cluster to another. You would have to manually deploy the workloads on the new
cluster. And you have the problem of configuring the services in the new cluster. You will need to use new IP addresses for the
services, modify the DNS to direct the client applications to the services from the new cluster. Very, very complicated. I would
exclude the answer that propose creation of new GKE clusters.
upvoted 
2 
times
Sur_Nikki
Sur_Nikki
 
1 year, 7 months agoSur_Nikki
Sur_Nikki
 
1 year, 7 months ago
Thanks again for such a descriptive and well convinced explanation
upvoted 
1 
times
gonlafer
gonlafer
 
2 years ago
Selected Answer: 
C
Question states problem cannot be replicated. So alerting is required to review the right logs at right time. Hence A is not adequate
solution and C is the right one
upvoted 
2 
timesTopic 1
 
Question #123
You need to deploy a stateful workload on Google Cloud. The workload can scale horizontally, but each instance needs to read
and write to the same POSIX filesystem. At high load, the stateful workload needs to support up to 100 MB/s of writes. What
should you do? 
A. 
Use a persistent disk for each instance.
B. 
Use a regional persistent disk for each instance.
C. 
Create a Cloud Filestore instance and mount it in each instance. 
Most Voted
D. 
Create a Cloud Storage bucket and mount it in each instance using gcsfuse.
Correct Answer:
 
C 
Community vote distribution
C (94%)
D (6%)
Comments
TotoroChina
TotoroChina
Highly Voted 
3 years, 6 months ago
Answer should be C,
https://cloud.google.com/storage/docs/gcs-fuse#notes
upvoted 
36 
times
JeffClarke111
JeffClarke111
3 years, 6 months ago
Agreed - C
upvoted 
5 
times
Urban_Life
Urban_Life
3 years ago
https://cloud.google.com/filestore
upvoted 
3 
times
elainexs
elainexs
2 years, 7 months ago
"“Cloud Storage FUSE is an open source [FUSE](http://fuse.sourceforge.net/) adapter that allows you to mount Cloud Storage buckets
as file systems on Linux or macOS systems. It also provides a way for applications to upload and download Cloud Storage objects
using standard file system semantics. Cloud Storage FUSE can be run anywhere with connectivity to Cloud Storage, including Google
Compute Engine VMs or on-premises systems[**1**](https://cloud.google.com/storage/docs/gcs-fuse#f1-note)." 
Exam Professional Cloud Architect 
All Actual Questions
        Compute Engine VMs or on-premises systems[**1**](https://cloud.google.com/storage/docs/gcs-fuse#f1-note)." 
D says "gcsfuse", should be D
upvoted 
4 
times
Frollo
Frollo
2 years, 2 months ago
FUSE is not posix
upvoted 
11 
times
CloudWars
CloudWars
1 year, 5 months ago
directly from the documentation of gcs fuse > While Cloud Storage FUSE has a file system interface, it is not like an NFS or CIFS file
system on the backend. Additionally, Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud,
see Filestore.
upvoted 
8 
times
ArtistS
ArtistS
1 year, 1 month ago
Google Cloud Storage Fuse is not POSIX compliant so C
upvoted 
3 
times
odacir
odacir
1 year, 1 month ago
Not, you need a file system not a blob storage...
upvoted 
2 
times
decw
decw
1 year ago
''While Cloud Storage FUSE has a file system interface, it is not like an NFS or CIFS file system on the backend. Additionally, Cloud
Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore.''
upvoted 
5 
times
XDevX
XDevX
Highly Voted 
3 years, 6 months ago
IMHO d) is wrong, the correct answer is c).
The requirement is explicitly POSIX filesystem - using gcsfuse Cloud Storage still remains an object storage - IMHO gcsfuse brings a
lot of downsizes compared with Filestore and in the question there are no indications that a non-POSIX filesystem shall be used.
upvoted 
16 
times
enado
enado
1 year, 6 months ago
Additional google explicitly states that Cloud Storage fuse is not POSIX compliant
https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations
upvoted 
2 
times
enado
enado
1 year, 6 months ago
So the correct answer is C
upvoted 
1 
times
kip21
kip21
Most Recent 
2 weeks, 4 days ago
Selected Answer: 
C
Cloud Storage FUSE is not POSIX compliant
upvoted 
1 
times
afsarkhan
afsarkhan
5 months, 3 weeks ago
Selected Answer: 
CSelected Answer: 
C
Answer is C
As filestore can be attached to multiple pods/vm in R/W mode
A & B are wrong for the same reason, PD can be attached to single pod for write operation.
upvoted 
1 
times
Gino17m
Gino17m
8 months, 2 weeks ago
Selected Answer: 
C
C
1. "each Instance needs to read and wrlte to the same POSIX filesystem"
2. Cloud Storage is not POSIX compliant filesystem but Object Storage and gcsfuse only "simulates" file system
3. See: https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations
"Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore.
upvoted 
1 
times
Polosaty
Polosaty
9 months, 3 weeks ago
Selected Answer: 
C
From https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations
Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore.
upvoted 
2 
times
Pime13
Pime13
11 months ago
Selected Answer: 
C
https://cloud.google.com/storage/docs/gcs-fuse#notes
upvoted 
1 
times
discuss24
discuss24
12 months ago
C is correct, Per documentation ( Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see
Filestore)
upvoted 
1 
times
iamleond
iamleond
1 year ago
Selected Answer: 
C
https://cloud.google.com/storage/docs/gcs-fuse, not POSIX complieant.
upvoted 
1 
times
[Removed]
[Removed]
1 year ago
C
Explanation:
A & B: persistent disk won't be shared. Question says "each instance needs to read and write to the same POSIX filesystem." Although,
now u can share persistent disk(https://cloud.google.com/compute/docs/disks/sharing-disks-between-
vms#:~:text=Note%3A%20You%20can%20share%20Persistent%20Disk%20volumes%20only%20with%20VMs%20that%20are%20in%2
0the%20same%20zone%20as%20the%20disk.) but question doesnt mention that all VMs are in same zone as disk. 
C: Filestore ideal for NFS and POSIX
D: shared access can be achieved using GCSFuse, still, it's not POSIX complaint (https://cloud.google.com/storage/docs/gcs-
fuse#differences-and-limitations)
upvoted 
1 
times
6b13108
6b13108
1 year, 1 month agoYes, the correct answer is C:
LIMITATIONS: "While Cloud Storage FUSE has a file system interface, it is not like an NFS or CIFS file system on the backend.
Additionally, Cloud Storage FUSE is not POSIX compliant. For a POSIX file system product in Google Cloud, see Filestore."
https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations
upvoted 
2 
times
odacir
odacir
1 year, 1 month ago
Selected Answer: 
D
You need a file system not a blob storage...
upvoted 
2 
times
thewalker
thewalker
1 year, 1 month ago
Selected Answer: 
C
As per the documentation, https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations. The option is C.
upvoted 
1 
times
bhinar
bhinar
1 year, 4 months ago
Selected Answer: 
C
firestore is POSIX
upvoted 
1 
times
coder36
coder36
1 year, 6 months ago
Could anyone please tell why A or B is wrong? Thanks
upvoted 
1 
times
BEE_HI_5
BEE_HI_5
1 year, 8 months ago
https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations 
Answer C because POSIX and FUSE are not compatible. Google recommends using Filestore to address POSIX file operations
upvoted 
3 
times
raselsys
raselsys
1 year, 9 months ago
Selected Answer: 
C
Answer is C
https://cloud.google.com/storage/docs/gcs-fuse
Clearly mentioned to see filestore for POSIX
upvoted 
2 
times
Exam name or code...
Exam name or code...
Log in to ExamTopics
×
Sign in:
Email or nicknameEmail or nickname
Password
Forgot my password
Log in
Don't have an account yet? just 
sign-up 
. 
Resend activation email
Close
Most Voted
A voting comment increases the vote count for the chosen answer by one. 
Upvoting a comment with a selected answer will also increase the vote count towards that answer by one. 
So if you see a comment that you already agree with,
you can upvote it instead of posting a new comment.
Save 
Cancel
Loading 
...
Report Comment
×
Is the comment made by 
USERNAME 
spam or abusive?
Yes
 
No
Commenting
×
In order to participate in the comments you need to be logged-in. 
You can 
sign-up 
or 
login 
(it's free).
Ok 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #124
Your company has an application deployed on Anthos clusters (formerly Anthos GKE) that is running multiple microservices.
The cluster has both Anthos Service 
Mesh and Anthos Config Management configured. End users inform you that the application is responding very slowly. You
want to identify the microservice that is causing the delay. What should you do? 
A. 
Use the Service Mesh visualization in the Cloud Console to inspect the telemetry between the microservices. 
Most Voted
B. 
Use Anthos Config Management to create a ClusterSelector selecting the relevant cluster. On the Google Cloud Console
page for Google Kubernetes Engine, view the Workloads and filter on the cluster. Inspect the configurations of the filtered
workloads.
C. 
Use Anthos Config Management to create a namespaceSelector selecting the relevant cluster namespace. On the
Google Cloud Console page for Google Kubernetes Engine, visit the workloads and filter on the namespace. Inspect the
configurations of the filtered workloads.
D. 
Reinstall istio using the default istio profile in order to collect request latency. Evaluate the telemetry between the
microservices in the Cloud Console.
Correct Answer:
 
A 
Comments
AnilKr
AnilKr
 
Highly Voted
 
3 years, 5 months ago
The Anthos Service Mesh pages in the Google Cloud Console provide both summary and in-depth metrics, charts, and graphs
that enable you to observe service behavior. You can monitor the overall health of your services, or drill down on a specific service
to set a service level objective (SLO) or troubleshoot an issue.
https://cloud.google.com/service-mesh/docs/observability/explore-dashboard
upvoted 
22 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
3 years, 5 months ago
Answer is A
upvoted 
13 
times
afsarkhan
afsarkhan
 
Most Recent
 
5 months, 3 weeks ago
Community vote distribution
A (100%)Selected Answer: 
A
Right answer is A
Service Mesh helps in monitoring at single place.
upvoted 
1 
times
odacir
odacir
 
1 year, 1 month ago
Selected Answer: 
A
Anthos Service Mesh Visualization
https://cloud.google.com/service-mesh/docs/observability-overview
upvoted 
3 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
ACE_ASPIRE
ACE_ASPIRE
 
2 years, 4 months ago
I got this question in exam.
upvoted 
3 
times
Deepak31
Deepak31
 
2 years, 1 month ago
what is the answer
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
A is right.
upvoted 
1 
times
technodev
technodev
 
2 years, 11 months ago
Got this question in my exam, answered A
upvoted 
5 
times
Deb2293
Deb2293
 
1 year, 10 months ago
How many question from this entire question bank were common?
upvoted 
1 
times
Sur_Nikki
Sur_Nikki
 
1 year, 7 months ago
Thanks dear
upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for A
upvoted 
1 
times
vincy2202
vincy2202
 
3 years, 1 month ago
A is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
4 
times
AnilKr
AnilKr
 
3 years, 5 months ago
Ans-A
https://cloud.google.com/service-mesh/docs/observability/explore-dashboardhttps://cloud.google.com/service-mesh/docs/observability/explore-dashboard
upvoted 
6 
times
VishalB
VishalB
 
3 years, 5 months ago
Ans : A
Anthos Service Mesh’s robust tracing, monitoring, and logging features give you deep insights into how your services are
performing, how that performance affects other processes, and any issues that might exist.
upvoted 
9 
times
victory108
victory108
 
3 years, 5 months ago
A. Use the Service Mesh visualization in the Cloud Console to inspect the telemetry between the microservices.
upvoted 
5 
times
Rom0817
Rom0817
 
3 years, 6 months ago
Answer: A, Service Mesh
https://cloud.google.com/anthos/service-mesh
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #125
You are working at a financial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these
approval documents must be uploaded as a separate approval file, so you want to ensure that these documents cannot be
deleted or overwritten for the next 5 years. What should you do? 
A. 
Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy. 
Most Voted
B. 
Create the bucket with uniform bucket-level access, and grant a service account the role of Object Writer. Use the
service account to upload new files.
C. 
Use a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.
D. 
Create the bucket with fine-grained access control, and grant a service account the role of Object Writer. Use the service
account to upload new files.
Correct Answer:
 
A 
Comments
VishalB
VishalB
 
Highly Voted
 
3 years, 5 months ago
Answer A
o If a bucket has a retention policy, objects in the bucket can only be deleted or replaced once their age is greater than the
retention period.
o Once you lock a retention policy, you cannot remove it or reduce the retention period it has.
upvoted 
31 
times
azureaspirant
azureaspirant
 
Highly Voted
 
2 years, 10 months ago
2/15/21 exam
upvoted 
7 
times
afsarkhan
afsarkhan
 
Most Recent
 
5 months, 3 weeks ago
Selected Answer: 
A
A is correct because retention policy on bucket with lock , make sure no one can delete the object until it expires.
upvoted 
1 
times
[Removed]
[Removed]
 
1 year ago
Community vote distribution
A (100%)A
https://cloud.google.com/storage/docs/bucket-lock#policy-locks
upvoted 
1 
times
examch
examch
 
1 year, 12 months ago
Selected Answer: 
A
A is the correct answer,
You can include a retention policy when creating a new bucket, or you can add a retention policy to an existing bucket. Placing a
retention policy on a bucket ensures that all current and future objects in the bucket cannot be deleted or replaced until they reach
the age you define in the retention policy. Attempts to delete or replace objects whose age is less than the retention period fail with
a 403 - retentionPolicyNotMet error.
https://cloud.google.com/storage/docs/bucket-lock#retention-policy
upvoted 
5 
times
ecruells
ecruells
 
2 years ago
It appeared in 12/12/22 Exam
upvoted 
2 
times
Deb2293
Deb2293
 
1 year, 10 months ago
How many question came from this question bank?
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
ACE_ASPIRE
ACE_ASPIRE
 
2 years, 4 months ago
I got this question in exam.
upvoted 
2 
times
cbarg
cbarg
 
2 years, 5 months ago
Selected Answer: 
A
A is the only applicable answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
A is right
upvoted 
1 
times
MOSES3009
MOSES3009
 
2 years, 6 months ago
A is correct
upvoted 
1 
times
ss909098
ss909098
 
2 years, 10 months ago
Selected Answer: 
A
A is correct. Create retention policy
upvoted 
1 
times
technodev
technodev
 
2 years, 11 months ago
Got this question in my exam, answered A
upvoted 
3 
times
Sur_Nikki
Sur_Nikki
 
1 year, 7 months ago
Thanks dear!upvoted 
1 
times
haroldbenites
haroldbenites
 
3 years ago
Go for A
upvoted 
1 
times
vincy2202
vincy2202
 
3 years, 1 month ago
A is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
3 
times
Chotebhaisahab
Chotebhaisahab
 
3 years, 2 months ago
I agree with A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #126
Your team will start developing a new application using microservices architecture on Kubernetes Engine. As part of the
development lifecycle, any code change that has been pushed to the remote develop branch on your GitHub repository should
be built and tested automatically. When the build and test are successful, the relevant microservice will be deployed
automatically in the development environment. You want to ensure that all code deployed in the development environment
follows this process. What should you do? 
A. 
Have each developer install a pre-commit hook on their workstation that tests the code and builds the container when
committing on the development branch. After a successful commit, have the developer deploy the newly built container
image on the development cluster.
B. 
Install a post-commit hook on the remote git repository that tests the code and builds the container when code is pushed
to the development branch. After a successful commit, have the developer deploy the newly built container image on the
development cluster.
C. 
Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in
Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on the
development cluster. Ensure only the deployment tool has access to deploy new versions. 
Most Voted
D. 
Create a Cloud Build trigger based on the development branch to build a new container image and store it in Container
Registry. Rely on Vulnerability Scanning to ensure the code tests succeed. As the final step of the Cloud Build process,
deploy the new container image on the development cluster. Ensure only Cloud Build has access to deploy new versions.
Correct Answer:
 
C 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years ago
Answer should be C, obviously.
upvoted 
46 
times
AdGlad
AdGlad
 
Highly Voted
 
2 years, 11 months ago
Questions say "relevant microservice will be deployed automatically in the development environment." Therefore A and B are
out. D says "Rely on Vulnerability Scanning to ensure the code tests succeed." Vulnerability Scanning is not test so D is out. The
correct Answer is therefore C.
Community vote distribution
C (97%)
D
(3%)correct Answer is therefore C.
upvoted 
42 
times
de1001c
de1001c
 
Most Recent
 
3 weeks, 6 days ago
Selected Answer: 
C
Automatic, no developer running stuff, plus relying in pre-commit hooks alone is not a CD strategy. The C over D is because the
pipeline in cloudbuild should run the tests, not the Vulnerability Scanner. While vulnerability scanner is useful, it's not required in
this context, and there're no tests run in answer D.
upvoted 
2 
times
Gino17m
Gino17m
 
2 months, 1 week ago
Automatic deployment required in the question and manual deployment by developer in answer marked as correct !
Ance again....Who is responsible for marking answers as correct on examtopics platform ? Enyone from examtopics read
responses from community and correct wrong answers ???
A few days after purchasing full access to this platform, I am disgusted :(
upvoted 
1 
times
ccpmad
ccpmad
 
3 weeks, 5 days ago
Do not be angry. That the answers are incorrect is on purpose. First so that the pdf is not shared and they are left without
business. And secondly so that Google does not change the questions because then there are no correct answers filtered.
upvoted 
1 
times
Gall
Gall
 
4 months, 3 weeks ago
Selected Answer: 
C
C but.... "build container and add to the registry" ???? 
Not container but image.
upvoted 
1 
times
convers39
convers39
 
5 months, 3 weeks ago
Selected Answer: 
C
Nah, no way to be A.
upvoted 
1 
times
AWS_Sam
AWS_Sam
 
6 months ago
The correct answer is C.
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
9 months ago
Selected Answer: 
C
Agreed with omermahgoub
Answer should be C.
upvoted 
1 
times
balajisreenivas
balajisreenivas
 
1 year, 3 months ago
Selected Answer: 
C
By elimination method, the answer is C.
upvoted 
1 
times
kopasz93
kopasz93
 
1 year, 3 months ago
Selected Answer: 
C
The answer is C.
upvoted 
1 
times
Dr_Ramzus
Dr_Ramzus
 
1 year, 5 months ago
Selected Answer: 
C
Clearly C
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months agoThe correct answer is C: Create a Cloud Build trigger based on the development branch that tests the code, builds the container,
and stores it in Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on
the development cluster. Ensure only the deployment tool has access to deploy new versions.
To automate the build and deployment process for your microservices in the development environment, you can use Cloud
Build to set up a trigger that listens for code pushes to the development branch on your GitHub repository. When a code
change is pushed to the branch, Cloud Build can test the code, build the container image, and store it in Container Registry. You
can then create a deployment pipeline that watches for new images in Container Registry and deploys them automatically on
the development cluster. To ensure that only code that has been properly tested and built is deployed in the development
environment, you should ensure that only the deployment tool has access to deploy new versions.
upvoted 
8 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option A is incorrect because installing a pre-commit hook on each developer's workstation does not ensure that the build
and test process is followed consistently for all code changes. It also does not provide a centralized way to track the
deployments in the development environment.
Option B is incorrect for the same reason. A post-commit hook on the remote repository does not provide a centralized way
to manage the build and deployment process for all code changes in the development environment.
Option D is incorrect because relying on Vulnerability Scanning alone is not sufficient to ensure that the code changes are
properly tested and built before being deployed in the development environment. A more comprehensive build and test
process, such as the one described in option C, is recommended to ensure the quality and reliability of the code being
deployed.
upvoted 
3 
times
madmike123
madmike123
 
1 year, 6 months ago
Selected Answer: 
C
"any code change that has been pushed to the remote develop branch on your GitHub repository should be built"...this
excludes A and B since both happen locally before a push.
Answer 'D' only performs security scanning (no test) and is not automatically deployed which is what was requested.
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
C
C. Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in
Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on the
development cluster. Ensure only the deployment tool has access to deploy new versions.
upvoted 
1 
times
Amit_arch
Amit_arch
 
1 year, 9 months ago
Selected Answer: 
D
C doesn't include test success. D should be the option.
upvoted 
1 
times
zellck
zellck
 
1 year, 9 months ago
Vulnerability Scanning is not relevant though for the question.
upvoted 
1 
times
zr79
zr79
 
1 year, 8 months ago
It does include tests succuss, the question is using CI/CD
upvoted 
1 
times
shayke
shayke
 
1 year, 11 months ago
Selected Answer: 
CC- the only answer
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #127
Your operations team has asked you to help diagnose a performance issue in a production application that runs on Compute
Engine. The application is dropping requests that reach it when under heavy load. The process list for affected instances shows
a single application process that is consuming all available CPU, and autoscaling has reached the upper limit of instances.
There is no abnormal load on any other related systems, including the database. You want to allow production traffic to be
served again as quickly as possible. Which action should you recommend? 
A. 
Change the autoscaling metric to agent.googleapis.com/memory/percent_used.
B. 
Restart the affected instances on a staggered schedule.
C. 
SSH to each instance and restart the application process.
D. 
Increase the maximum number of instances in the autoscaling group. 
Most Voted
Correct Answer:
 
D 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years ago
Answer should be D.
I doubt it is intended to provide wrong answer.
upvoted 
44 
times
poseidon24
poseidon24
 
2 years, 11 months ago
Agree. 
Cannot be A), since changing the metric used for autoscaling will not solve the issue, the CPU is already over utilized, hence the
unique "workaround" meanwhile the application causing the issue is fixed (connection leaks, infinite loops, etc.) is to allow
introducing new nodes/workers/VMs.
upvoted 
9 
times
victorlie
victorlie
 
2 years, 10 months ago
why almost all answers are wrong?
upvoted 
15 
times
 
1 year, 8 months ago
Community vote distribution
D (86%)
Other (14%)zr79
zr79
 
1 year, 8 months ago
to prevent us from memorizing the answers and hopefully, the site can not be shut down
upvoted 
10 
times
Action
Action
 
6 months, 2 weeks ago
Yes I always consider this to be the reason behind so many obviously wrong answers too... but who knows
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
2 years, 12 months ago
Answer is D
upvoted 
12 
times
AWS_Sam
AWS_Sam
 
Most Recent
 
6 months ago
The question is not asking for a permanent solution to the problem, it is asking what to do to have the production traffic to be
served again as quickly as possible. 
Therefore, the best answer is D.
upvoted 
4 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
D
Answer D is correct.
Prioritize the availability on production environment.
upvoted 
2 
times
[Removed]
[Removed]
 
10 months, 2 weeks ago
Selected Answer: 
D
it says "autoscaling has reached the upper limit of instances" and there are no abnormal errors... so the upper limit for autoscaling
has to be increased.
upvoted 
5 
times
gary_cooper
gary_cooper
 
11 months, 3 weeks ago
Selected Answer: 
D
Increase the maximum number of instances in the autoscaling group
upvoted 
1 
times
red_panda
red_panda
 
1 year ago
Selected Answer: 
D
Answer D is correct.
In order to prioritize the availability on production environment (as per question), first we need to increase the number of max
instances in the instance group, then, for sure we can investigate and restart application process.
Be careful, often the answer is in the question
upvoted 
1 
times
grejao
grejao
 
1 year, 3 months ago
I choose for A, but D is the best choice. 
The trick is: "process that is consuming all available CPU" and "autoscaling has reached the upper limit of instances"
If the process is consuming all available CPU, we need to reconfigure our metrics for best tresholds (Option A)
AND
if the autoscaling reached the upper limit of instances, so we need to increase this limit (Option D), 
BUT, after 
reached the upper limit of instances, it doesn't matter the tresholds, the process will consume all resources that have
available. So, option D is the best option.
upvoted 
9 
times
Sur_Nikki
Sur_Nikki
 
1 year, 1 month ago
Loved the way you made us travel through the roots of the question
upvoted 
2 
times
 
1 year, 3 months agoCGS22
CGS22
 
1 year, 3 months ago
Selected Answer: 
D
The application is dropping requests because the available CPU is exhausted. Autoscaling has reached the upper limit of
instances, so it cannot increase the number of instances to meet the demand. The best way to allow production traffic to be
served again is to increase the maximum number of instances in the autoscaling group.
This will allow autoscaling to increase the number of instances to meet the demand without exhausting the available CPU.
Restarting the affected instances or SSHing to each instance and restarting the application process will not solve the problem
because the root cause is that there are not enough instances to meet the demand.
upvoted 
6 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The correct answer is D: Increase the maximum number of instances in the autoscaling group.
If the application is dropping requests under heavy load and the process list for affected instances shows a single application
process consuming all available CPU, increasing the maximum number of instances in the autoscaling group may help to alleviate
the performance issue. By adding more instances to the group, you can distribute the load across multiple instances, which
should help to reduce the strain on any single instance. This will allow production traffic to be served again more quickly.
upvoted 
7 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option A is incorrect because changing the autoscaling metric to agent.googleapis.com/memory/percent_used will not address
the root cause of the performance issue. The issue is related to CPU utilization, not memory usage.
Option B is incorrect because restarting the affected instances on a staggered schedule will not address the root cause of the
performance issue. It may provide temporary relief, but the issue is likely to recur once the instances are under heavy load again.
Option C is incorrect because restarting the application process on each instance will not address the root cause of the
performance issue. It may provide temporary relief, but the issue is likely to recur once the instances are under heavy load again.
Increasing the maximum number of instances in the autoscaling group is a more effective solution in this case.
upvoted 
4 
times
SureshbabuK
SureshbabuK
 
1 year, 6 months ago
Selected Answer: 
B
Given the is not no abnormal load, autoscaling will is not required, restarting should kill the single application process consuming
excess CPU
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
sanait100
sanait100
 
1 year, 7 months ago
The keyword is "You want to allow production traffic to be served again as quickly as possible" so D should be the only answer so
as to resume production traffic and then you can do a root cause analysis and take further action depending upon the findings.
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
D
It all depends on how you want to troubleshoot the issue. Do you want to check the application before or after increasing the max
number of instances in the scaling group. I guess in real life people will ask for an increase in the max number of instances and if
the application process continues to consume all the CPU then they will probably stop/restart the app. 
D is the only sensible option.
A is not an option
B you could restart but you dont know if that will fix the issueB you could restart but you dont know if that will fix the issue
C SSH assumes unix vm's (?)....!
upvoted 
3 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
autoscaling has reached the upper limit of instances. There is no abnormal load on any other related systems, including the
database. This so junk question, only D seems viable option.
upvoted 
2 
times
Jay_Krish
Jay_Krish
 
1 year, 10 months ago
Selected Answer: 
D
I feel increasing the autoscale limit seems to be the logical answer
upvoted 
3 
times
ijazahmad722
ijazahmad722
 
1 year, 10 months ago
Selected Answer: 
D
D seems to be least wrong
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #128
You are implementing the infrastructure for a web service on Google Cloud. The web service needs to receive and store the
data from 500,000 requests per second. The data will be queried later in real time, based on exact matches of a known set of
attributes. There will be periods where the web service will not receive any requests. The business wants to keep costs low.
Which web service platform and database should you use for the application? 
A. 
Cloud Run and BigQuery
B. 
Cloud Run and Cloud Bigtable 
Most Voted
C. 
A Compute Engine autoscaling managed instance group and BigQuery
D. 
A Compute Engine autoscaling managed instance group and Cloud Bigtable
Correct Answer:
 
B 
Comments
Enzian
Enzian
 
Highly Voted
 
3 years, 6 months ago
Any correct answer must involve Cloud Bigtable over BigQuery since Bigtable is optimized for heavy write loads. That leaves B
and D. I would suggest B b/c it is lower cost ("The business wants to keep costs low")
upvoted 
80 
times
AmitRBS
AmitRBS
 
2 years, 7 months ago
B. Agree. Additionally data need to store now so use Bigtable as question is not for analysing or data Analytics 
etc
upvoted 
4 
times
zanfo
zanfo
 
2 years, 9 months ago
the correct is B
upvoted 
2 
times
pakilodi
pakilodi
 
3 years ago
Not only: occasionally there will be no requests. so Cloud Run will scale to zero
upvoted 
21 
times
Petya27
Petya27
 
1 year, 7 months ago
Community vote distribution
B (61%)
D (36%)
A
(3%)Petya27
Petya27
 
1 year, 7 months ago
Plus, we are talking about a predefined set of queries. For any predefined list of (simple) queries, we use Bigtable, and for
any (complex) queries that we do not know ahead of time, we use BigQuery.
upvoted 
3 
times
kshlgpt
kshlgpt
 
1 year ago
But cloud run can't support 50,000 request per second. Even cloud run 2nd gen supports 1000 requests per second. B is
eliminated.
upvoted 
1 
times
pancakes22
pancakes22
 
11 months, 2 weeks ago
That's incorrect. https://cloud.google.com/run/quotas
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
3 years, 6 months ago
B is correct answer.
upvoted 
16 
times
Armne96X
Armne96X
 
Most Recent
 
4 months, 2 weeks ago
Selected Answer: 
B
Cloud Run and Cloud Bigtable is the best choice because it meets all the requirements:
Cloud Run can scale automatically to handle 500,000 requests per second and scales to zero during periods of no requests,
reducing costs. Cloud Bigtable is designed for real-time queries with exact match attributes. 
Autoscaling Managed Instance Group can not scale to zero during periods of no requests*
upvoted 
1 
times
afsarkhan
afsarkhan
 
5 months, 3 weeks ago
Selected Answer: 
D
It's hard for Cloud Run to scale to accept 500k rps so choosing Option D
upvoted 
1 
times
yas_cloud
yas_cloud
 
10 months ago
Not sure why this is voted between B and D. It should be A. 
MIG wont support, that rules out C and D.
between BQ and BT, please see that "data will be queried later in real time, based on exact matches of a known set of
attributes". This is supported by BQ alone. So I would go with A.
upvoted 
1 
times
Tirthankar17
Tirthankar17
 
10 months, 2 weeks ago
Selected Answer: 
B
B is correct
upvoted 
2 
times
pancakes22
pancakes22
 
11 months, 2 weeks ago
Selected Answer: 
B
https://cloud.google.com/run/quotas
There is no direct limit for:
The size of container images you can deploy.
The number of concurrent requests served by a Cloud Run service.
upvoted 
4 
times
the1dv
the1dv
 
11 months, 3 weeks ago
Selected Answer: 
D
Cloud Run can handle this amount if there were like 500 instances which would cost a pretty ridiculous amount per minute, so
unfortunately there isnt enough information in this question around how long the gaps are without data to make a proper
decision.Autoscaling Managed Instance Groups can scale to zero and 500k per second would be relatively easily handled by a few
instances.
upvoted 
2 
times
convers39
convers39
 
11 months, 4 weeks ago
Selected Answer: 
B
50,000 rps
At first I thought Cloud Run could not handle this request rate and then chose D. After a little bit of research on the docs I
changed my mind to B. 
On each instance concurrency, it clearly says
> By default each Cloud Run instance can receive up to 80 requests at the same time; you can increase this to a maximum of
1000
https://cloud.google.com/run/docs/about-concurrency
The maximum number of auto-scaling instances by default is 100, which can be configured depending on the regional quota.
With the default max instances it can already handle 100 * 1000 = 100,000 requests concurrently, which should be able to
achieve the 50,000 rps requirement.
https://cloud.google.com/run/docs/about-instance-autoscaling
upvoted 
4 
times
afsarkhan
afsarkhan
 
5 months, 3 weeks ago
question says it's 500k and not 50k rps
upvoted 
1 
times
kshlgpt
kshlgpt
 
1 year ago
Selected Answer: 
D
Cloud Run can't support 50,000 requests per second.
Correct answer should be D.
upvoted 
1 
times
kshlgpt
kshlgpt
 
1 year ago
Cloud Run can't handle 50,000 requests per second
A & B is eliminated
upvoted 
1 
times
wly_al
wly_al
 
1 year ago
Not receive any request = Cloud Run
upvoted 
1 
times
tamer_m_Saleh
tamer_m_Saleh
 
1 year ago
Selected Answer: 
D
At first I through its B, but then I thought about the number of requests that will be over 1 minute, if we calculated it = 30
million request per minute, and based on cloud run pricing this will cost only for the number of requests: 24 USD. so cloud run
will cost the company 24 USD / min. which might be a very costly option. 
But in the cloud run pricing there is 2 modes: 
- CPU allocated when receiving requests: and there is a cost for CPU and requests
- CPU always allocated: and there is only a cost for the CPU and zero price for the number of requests. 
I think we need someone experiencing the billing of a cloud run under a heavy load like this :)
upvoted 
2 
times
Andoameda9
Andoameda9
 
1 year ago
Selected Answer: 
B
Apart from the reason that cloud run can scale to zero, another benefit in this scenario is the fact Cloud run will provide out of
the box revision maintenance for the web service.
upvoted 
1 
times
 
1 year, 1 month agoodacir
odacir
 
1 year, 1 month ago
Selected Answer: 
B
Compute:
Cloud Run vs. Compute Engine autoscaling managed instance group
Cloud Run wins because can scale down up to 0 instances -> in Spike workflows will be cheaper.
Storage:
BigQuery vs. Big Table.
500,000 requests per second it’s not suitable in BQ:
https://cloud.google.com/bigquery/quotas
“A user can make up to 100 API requests per second to an API method”
So answer most be B.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
MIGs cannot scale the VMs to 0 as per https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-
metrics#configure_utilization_target
So B is the answer.
upvoted 
1 
times
DinRush
DinRush
 
1 year, 2 months ago
Cloud Functions can also scale to 0. But I guess because it manageable scaling can be done faster on functions level
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #129
You are developing an application using different microservices that should remain internal to the cluster. You want to be able
to configure each microservice with a specific number of replicas. You also want to be able to address a specific microservice
from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You need to
implement this solution on Google Kubernetes Engine. What should you do? 
A. 
Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service
DNS name to address it from other microservices within the cluster. 
Most Voted
B. 
Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress
IP address to address the Deployment from other microservices within the cluster.
C. 
Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to
address the microservice from other microservices within the cluster.
D. 
Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address name
to address the Pod from other microservices within the cluster.
Correct Answer:
 
A 
Comments
MamthaSJ
MamthaSJ
 
Highly Voted
 
2 years, 12 months ago
Answer is A
upvoted 
29 
times
PeppaPig
PeppaPig
 
Highly Voted
 
2 years, 11 months ago
Answer is A 100%
B is incorrect. Ingress comes with a HTTP(S) LB with external IP hence is not needed for communications within the cluster
internally.
upvoted 
19 
times
AWS_Sam
AWS_Sam
 
Most Recent
 
6 months ago
Correct answer is A
upvoted 
1 
times
Community vote distribution
A (100%)heretolearnazure
heretolearnazure
 
10 months, 1 week ago
A is correct answer
upvoted 
1 
times
wooloo
wooloo
 
11 months, 1 week ago
each microservice with a specific number of replicas = Deployment
internal to the cluster = Service
upvoted 
8 
times
[Removed]
[Removed]
 
1 year, 5 months ago
Selected Answer: 
A
answer is A
upvoted 
2 
times
jay9114
jay9114
 
1 year, 6 months ago
Selected Answer: 
A
Benefit of using Service
Leveraging service allows for you to set up your environment with static IP addresses. So when your pods die and restart the IP
address associated with the deceased pod remains
for the new pod that replaces it (ephemeral). I think using "Service" is helpful if you are setting up your pods to be able to
communicate with specific pods in the cluster.
Benefit of using DNS for Service
Using DNS for your Service (static IP) you can look up Services and/or Pods by name instead of IP. Addressability by name
instead of IP is easier for me.
upvoted 
9 
times
jay9114
jay9114
 
1 year, 6 months ago
l
l
Benefit of using Service
Leveraging service allows for you to set up your environment with static IP addresses. So when your pods die and restart the IP
address associated with the deceased pod remains 
for the new pod that replaces it (ephemeral). I think using "Service" is helpful if you are setting up your pods to be able to
communicate with specific pods in the cluster. 
Benefit of using DNS for Service
Using DNS for your Service (static IP) you can look up Services and/or Pods by name instead of IP. Addressability by name
instead of IP is easier for me.
upvoted 
3 
times
Sreenivasa739
Sreenivasa739
 
1 year, 6 months ago
A is ok
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
A
A. Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS
name to address it from other microservices within the cluster.
upvoted 
1 
times
ACE_ASPIRE
ACE_ASPIRE
 
1 year, 10 months ago
I got this question in exam.
upvoted 
4 
times
 
1 year, 1 month agoSur_Nikki
Sur_Nikki
 
1 year, 1 month ago
I have received this sentence from u in every comment posted by u
upvoted 
2 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Selected Answer: 
A
ngress comes with a HTTP(S) LB with external IP hence is not needed for communications within the cluster
internally.Microservice as Deployment - used to create replicas as per this request
DNS name - used as an alias service name for External name which is user for internal requests
upvoted 
3 
times
backhand
backhand
 
1 year, 11 months ago
vote A
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years ago
A is right, There is no need of Ingress here because all service need to communicate internally...
upvoted 
1 
times
JoeyCASD
JoeyCASD
 
2 years, 1 month ago
Vote A
1. Based on the description "You want to be able to configure each microservice with a specific number of replicas.", It's a hint
to use either Deployment or StatefulSet based on the service type is stateless or stateful, since the option only has Deployment,
thus Option C and D is out.
2. Based on the description "You also want to be able to address a specific microservice from any other microservice in a
uniform way, regardless of the number of replicas the microservice scales to." the later part is the key point, which means the
traffic direct to each service is based on some certain rules, in K8S this means URL, which is Ingress with external HTTP LB.
upvoted 
8 
times
ss909098
ss909098
 
2 years, 4 months ago
Selected Answer: 
A
Of course it is A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #130
Your company has a networking team and a development team. The development team runs applications on Compute Engine
instances that contain sensitive data. The development team requires administrative permissions for Compute Engine. Your
company requires all network resources to be managed by the networking team. The development team does not want the
networking team to have access to the sensitive data on the instances. What should you do? 
A. 
1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a
second project with a standalone VPC and assign the Compute Admin role to the development team. 3. Use Cloud VPN to
join the two VPCs.
B. 
1. Create a project with a standalone Virtual Private Cloud (VPC), assign the Network Admin role to the networking team,
and assign the Compute Admin role to the development team.
C. 
1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second
project without a VPC, configure it as a Shared VPC service project, and assign the Compute Admin role to the
development team. 
Most Voted
D. 
1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a
second project with a standalone VPC and assign the Compute Admin role to the development team. 3. Use VPC Peering to
join the two VPCs.
Correct Answer:
 
C 
Comments
Nalo1
Nalo1
 
Highly Voted
 
3 years ago
Selected Answer: 
B
For the same project , same VPC, Network Admin role to the networking team, and Compute Admin role to the development team.
What is the need for another project?
upvoted 
55 
times
TonyKGH
TonyKGH
 
1 year, 6 months ago
For full separation of the teams you will need to use a shared VPC in this case. If you compare the two roles you will see that
Compute Admin includes the permissions of the Network Admin so with option B you don't separate the teams as Compute
Admin includes compute.network.* permissions (and others). https://cloud.google.com/iam/docs/understanding-roles
Community vote distribution
C (57%)
B (43%)Admin includes compute.network.* permissions (and others). https://cloud.google.com/iam/docs/understanding-roles
upvoted 
13 
times
leslie19671
leslie19671
 
1 year, 3 months ago
Complete separation was not required. However, the networking team shouldn't have access to the compute engine. For this,
no need a full separation. Any better idea?
upvoted 
2 
times
hogtrough
hogtrough
 
12 months ago
They're getting the Compute Admin permissions either way. The key words in the statement are actually "Create a second
project without a VPC, configure it as a Shared VPC service project." Since the VPC being used doesn't exist in their project,
they're unable to manage network changes.
upvoted 
4 
times
awsgcparch
awsgcparch
 
5 months, 1 week ago
love this explanation
upvoted 
1 
times
victory108
victory108
 
Highly Voted
 
3 years, 5 months ago
C. 1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second project
without a VPC, configure it as a Shared VPC service project, and assign the Compute Admin role to the development team.
upvoted 
30 
times
Nick89GR
Nick89GR
 
2 years, 8 months ago
I do not understand why do we need to have a shared VPC. With B the dev team will not be able to make any network change
and the Network team will not be able to do any change on the CE. I would say B is the correct answer although C does not
seem wrong
upvoted 
18 
times
medi01
medi01
 
1 year, 8 months ago
Because Compute Admin has compute.* permissions, which includes Network Admin's.
upvoted 
6 
times
sfsdeniso
sfsdeniso
 
2 years, 2 months ago
because dev team will have several projects - for dev, qa and prod per app they are developing so C is most scalable solution
upvoted 
6 
times
BeCalm
BeCalm
 
1 year, 9 months ago
How does 1 additional VPC solve what you're expressing -- that the dev team needs a dev and qa environment.
upvoted 
1 
times
pcamaster
pcamaster
 
Most Recent
 
3 months, 1 week ago
This is tricky. Both B & C could seem okay, but C is the right answer.
The compute.networkAdmin role gives broad permissions on the project, which also affects compute instances. For instance, it
gives "compute.instances.get" and "compute.instances.use" permissions. Even though this roles does not grant permissions to
start/stop/create/delete instances, it still gives broad permissions on compute instances.
This gets much clearer if we do the same analysis on the other role: compute.Admin.
This role gives permissions on "compute.*", which also includes "compute.networks.*". That is exactly what we don't want to
happen. If we spawn the VPC and the compute VMs in the same project, then compute admins will be able to mess around with
the VPC. 
That is why we need to separate networks and compute within 2 projects, unless creating custom roles, etc. Shared VPC are
aimed at that. Therefore, C is the right answer.
upvoted 
2 
times
Toothpick
Toothpick
 
5 months, 1 week ago
If it was compute instance admin instead of compute admin, then B would do it,
But since They specifically mention Compute Admin, C is the only option.But since They specifically mention Compute Admin, C is the only option.
But that being said, there's nothing stopping the apps team from creating a vpc in the new project since they have all the required
permissions for it, kind of an oversight
upvoted 
2 
times
yas_cloud
yas_cloud
 
10 months ago
While IAM roles can technically achieve some separation in option B, Shared VPC (option C) offers a more secure, well-defined,
and recommended approach for this scenario. It reduces the risk of accidental access and promotes a cleaner separation of
duties between the networking and development teams.
upvoted 
2 
times
Amrita2012
Amrita2012
 
10 months, 2 weeks ago
Selected Answer: 
C
Shared ~VPC provide a feature to have segregation of such roles.
upvoted 
2 
times
Tirthankar17
Tirthankar17
 
10 months, 3 weeks ago
Selected Answer: 
C
The project I am part of follows exactly the same architecture as Option C.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
11 months ago
Selected Answer: 
C
C is ok.
upvoted 
1 
times
spuyol
spuyol
 
11 months ago
No one is a valid answer (sorry for that) because all answers assign COMPUTE ADMIN to the developers.
If Your company requires all network resources to be managed by the networking team you must NOT assigne COMPUTE ADMIN
to the developers because you give them the power to managed the network. No matter the project they are assigned, they could
do things forbidden for the requeriment.
upvoted 
1 
times
Wiss7
Wiss7
 
1 year ago
Selected Answer: 
C
Not B because Compute Admin has networking roles.
upvoted 
2 
times
rsvd
rsvd
 
1 year, 2 months ago
Selected Answer: 
C
https://cloud.google.com/vpc/docs/shared-vpc
When you use Shared VPC, you designate a project as a host project and attach one or more other service projects to it. The VPC
networks in the host project are called Shared VPC networks. Eligible resources from service projects can use subnets in the
Shared VPC network.
Shared VPC lets organization administrators delegate administrative responsibilities, such as creating and managing instances, to
Service Project Admins while maintaining centralized control over network resources like subnets, routes, and firewalls.
upvoted 
5 
times
arpana_naa
arpana_naa
 
1 year, 2 months ago
Selected Answer: 
C
Two reasons for me to select C:
1. Compute admin has network admin roles included
2. If Dev team adds more projects for testing/staging later, they can be centrally handled by the host project and Google also
recommends separation of duties
upvoted 
4 
times
AdityaGupta
AdityaGupta
 
1 year, 2 months ago
Selected Answer: 
C
Option B suggests creating a single project with a standalone VPC, and assigning both the Network Admin and Compute AdminOption B suggests creating a single project with a standalone VPC, and assigning both the Network Admin and Compute Admin
roles to the respective teams. However, this solution does not enforce the required separation of duties between the networking
and development teams.
Option C suggests using a Shared VPC. A Shared VPC allows for separation of duties between teams while sharing network
resources. The networking team can manage the Shared VPC, and the development team can create Compute Engine instances
in the Shared VPC without the networking team having access to the sensitive data on the instances. The development team can
be assigned the Compute Admin role for the Shared VPC service project, and the networking team can be assigned the Network
Admin role for the Shared VPC host project.
upvoted 
3 
times
piiizu
piiizu
 
1 year, 3 months ago
Can this be handled with IAM roles? Yes. Why go through extra effort, in the event you want to add a feature or couple other
services, would you still continue tweaking?
upvoted 
1 
times
PKookNN
PKookNN
 
1 year, 4 months ago
Selected Answer: 
C
to prevent dev team from having network admin (same project will mean dev team with compute admin will also have network
permission). 
That's my take on this one.
upvoted 
2 
times
capt2101akash
capt2101akash
 
1 year, 5 months ago
Selected Answer: 
C
It talks about managing all network resources in a company. Google always recommends having a shared VPC to maintain
network resources in an organization. The separation of roles adds to the favour of having a shared vpc.
upvoted 
1 
times
riyaztmd
riyaztmd
 
1 year, 5 months ago
Selected Answer: 
B
C is not correct as it doesn't make sense to have 2 vpcs for managing roles
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #131
Your company wants you to build a highly reliable web application with a few public APIs as the backend. You don't expect a lot
of user traffic, but traffic could spike occasionally. You want to leverage Cloud Load Balancing, and the solution must be cost-
effective for users. What should you do? 
A. 
Store static content such as HTML and images in Cloud CDN. Host the APIs on App Engine and store the user data in
Cloud SQL.
B. 
Store static content such as HTML and images in a Cloud Storage bucket. Host the APIs on a zonal Google Kubernetes
Engine cluster with worker nodes in multiple zones, and save the user data in Cloud Spanner.
C. 
Store static content such as HTML and images in Cloud CDN. Use Cloud Run to host the APIs and save the user data in
Cloud SQL.
D. 
Store static content such as HTML and images in a Cloud Storage bucket. Use Cloud Functions to host the APIs and save
the user data in Firestore. 
Most Voted
Correct Answer:
 
D 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years, 6 months ago
Answer should be D,
https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless#gcloud:-cloud-functions
https://cloud.google.com/blog/products/networking/better-load-balancing-for-app-engine-cloud-run-and-functions
upvoted 
54 
times
dguillenca
dguillenca
 
3 years, 6 months ago
D not use CDN, is D correct answer?
upvoted 
1 
times
PeppaPig
PeppaPig
 
3 years, 5 months ago
CDN is not needed here. You don't need to service users globally thus latency and locality isn't critical
upvoted 
6 
times
Community vote distribution
D (64%)
B (16%)
C (15%)
Other
(5%)letonphat
letonphat
 
3 years, 2 months ago
IMHO CDN is not storage solution to store static html or image
upvoted 
5 
times
Warlock7
Warlock7
 
2 years, 10 months ago
You should look at this
https://cloud.google.com/storage/docs/caching
upvoted 
1 
times
diluviouniv
diluviouniv
 
3 years, 5 months ago
Spanner is expensive
upvoted 
10 
times
BrunoTostes
BrunoTostes
 
3 years, 2 months ago
but is it Cloud Functions used for hosting APIs?
upvoted 
5 
times
turbo8p
turbo8p
 
2 years, 1 month ago
Can be hosted. It's cost effective since you get charged on per call basis. If no traffic then no cost will be charged.
upvoted 
4 
times
parthkulkarni998
parthkulkarni998
 
1 year ago
If CF you want to use for hosting APIs why not option C. to use CloudRun? It too autoscales to 0 instances for no traffic..
upvoted 
1 
times
huuthanhdlv
huuthanhdlv
 
7 months, 1 week ago
CE using cloud CDN to host static contents which is incorrect.
upvoted 
1 
times
mikesp
mikesp
 
3 years, 2 months ago
IMHO, i agree with you. Furthermore:
Cloud Storage buckets are a good choice for static web content. Cloud storage buckets behave like a CDN Network:
https://cloud.google.com/storage/docs/caching
So it is lower cost than CDN.
upvoted 
8 
times
XDevX
XDevX
 
Highly Voted
 
3 years, 6 months ago
IMHO it is d), not b).
Reason is that you don't need Cloud Spanner just to store user data - FireStore is the better solution. Additionally, I see no
indications concerning the requirement to use GKE... Please correct me when I am wrong.
upvoted 
17 
times
Andrea67
Andrea67
 
3 years ago
agree with u
upvoted 
3 
times
tosinogunfile
tosinogunfile
 
1 year ago
That's correct
upvoted 
1 
times
tosinogunfile
tosinogunfile
 
1 year ago
That's correct
upvoted 
1 
times
VegasDegenerate
VegasDegenerate
 
Most Recent
 
2 days, 22 hours ago
Selected Answer: 
DB is absolutely diabolical for "Cost effective"
upvoted 
1 
times
NitinV
NitinV
 
5 months, 3 weeks ago
As per ChatGPT , answer is C
upvoted 
1 
times
mshafa
mshafa
 
5 months, 3 weeks ago
xcvxcv
upvoted 
1 
times
Diwz
Diwz
 
9 months, 1 week ago
Answer is D. 
Solution should be cost effective and highly reliable. Cloud storage and firestore is suitable .
upvoted 
1 
times
krokskan
krokskan
 
10 months, 1 week ago
Selected Answer: 
D
absolutely D because kubernetes is not cost effective. cloud functions are a better shot
upvoted 
1 
times
convers39
convers39
 
11 months, 4 weeks ago
Selected Answer: 
D
CDN is for caching. For static website hosting a storage bucket is a good choice, thus A and C are eliminated.
B, Cloud Spanner, my nightmare. I accidentally created an empty cloud spanner and it burned like 30-40 USD per day! I got a huge
amount of billing from GCP that month! No way to be cost-effective.
D, Cloud Functions are good for simple API services and have no cost if not in use. SQL or NoSQL for user data is not a strong
factor here, either should be fine.
upvoted 
4 
times
AWS_Sam
AWS_Sam
 
1 year ago
The correct answer with the lowest cost is D
upvoted 
1 
times
tamer_m_Saleh
tamer_m_Saleh
 
1 year ago
in B) it says zonal GKE with worker nodes in multiple zones
how zonal and in multiple zones?! 
And that 100% eliminate B
upvoted 
2 
times
Gino17m
Gino17m
 
8 months, 2 weeks ago
"zonal" is for conrol plane, workers can be in multiple zones in multi-zone zonal cluster - the wording is just strange, see:
https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster
Nevertheless B is still incorrect
upvoted 
1 
times
Jconnor
Jconnor
 
1 year, 1 month ago
Actually it is B. Highly reliable, using Load Balancer, Spikes and several API. D is less reliable, as functions will take long to cold
start and will have a time out and not using LB. The only draw back is cost.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
D
User data in a relational database is not a good option, A B C are ruled out. 
Left with D - User data in Firestore.
upvoted 
1 
timesMiguelMiguel
MiguelMiguel
 
1 year, 2 months ago
D is the correct option since CDN is not for storage of html content, so you have only the option B and D. And the option B required
that you have a containerezed application in other way, you can't use k8s. So the option is D.
upvoted 
1 
times
Arun_m_123
Arun_m_123
 
1 year, 2 months ago
Selected Answer: 
D
D is the correct answer
1. Cloud CDN is not meant for storing contents. It can only cache but not act as the source-of-truth and moreover the question
tells that the user traffic is less. Cloud CDN performs better when the user traffic is high
2. The spikes are very rare and the app gets low traffic most of the time. That said, putting a cluster is an expensive option.
choosing a serverless supported app-platform (Cloud Run) and serverless supported DB (Firestore) makes perfect choice here to
handle cost during low traffic and also handle any incoming sudden spikes
upvoted 
3 
times
JPA210
JPA210
 
1 year, 2 months ago
Cloud Spanner is expensive, so it cannot be that option.
upvoted 
2 
times
ManishKS
ManishKS
 
1 year, 5 months ago
Correct Answer Is D
upvoted 
1 
times
gary_cooper
gary_cooper
 
1 year, 5 months ago
Selected Answer: 
D
Store static content such as HTML and images in a Cloud Storage bucket. Use Cloud Functions to host the APIs and save the
user data in Firestore
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #132
Your company sends all Google Cloud logs to Cloud Logging. Your security team wants to monitor the logs. You want to ensure
that the security team can react quickly if an anomaly such as an unwanted firewall change or server breach is detected. You
want to follow Google-recommended practices. What should you do? 
A. 
Schedule a cron job with Cloud Scheduler. The scheduled job queries the logs every minute for the relevant events.
B. 
Export logs to BigQuery, and trigger a query in BigQuery to process the log data for the relevant events.
C. 
Export logs to a Pub/Sub topic, and trigger Cloud Function with the relevant log events. 
Most Voted
D. 
Export logs to a Cloud Storage bucket, and trigger Cloud Run with the relevant log events.
Correct Answer:
 
C 
Comments
kopper2019
kopper2019
 
Highly Voted
 
2 years, 12 months ago
I think C using BigQuery can get expensive if you have somehow check the logs for anomalies
https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event
check there is a diagram
upvoted 
48 
times
poseidon24
poseidon24
 
2 years, 11 months ago
Thanks for pointing out the reference. C is the correct one.
Nevertheless the question and all the answers are missleading, even C) sounds like sending all the logs to pub/sub, it should
mention about "filtering" prior to send to Pub/Sub.
upvoted 
10 
times
AzureDP900
AzureDP900
 
1 year, 9 months ago
C is absolutely make sense, Thank you for sharing the link.
upvoted 
3 
times
amxexam
amxexam
 
2 years, 9 months ago
Community vote distribution
C (100%)It may get expensive but GCP recommended way , they not asking for self alternative for cheap solution.
upvoted 
4 
times
Urban_Life
Urban_Life
 
2 years, 6 months ago
cloud function is also key point
upvoted 
2 
times
manmohan15
manmohan15
 
Highly Voted
 
2 years, 12 months ago
c) is correct as quickly action is required for unwanted event/access should be actioned.
upvoted 
9 
times
thewalker
thewalker
 
Most Recent
 
7 months, 3 weeks ago
Selected Answer: 
C
Option is C
The clean and neat way to architect the solution is C.
upvoted 
2 
times
Bhargav2000
Bhargav2000
 
1 year ago
One of your key employees received a job offer from another cloud company. S/he left the Organization without giving notice.
His Google Account was kept active for 3 weeks. How can you find out if the employee accessed any sensitive data after s/he
left?
upvoted 
1 
times
Romio2023
Romio2023
 
5 months, 3 weeks ago
use user activity log
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C Is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
minmin2020
minmin2020
 
1 year, 8 months ago
Selected Answer: 
C
C - check https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event
upvoted 
2 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Selected Answer: 
C
https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years ago
Pub/Sub & Cloud Function serves the purpose , I am choosing C as right !
upvoted 
1 
times
ss909098
ss909098
 
2 years, 4 months agoss909098
ss909098
 
2 years, 4 months ago
Selected Answer: 
C
C is the correct one
upvoted 
1 
times
azureaspirant
azureaspirant
 
2 years, 4 months ago
2/15/21 exam
upvoted 
2 
times
ahsangh
ahsangh
 
2 years, 4 months ago
21 or 22 ?
upvoted 
2 
times
[Removed]
[Removed]
 
2 years, 4 months ago
Selected Answer: 
C
I got similar question on my exam. Answered C.
upvoted 
3 
times
DoVale
DoVale
 
2 years, 5 months ago
B is correct because exported logs can be analyzed in Bigquery to identity anomalies by executing scheduled queries on the
exported data.
upvoted 
2 
times
DoVale
DoVale
 
2 years, 5 months ago
B is correct because exported logs can be analyzed in Bigquery to identity anomalies by executing scheduled queries on the
exported data.
upvoted 
2 
times
ehgm
ehgm
 
2 years, 6 months ago
The logs already on Cloud Logging, we can just create a metric and an alert for it. No need any development.
upvoted 
4 
times
haroldbenites
haroldbenites
 
2 years, 6 months ago
Go for C
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #133
You have deployed several instances on Compute Engine. As a security requirement, instances cannot have a public IP
address. There is no VPN connection between Google Cloud and your office, and you need to connect via SSH into a specific
machine without violating the security requirements. What should you do? 
A. 
Configure Cloud NAT on the subnet where the instance is hosted. Create an SSH connection to the Cloud NAT IP address
to reach the instance.
B. 
Add all instances to an unmanaged instance group. Configure TCP Proxy Load Balancing with the instance group as a
backend. Connect to the instance using the TCP Proxy IP.
C. 
Configure Identity-Aware Proxy (IAP) for the instance and ensure that you have the role of IAP-secured Tunnel User. Use
the gcloud command line tool to ssh into the instance. 
Most Voted
D. 
Create a bastion host in the network to SSH into the bastion host from your office location. From the bastion host, SSH
into the desired instance.
Correct Answer:
 
C 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years, 6 months ago
Answer is C.
https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh
upvoted 
60 
times
ShadowLord
ShadowLord
 
2 years, 4 months ago
https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh
"IAP TCP forwarding allows you to establish an encrypted tunnel over which you can forward SSH, RDP, and other traffic to VM
instances"
But Options C says ,,,,, SSH from IAP .. which is not true.
upvoted 
2 
times
meh009
meh009
 
3 years, 2 months ago
100% Agree. I use IAP all the time which allows me to reduce exposure to VM from public internet. Ans is C
Community vote distribution
C (73%)
D (27%)100% Agree. I use IAP all the time which allows me to reduce exposure to VM from public internet. Ans is C
upvoted 
11 
times
mikesp
mikesp
 
3 years, 2 months ago
Agee too. Bastion host violates security requirements due to it has public IP :)
upvoted 
12 
times
ank82
ank82
 
Highly Voted
 
3 years, 5 months ago
And D seems correct, bastion host is specifically used for this purpose, using option C user can connect through cloud only.
By using a bastion host, you can connect to an VM that does not have an external IP address. This approach allows you to
connect to a development environment or manage the database instance for your external application, for example, without
configuring additional firewall rules.
https://cloud.google.com/solutions/connecting-securely
upvoted 
16 
times
eascen
eascen
 
3 years, 2 months ago
Except the policy is no machines can have public IP's, how do you connect to the bastion?
upvoted 
6 
times
elainexs
elainexs
 
2 years, 7 months ago
It's never mentioned that there's no public IP in all GCP services, it just said instances no public IP, which is very normal. that's
why bastion inward, and NAT outward.
upvoted 
2 
times
learner311
learner311
 
2 years, 8 months ago
C. no network connection between office and cloud. Can't use bastion. What C fails to say or specify is if you are either using
cloud shell gcloud or you downloaded the sdk on local. Dumb question without clarification. Assuming silly test writers conflate
gcloud always being used in cloud shell. So you are in cloud shell, you have internal access since the shell resides inside the
VPC network with all perms.
upvoted 
2 
times
orest
orest
 
2 years, 5 months ago
" There is no VPN connection between Google Cloud and your office". If there would be no network connection betweek office
and the cloud you could not use any of google services
upvoted 
1 
times
ShadowLord
ShadowLord
 
2 years, 4 months ago
But you can always SSH to bastion host from internet .. as ports are open usually
https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh
"IAP TCP forwarding allows you to establish an encrypted tunnel over which you can forward SSH, RDP, and other traffic to
VM instances". it is Traffic forwarding ... 
But Options C says ,,,,, SSH from IAP .. which is not true.
upvoted 
1 
times
turbo8p
turbo8p
 
2 years, 1 month ago
If you're looking for word precision then:
Option C says "Use the gcloud command line tool to ssh into the instance. Most Voted"
So I think C is still correct.
upvoted 
1 
times
[Removed]
[Removed]
 
1 year, 4 months ago
Question states: "As a security requirement, instances cannot have a public IP address"
If you install a Public IP on the GCE Bastion host you violate the security requirement.
If you install a Private IP on the GCE bastion host you need a private route (e.g. VPN) or NAT to it.
The question scenario seems specific to point to the IAP SSH tunnel feature.
upvoted 
1 
times3fd692e
3fd692e
 
Most Recent
 
3 months ago
Selected Answer: 
C
C is the answer. D also works as a solution but it's not using GCP native features when available. Since it's a test about GCP,
always go with the answer that uses GCP services.
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 1 week ago
According to https://cloud.google.com/solutions/connecting-securely : "Using SSH with IAP's TCP forwarding feature wraps an
SSH connection inside HTTPS. IAP's TCP forwarding feature then sends it to the remote VM."
So is it ssh or http connection ? Very tricky question.....
upvoted 
1 
times
a53fd2c
a53fd2c
 
8 months, 4 weeks ago
https://cloud.google.com/iap/docs/tutorial-gce
Answer is D. 
For C to be corrected it should mention the IAP-secured Web App User role. No the one listed on the question which
is wrong
upvoted 
1 
times
a53fd2c
a53fd2c
 
8 months, 4 weeks ago
Answer is D. 
Wrong user mentioned on C
Step 6: Test IAP
To test that IAP is working correctly, follow the steps below:
In your web browser, navigate to your domain.
If you see "Unauthorized request", try again in a few minutes.
When you see a Google sign-in screen, sign in using the Google Account you gave access to in the previous step.
You should see a message like "Hi,
user@example.com
. I am my-managed-instance-group-29z6."
Try refreshing the page. Your browser should show the names of the 3 machines in your managed instance group. This is the load
balancer distributing traffic across the VMs in the group.
upvoted 
1 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
you are wrong, study first IAP. It is C.
upvoted 
1 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
C
C is correct: IAP offers a secure and controlled way to access internal instances without assigning them public IP addresses. It
uses IAM permissions to restrict access only to authorized users and provides a temporary connection tunnel for SSH access
using the gcloud command-line tool.
upvoted 
1 
times
hzaoui
hzaoui
 
11 months, 2 weeks ago
Selected Answer: 
C
C is the best answer
upvoted 
1 
times
convers39
convers39
 
11 months, 4 weeks ago
Selected Answer: 
C
For D, 
Create a bastion host in the network to SSH into the bastion host from your office location. From the bastion host, SSH into
the desired instance.
How could you SSH into the bastion host? All VMs do not have public IP
upvoted 
1 
times
Wiss7
Wiss7
 
1 year ago
Selected Answer: 
CSelected Answer: 
C
C is ok
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
C
As per the documentation, https://cloud.google.com/iap/docs/tcp-forwarding-overview/ The option is C
upvoted 
1 
times
smlabonia
smlabonia
 
1 year, 3 months ago
Selected Answer: 
C
C is the correct answer in this case.
Question quote "There is no VPN connection between Google Cloud and your office"
Answer D "...from your office location..."
The only way to achieve this with Bastion Host is giving it a Public IP. At least in this case.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
I think it's Bastion host. 
In my org (large bluechip) all connections are via bastion host to provide a single point of audit and control.
upvoted 
2 
times
kapa900
kapa900
 
1 year, 6 months ago
Instances cannot have public IP bastian host will still need IP
upvoted 
3 
times
Atanu
Atanu
 
1 year, 7 months ago
Selected Answer: 
D
Bastion host service is specifically designed for this purpose. No need to do over-engineering too much here.
upvoted 
2 
times
desertlotus1211
desertlotus1211
 
1 month ago
Answer C is perferred
upvoted 
1 
times
mraza
mraza
 
1 year, 8 months ago
Selected Answer: 
D
As per ChatGPT:
Since instances cannot have a public IP address, the best option is to use a bastion host to access the instance securely.
Therefore, option D is the correct choice.
Here's what you would do:
Create a new instance that will serve as a bastion host. Assign it a static IP address.
Configure the firewall rules for the bastion host to allow incoming SSH traffic from your office location.
Connect to the bastion host via SSH from your office location.
Once connected to the bastion host, use SSH to connect to the desired instance on the same network.
This way, you can securely access the instance without violating the security requirements.
upvoted 
1 
times
jlambdan
jlambdan
 
1 year, 9 months ago
Selected Answer: 
C
https://cloud.google.com/iap/docs/using-tcp-forwarding
upvoted 
1 
times
VarunGo
VarunGo
 
1 year, 10 months ago
Selected Answer: 
C
As per chatGPT answer is C.As per chatGPT answer is C.
Identity-Aware Proxy (IAP) is a Google Cloud service that provides secure access to VM instances without exposing them to the
internet. It allows you to establish a secure SSH connection to a VM instance via the Google Cloud Console or the gcloud
command-line tool, using OAuth 2.0-based authentication and authorization. With IAP, you can set up secure, encrypted tunnels to
your VM instances, without the need for a VPN or an external bastion host.
By configuring IAP for the instance and ensuring that you have the IAP-secured Tunnel User role, you can securely access the
instance using the gcloud command-line tool to SSH into the instance, without violating the security requirements.
upvoted 
5 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #134
Your company is using Google Cloud. You have two folders under the Organization: Finance and Shopping. The members of the
development team are in a 
Google Group. The development team group has been assigned the Project Owner role on the Organization. You want to
prevent the development team from creating resources in projects in the Finance folder. What should you do? 
A. 
Assign the development team group the Project Viewer role on the Finance folder, and assign the development team
group the Project Owner role on the Shopping folder.
B. 
Assign the development team group only the Project Viewer role on the Finance folder.
C. 
Assign the development team group the Project Owner role on the Shopping folder, and remove the development team
group Project Owner role from the Organization. 
Most Voted
D. 
Assign the development team group only the Project Owner role on the Shopping folder.
Correct Answer:
 
C 
Comments
kopper2019
kopper2019
 
Highly Voted
 
2 years, 6 months ago
It is C
upvoted 
32 
times
milan74
milan74
 
Highly Voted
 
2 years, 5 months ago
Answer C is correct.
Answer A and B are both overridden by the less-restrictive permission on Organization level.
Answer D permission was already there on Organization level, and does not remove the project owner permission on the other
folder
upvoted 
15 
times
thewalker
thewalker
 
Most Recent
 
1 month, 2 weeks ago
Selected Answer: 
C
Out of the four options given + principle of least privileges - C is the best option.
upvoted 
1 
times
 
3 months, 2 weeks ago
Community vote distribution
C (91%)
Other (9%)rakp
rakp
 
3 months, 2 weeks ago
Selected Answer: 
C
C is correct. It is required to remove the Project Owner role from the Organization.
upvoted 
1 
times
ManishKS
ManishKS
 
5 months ago
Option C suggests assigning the development team group the Project Owner role on the Shopping folder and removing the
development team group's Project Owner role from the Organization. However, this approach would not achieve the desired
outcome of preventing the development team from creating resources in projects within the Finance folder.
Correct Answer is A
upvoted 
1 
times
AugustoKras011111
AugustoKras011111
 
10 months, 3 weeks ago
C seems right, you have to remove the role from organization level
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 2 months ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
alexandercamachop
alexandercamachop
 
1 year, 3 months ago
Selected Answer: 
C
It says "The development team has received Project Owner Role in the Organization level"
This means the only answer viable to not be able to access Finance folder resources is to remove that initial role that they were
assigned and assign the new one, hence only C mentions removing the role. 
Remember IAM roles are hereditary downwards, therefor even if we assign Project Viewer to that Group, they still keep their
Project Owner in the Project, unless removed.
upvoted 
10 
times
theBestStudent
theBestStudent
 
1 month, 3 weeks ago
excellent explanation
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 3 months ago
C is perfect for this scenario.
upvoted 
2 
times
backhand
backhand
 
1 year, 4 months ago
selected answer C
basic concept
upvoted 
2 
times
szefco
szefco
 
1 year, 5 months ago
Selected Answer: 
C
C is correct
upvoted 
1 
timesupvoted 
1 
times
vs_s
vs_s
 
1 year, 6 months ago
Selected Answer: 
C
C is right
upvoted 
1 
times
dragos_dragos62000
dragos_dragos62000
 
1 year, 7 months ago
Selected Answer: 
C
Answer is C, adding Project Viewer on folder doesn't overwrite the Project Owner at org level, so you have to delete it.
upvoted 
2 
times
amxexam
amxexam
 
1 year, 7 months ago
Selected Answer: 
A
A is better than B. And Correct.
upvoted 
1 
times
amxexam
amxexam
 
1 year, 7 months ago
Selected Answer: 
B
It is obviously B
upvoted 
1 
times
azureaspirant
azureaspirant
 
1 year, 10 months ago
2/15/21 exam
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 6 months ago
what do u mean by the date, It doesn't help anyone
upvoted 
1 
times
AWS2GCP
AWS2GCP
 
1 year, 10 months ago
give some context on your 2/15/21 post in every discussion
upvoted 
4 
times
saggianki
saggianki
 
1 year, 8 months ago
Seriously :D
upvoted 
1 
times
satamex
satamex
 
1 year, 5 months ago
I guess he got this question on that day exam.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #135
You are developing your microservices application on Google Kubernetes Engine. During testing, you want to validate the
behavior of your application in case a specific microservice should suddenly crash. What should you do? 
A. 
Add a taint to one of the nodes of the Kubernetes cluster. For the specific microservice, configure a pod anti-affinity
label that has the name of the tainted node as a value.
B. 
Use Istio's fault injection on the particular microservice whose faulty behavior you want to simulate. 
Most Voted
C. 
Destroy one of the nodes of the Kubernetes cluster to observe the behavior.
D. 
Configure Istio's traffic management features to steer the traffic away from a crashing microservice.
Correct Answer:
 
B 
Comments
TotoroChina
TotoroChina
 
Highly Voted
 
3 years ago
Answer is B.
application crash, not node.
upvoted 
45 
times
XDevX
XDevX
 
3 years ago
I see it the same way - it is b)
upvoted 
5 
times
XDevX
XDevX
 
Highly Voted
 
3 years ago
I think that c) is not the correct answer.
I am not a GKE or Kubernetes expert, so maybe I am wrong.
My understanding is, that in Kubernetes a microservice can run on pods on different nodes and one node can contain pods
running differend microservices - so to kill one node will not kill a microservice but several pods running on that node. Please
correct me if I am wrong.
upvoted 
21 
times
de1001c
de1001c
 
Most Recent
 
3 weeks, 6 days ago
Selected Answer: 
B
Community vote distribution
B (88%)
C (12%)Between B and C.
C is the wrong answer as the microservice may very well be running in one of the other nodes that were not destroyed.
upvoted 
2 
times
hzaoui
hzaoui
 
5 months, 1 week ago
Selected Answer: 
B
Fault injection is a technique used in chaos engineering to deliberately introduce errors into a system to test its resilience and
observe its behavior under failure conditions. Istio is a service mesh that can manage the traffic between microservices. It
includes fault injection capabilities that enable you to simulate failures such as delays or crashed services without actually
stopping the service or damaging the environment. This allows you to validate how the rest of your application reacts to the
failure of a specific microservice.
upvoted 
4 
times
ammonia_free
ammonia_free
 
5 months, 3 weeks ago
Selected Answer: 
B
Testing Objective: The choice between options B and C depends on the testing objectives. If the goal is to understand the
behavior of the system when a specific microservice fails (this is how the question is formulated), then a targeted approach
(Option B) is more appropriate. If the goal is to understand the broader resiliency of the system to node failures, then Option C
would be more relevant.
Microservice Focus: Given the question's focus on a specific microservice, Istio's fault injection (Option B) provides a more direct
and controlled way to simulate the failure of that microservice and observe the system's response, aligning better with the
scenario described.
upvoted 
1 
times
spuyol
spuyol
 
6 months ago
Selected Answer: B
Kill one randomly selected k8s cluster node doesn't guarantee you kill pods of the microservice. The node could or colud not
have microservices of that app running on it.
upvoted 
1 
times
JoeJoe
JoeJoe
 
7 months, 2 weeks ago
IMHO the right answer is C. This link clearly explains why (remember that we are in a testing environment, so what's the
problem in generating a cras of an entire node of the cluster?):
https://www.linkedin.com/pulse/google-cloud-architect-case-study-5-biswa-prakash-nayak/
upvoted 
1 
times
Gino17m
Gino17m
 
2 months, 1 week ago
The problem is that according to the question: "During testing, you want to validate the behavior of your application in case a
specific micsoservice should suddenly crash"....Even on testing environment specific microservice can run on more then one
node and not on the one you destroyed.....moreover you should not affect other parts o the application (other microservices
running on the destroyed node)
upvoted 
1 
times
MiguelMiguel
MiguelMiguel
 
8 months, 1 week ago
B is the correct option, if you deleted a node you will deleted more than the microservices that you wanted to eliminated.
upvoted 
1 
times
someone2011
someone2011
 
9 months, 1 week ago
https://istiobyexample.dev/fault-injection/
In a Kubernetes environment, you can approach chaos testing at different layers - for instance, by deleting pods at random, or
shutting off entire nodes.
But failures also happen at the application layer. Infinite loops, broken client libraries - application code can fail in an infinite
number of ways! This is where Istio fault injection comes in. You can use Istio VirtualServices to do chaos testing at the
application layer, by injecting timeouts or HTTP errors into your services, without actually updating your app code. Let’s see
how.
So both C and B work.
upvoted 
2 
times
someone2011
someone2011
 
9 months, 1 week ago
Probably C is the best to simulate the entire microservice crashing.Probably C is the best to simulate the entire microservice crashing.
upvoted 
2 
times
rakp
rakp
 
9 months, 2 weeks ago
B seems correct. Istio's fault injection is right way to introduce fault.
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
I think perhaps C is correct due to the very specific test scenario. 
Chaos testing would be great for generally resilience test but it
wants to test the behaviour of the app when the microservice crashed. 
Shutting down the microservice seems the simplest way
to test this scenario.
upvoted 
1 
times
JoeJoe
JoeJoe
 
7 months, 2 weeks ago
I agree, mostly because we are in a testing environment, so a node crash is not a great disaster even if there are other
microservices running in thst node's pods.
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 4 months ago
Isn’t Istio used in the context of Anthos vs. GKE?
upvoted 
1 
times
roaming_panda
roaming_panda
 
1 year, 6 months ago
Selected Answer: 
B
Istio fault injection 
:https://istiobyexample.dev/fault-injection/
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
B
B is the correct answer
upvoted 
1 
times
ale_brd_111
ale_brd_111
 
1 year, 7 months ago
Selected Answer: 
B
Istio is replicated and replaced by Anthos service mesh, please update the answer.
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
destroying a node is not really testing microservices failure
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #136
Your company is developing a new application that will allow globally distributed users to upload pictures and share them with
other selected users. The application will support millions of concurrent users. You want to allow developers to focus on just
building code without having to create and maintain the underlying infrastructure. Which service should you use to deploy the
application? 
A. 
App Engine 
Most Voted
B. 
Cloud Endpoints
C. 
Compute Engine
D. 
Google Kubernetes Engine
Correct Answer:
 
A 
Comments
kopper2019
kopper2019
 
Highly Voted
 
2 years, 6 months ago
A, App Engine, you just want you people dedicated to the App
upvoted 
27 
times
Rzla
Rzla
 
Highly Voted
 
2 years, 3 months ago
AppEngine is regional. 
Millions of distributed global users = GkE.
upvoted 
13 
times
joe2211
joe2211
 
2 years, 1 month ago
But "focus on just building code without having to create and maintain the underlying infrastructure" => A right
upvoted 
7 
times
thewalker
thewalker
 
Most Recent
 
1 month ago
Selected Answer: 
D
D
GKE - Auto mode is free from managing the underlying infra.
upvoted 
1 
times
Community vote distribution
A (92%)
D (8%)rakp
rakp
 
3 months, 2 weeks ago
Selected Answer: 
A
Compute Engine & Google Kubernetes Engine are not pure PaaS, so they will require some management and maintenance. So C
& D are eliminated. Cloud Endpoints is also not best fit for the given requirement, so B is also eliminated.
App Engine is fully managed platform.
upvoted 
6 
times
Aninina
Aninina
 
1 year, 1 month ago
Selected Answer: 
A
App Engine would work
upvoted 
1 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 6 months ago
App Engine is right choice, Developer can focus on developing code rather than worry about infrastructure. A is right
upvoted 
3 
times
Jabree12
Jabree12
 
1 year, 7 months ago
The users are global but doesn't mean that app can't be regional. A is the correct answer it seems.
upvoted 
1 
times
learner311
learner311
 
1 year, 8 months ago
Another dumb question. Does the company have an operations team? K8s is specifically meant for appcode devs to be
abstracted away from infra and your devops/ops handles the infra making it so all a dev has to do is say "here's my built container
image that was built in a fully automated fashion that was completely abstracted from me after I merged my pull request". Then
again, test writes are def going for app engine here.
upvoted 
2 
times
azureaspirant
azureaspirant
 
1 year, 10 months ago
2/15/21 exam
upvoted 
2 
times
mshry
mshry
 
1 year, 10 months ago
Selected Answer: 
A
I am seeing some confusion in answers for questions relating to requests or concurrent sessions being served by a solution. I
think we ought not to look at API rate limits being synonymous to data handling limits, as these are programming limits to that API.
An analogy would be comparing the control plane request limits to the data limits.
upvoted 
1 
times
haroldbenites
haroldbenites
 
1 year, 10 months ago
go for A
upvoted 
1 
times
anjuagrawal
anjuagrawal
 
2 years ago
Why not Cloud Endpoint?
upvoted 
1 
times
Urban_Life
Urban_Life
 
2 years ago
Not to manage infra so it's A
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 1 month ago
Selected Answer: 
AA is the correct answer
upvoted 
2 
times
ravisar
ravisar
 
2 years, 1 month ago
In GKE, you have to create underlying infrastructure. It is not PAAS. Only app engine provide capability for developers to focus on
code. The GKE, you need to configure many other items Apart from Code. So A seems to be more accurate. Regarding global
nature, the app engine application servers users globally. I agree that there may be little latency for regions other than NA, however
since the focus of the question is "Code", I would select A.
upvoted 
4 
times
XAliX
XAliX
 
2 years, 1 month ago
App Engine support limited numbers of languages, they does not mention which interface, so for flexibity i will go for D
upvoted 
1 
times
danielfootc
danielfootc
 
2 years, 2 months ago
I would say it's App Engine for developers to focus on code.
upvoted 
2 
timesTopic 1
 
Question #137
Your company provides a recommendation engine for retail customers. You are providing retail customers with an API where they
can submit a user ID and the 
API returns a list of recommendations for that user. You are responsible for the API lifecycle and want to ensure stability for your
customers in case the API makes backward-incompatible changes. You want to follow Google-recommended practices. What
should you do? 
A. 
Create a distribution list of all customers to inform them of an upcoming backward-incompatible change at least one
month before replacing the old API with the new API.
B. 
Create an automated process to generate API documentation, and update the public API documentation as part of the
CI/CD process when deploying an update to the API.
C. 
Use a versioning strategy for the APIs that increases the version number on every backward-incompatible change. 
Most
Voted
D. 
Use a versioning strategy for the APIs that adds the suffix 
ג
€DEPRECATED
ג
 €to the current API version number on every
backward-incompatible change. Use the current version number for the new API.
Correct Answer:
 
C 
Community vote distribution
C (100%)
Comments
kopper2019
kopper2019
Highly Voted 
3 years ago
It is C
upvoted 
35 
times
sebafranek
sebafranek
2 years, 11 months ago
https://cloud.google.com/apis/design/versioning
upvoted 
9 
times
[Removed]
[Removed]
1 year, 6 months ago
more specifically: https://cloud.google.com/apis/design/versioning#release-based_versioning
upvoted 
5 
times
VishalB
VishalB
Exam Professional Cloud Architect 
All Actual Questions
        Highly Voted 
2 years, 11 months ago
Answer C
All Google API interfaces must provide a major version number, which is encoded at the end of the protobuf package, and included as
the first part of the URI path for REST APIs. If an API introduces a breaking change, such as removing or renaming a field, it must
increment its API version number to ensure that existing user code does not suddenly break.
upvoted 
23 
times
de1001c
de1001c
Most Recent 
3 weeks, 6 days ago
Selected Answer: 
C
Breaking changes = version bump in the API url
upvoted 
1 
times
spuyol
spuyol
5 months ago
Answer is A
Question asks: You are responsible for the API lifecycle and want to ensure stability for your customers in case the API makes
backward-incompatible changes.
Option C is a must for the API lifecycle but doing only that will not help your customers. In my opinion they have to be notified of
backward-incompatible changes with time enough to stay ready
upvoted 
1 
times
convers39
convers39
5 months, 3 weeks ago
Selected Answer: 
C
This is not a GCP-specific or even cloud-specific question, but a development best practice. No need to look at other options.
upvoted 
3 
times
mastrrrr
mastrrrr
7 months ago
I had a test today. The overall question was almost the same, but question mentioned 
Apigee.
upvoted 
4 
times
heretolearnazure
heretolearnazure
10 months, 1 week ago
C is correct
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
1 year ago
Maybe A on top of C but are we really going to mandate customers only have 1 month to be ready? 
I don't think so, use the version
strategy and let them migrate when ready!
upvoted 
1 
times
Atanu
Atanu
1 year, 1 month ago
Anyways option A is also a recommended best practice. Its version incompatible change afterall.
upvoted 
1 
times
gu_pp
gu_pp
1 year, 2 months ago
Selected Answer: 
C
Clearly C
upvoted 
1 
times
bharath2k5
bharath2k5
1 year, 3 months ago
Selected Answer: 
CSelected Answer: 
C
c version numbers cannot be changed as given in d
upvoted 
1 
times
CGS22
CGS22
1 year, 3 months ago
Selected Answer: 
C
The correct answer is: C. Use a versioning strategy for the APIs that increases the version number on every backward-incompatible
change.
A versioning strategy for the APIs that increases the version number on every backward-incompatible change is the best way to ensure
stability for your customers in case the API makes backward-incompatible changes. This will allow you to track the changes that have
been made to the API and allow your customers to easily identify the latest version of the API.
upvoted 
3 
times
zerg0
zerg0
1 year, 5 months ago
Selected Answer: 
C
See @seafranek comment
upvoted 
1 
times
HenkH
HenkH
1 year, 6 months ago
If 'A' is the correct answer, one would wonder why.
The new version might be back-ward incompatible but does not necessarily mean its a breaking one. In that case A might be sufficient,
although C remains mandatoryBest Practice imho.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
megumin
megumin
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Balaji_Sakthi
Balaji_Sakthi
1 year, 8 months ago
option C
upvoted 
1 
times
Exam name or code...
Exam name or code...
Log in to ExamTopics
×
Sign in:Sign in:
Email or nickname
Password
Forgot my password
Log in
Don't have an account yet? just 
sign-up 
. 
Resend activation email
Close
Most Voted
A voting comment increases the vote count for the chosen answer by one. 
Upvoting a comment with a selected answer will also increase the vote count towards that answer by one. 
So if you see a comment that you already agree with,
you can upvote it instead of posting a new comment.
Save 
Cancel
Loading 
...
Report Comment
×
Is the comment made by 
USERNAME 
spam or abusive?
Yes
 
No
Commenting
×
In order to participate in the comments you need to be logged-in. 
You can 
sign-up 
or 
login 
(it's free).
Ok 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #138
Your company has developed a monolithic, 3-tier application to allow external users to upload and share files. The solution
cannot be easily enhanced and lacks reliability. The development team would like to re-architect the application to adopt
microservices and a fully managed service approach, but they need to convince their leadership that the effort is worthwhile.
Which advantage(s) should they highlight to leadership? 
A. 
The new approach will be significantly less costly, make it easier to manage the underlying infrastructure, and
automatically manage the CI/CD pipelines.
B. 
The monolithic solution can be converted to a container with Docker. The generated container can then be deployed into
a Kubernetes cluster.
C. 
The new approach will make it easier to decouple infrastructure from application, develop and release new features,
manage the underlying infrastructure, manage CI/CD pipelines and perform A/B testing, and scale the solution if
necessary. 
Most Voted
D. 
The process can be automated with Migrate for Compute Engine.
Correct Answer:
 
C 
Comments
Mitra123
Mitra123
 
Highly Voted
 
3 years ago
decoupling, new features, CI/CD, A/B testing, scaling is the advantage so C
upvoted 
23 
times
kopper2019
kopper2019
 
Highly Voted
 
2 years, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
12 
times
bishalsainju
bishalsainju
 
2 years, 8 months ago
Hey man do you know if in second attempt as well, we get these same questions?
upvoted 
4 
times
tannV
tannV
 
2 years, 1 month ago
Community vote distribution
C (88%)
B (12%)tannV
tannV
 
2 years, 1 month ago
+1. Please let me know as well.
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
5 months, 3 weeks ago
Almost questions ware replaced
、
when i 2nd try.Good luck.
upvoted 
1 
times
YAS007
YAS007
 
2 years ago
+ 1 Please let me know as well.
upvoted 
2 
times
PST21
PST21
 
Most Recent
 
1 year, 4 months ago
Here they need to convince why the effort is worthwhile - to prove that you will talk on enhancements hence C. All of these lead to
cost reduction/saving eventually.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
FenixRa73
FenixRa73
 
1 year, 7 months ago
I had the same question on the exam, my choice was A, passed
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
samsonakala
samsonakala
 
1 year, 8 months ago
C most definitely. Following the best practice.
upvoted 
1 
times
Sitender
Sitender
 
1 year, 11 months ago
C is better: Management like to understand business benefit in simple language. They don't 
bother about the technology
upvoted 
2 
times
rakp
rakp
 
9 months, 2 weeks ago
Agree with you. Isn't A the simplest answer. It says all the same things as C, just without any technicality.
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years ago
C clearly states all of the benefits, C is right.
upvoted 
3 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
B
There is very less discussion for this question.
.I thing we are forgetting reliablity or HA.
I am voting for B
upvoted 
2 
times
9xnine
9xnine
 
2 years, 1 month ago
HA is not mentioned in the question, reliability is, but decoupling the architecture improves reliability.upvoted 
1 
times
ryzior
ryzior
 
2 years, 2 months ago
Devs will talk to their managers - so more technically, then the managers will talk to their managers (maybe CxO level) and they
will talk 'money'. So I think the correct answer is C but A is still quite possible in very flat organizations. Again the question is asked
in some context which is not expressed :(
upvoted 
2 
times
ryzior
ryzior
 
2 years, 2 months ago
and CxOs are not interested in 'easier' or 'automatically' which are parts of A answer, the job to make things easier/less
cumbersome are on the low level managers or leads of the streams :)
upvoted 
1 
times
wilwong
wilwong
 
2 years, 2 months ago
Selected Answer: 
C
C 
to decouple infrastructure from application, 
the development team want a fully managed service approach
upvoted 
2 
times
learner311
learner311
 
2 years, 2 months ago
C. "develop and release new features". Work as a dev in any environment. All leadership cares about is "first to market" and
differentiators.
upvoted 
2 
times
[Removed]
[Removed]
 
2 years, 4 months ago
Selected Answer: 
C
I got similar question on my exam. Answered C.
upvoted 
5 
times
technodev
technodev
 
2 years, 5 months ago
Got this question in my exam, answered C
upvoted 
2 
times
pgarg2000
pgarg2000
 
2 years, 5 months ago
do you have the same response to every question
upvoted 
6 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
I vote A.
Management is concerned about costs.
A is the only option that touches on costs.
upvoted 
4 
times
anjuagrawal
anjuagrawal
 
2 years, 6 months ago
The answer should be between A or C. Choice is to be made between cost effective or Scaling 
respectively. They have to
convince leadership on the effort and not cost so I think we can go for C.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #139
Your team is developing a web application that will be deployed on Google Kubernetes Engine (GKE). Your CTO expects a
successful launch and you need to ensure your application can handle the expected load of tens of thousands of users. You
want to test the current deployment to ensure the latency of your application stays below a certain threshold. What should you
do? 
A. 
Use a load testing tool to simulate the expected number of concurrent users and total requests to your application, and
inspect the results. 
Most Voted
B. 
Enable autoscaling on the GKE cluster and enable horizontal pod autoscaling on your application deployments. Send curl
requests to your application, and validate if the auto scaling works.
C. 
Replicate the application over multiple GKE clusters in every Google Cloud region. Configure a global HTTP(S) load
balancer to expose the different clusters over a single global IP address.
D. 
Use Cloud Debugger in the development environment to understand the latency between the different microservices.
Correct Answer:
 
A 
Comments
kopper2019
kopper2019
 
Highly Voted
 
3 years, 5 months ago
21 NEw Qs - 
July 12, 2021
# 15. An application development team has come to you for advice.They are planning to write and deploy an HTTP(S) API using
Go 1.12. The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize
operational overhead for this application. What approach should you recommend?
a. Use a Managed Instance Group when deploying to Compute Engine
b. Develop an application with containers, and deploy to Google Kubernetes Engine (GKE)
c. Develop the application for App Engine standard environment
d. Develop the application for App Engine Flexible environment using a custom runtime
Answer C, , 
please share you answers
upvoted 
22 
times
muhasinem
muhasinem
 
3 years, 5 months ago
C is ok.
Community vote distribution
A (83%)
B (17%)C is ok.
upvoted 
5 
times
namanp12345
namanp12345
 
3 years, 4 months ago
Answer A
upvoted 
1 
times
namanp12345
namanp12345
 
3 years, 4 months ago
Sorry C
upvoted 
1 
times
Neo_ACE
Neo_ACE
 
3 years, 1 month ago
Answer is C
upvoted 
2 
times
learner311
learner311
 
2 years, 8 months ago
C. Unpredictable workload. Flexible does not scale nearly as fast as standard.
upvoted 
2 
times
juccjucc
juccjucc
 
Highly Voted
 
3 years, 5 months ago
Anyone can tell please if at the new exam there are also questions from the old set(before question 115)?
upvoted 
13 
times
kopper2019
kopper2019
 
3 years, 3 months ago
old ones are not 
removed
upvoted 
4 
times
ale183
ale183
 
3 years, 3 months ago
really ? the old ones are are still on exam ? from 1-100 ? how about old case study questions ?
upvoted 
2 
times
LDAP_Anand
LDAP_Anand
 
Most Recent
 
3 weeks, 5 days ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
de1001c
de1001c
 
7 months ago
Selected Answer: 
A
Test current status, load test tools only purpouse is to test load and consider success/failure based on latency / response times.
upvoted 
2 
times
35cd41b
35cd41b
 
11 months, 3 weeks ago
Selected Answer: 
A
A is correct, jmeter, k6
upvoted 
1 
times
ammonia_free
ammonia_free
 
11 months, 3 weeks ago
Selected Answer: 
A
IMO, something here should be a multiple-choice option, and the examinee should have selected A and B. Otherwise, I am picking
option A.
upvoted 
1 
times
kubinho
kubinho
 
1 year, 6 months ago
Admin should really deleting the wrong answers.. I think 90% of people here just copying explanations and really have no idea what
they are talking about. every second question has different answer with different "voting"answer..
upvoted 
2 
times
 
1 year, 6 months agoBiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
Using Curl seems weird, how is Curl going to inject the load? 
Curl would be fine if we just wanted to test the underlying latency of
the system.
upvoted 
1 
times
gu_pp
gu_pp
 
1 year, 8 months ago
Selected Answer: 
A
It is A.
You want to TEST the deployment, not changing anything (yet).
upvoted 
1 
times
CGS22
CGS22
 
1 year, 10 months ago
Selected Answer: 
A
The correct answer is: A. Use a load testing tool to simulate the expected number of concurrent users and total requests to your
application, and inspect the results.
A load testing tool can be used to simulate the expected number of concurrent users and total requests to your application. This
will allow you to test how your application handles the expected load and to identify any potential problems.
Enabling autoscaling on the GKE cluster and enabling horizontal pod autoscaling on your application deployments will not help you
to test the latency of your application. This will only help to ensure that your application can handle the expected load.
upvoted 
4 
times
MaryMei
MaryMei
 
1 year, 10 months ago
Selected Answer: 
B
is curl returns latency info?
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
sanait100
sanait100
 
2 years ago
Also, there is no expected numbers, the users can be tens of thousands so whatever testing you do with A may not be sufficient
so it's better to keep autoscaling for whatsoever load comes in and curl test ensures autoscaling happens when required
upvoted 
2 
times
sanait100
sanait100
 
2 years ago
The question specifically asks that your CTO expects a successful launch and you need to ensure your application can handle the
expected load of tens of thousands of users. In A, you are just testing and not taking an action. In B, you are not only testing with
curl commands to check for latency but also taking action to enable the cluster to acquire more resources. So, I will go with B.
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A is the correct, no load ==> no latency checking
upvoted 
1 
times
AHUI
AHUI
 
2 years, 2 months ago
Ans A: the question asks you want to test
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #140
Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the
application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered
that the application cannot process the messages in real time. Most of them wait for minutes before being processed. You
need to scale the elaboration process that is I/O-intensive. What should you do? 
A. 
Use kubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to configure Kubernetes autoscaling
deployment.
B. 
Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric.
C. 
Use the --enable-autoscaling flag when you create the Kubernetes cluster.
D. 
Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric.
Most Voted
Correct Answer:
 
D 
Comments
Rzla
Rzla
 
Highly Voted
 
3 years, 4 months ago
Answer is D. 
num_undelivered_messages metric can indicate if subscribers are keeping up with message submissions. 
https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog
upvoted 
40 
times
rishab86
rishab86
 
3 years, 3 months ago
D is correct !
upvoted 
2 
times
PATILDXB
PATILDXB
 
2 years, 1 month ago
The provided link is not relevant to kubernetes, but pertains to cloud pub/sub....the num_undelivered_messages metric is not
available for kubernetes autoscaling...C is correct
upvoted 
2 
times
huuthanhdlv
huuthanhdlv
 
7 months, 1 week ago
Community vote distribution
D (87%)
B (13%)Kubernetes allow autoscaling based on external metrics
upvoted 
1 
times
GopeshSahu
GopeshSahu
 
1 year, 11 months ago
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub
upvoted 
4 
times
aut0pil0t
aut0pil0t
 
Highly Voted
 
2 years, 4 months ago
Selected Answer: 
D
Direct answer - D
https://cloud.google.com/kubernetes-engine/docs/samples/container-pubsub-horizontal-pod-autoscaler
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
name: pubsub
spec:
minReplicas: 1
maxReplicas: 5
metrics:
- external:
metric:
name: pubsub.googleapis.com|subscription|num_undelivered_messages
selector:
matchLabels:
resource.labels.subscription_id: echo-read
target:
type: AverageValue
averageValue: 2
type: External
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: pubsub
upvoted 
22 
times
awsgcparch
awsgcparch
 
Most Recent
 
5 months, 1 week ago
Selected Answer: 
D
Subscription Metric: Scaling based on the subscription/num_undelivered_messages metric directly ties the scaling behavior to the
number of unprocessed messages in Pub/Sub. This ensures that your application scales out when there are more messages to
process and scales in when the queue is short.
Relevant Metric: This metric is relevant for an I/O-intensive application that processes messages from Pub/Sub, ensuring that the
scaling is directly responsive to the message processing demand.
upvoted 
1 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
D
D is the correct answer: subscription/num_undelivered_messages directly indicates the number of messages waiting to be
processed, making it a perfect indicator of the workload on the application.
upvoted 
1 
times
ductrinh
ductrinh
 
1 year, 3 months ago
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub
here . so this is D
upvoted 
1 
times
RaviRS
RaviRS
 
1 year, 3 months ago
Option C (--enable-autoscaling flag for the entire cluster): Enabling autoscaling at the cluster level doesn't provide fine-grained
control over scaling individual pods or deployments based on specific workload metrics.
upvoted 
1 
times
Zahasan
Zahasan
 
1 year, 4 months agoCan someone tell that these answers are right like people voted D but answer is C.
upvoted 
2 
times
sampon279
sampon279
 
1 year, 6 months ago
Answer D
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
B & C refer to using auto scaler with custom metrics but
"Cluster autoscaler makes these scaling decisions based on the resource requests (rather than actual resource utilization) of
Pods running on that node pool's nodes. "
A makes no sense as it defines a min and max that we don't know
D seems like part of the solution so D.
upvoted 
1 
times
natpilot
natpilot
 
1 year, 8 months ago
is D - https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub
upvoted 
1 
times
WFCheong
WFCheong
 
1 year, 11 months ago
Selected Answer: 
B
I vot for B. Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric. as the
metric should be based on latency instead of num_undelivered_messages metric in D.
upvoted 
2 
times
gcppandit
gcppandit
 
1 year, 11 months ago
The problems says PULL request and B is related to PUSH request. I do not think it is related.
upvoted 
4 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
ale_brd_111
ale_brd_111
 
2 years, 1 month ago
Selected Answer: 
D
D is the correct one
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
newuser111
newuser111
 
2 years, 2 months ago
Selected Answer: 
D
D
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub
upvoted 
2 
times
bossdellacert
bossdellacert
 
2 years, 4 months ago
Selected Answer: 
D
This seems relevant 
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub
even if it uses Deployment + HorizontalPodAutoscaler which is not mentioned in the context of the question/answereven if it uses Deployment + HorizontalPodAutoscaler which is not mentioned in the context of the question/answer
upvoted 
2 
times
tycho
tycho
 
2 years, 4 months ago
I think it wrong for application to be in pod it should be a deployment, and deployment would scale
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #141
Your company is developing a web-based application. You need to make sure that production deployments are linked to source
code commits and are fully auditable. What should you do? 
A. 
Make sure a developer is tagging the code commit with the date and time of commit.
B. 
Make sure a developer is adding a comment to the commit that links to the deployment.
C. 
Make the container tag match the source code commit hash. 
Most Voted
D. 
Make sure the developer is tagging the commits with latest.
Correct Answer:
 
C 
Comments
djosani
djosani
 
Highly Voted
 
3 years, 4 months ago
Developer shouldn't tag or comment every commit with some specific data, like timestamps or something else. There might be an
app version, but it's not mentioned. I'd go with C as it's an automated, error-less approach that answers the question.
upvoted 
35 
times
Urban_Life
Urban_Life
 
3 years ago
@Kopper2019- what do you think about ans C?
upvoted 
2 
times
victory108
victory108
 
Highly Voted
 
3 years, 4 months ago
C. Make the container tag match the source code commit hash.
upvoted 
16 
times
amxexam
amxexam
 
3 years, 3 months ago
Not sure how the container tag match with the commit will help to audit, can someone explain?
upvoted 
2 
times
ynoot
ynoot
 
3 years, 1 month ago
if you got the commit hash from the container you can check the corresponding commit in the git repository. So the change, that
Community vote distribution
C (100%)if you got the commit hash from the container you can check the corresponding commit in the git repository. So the change, that
was made and deployed into your environment can be audited.
upvoted 
10 
times
Sephethus
Sephethus
 
Most Recent
 
6 months, 2 weeks ago
Selected Answer: 
C
Linking Deployments to Commits: By tagging the container image with the source code commit hash, you create a direct link
between the deployed container and the specific state of the source code. This provides a clear and auditable trail from the
deployed application back to the exact source code that was used to build it.
Auditability: Using the commit hash as the container tag ensures that each deployment can be traced back to a unique and
immutable source code commit. This makes it easy to audit deployments and verify which version of the code is running in
production.
upvoted 
2 
times
RaviRS
RaviRS
 
1 year, 3 months ago
Selected Answer: 
C
Can't fathom A. This is what ChatGPT says about A - I agree to this.
Option A (tagging with date and time): Using date and time as tags may not be precise enough to identify the exact code version
associated with a deployment, especially if multiple commits occurred within the same time window.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
Really C should say image?
We have to seperate systems: source code repo & container repo.
How do we link the two together? 
C is the only attempt at solving the problem.
upvoted 
1 
times
WFCheong
WFCheong
 
1 year, 11 months ago
Selected Answer: 
C
Agreed with C instead of A with them.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
KumarSelvaraj
KumarSelvaraj
 
2 years, 1 month ago
Answer is C
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
C is correct "By design, the Git commit hash is immutable and references a specific version of your software." as per
https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash
upvoted 
4 
times
zellck
zellck
 
2 years, 3 months ago
Selected Answer: 
C
C is the answer.
https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash
You can use this commit hash as a version number for your software, but also as a tag for the Docker image built from this
specific version of your software. Doing so makes Docker images traceable: because in this case the image tag is immutable, you
instantly know which specific version of your software is running inside a given container.instantly know which specific version of your software is running inside a given container.
upvoted 
5 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
Every Git commit with timestamp A doesn't make since. C is right
upvoted 
3 
times
munnysh
munnysh
 
2 years, 6 months ago
Selected Answer: 
C
No manual intervention is preferred in automatic deployments. Only automating the container tag to match the commit hash will be
fully auditable with the help of the scm.
upvoted 
4 
times
ridyr
ridyr
 
2 years, 8 months ago
Selected Answer: 
C
From: https://cloud.google.com/architecture/best-practices-for-building-containers
Under: Tagging using the Git commit hash 
(bottom of page almost)
"In this case, a common way of handling version numbers is to use the Git commit SHA-1 hash (or a short version of it) as the
version number. By design, the Git commit hash is immutable and references a specific version of your software.
You can use this commit hash as a version number for your software, but also as a tag for the Docker image built from this
specific version of your software. Doing so makes Docker images traceable: because in this case the image tag is immutable, you
instantly know which specific version of your software is running inside a given container."
upvoted 
7 
times
SCVinod
SCVinod
 
2 years, 10 months ago
It's got to be A. Option C talks about containers whereas there is no mention of containers in the question.
upvoted 
4 
times
[Removed]
[Removed]
 
2 years, 10 months ago
Selected Answer: 
C
I got similar question on my exam. Answered C.
upvoted 
5 
times
Narinder
Narinder
 
2 years, 11 months ago
I think answer is A.
In Git, tag is used to mark release points (v1.0, v2.0 and so on). You can tag the release based on the time stamp and using 
git
show <tag-name> command, you can see the commit detailed history. 
Reference: https://git-scm.com/book/en/v2/Git-Basics-Tagging
C could be the correct answer for the case if you are going with container based solution which is not mentioned anywhere in the
question.
upvoted 
5 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #142
An application development team has come to you for advice. They are planning to write and deploy an HTTP(S) API using Go
1.12. The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize
operational overhead for this application. Which approach should you recommend? 
A. 
Develop the application with containers, and deploy to Google Kubernetes Engine.
B. 
Develop the application for App Engine standard environment. 
Most Voted
C. 
Use a Managed Instance Group when deploying to Compute Engine.
D. 
Develop the application for App Engine flexible environment, using a custom runtime.
Correct Answer:
 
B 
Comments
vladik820
vladik820
Highly Voted 
2 years, 10 months ago
B is ok
upvoted 
21 
times
SweetieS
SweetieS
Highly Voted 
2 years, 10 months ago
B is ok.
https://cloud.google.com/appengine/docs/the-appengine-environments
upvoted 
12 
times
cugena
cugena
2 years, 9 months ago
Source code is written in specific versions of the supported programming languages:
Python 2.7, Python 3.7, Python 3.8, Python 3.9
Java 8, Java 11
Node.js 10, Node.js 12, Node.js 14, Node.js 16 (preview)
PHP 5.5, PHP 7.2, PHP 7.3, and PHP 7.4
Ruby 2.5, Ruby 2.6, and Ruby 2.7
Go 1.11, Go 1.12, Go 1.13, Go 1.14, Go 1.15, and Go 1.16 (preview)
upvoted 
7 
times
cugena
cugena
2 years, 9 months ago
Community vote distribution
B (96%)
A
(4%)2 years, 9 months ago
Intended to run for free or at very low cost, where you pay only for what you need and when you need it. For example, your application
can scale to 0 instances when there is no traffic.
Experiences sudden and extreme spikes of traffic which require immediate scaling.
upvoted 
4 
times
nairj
nairj
Most Recent 
3 months, 3 weeks ago
Both B and D are okay, however, the need for App engine Flexible environment is not required unless you want to run docker
containers, have more control over the instance used and so on, hence in this case B works well. 
https://cloud.google.com/appengine/docs/the-appengine-environments
upvoted 
3 
times
thewalker
thewalker
7 months, 1 week ago
Selected Answer: 
A
A
GKE is much reliable compared to the other options provided here.
upvoted 
1 
times
rakp
rakp
9 months, 2 weeks ago
Selected Answer: 
B
B is correct. It supports Go 1.12, and can handle sudden spikes.
https://cloud.google.com/appengine/docs/the-appengine-environments
upvoted 
1 
times
gotcertified
gotcertified
1 year ago
Can someone explain why we cannot use AppEngine Flexible environment ?
upvoted 
1 
times
anjanc
anjanc
6 months, 3 weeks ago
Bcs They want to minimize operational overhead for this application
upvoted 
1 
times
edoo
edoo
4 months, 3 weeks ago
I guess it can't scale down to 0.
upvoted 
1 
times
sampon279
sampon279
1 year ago
Selected Answer: 
B
App engine standard provides go env.
upvoted 
1 
times
AugustoKras011111
AugustoKras011111
1 year, 4 months ago
Selected Answer: 
B
App Engine Std. Can run this Go version and Scales to 0.
upvoted 
1 
times
zerg0
zerg0
1 year, 4 months ago
Selected Answer: 
B
Standard AppEngine Environment supports Go 1.2. The AppEngine can be low cost if no or low traffic. It has free quotas.
upvoted 
1 
times
zerg0
zerg0Exam name or code...
Exam name or code...
Log in to ExamTopics
zerg0
zerg0
1 year, 5 months ago
Selected Answer: 
B
AppEngine scales well, only dev effort. No infrastructure. go is supported in the standard distribution.
upvoted 
1 
times
NodummyIQ
NodummyIQ
1 year, 6 months ago
The answer is A. B option is not correct. It is not recommended to use App Engine Standard environment for an HTTP(S) API with a
very unpredictable workload because App Engine Standard environment has certain limitations and constraints that may not be
suitable for an API with an unpredictable workload. For example, App Engine Standard environment has a maximum request timeout
of 60 seconds, which may not be sufficient for an API with a very unpredictable workload.
upvoted 
1 
times
megumin
megumin
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
2 years ago
B is correct ..https://cloud.google.com/appengine/docs/the-appengine-environments
Experiences sudden and extreme spikes of traffic which require immediate scaling.
upvoted 
2 
times
munnysh
munnysh
2 years ago
Selected Answer: 
B
https://cloud.google.com/appengine/docs/the-appengine-environments App engine standard environment support go 1.13 and also
handles the unpredictable load.
upvoted 
3 
times
TitaniumBurger
TitaniumBurger
2 years, 4 months ago
B. Unpredictable traffic & low overhead.
upvoted 
2 
times
tmnd91
tmnd91
2 years, 5 months ago
Selected Answer: 
B
App Engine standard has autoscaling out of the box, supports Go 1.12 and can scale down to 0 to save money
upvoted 
6 
times
lxgywil
lxgywil
2 years, 5 months ago
B is ok.
upvoted 
1 
timesLog in to ExamTopics
×
Close
Most Voted
A voting comment increases the vote count for the chosen answer by one. 
Upvoting a comment with a selected answer will also increase the vote count towards that answer by one. 
So if you see a comment that you already agree with,
you can upvote it instead of posting a new comment.
Save 
Cancel
Loading 
...
Report Comment
×
Is the comment made by 
USERNAME 
spam or abusive?
Yes
 
No
Commenting
×
In order to participate in the comments you need to be logged-in. 
You can 
sign-up 
or 
login 
(it's free).
Ok
Sign in:
Email or nickname
Password
Forgot my password
Log in
Don't have an account yet? just 
sign-up 
. 
Resend activation email 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #143
Your company is designing its data lake on Google Cloud and wants to develop different ingestion pipelines to collect
unstructured data from different sources. 
After the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine for
end users on the website. The structure of the data retrieved from the source systems can change at any time. The data must
be stored exactly as it was retrieved for reprocessing purposes in case the data structure is incompatible with the current
processing pipelines. You need to design an architecture to support the use case after you retrieve the data. What should you
do? 
A. 
Send the data through the processing pipeline, and then store the processed data in a BigQuery table for reprocessing.
B. 
Store the data in a BigQuery table. Design the processing pipelines to retrieve the data from the table.
C. 
Send the data through the processing pipeline, and then store the processed data in a Cloud Storage bucket for
reprocessing.
D. 
Store the data in a Cloud Storage bucket. Design the processing pipelines to retrieve the data from the bucket.
Most Voted
Correct Answer:
 
D 
Comments
vladik820
vladik820
 
Highly Voted
 
2 years, 10 months ago
D is ok
The data needs to be stored as it is retrieved. This would mean that any processing should be done after it is stored.
upvoted 
28 
times
MaxNRG
MaxNRG
 
Highly Voted
 
2 years, 8 months ago
D, store RAW unstructured data as-is in Cloud Storage, and then define how to process it.
Classical Data Lake ELT (Extract -> Load -> Transform )
upvoted 
7 
times
Gino17m
Gino17m
 
Most Recent
 
2 months, 1 week ago
Selected Answer: 
D
Community vote distribution
D (100%)D
Unstructured data - GCS
Data stored axactly as it was retrieved - store before processing
upvoted 
1 
times
anjanc
anjanc
 
6 months, 3 weeks ago
Key word is "The data must be stored exactly as it was retrieved for reprocessing purposes in case the data structure is
incompatible with the current processing pipelines." and hence D
upvoted 
1 
times
devnul
devnul
 
10 months, 2 weeks ago
D. It aligns with an example in the Cloud Architecture Framework
https://cloud.google.com/architecture/big-data-analytics/analytics-lakehouse
upvoted 
2 
times
BeCalm
BeCalm
 
1 year, 3 months ago
What is the point of data being in a lake and then being dumped into GCS without processing. What purpose is served with
GCS being a copy of lake?
upvoted 
1 
times
jlambdan
jlambdan
 
1 year, 3 months ago
here gcs is the lake. Not a copy.
The data warehouse will be what comes out of the pipelines.
upvoted 
3 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
D
D is ok
upvoted 
2 
times
jmblancof
jmblancof
 
1 year, 8 months ago
D is ok
upvoted 
2 
times
Nirca
Nirca
 
1 year, 9 months ago
Selected Answer: 
D
D is ok
The data needs to be stored as it is retrieved. This would mean that any processing should be done after it is stored in GCS.
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years ago
storing and retrieving data in cloud storage solve the purpose of this use case. D is perfect answer.
upvoted 
1 
times
snwbr
snwbr
 
2 years, 3 months ago
Although... wouldn't be Bigtable or Datastore better than GCS?
upvoted 
1 
times
wykofc
wykofc
 
1 year, 11 months ago
Both BigTable and DataStore are NoSQL Databases, qns mentioned that data structure may change anytime
upvoted 
2 
times
[Removed]
[Removed]
 
2 years, 4 months ago
Selected Answer: 
D
I got similar question on my exam. Answered D.
upvoted 
4 
times
technodev
technodev
 
2 years, 5 months agotechnodev
technodev
 
2 years, 5 months ago
Got this question in my exam, answered D
upvoted 
4 
times
lxgywil
lxgywil
 
2 years, 5 months ago
D is ok
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 7 months ago
D is the correct answer
upvoted 
1 
times
pakilodi
pakilodi
 
2 years, 7 months ago
Selected Answer: 
D
D is correct
upvoted 
1 
times
TheCloudBoy77
TheCloudBoy77
 
2 years, 7 months ago
D - Data must be stored as it is before and after so use Cloud storage and then build pipelines as needed.
upvoted 
2 
timesTopic 1
 
Question #144
You are responsible for the Google Cloud environment in your company. Multiple departments need access to their own projects,
and the members within each department will have the same project responsibilities. You want to structure your Google Cloud
environment for minimal maintenance and maximum overview of 
IAM permissions as each department's projects start and end. You want to follow Google-recommended practices. What should
you do? 
A. 
Grant all department members the required IAM permissions for their respective projects.
B. 
Create a Google Group per department and add all department members to their respective groups. Create a folder per
department and grant the respective group the required IAM permissions at the folder level. Add the projects under the
respective folders. 
Most Voted
C. 
Create a folder per department and grant the respective members of the department the required IAM permissions at the
folder level. Structure all projects for each department under the respective folders.
D. 
Create a Google Group per department and add all department members to their respective groups. Grant each group the
required IAM permissions for their respective projects.
Correct Answer:
 
B 
Community vote distribution
B (100%)
Comments
Manh
Manh
Highly Voted 
2 years, 10 months ago
it's B
upvoted 
16 
times
victory108
victory108
Highly Voted 
2 years, 10 months ago
B. Create a Google Group per department and add all department members to their respective groups. Create a folder per department
and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.
upvoted 
10 
times
afsarkhan
afsarkhan
Most Recent 
1 month, 3 weeks ago
it's B
upvoted 
1 
times
megumin
megumin
Exam Professional Cloud Architect 
All Actual Questions
        megumin
megumin
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
zellck
zellck
1 year, 9 months ago
Selected Answer: 
B
B is the answer.
https://cloud.google.com/resource-manager/docs/access-control-folders#best-practices-folders-iam
Use groups whenever possible to manage principals.
https://cloud.google.com/resource-manager/docs/creating-managing-folders
A folder can contain projects, other folders, or a combination of both. Organizations can use folders to group projects under the
organization node in a hierarchy. For example, your organization might contain multiple departments, each with its own set of Google
Cloud resources. Folders allow you to group these resources on a per-department basis.
upvoted 
5 
times
Nirca
Nirca
1 year, 9 months ago
Selected Answer: 
B
B is most appropriate for the use case and principle of least privilege.
upvoted 
1 
times
AzureDP900
AzureDP900
2 years ago
B is most appropriate for the use case and principle of least privilege.
upvoted 
1 
times
coutcin
coutcin
2 years, 1 month ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
lxgywil
lxgywil
2 years, 5 months ago
B is ok
upvoted 
1 
times
edilramos
edilramos
2 years, 6 months ago
B is ideal for minimal maintenance and maximum overview of IAM permissions as each department's projects start and end.
Manage the users inside Groups will turn it easer.
upvoted 
5 
times
anjuagrawal
anjuagrawal
2 years, 6 months ago
Voted B
upvoted 
1 
times
vincy2202
vincy2202
2 years, 7 months ago
B is the correct answer
upvoted 
1 
times
nqthien041292
nqthien041292nqthien041292
nqthien041292
2 years, 7 months ago
Selected Answer: 
B
Vote B
upvoted 
2 
times
danielfootc
danielfootc
2 years, 8 months ago
I would select B.
upvoted 
2 
times
AnilKr
AnilKr
2 years, 9 months ago
B is correct, folder restructure per department and IAM permission for Group is recommended.
upvoted 
4 
times
Sonu_xyz
Sonu_xyz
2 years, 9 months ago
Answer is B
upvoted 
2 
times
diaga2
diaga2
2 years, 10 months ago
Yes, B
upvoted 
4 
times
Exam name or code...
Exam name or code...
Log in to ExamTopics
×
Sign in:
Email or nickname
Password
Forgot my password
Log in
Don't have an account yet? just 
sign-up 
. 
Resend activation email
Close
Most Voted
A voting comment increases the vote count for the chosen answer by one. Upvoting a comment with a selected answer will also increase the vote count towards that answer by one. 
So if you see a comment that you already agree with,
you can upvote it instead of posting a new comment.
Save 
Cancel
Loading 
...
Report Comment
×
Is the comment made by 
USERNAME 
spam or abusive?
Yes
 
No
Commenting
×
In order to participate in the comments you need to be logged-in. 
You can 
sign-up 
or 
login 
(it's free).
Ok 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #145
Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. You have separate
clusters for development, staging, and production. You have discovered that the team is able to deploy a Docker image to the
production cluster without first testing the deployment in development and then staging. You want to allow the team to have
autonomy but want to prevent this from happening. You want a Google Cloud solution that can be implemented quickly with
minimal effort. What should you do? 
A. 
Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given
environment.
B. 
Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker
image was tested in an earlier environment.
C. 
Configure binary authorization policies for the development, staging, and production clusters. Create attestations as
part of the continuous integration pipeline. 
Most Voted
D. 
Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the
given environment.
Correct Answer:
 
C 
Comments
diaga2
diaga2
 
Highly Voted
 
1 year, 10 months ago
C is s fine.
upvoted 
15 
times
[Removed]
[Removed]
 
Highly Voted
 
1 year, 4 months ago
Selected Answer: 
C
I got similar question on my exam. Answered C.
upvoted 
11 
times
Deb2293
Deb2293
 
Most Recent
 
3 months, 3 weeks ago
Selected Answer: 
C
C it is
Community vote distribution
C (100%)C it is
upvoted 
2 
times
omermahgoub
omermahgoub
 
6 months, 1 week ago
A good option for quickly implementing a solution to prevent deployments to the production cluster without first testing in
development and staging would be to configure binary authorization policies for the development, staging, and production clusters.
You can then create attestations as part of the continuous integration pipeline.
Option C, "Configure binary authorization policies for the development, staging, and production clusters. Create attestations as
part of the continuous integration pipeline," would be the correct choice for this scenario.
Binary authorization is a feature of Google Kubernetes Engine that allows you to enforce policies on the images that are deployed
to your clusters. By configuring binary authorization policies for the development, staging, and production clusters, you can ensure
that only images that have been attested by an authorized entity are allowed to be deployed to those clusters. You can create the
attestations as part of the continuous integration pipeline, which will allow you to verify that the image has been tested before it is
deployed to the next environment.
upvoted 
10 
times
omermahgoub
omermahgoub
 
6 months, 1 week ago
Option A, "Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given
environment," would not be a good choice because it would not prevent the deployment of the container to the cluster in the first
place.
Option D, "Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the
given environment," would also not be a good choice because it would not prevent the deployment of the container to the cluster
in the first place.
Option B, "Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker
image was tested in an earlier environment," would be a good option, but it would not be as effective as using binary authorization
policies, as it would rely on the team following the policy rather than enforcing it automatically.
upvoted 
2 
times
megumin
megumin
 
7 months, 3 weeks ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Thornadoo
Thornadoo
 
11 months, 1 week ago
Why not A? Need something to be implemented quickly is what the q asks.
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year ago
C is right.. 
Binary Authorization implements a policy model, where a policy is a set of rules that governs the deployment of container images.
Rules in a policy provide specific criteria that an image must satisfy before it can be deployed.
For more information about the Binary Authorization policy model and other concepts, see Key concepts.
upvoted 
4 
times
AzureDP900
AzureDP900
 
1 year ago
https://cloud.google.com/binary-authorization/docs/overview#policy_model
upvoted 
3 
times
yogi_508
yogi_508
 
1 year, 6 months ago
where the case study questions are 
available in this website?
upvoted 
1 
times
vincy2202
vincy2202
 
1 year, 7 months ago
C is the correct answer
https://cloud.google.com/binary-authorization/docs/overview
upvoted 
6 
timesJimjiang
Jimjiang
 
1 year, 8 months ago
C is fine
upvoted 
1 
times
danielfootc
danielfootc
 
1 year, 8 months ago
I think C is the correct answer.
upvoted 
1 
times
AnilKr
AnilKr
 
1 year, 9 months ago
C is correct, binary authorization is the solution.
upvoted 
2 
times
victory108
victory108
 
1 year, 10 months ago
C. Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the
continuous integration pipeline.
upvoted 
2 
times
serious_user
serious_user
 
1 year, 10 months ago
C is ok
upvoted 
2 
times
vladik820
vladik820
 
1 year, 10 months ago
C is ok
upvoted 
2 
times
SweetieS
SweetieS
 
1 year, 10 months ago
Sorry, it's C : Configure binary authorization policies for the development, staging, and production clusters. Create attestations as
part of the continuous integration pipeline.
upvoted 
3 
times
SweetieS
SweetieS
 
1 year, 10 months ago
D is ok.
https://cloud.google.com/binary-authorization/docs/overview
upvoted 
1 
times
cugena
cugena
 
1 year, 9 months ago
You meant C I guess
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #146
Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it
takes to complete this activity, the overall cost, and database load. The bandwidth between the on-premises environment and
Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do? 
A. 
Develop a Dataflow job to read data directly from the database and write it into Cloud Storage.
B. 
Use the Data Transfer appliance to perform an offline migration. 
Most Voted
C. 
Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud
Storage.
D. 
Compress the data and upload it with gsutil -m to enable multi-threaded copy.
Correct Answer:
 
B 
Comments
pr2web
pr2web
 
Highly Voted
 
3 years, 3 months ago
This is pretty simple. 
Time to transfer using Transfer Appliance: 1-3 weeks (I've used it twice and had a 2-3 week turnaround total)
Time to transfer using 1Gbps : 30 hours (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-
large-datasets) 
Answer is D, using gsutil
upvoted 
100 
times
mickeythecraycray
mickeythecraycray
 
2 years, 9 months ago
Will that not increase the Database load?, one of the requirement is to reduce the load of the DB during this operation.
upvoted 
3 
times
Aiffone
Aiffone
 
3 years ago
If I can do it in 30hrs, why choose 1 
week? i'd go with B
upvoted 
3 
times
Aiffone
Aiffone
 
3 years ago
Community vote distribution
B (55%)
D (37%)
A (8%)I mean I'd go with A rather...questions says to spend minimum time and we have 1Gbps to do 10Tb in 30hrs
upvoted 
2 
times
Aiffone
Aiffone
 
2 years, 11 months ago
Transfer appliance -A
upvoted 
2 
times
Deb2293
Deb2293
 
1 year, 9 months ago
Go home you are drunk
upvoted 
5 
times
joe2211
joe2211
 
3 years, 1 month ago
Not about time but "Google-recommended practices"
upvoted 
8 
times
MikeB19
MikeB19
 
3 years, 3 months ago
This is the correct article to support this question but the article proves the transfer appliance is the correct answer. Right
below the transfer calc chart is recommended amount of data for gsutil. Gsutil should be used for data transfer under 1 tb
“Your private data center to Google Cloud Enough bandwidth to meet your project deadline
for less than 1 TB of data gsutil”
upvoted 
3 
times
gingerbeer
gingerbeer
 
Highly Voted
 
3 years, 3 months ago
No perfect answer as B and D both have flaws. B is time latency as transfer appliance usually takes weeks; D gsutil applies for
less than 1TB. The answer should be storage transfer service for on-premises data, which is not available here. 
If have to choose one I go for B
upvoted 
21 
times
RitwickKumar
RitwickKumar
 
2 years, 4 months ago
Storage transfer service is for online data. It can't serve the purpose if you don't have the connectivity established between on
prem and gcp. Which is what we can't assume ourselves in this question.
upvoted 
1 
times
T12344223
T12344223
 
Most Recent
 
4 weeks ago
Selected Answer: 
B
B and D sounds feasible but gsutil is not recommended any more so definitely B.
https://cloud.google.com/storage/docs/gsutil
However, I'm not sure if D is replaced with gcloud storage cp.
upvoted 
1 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
Selected Answer: 
B
D says compress data, ¿in a single file? it will be more than the limit 5 TB of gsutil, so it is B.
upvoted 
1 
times
huuthanhdlv
huuthanhdlv
 
7 months, 1 week ago
I think the answer is B.
The main consideration is between B and D. Just thinking if they want the answer to be online transfer, they should have added
Online Transfer Service instead of gsutils. Just guessing Google must want us to choose B :)
upvoted 
1 
times
seetpt
seetpt
 
7 months, 2 weeks ago
Selected Answer: 
B
B fo sho
upvoted 
1 
timesafsarkhan
afsarkhan
 
7 months, 4 weeks ago
D will be most cost effective where as B will incur cost (question asking to consider cost effective solution as well) so D is my
answer
upvoted 
2 
times
MFay
MFay
 
8 months, 1 week ago
Selected Answer: 
B
Option B (Data Transfer appliance) is the best choice for efficient and cost-effective data migration while minimizing database
load and transfer time. This solution bypasses network limitations and reduces the impact on the on-premises environment,
making it ideal for migrating large data sets to the cloud.
upvoted 
2 
times
gbemimatti
gbemimatti
 
8 months, 2 weeks ago
Selected Answer: 
B
Compressing the data and uploading it with gsutil -m can be a good optimization for your transfer, but it has limitations to
consider:
Compression Overhead: While compressing the data can reduce upload size and potentially speed up transfer, the compression
and decompression processes themselves take time and resources. Depending on your data type, the benefit of reduced size
might be offset by the processing overhead.
Transfer Appliance: The recommended approach with the Transfer Appliance already utilizes parallel transfers for faster
uploads, potentially making gsutil -m less impactful.
I will go with B
upvoted 
2 
times
gbemimatti
gbemimatti
 
8 months, 2 weeks ago
Compressing the data and uploading it with gsutil -m can be a good optimization for your transfer, but it has limitations to
consider:
Compression Overhead: While compressing the data can reduce upload size and potentially speed up transfer, the compression
and decompression processes themselves take time and resources. Depending on your data type, the benefit of reduced size
might be offset by the processing overhead.
Transfer Appliance: The recommended approach with the Transfer Appliance already utilizes parallel transfers for faster
uploads, potentially making gsutil -m less impactful.
I will go with B
upvoted 
1 
times
342f1c6
342f1c6
 
9 months, 1 week ago
Selected Answer: 
D
with 1 Gbps it will take only 30 hrs so best option is D
upvoted 
2 
times
RajSelvaraj
RajSelvaraj
 
9 months, 3 weeks ago
Option B and D are most feasible options
Option B will be okay if the size of the data is too huge
Option D will be good for a few TBs of data. I am assuming 10 TB will fit in this case.
https://cloud.google.com/blog/topics/developers-practitioners/how-transfer-your-data-google-cloud
upvoted 
1 
times
madcloud32
madcloud32
 
10 months, 1 week ago
Selected Answer: 
B
Answer B. Cp limit is 5 TB max
upvoted 
3 
times
OrangeTiger
OrangeTiger
 
11 months ago
Selected Answer: 
D
I chose D.
According to the link below, 10TB of data can be transferred in 30h. The light blue area is the acceptable line for online
transfer.
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets?
hl=ja#online_versus_offline_transferhl=ja#online_versus_offline_transfer
upvoted 
2 
times
ccpmad
ccpmad
 
6 months, 4 weeks ago
D says compress data, in a single file? it will be more than the limit 5 TB of gsutil
upvoted 
1 
times
Pime13
Pime13
 
11 months ago
Selected Answer: 
D
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets
upvoted 
2 
times
Pime13
Pime13
 
11 months, 1 week ago
Selected Answer: 
D
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-
datasets#online_versus_offline_transfer
upvoted 
1 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Selected Answer: 
B
It is B
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #147
Your company has an enterprise application running on Compute Engine that requires high availability and high performance.
The application has been deployed on two instances in two zones in the same region in active-passive mode. The application
writes data to a persistent disk. In the case of a single zone outage, that data should be immediately made available to the
other instance in the other zone. You want to maximize performance while minimizing downtime and data loss. 
What should you do? 
A. 
1. Attach a persistent SSD disk to the first instance. 2. Create a snapshot every hour. 3. In case of a zone outage,
recreate a persistent SSD disk in the second instance where data is coming from the created snapshot.
B. 
1. Create a Cloud Storage bucket. 2. Mount the bucket into the first instance with gcs-fuse. 3. In case of a zone outage,
mount the Cloud Storage bucket to the second instance with gcs-fuse.
C. 
1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the
other instance. 
Most Voted
D. 
1. Attach a local SSD to the first instance disk. 2. Execute an rsync command every hour where the target is a persistent
SSD disk attached to the second instance. 3. In case of a zone outage, use the second instance.
Correct Answer:
 
C 
Comments
juma_david
juma_david
 
Highly Voted
 
3 years, 4 months ago
Answer C
https://cloud.google.com/compute/docs/disks/repd-failover
upvoted 
44 
times
[Removed]
[Removed]
 
Highly Voted
 
3 years, 2 months ago
C is right answer. 
C. 1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other
instance.
gcs-fuse is slower than of regional SSD PD. 
**** Admin: You need to correct lots of questions. Some of the marked answers are nonsense, these must be revisited based on
experts comments.
Community vote distribution
C (88%)
B (12%)upvoted 
42 
times
Sephethus
Sephethus
 
Most Recent
 
6 months, 2 weeks ago
Selected Answer: 
B
BigQuery cannot use customer supplied KMs keys only customer managed keys. The other options add too much complexity to
the problem.
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
Somehow I commented on the wrong answer please delete.
upvoted 
1 
times
afsarkhan
afsarkhan
 
7 months, 4 weeks ago
Selected Answer: 
C
C makes a better sense than any other option
upvoted 
1 
times
dija123
dija123
 
8 months, 2 weeks ago
Selected Answer: 
C
Agree with Regional SSD persistent
upvoted 
1 
times
[Removed]
[Removed]
 
1 year ago
C
In the event that the primary zone fails, you can fail over your regional Persistent Disk volume to a VM in another zone by using
a force-attach operation. When there's a failure in the primary zone, you might not be able to detach the disk from the VM
because the VM can't be reached to perform the detach operation. Force-attach operation lets you attach a regional Persistent
Disk volume to a VM even if that volume is attached to another VM
upvoted 
2 
times
RaviRS
RaviRS
 
1 year, 3 months ago
Selected Answer: 
C
I don't get why B has been given as answer... GCS-FUSE brings in additional complexity and it also doesn't serve the same
purpose as effectively as regional SSD does.
upvoted 
1 
times
DS2023
DS2023
 
1 year, 6 months ago
Selected Answer: 
C
Ans: C, please check - https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk
upvoted 
1 
times
dbsmk
dbsmk
 
1 year, 9 months ago
https://cloud.google.com/compute/docs/disks/repd-failover
Seems C is correct
upvoted 
1 
times
examch
examch
 
1 year, 12 months ago
C is the correct answer,
https://cloud.google.com/compute/docs/disks/repd-failover#zonal_failures
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
C
Answer is C
upvoted 
1 
timeszetalexg
zetalexg
 
2 years ago
Admins please take some time and redo the answers, put them to match at least the most voted ones, would help a lot.
upvoted 
7 
times
ashrafh
ashrafh
 
2 years, 1 month ago
Selected Answer: 
C
Regional persistent disk is a storage option that provides synchronous replication of data between two zones in a region.
Regional persistent disks can be a good building block to use when you implement HA services in Compute Engine.
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
C is the right answer
upvoted 
1 
times
Nirca
Nirca
 
2 years, 3 months ago
Selected Answer: 
C
You want to maximize performance while minimizing downtime and data loss
upvoted 
1 
times
RitwickKumar
RitwickKumar
 
2 years, 4 months ago
Selected Answer: 
C
Inline with the current architecture itself "The application writes data to a persistent disk."
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #148
You are designing a Data Warehouse on Google Cloud and want to store sensitive data in BigQuery. Your company requires you
to generate the encryption keys outside of Google Cloud. You need to implement a solution. What should you do? 
A. 
Generate a new key in Cloud Key Management Service (Cloud KMS). Store all data in Cloud Storage using the customer-
managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new
BigQuery dataset.
B. 
Generate a new key in Cloud KMS. Create a dataset in BigQuery using the customer-managed key option and select the
created key.
C. 
Import a key in Cloud KMS. Store all data in Cloud Storage using the customer-managed key option and select the
created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset.
D. 
Import a key in Cloud KMS. Create a dataset in BigQuery using the customer-supplied key option and select the created
key. 
Most Voted
Correct Answer:
 
D 
Comments
alexandercamachop
alexandercamachop
 
Highly Voted
 
2 years, 3 months ago
Selected Answer: 
D
The answer is easy. It says keys must be left outside of Google Cloud.
This automatically eliminates A / B.
Now the C option says decrypts before storing it in BigQuery which the point is to encrypt the data while been in BigQuery, D is the
only possible answer.
upvoted 
17 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
Except that BigQuery doesn't support customer supplied keys outside of GCP.
upvoted 
3 
times
SweetieS
SweetieS
 
Highly Voted
 
3 years, 4 months ago
D is OK
Community vote distribution
D (73%)
C (21%)
B (6%)upvoted 
17 
times
SR23222
SR23222
 
1 year, 6 months ago
But CSEK is not supported in BigQuery
upvoted 
2 
times
[Removed]
[Removed]
 
1 year, 4 months ago
It is a tricky distinction because of the term collision.
However, "import key to KMS" does not mean CSEK.
CSEK does not get imported or stored in KMS at all. 
CSEK "customer supplied" is per-transaction uploaded by every API call by
the user/client (no KMS). 
This situation "customer supplied" means created from non-GCP KMS (could be on-prem or EKM). 
Once a key is imported to
KMS it is treated as CMEK. 
The API client calling GCS doesn't need to upload the key. 
It lives in KMS. 
That is not the same "per-
transaction" upload as CSEK.
upvoted 
2 
times
[Removed]
[Removed]
 
1 year, 4 months ago
I mean after being imported to KMS you key is handled like a CMEK and available to BQ service.
upvoted 
1 
times
25lion52
25lion52
 
Most Recent
 
3 months, 1 week ago
Selected Answer: 
D
C - won't encrypt data in BQ with customer key.
A,B - you will generate key inside the GCP (what is also wrong by requirements)
D - looks good, but say to select CSEK... but after importing the key to KMS it becomes a customer-managed.
I would select the D
upvoted 
2 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
Selected Answer: 
B
The answer cannot be D since BigQuery does not support customer provided keys, only customer managed keys generated in
Cloud KMS. So B is the only viable option that doesn't add complexity.
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
It cannot be D, BigQuery does not support customer supplied KMS keys, only customer managed keys, B.
upvoted 
1 
times
odacir
odacir
 
1 year, 1 month ago
Selected Answer: 
D
https://cloud.google.com/bigquery/docs/customer-managed-encryption
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
A, B, C are ruled out as 
they say Customer Managed keys.
Hence, D.
upvoted 
2 
times
devnul
devnul
 
1 year, 4 months ago
GCP docu says "BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK)." 
However, I just tested it and it worked:
1. Create Key
openssl rand 32 > ./key2
2. Import into KMS
gcloud kms keys versions import --import-job csek1 --location us-west1 --keyring csek --key csek --algorithm google-symmetric-
encryption --target-key-file ./key23. In Cloud Console: select the key when creating a new data set and table in BigQuery
upvoted 
4 
times
[Removed]
[Removed]
 
1 year, 4 months ago
Right, term collision with "customer supplied" key. 
However, "import key to KMS" does not mean CSEK.
upvoted 
2 
times
jits1984
jits1984
 
1 year, 8 months ago
Selected Answer: 
C
C - as BigQuery doesn't support Customer Supplier Keys.
upvoted 
2 
times
n_nana
n_nana
 
1 year, 9 months ago
Selected Answer: 
C
BigQuery doesn't support CSEK
upvoted 
1 
times
medi01
medi01
 
1 year, 8 months ago
BG DOES support CSEK.
upvoted 
1 
times
n_nana
n_nana
 
1 year, 9 months ago
Sorry even C is not correct, why to store the data in bq without encryption.
data should be passed encrypted from storage to bq.
then Answer is B
upvoted 
1 
times
A21325412
A21325412
 
1 year, 2 months ago
I would go with C.
https://cloud.google.com/bigquery/docs/customer-managed-encryption
Read that document in the link carefully.
1st paragraph: "By Default, BigQuery 
encrypts your content stored at rest";
1st bullet point, 2nd paragraph under the [Before you Begin] section: "BigQuery and BigLake tables don't support Customer-
Supplied Encryption Keys (CSEK)"
There is also a difference between CMEK and CSEK.
CMEK: you can create and manage a key using Cloud KMS;
CSEK: you specify the contents of the key;
Ref for CMEK vs CSEK:
https://cloud.google.com/sql/docs/mysql/cmek#:~:text=Note%3A%20Customer%2Dmanaged%20encryption%20keys,specific
%20resources%20across%20Google%20Cloud.
upvoted 
1 
times
A21325412
A21325412
 
1 year, 2 months ago
Even though I'll chose C for the answer over D, because of the terminology in "BQ using customer-supplied key", I have an
issue with this: 
To me it does not make any sense.
The data is being encrypted by some key say K1 to store in Cloud Storage, then Decrypted, to be Re-Encrypted (automatically
by say K2 [a google created key]) by BigQuery when being stored. This negates the use of K1 on your Data Storage in
BigQuery. 
It makes no sense. If someone sees this differently, I'd love to hear it. Thanks.
upvoted 
1 
times
nandoD
nandoD
 
1 year, 8 months ago
If you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. 
https://cloud.google.com/bigquery/docs/customer-managed-encryptionhttps://cloud.google.com/bigquery/docs/customer-managed-encryption
upvoted 
1 
times
SR23222
SR23222
 
1 year, 6 months ago
There is a difference between customer managed and customer supplied. Link that you have shared talks about customer
managed and not customer supplied
upvoted 
1 
times
AugustoKras011111
AugustoKras011111
 
1 year, 10 months ago
Selected Answer: 
D
Key work: "keys outside of Google Cloud" so you have to import the key. between C and D I go with D.
upvoted 
1 
times
smachnio
smachnio
 
1 year, 11 months ago
Selected Answer: 
D
D is correct. I had this question on the exam toaday and I go with D.
Explanation is - Generate the key outside the GCP so C and D are correct.
"Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset" is not correct becuase it means that data
exist on GCP what is not correct. Only D is correct.
upvoted 
2 
times
examch
examch
 
1 year, 12 months ago
Selected Answer: 
C
Yes, BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK). Answer must be either A or C,
since the say generate key outside Google Cloud, import the key, hence I go for the answer C.
https://cloud.google.com/bigquery/docs/customer-managed-encryption#before_you_begin
https://cloud.google.com/kms/docs/importing-a-key
upvoted 
2 
times
NodummyIQ
NodummyIQ
 
2 years ago
Answer D is incorrect because BigQuery does not support the use of customer-supplied keys to encrypt data at rest. Instead, you
can use customer-managed encryption keys in Cloud KMS to encrypt the data in BigQuery. To do this, you can either generate a
new key in Cloud KMS (answer A) or import an existing key (answer C). Once you have a key in Cloud KMS, you can create a
BigQuery dataset and select the key as the customer-managed key for the dataset. This will enable BigQuery to use the key to
encrypt the data in the dataset.
upvoted 
4 
times
examch
examch
 
1 year, 12 months ago
Yes, BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK). Answer must be either A or C,
since the say generate key outside Google Cloud, import the key, hence I go for the answer C.
upvoted 
1 
times
examch
examch
 
1 year, 12 months ago
https://cloud.google.com/bigquery/docs/customer-managed-encryption#before_you_begin
upvoted 
1 
times
examch
examch
 
1 year, 12 months ago
https://cloud.google.com/kms/docs/importing-a-key
upvoted 
2 
times
[Removed]
[Removed]
 
1 year, 4 months ago
You have to know the difference between CSEK and "imported keys to KMS". 
Those are not the same things. 
CSEK is never
stored in KMS, obviously an imported key is. 
It is then as available as any CMEK to BQ.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
D
Correct answer is DCorrect answer is D
upvoted 
1 
times
ale_brd_111
ale_brd_111
 
2 years, 1 month ago
Selected Answer: 
D
answer 
is D
https://cloud.google.com/bigquery/docs/customer-managed-encryption
upvoted 
1 
times
tomahawk003
tomahawk003
 
2 years, 1 month ago
Answer D.
Questions says "...design data warehouse..." - would prefer BigQuery
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #149
Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to
rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow
Google-recommended practices for security. What should you do? 
A. 
Create a key with Cloud Key Management Service (KMS). Encrypt the data using the encrypt method of Cloud KMS.
B. 
Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.
Most Voted
C. 
Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.
D. 
Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys
feature.
Correct Answer:
 
B 
Comments
victory108
victory108
 
Highly Voted
 
2 years, 10 months ago
B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.
upvoted 
32 
times
SweetieS
SweetieS
 
Highly Voted
 
2 years, 10 months ago
B is OK
https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys#add-object-key
upvoted 
9 
times
Pime13
Pime13
 
Most Recent
 
5 months ago
Selected Answer: 
B
https://cloud.google.com/storage/docs/encryption/customer-managed-keys#key-rotation
upvoted 
2 
times
Roro_Brother
Roro_Brother
 
6 months, 3 weeks ago
Selected Answer: 
B
Community vote distribution
B (87%)
Other (13%)It's B, off course
upvoted 
1 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
B
https://cloud.google.com/storage/docs/encryption/customer-managed-keys#key-rotation
upvoted 
1 
times
vc1011
vc1011
 
8 months, 3 weeks ago
Selected Answer: 
B
The following restrictions apply when using customer-managed encryption keys:
You cannot encrypt an object with a customer-managed encryption key by updating the object's metadata. Include the key as part
of a rewrite of the object instead.
gcloud storage uses the objects update command to set encryption keys on objects, but the command rewrites the object as part
of the request.
this makes rotating keys difficult
upvoted 
2 
times
someone2011
someone2011
 
10 months, 1 week ago
Probably B:
https://cloud.google.com/storage/docs/encryption/customer-managed-keys#key-replacement
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
It says customer wants to manage the rotation not the supplying of key. 
Hence B not D. 
Seen some people say with customer
managed you cannot rotate but this document suggests you can https://cloud.google.com/storage/docs/encryption/customer-
managed-keys#key-rotation.
upvoted 
1 
times
jlambdan
jlambdan
 
1 year, 3 months ago
B does not allow to rotate assymetric key.
https://cloud.google.com/kms/docs/key-rotation
=> Cloud Key Management Service does not support automatic rotation of asymmetric keys. See Considerations for asymmetric
keys below.
I go for D.
upvoted 
1 
times
medi01
medi01
 
1 year, 2 months ago
GC uses symmetric key.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 3 months ago
Selected Answer: 
B
B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.
To rotate the encryption key used to encrypt data in a Cloud Storage bucket, it is recommended to use Cloud KMS. You can create
a new key version, set it as the primary version, and update the bucket's default KMS key to the new key version. This allows you
to rotate the encryption key while still allowing access to the data. You can then process the data in Dataproc while the encryption
key is being rotated. This approach provides security and compliance with regulations, as well as easy key rotation without
disrupting access to data.
upvoted 
4 
times
JC0926
JC0926
 
1 year, 3 months ago
Selected Answer: 
B
Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to
rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow
Google-recommended practices for security. What should you do?
A. Create a key with Cloud Key Management Service (KMS). Encrypt the data using the encrypt method of Cloud KMS.
B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.
C. Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.
D. Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature.
upvoted 
1 
times
examch
examch
 
1 year, 6 months ago
Selected Answer: 
B
B is the correct answer, we can encrypt the data in the bucket using CMEK. And the key can be rotated as per requirement.
https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys#add-object-key
https://cloud.google.com/storage/docs/samples/storage-rotate-encryption-key#storage_rotate_encryption_key-python
upvoted 
1 
times
nhorcajada
nhorcajada
 
1 year, 7 months ago
Selected Answer: 
C
B is ok
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
KongsMom
KongsMom
 
1 year, 8 months ago
B. rotation and dataproc ... trendmicro talk about this in https://www.trendmicro.com/cloudoneconformity/knowledge-
base/gcp/Dataproc/enable-encryption-with-cmks-for-dataproc-clusters.html
Ensure that your Google Cloud Dataproc clusters on Compute Engine are encrypted with Customer-Managed Keys (CMKs) in
order to control the cluster data encryption/decryption process. You can create and manage your own Customer-Managed Keys
(CMKs) with Cloud Key Management Service (Cloud KMS). Cloud KMS provides secure and efficient encryption key management,
controlled key rotation, and revocation mechanisms.
This rule resolution is part of the Conformity Security & Compliance tool for GCP.
upvoted 
1 
times
RitwickKumar
RitwickKumar
 
1 year, 10 months ago
Selected Answer: 
B
As per question: " your company must be able to rotate the encryption key"
It is easily possible with KMS: 
https://cloud.google.com/kms/docs/rotating-keys#kms-create-key-rotation-schedule-gcloud
upvoted 
3 
times
Ric350
Ric350
 
1 year, 11 months ago
"Your company must be able to rotate the encryption key" is the requirement which eliminates CMEK and why you need a CSEK.
You have to use a boto config file to do this and is part of one of the labs.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #150
Your team needs to create a Google Kubernetes Engine (GKE) cluster to host a newly built application that requires access to
third-party services on the internet. 
Your company does not allow any Compute Engine instance to have a public IP address on Google Cloud. You need to create a
deployment strategy that adheres to these guidelines. What should you do? 
A. 
Configure the GKE cluster as a private cluster, and configure Cloud NAT Gateway for the cluster subnet. 
Most Voted
B. 
Configure the GKE cluster as a private cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).
C. 
Configure the GKE cluster as a route-based cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).
D. 
Create a Compute Engine instance, and install a NAT Proxy on the instance. Configure all workloads on GKE to pass
through this proxy to access third-party services on the Internet.
Correct Answer:
 
A 
Comments
ACE_ASPIRE
ACE_ASPIRE
 
Highly Voted
 
3 years, 3 months ago
Cloud NAT is the correct answer
upvoted 
32 
times
RitwickKumar
RitwickKumar
 
Highly Voted
 
2 years, 4 months ago
Selected Answer: 
A
** Admins: More than 60% of the answers you have selected are wrong. Please correct them ASAP. I must appreciate community
here for taking out time to share their perspective and help fellow learners.
"B" can never be an answer here as the Private Google Access enables internal access to Google APIs only whereas in question
the ask is "access to third-party services on the internet"
upvoted 
26 
times
ArtistS
ArtistS
 
1 year, 1 month ago
If they provide the correct answer, you will never see this website any more
upvoted 
7 
times
Community vote distribution
A (98%)
B
(2%)Sephethus
Sephethus
 
6 months, 2 weeks ago
True, but then if it were shut down literally nobody could pass this ridiculous test where half the questions are so badly worded
and confusing with debatable options.
upvoted 
2 
times
jlambdan
jlambdan
 
1 year, 9 months ago
This is most likely on purpose. Otherwise google will do something in order for the exam dump to be shutdown.
upvoted 
13 
times
19040e5
19040e5
 
Most Recent
 
7 months, 2 weeks ago
Selected Answer: 
A
Cloud NAT, Private Service Connect is for Google API Access.
upvoted 
1 
times
kahinah
kahinah
 
9 months, 4 weeks ago
Selected Answer: 
A
Cloud NAT to access to the internet
upvoted 
1 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Selected Answer: 
A
It is A
upvoted 
1 
times
techtitan
techtitan
 
1 year, 1 month ago
Selected Answer: 
A
Needs Nat to connect to 3rd party apps
upvoted 
1 
times
6b13108
6b13108
 
1 year, 1 month ago
B is only part of the solution, but needs Cloud Nat to get access on the internet with third-party services, then the correct answer is
A . See doc:
https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
upvoted 
1 
times
tamj123
tamj123
 
1 year, 2 months ago
Selected Answer: 
A
go for Cloud NAT
upvoted 
1 
times
RaviRS
RaviRS
 
1 year, 3 months ago
Selected Answer: 
A
I am not sure who's writing these answers
Private Google Access is useful for allowing Google Cloud resources, including GKE clusters, to access Google services without
public IPs, but it doesn't provide access to third-party services on the internet.
upvoted 
2 
times
[Removed]
[Removed]
 
1 year, 6 months ago
Selected Answer: 
A
Cloud NAT A
upvoted 
1 
times
DS2023
DS2023
 
1 year, 7 months ago
Selected Answer: 
A
Cloud NAT allows the resources in private subnet to access the internet—for updates, patching, config management, and more—
in a controlled and efficient manner.
upvoted 
1 
timesupvoted 
1 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
Yeah agree as GKE admin
upvoted 
1 
times
DS2023
DS2023
 
1 year, 7 months ago
Selected Answer: A. Cloud NAT allows the resources in private subnet to access the internet—for updates, patching, config
management, and more—in a controlled and efficient manner.
upvoted 
1 
times
dbsmk
dbsmk
 
1 year, 9 months ago
A.
https://cloud.google.com/kubernetes-engine/docs/how-to/private-
clusters#workloads_on_private_clusters_unable_to_access_internet
upvoted 
3 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
B
Private Google Access allows resources in a VPC network to access Google Cloud services without an external IP address. By
configuring the GKE cluster as a private cluster, the nodes and services inside the cluster will not have a public IP address, and
only resources within the VPC network will be able to communicate with them. With Private Google Access enabled, the GKE
cluster can access third-party services on the internet via Google APIs and services without requiring a public IP address.
Therefore, the correct option is:
B. Configure the GKE cluster as a private cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).
upvoted 
1 
times
r1ck
r1ck
 
1 year, 10 months ago
answer should be "B"
https://cloud.google.com/vpc/docs/private-access-options
upvoted 
2 
times
examch
examch
 
1 year, 12 months ago
Selected Answer: 
A
A is the correct answer,
Granting private nodes outbound internet access
To provide outbound internet access for your private nodes, such as to pull images from an external registry, use Cloud NAT to
create and configure a Cloud Router. Cloud NAT lets private clusters establish outbound connections over the internet to send and
receive packets.
The Cloud Router allows all your nodes in the region to use Cloud NAT for all primary and alias IP ranges. It also automatically
allocates the external IP addresses for the NAT gateway.
For instructions to create and configure a Cloud Router, refer to Create a Cloud NAT configuration using Cloud Router in the Cloud
NAT documentation.
https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#private-nodes-outbound
upvoted 
4 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
A
A is the correct answer
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #151
Your company has a support ticketing solution that uses App Engine Standard. The project that contains the App Engine
application already has a Virtual Private 
Cloud (VPC) network fully connected to the company's on-premises environment through a Cloud VPN tunnel. You want to
enable the App Engine application to communicate with a database that is running in the company's on-premises environment.
What should you do? 
A. 
Configure private Google access for on-premises hosts only.
B. 
Configure private Google access.
C. 
Configure private services access.
D. 
Configure serverless VPC access. 
Most Voted
Correct Answer:
 
D 
Comments
Roncy
Roncy
 
Highly Voted
 
2 years, 3 months ago
D is right , refer to https://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases
upvoted 
42 
times
cloudguy2
cloudguy2
 
2 years, 1 month ago
D) is correct. Use case example: Your serverless environment needs to access data from your on-premises database through
Cloud VPN.
upvoted 
10 
times
Besss
Besss
 
Highly Voted
 
2 years, 3 months ago
D. Configuring serverless VPC access App Engine can connect to the VPC and then through VPN tunnel to the on-prem DB
upvoted 
18 
times
PKKim
PKKim
 
Most Recent
 
1 month, 1 week ago
The answer is D.
The option B is for the other way. The option B is for the on-premise services to be able to use Google API through VPN
upvoted 
3 
times
Community vote distribution
D (92%)
B (8%)upvoted 
3 
times
theBestStudent
theBestStudent
 
1 month, 2 weeks ago
Selected Answer: 
D
Answer is D. Here the explanation since I didn't see any good answer:
1- 
We have a VPC.
2- 
We have an onpremisses DB.
3- We have App Engine (that runs on a isolated network that does not belong to the VPC).
4- We can connect the VPC to the onpremisses network using Cloud VPN, which is the main purpose of Cloud VPN (let's say to
simplify this answer).
5 - Now how we connect the AppEngine that is isolated from the VPC and needs to use "something" to reach out the
onpremisses DB directly (no public ip, only private ip)? Here we will have to have somehow access to the VPC and then the VPN
and then the on premisses DB. That is the serverless vpc access.
6- 
So flow can be something like app engine --> serverless vpc access --> cloud VPN ---> on premessises db through private
ip.
upvoted 
12 
times
odacir
odacir
 
1 month, 2 weeks ago
Selected Answer: 
D
D:
Use cases
...
Your serverless environment needs to access data from your on-premises database through Cloud VPN.
https://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases
upvoted 
1 
times
thewalker
thewalker
 
1 month, 3 weeks ago
Selected Answer: 
D
Read this article: https://cloud.google.com/vpc/docs/serverless-vpc-access
That makes me conclude for D.
upvoted 
1 
times
Prakzz
Prakzz
 
3 months ago
It's App Engine Standard and VPN cannot be used with Standard version
upvoted 
1 
times
RaviRS
RaviRS
 
3 months, 4 weeks ago
Selected Answer: 
D
That's the whole purpose of serverless google access
upvoted 
1 
times
sampon279
sampon279
 
6 months, 1 week ago
Selected Answer: 
D
Private google service and private google access seem to provide same level of access:
https://googlecloudarchitect.us/private-service-access-vs-google-private-access/ Based on elimination both can be eliminated.
Hence D.
upvoted 
1 
times
natpilot
natpilot
 
8 months, 3 weeks ago
D is Right . You can use a Serverless VPC Access connector to let Cloud Run, App Engine standard, and Cloud Functions
environments send packets to the internal IPv4 addresses of resources in a VPC network. Serverless VPC Access also supports
sending packets to other networks connected to the selected VPC network.
upvoted 
3 
times
JC0926
JC0926
 
9 months, 2 weeks ago
Selected Answer: 
D
Private Google Access (option B) is used to enable VM instances in a VPC network to reach Google APIs and services using an
internal IP address, but it does not allow communication to on-premises resources.
Private Services Access (option C) allows you to access supported Google Cloud services through private IP addresses rather
than public IP addresses, but it does not help in communicating with on-premises resources.Configuring Private Google Access for on-premises hosts only (option A) is not a valid option as this configuration is not
available.
upvoted 
6 
times
Mohtasham9
Mohtasham9
 
10 months, 2 weeks ago
C. Configure private services access.
To enable an App Engine application to communicate with a database running in the company's on-premises environment over
a VPC network that is fully connected to the company's on-premises environment through a Cloud VPN tunnel, the
recommended approach is to use Private Service Access (PSA). Therefore, the correct answer is C. Configure private services
access.
Private Service Access (PSA) allows you to create private connections between your VPC network and services like Cloud SQL,
Cloud Storage, and other Google APIs and services. With PSA, you can access these services using their private IP addresses,
which are only accessible from within your VPC network, and not over the public internet. This provides better security and
reduces the risk of data exfiltration or unauthorized access.
upvoted 
1 
times
SLChief
SLChief
 
10 months, 4 weeks ago
D is right. Configuring serverless VPC access is the option for app engine to have Google private access
upvoted 
1 
times
[Removed]
[Removed]
 
11 months ago
Selected Answer: 
D
You can use a Serverless VPC Access connector to let Cloud Run, App Engine standard, and Cloud Functions environments send
packets to the internal IPv4 addresses of resources in a VPC network. Serverless VPC Access also supports sending packets to
other networks connected to the selected VPC network.
https://cloud.google.com/vpc/docs/private-access-options
upvoted 
1 
times
jay9114
jay9114
 
11 months ago
Upvote if there was no mention of "serverless VPC access" in the training videos and study guides you used to prepare for this
exam.
upvoted 
9 
times
GopeshSahu
GopeshSahu
 
11 months, 2 weeks ago
Selected Answer: 
B
I am surprised 95% selected option D without understanding the use case. 
Very basic ask
AppEngine ->Private Google Access->On-Prem DB 
Google Private Access to so enable any Services no matter running in VPC 
to connect to on-prem DB via VPN tunnel. 
https://cloud.google.com/vpc/docs/private-google-access-hybrid
https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid
AppEngine -> Serveless-VPV-Access -> Any GCP Resources/Services(with private IPs)
upvoted 
3 
times
jake_edman
jake_edman
 
11 months, 1 week ago
I still think it is D - the example you linked to for Private Google Access is to allow on-prem resources to contact Google
Services, not the other way round. 
https://cloud.google.com/vpc/docs/private-google-access-hybrid
But the example others link to explicitly says a use case is "Your serverless environment needs to access data from your on-
premises database through Cloud VPN
https://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases
upvoted 
2 
times
gcppandit
gcppandit
 
11 months, 1 week ago
Private Google Access provides access to Google Services via Private IP and this can be used to call the App Engine from On-
Prem. Here the usecase is exactly the opposite. Here only option to set up the Serverless VPC access to allow Serverless
components to access Private resources (including on-Prem if proper VPN is already setup)
upvoted 
3 
times
 
12 months agoexamch
examch
 
12 months ago
Selected Answer: 
D
D is the correct answer,
Serverless VPC Access
bookmark_border
Serverless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud network from serverless
environments such as Cloud Run, App Engine, or Cloud Functions. Configuring Serverless VPC Access allows your serverless
environment to send requests to your VPC network using internal DNS and internal IP addresses (as defined by RFC 1918 and
RFC 6598). The responses to these requests also use your internal network.
There are two main benefits to using Serverless VPC Access:
Requests sent to your VPC network are never exposed to the internet.
Communication through Serverless VPC Access can have less latency compared to the internet.
https://cloud.google.com/vpc/docs/serverless-vpc-access#use_case
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #152
Your company is planning to upload several important files to Cloud Storage. After the upload is completed, they want to verify
that the uploaded content is identical to what they have on-premises. You want to minimize the cost and effort of performing
this check. What should you do? 
A. 
1. Use Linux shasum to compute a digest of files you want to upload. 2. Use gsutil -m to upload all the files to Cloud
Storage. 3. Use gsutil cp to download the uploaded files. 4. Use Linux shasum to compute a digest of the downloaded files.
5. Compare the hashes.
B. 
1. Use gsutil -m to upload the files to Cloud Storage. 2. Develop a custom Java application that computes CRC32C
hashes. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the
hashes.
C. 
1. Use gsutil -m to upload all the files to Cloud Storage. 2. Use gsutil cp to download the uploaded files. 3. Use Linux diff
to compare the content of the files.
D. 
1. Use gsutil -m to upload the files to Cloud Storage. 2. Use gsutil hash -c FILE_NAME to generate CRC32C hashes of all
on-premises files. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4.
Compare the hashes. 
Most Voted
Correct Answer:
 
D 
Comments
vladik820
vladik820
 
Highly Voted
 
2 years, 4 months ago
D is ok .
https://cloud.google.com/storage/docs/gsutil/commands/hash
upvoted 
39 
times
Bahubali1988
Bahubali1988
 
Highly Voted
 
1 year, 3 months ago
Seems most of the questions are having wrong answers.. If there is no discussion , its highly difficult to get the right answers.
upvoted 
17 
times
tamj123
tamj123
 
Most Recent
 
2 months, 2 weeks ago
Selected Answer: 
D
Community vote distribution
D (95%)
C (5%)created hash and compare after is way to go.
upvoted 
2 
times
RaviRS
RaviRS
 
3 months, 4 weeks ago
Selected Answer: 
D
I am losing faith on the answers given... Option C is downright absurd.
upvoted 
1 
times
Jerar
Jerar
 
5 months ago
Selected Answer: 
D
https://cloud.google.com/storage/docs/gsutil/commands/hash
Calculate hashes on local files, which can be used to compare with gsutil ls -L output.
-c
Calculate a CRC32c hash for the specified files.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
6 months, 3 weeks ago
Downloading before hashing cannot be right. 
The upload might be fine but if the download could corrupt
upvoted 
1 
times
[Removed]
[Removed]
 
7 months, 2 weeks ago
The correct answer should be D: https://cloud.google.com/storage/docs/gsutil/commands/hash
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year ago
Selected Answer: 
D
D is the correct answer
upvoted 
2 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
Nuwan_SriLanka
Nuwan_SriLanka
 
1 year, 2 months ago
Selected Answer: 
D
Calculate hashes on local files, which can be used to compare with gsutil ls -L output. If a specific hash option is not provided,
this command calculates all gsutil-supported hashes for the files.
Note that gsutil automatically performs hash validation when uploading or downloading files, so this command is only needed
if you want to write a script that separately checks the hash.
If you calculate a CRC32c hash for files without a precompiled crcmod installation, hashing will be very slow. See gsutil help
crcmod for details.
https://cloud.google.com/storage/docs/gsutil/commands/hash
upvoted 
5 
times
Mahmoud_E
Mahmoud_E
 
1 year, 2 months ago
Selected Answer: 
D
D is the right answer per this doc https://cloud.google.com/storage/docs/gsutil/commands/hash
upvoted 
1 
times
Jay_Krish
Jay_Krish
 
1 year, 3 months ago
Selected Answer: 
C
All those who answered D.. can one of you if you're genuine tell how is this even possible - The second step in the option D?
2. Use gsutil hash -c FILE_NAME to generate CRC32C hashes of all on-premises files.
upvoted 
2 
times
Jay_Krish
Jay_Krish
 
1 year, 3 months agoJay_Krish
Jay_Krish
 
1 year, 3 months ago
Reading again it's probably not C because it talks about Linux commands but what if the environment is Windows.. 
but I still have my doubts on D if someone could clarify?
upvoted 
2 
times
binpan
binpan
 
1 year, 5 months ago
Correct Answer C
A- digest comparison does not gurantee file contents are same. moreover lot of extra steps. - not correct
B - custom Java code - lot of effort - not correct
D - gs util cannot be used for creating hash for on prem files stored on on prem filestore/database. Not correct
C - not the best option but right answer for the options available.
upvoted 
2 
times
luamail
luamail
 
1 year, 2 months ago
dowload file has cost, C no is a option
upvoted 
1 
times
SIMMEAT
SIMMEAT
 
1 year, 4 months ago
there is a hash options in gsutil for local files.
https://cloud.google.com/storage/docs/gsutil/commands/hash
upvoted 
2 
times
kaito789
kaito789
 
1 year, 5 months ago
D is correct. you only need gs util to generate hash for cloud storage. you would use your own utility to create ash for on prem
and then compare the two.
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 6 months ago
D is correct , there is no need to build custom java script.
upvoted 
2 
times
Superr
Superr
 
1 year, 7 months ago
Selected Answer: 
D
D seems valid
upvoted 
1 
times
amxexam
amxexam
 
1 year, 7 months ago
Selected Answer: 
D
I am eliminating tedious approaches that is downloading and doing custom coding so A B C are eliminated.
D is the solution.
upvoted 
1 
times
cmamiusa
cmamiusa
 
1 year, 8 months ago
Selected Answer: 
D
D makes sense
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #153
You have deployed an application on Anthos clusters (formerly Anthos GKE). According to the SRE practices at your company,
you need to be alerted if request latency is above a certain threshold for a specified amount of time. What should you do? 
A. 
Install Anthos Service Mesh on your cluster. Use the Google Cloud Console to define a Service Level Objective (SLO),
and create an alerting policy based on this SLO. 
Most Voted
B. 
Enable the Cloud Trace API on your project, and use Cloud Monitoring Alerts to send an alert based on the Cloud Trace
metrics.
C. 
Use Cloud Profiler to follow up the request latency. Create a custom metric in Cloud Monitoring based on the results of
Cloud Profiler, and create an Alerting policy in case this metric exceeds the threshold.
D. 
Configure Anthos Config Management on your cluster, and create a yaml file that defines the SLO and alerting policy
you want to deploy in your cluster.
Correct Answer:
 
A 
Comments
vladik820
vladik820
 
Highly Voted
 
2 years, 4 months ago
A is ok. 
https://cloud.google.com/service-mesh/docs/observability/slo-overview
upvoted 
25 
times
Ouss_123
Ouss_123
 
Most Recent
 
6 days, 8 hours ago
Selected Answer: 
A
Cloud Service Mesh displays a Latency graph on the Metrics page for each of your services. The Latency graph shows you the
latency over time, which can help you determine a latency threshold or upper bound for a service.
https://cloud.google.com/service-mesh/docs/observability/slo-overview
upvoted 
1 
times
cchiaramelli
cchiaramelli
 
2 months, 1 week ago
Selected Answer: 
B
"Google Cloud Console to define a Service Level Objective (SLO)" seems odd, B doesn't seem wrong
Community vote distribution
A (78%)
B (22%)"Google Cloud Console to define a Service Level Objective (SLO)" seems odd, B doesn't seem wrong
upvoted 
4 
times
tamj123
tamj123
 
2 months, 2 weeks ago
Answer A looks correct
upvoted 
1 
times
examch
examch
 
12 months ago
Selected Answer: 
A
Cloud Monitoring can trigger an alert when a Service is on track to violate an SLO. You can create an alerting policy based on
the rate of consumption of your error budget. All alerts on error budgets have the same basic condition: a specified percentage
of the error budget for the compliance period is consumed in a lookback period, which is a time period, such as the previous 60
minutes. When you create the alerting policy, Anthos Service Mesh automatically sets most of the conditions for the alert based
on the settings in the SLO. You specify the lookback period and the consumption percentage.
https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo
upvoted 
4 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
2 
times
Jay_Krish
Jay_Krish
 
1 year, 3 months ago
Selected Answer: 
A
A seems correct
upvoted 
2 
times
RitwickKumar
RitwickKumar
 
1 year, 4 months ago
Selected Answer: 
A
https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo
upvoted 
2 
times
igor_nov1
igor_nov1
 
1 year, 5 months ago
Use the Google Cloud Console to define a Service Level Objective (SLO)
WAAAAT ?
How Console help you to define SLO?
upvoted 
1 
times
AMohanty
AMohanty
 
1 year, 5 months ago
Specific Purpose of Cloud Trace API is to get info regarding Latency.
Would go with B.
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 6 months ago
A is right
upvoted 
1 
times
sivre
sivre
 
1 year, 9 months ago
Why not B....
Cloud Trace is a distributed tracing system that collects latency data from the applications and displays it in near real-time. It
allows you to follow a sample request through your distributed system, observe the network calls and profile your system end
to end.
Note that Cloud Trace is disabled by default.
The Anthos Service Mesh pages provide a link to the traces in the Cloud Trace page in the Cloud Console.
https://cloud.google.com/service-mesh/docs/observability/accessing-traces 
In Anthos clusters you need to install Anthos service mesh? From this link you need to install it only on GKE and on-premises
platforms
https://cloud.google.com/service-mesh/docs/observability/accessing-traces
upvoted 
3 
times
 
1 year, 6 months agoryzior
ryzior
 
1 year, 6 months ago
I think A is about monitoring and alerting without any further investigation, while Trace is for finding the root cause/detective
purposes, when you look into a call and track this call step by step through each endpoint, the call is going through.
upvoted 
3 
times
kimharsh
kimharsh
 
1 year, 7 months ago
Can you create an Alert when you use Cloud Trace?
upvoted 
2 
times
Shawnn
Shawnn
 
9 months, 3 weeks ago
yep, you can
upvoted 
2 
times
[Removed]
[Removed]
 
1 year, 10 months ago
I got same question on my exam.
upvoted 
2 
times
haroldbenites
haroldbenites
 
1 year, 10 months ago
Go for A
upvoted 
1 
times
technodev
technodev
 
1 year, 11 months ago
Got this question in my exam, answered A
upvoted 
4 
times
vincy2202
vincy2202
 
2 years ago
Selected Answer: 
A
A is the correct answer
upvoted 
2 
times
nqthien041292
nqthien041292
 
2 years, 1 month ago
Selected Answer: 
A
Vote A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #154
Your company has a stateless web API that performs scientific calculations. The web API runs on a single Google Kubernetes
Engine (GKE) cluster. The cluster is currently deployed in us-central1. Your company has expanded to offer your API to
customers in Asia. You want to reduce the latency for users in Asia. 
What should you do? 
A. 
Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the
public IPs to the Cloud DNS zone.
B. 
Use a global HTTP(s) load balancer with Cloud CDN enabled.
C. 
Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer. 
Most Voted
D. 
Increase the memory and CPU allocated to the application in the cluster.
Correct Answer:
 
C 
Comments
vladik820
vladik820
 
Highly Voted
 
2 years, 10 months ago
C is ok .
https://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci
upvoted 
36 
times
rishab86
rishab86
 
2 years, 8 months ago
After going through the link I feel its C
upvoted 
3 
times
mikesp
mikesp
 
2 years, 8 months ago
Mee too. 
CDN does not make sense
upvoted 
6 
times
bandegg
bandegg
 
6 months, 1 week ago
Indeed. We don't know if the API is authenticated, reveals private data, static or not.
Community vote distribution
C (68%)
A (16%)
B (16%)Indeed. We don't know if the API is authenticated, reveals private data, static or not.
upvoted 
1 
times
Lk9876
Lk9876
 
Highly Voted
 
2 years, 9 months ago
I'm not sure about C. kubemci is deprecated and is not part anymore of cloud sdk in favor of ingress for anthos. I'll go with A
upvoted 
11 
times
MikeB19
MikeB19
 
2 years, 9 months ago
I think either a or c is correct. I chose c base on the article ref in the chat. Do u have supporting article ref kubemci is deprecated?
I also found some chatter about kubemci being deprecated but couldn’t find anything offical
upvoted 
1 
times
Linus11
Linus11
 
2 years, 9 months ago
It is hee -- https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress
upvoted 
1 
times
Rzla
Rzla
 
2 years, 9 months ago
Problem with A is that a service load bancer is not l7 https. 
The question is outdated, the answer will have been C. 
Now it would
be Anthos multi cluster ingress -https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress
upvoted 
14 
times
cotam
cotam
 
2 years, 8 months ago
That's actually not true. Service of type: LoadBalancer, is a service from "K8s" point of view, which creates 
L7 HTTP(S) Load
Balancer.
upvoted 
5 
times
huuthanhdlv
huuthanhdlv
 
1 month, 1 week ago
Nope, service is L4 Network/Internal load balancer
upvoted 
1 
times
dija123
dija123
 
2 months, 2 weeks ago
Agree with you
upvoted 
1 
times
gcloud007
gcloud007
 
Most Recent
 
2 days, 18 hours ago
Selected Answer: 
C
C is correct .. https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress
upvoted 
1 
times
svkds
svkds
 
1 month, 3 weeks ago
Selected Answer: 
B
To reduce latency for users in Asia while maintaining high availability and scalability, the most appropriate option would be:
B. Use a global HTTP(s) load balancer with Cloud CDN enabled.
upvoted 
2 
times
Diwz
Diwz
 
3 months ago
Selected Answer: 
B
B is answer
upvoted 
2 
times
Pime13
Pime13
 
5 months, 1 week ago
Selected Answer: 
C
question is old but it should be c. however currently should be multi cluster ingress
https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress
upvoted 
4 
times
AzFarid
AzFarid
 
6 months, 1 week agoAzFarid
AzFarid
 
6 months, 1 week ago
really powefull 
kubernetes-engine-with-kubemci
upvoted 
1 
times
theBestStudent
theBestStudent
 
7 months, 2 weeks ago
Well it should be C, but it is deprecated in favor of ingress for Anthos as can be read here
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress
upvoted 
2 
times
tamj123
tamj123
 
8 months, 2 weeks ago
go for C
upvoted 
1 
times
RaviRS
RaviRS
 
10 months ago
Selected Answer: 
C
B is funny
upvoted 
2 
times
Vignesh_Krishnamurthi
Vignesh_Krishnamurthi
 
12 months ago
If it is an API performing scientific calculations then its customer base is a very specific targeted group. 
It should not be considered
as a mass market app that is used by lots of people all over the world. 
Considering the business purpose of the API, option B
would be more than sufficient to serve the need of customers anywhere in the world.
upvoted 
1 
times
kapara
kapara
 
12 months ago
IDK if this question will be in the exam bc the answer should be C.
but kubemci has now been deprecated in favor of Ingress for Anthos. 
Ingress for Anthos is the recommended way to deploy multi-cluster ingress.
upvoted 
3 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
The problem with B is the question very much infers we are dealing with dynamic content & not static.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
It's A or C but I think A might be better.
A is a simpler solution. 
Cloud DNS allows you to add multiple targets and part of its decision making is the latency.
https://cloud.google.com/dns/docs/zones/zones-overview
Tough but I think A because it's only one API being exposed. 
Ingress comes into its own when exposing many services.
upvoted 
1 
times
r1ck
r1ck
 
1 year, 4 months ago
kubemci - deprecated
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress
upvoted 
4 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
A good option for reducing latency for users in Asia accessing the web API would be to create a second GKE cluster in asia-
southeast1 and use kubemci to create a global HTTP(s) load balancer.
Option C, "Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer," would be
the correct choice for this scenario.
By creating a second GKE cluster in asia-southeast1, you can reduce latency for users in Asia by serving the API from a closer
location. You can then use kubemci, a command-line tool that simplifies the process of creating a global HTTP(s) load balancer, to
expose the APIs from both clusters through a single global IP address. This allows users to access the API with low latency,
regardless of their location.
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months agoomermahgoub
omermahgoub
 
1 year, 6 months ago
Option A, "Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the
public IPs to the Cloud DNS zone," would not be a good choice because it would not provide a single global IP address for users
to access the API, which would increase latency and complexity.
Option B, "Use a global HTTP(s) load balancer with Cloud CDN enabled," would not be a good choice because it would not allow
you to serve the API from a closer location for users in Asia.
Option D, "Increase the memory and CPU allocated to the application in the cluster," would not be a good choice because it
would not address the issue of latency for users in Asia accessing the API.
upvoted 
2 
times
ale_brd_111
ale_brd_111
 
1 year, 7 months ago
Selected Answer: 
C
Answer is C but kubemci is deprecated, now you have to go with:
Multi Cluster Ingress is a cloud-hosted controller for Google Kubernetes Engine (GKE) clusters. It's a Google-hosted service that
supports deploying shared load balancing resources across clusters and across regions. To deploy Multi Cluster Ingress across
multiple clusters, complete Setting up Multi Cluster Ingress then see Deploying Ingress across multiple clusters.
https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress
upvoted 
5 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #155
You are migrating third-party applications from optimized on-premises virtual machines to Google Cloud. You are unsure about
the optimum CPU and memory options. The applications have a consistent usage pattern across multiple weeks. You want to
optimize resource usage for the lowest cost. What should you do? 
A. 
Create an instance template with the smallest available machine type, and use an image of the third-party application
taken from a current on-premises virtual machine. Create a managed instance group that uses average CPU utilization to
autoscale the number of instances in the group. Modify the average CPU utilization threshold to optimize the number of
instances running.
B. 
Create an App Engine flexible environment, and deploy the third-party application using a Dockerfile and a custom
runtime. Set CPU and memory options similar to your application's current on-premises virtual machine in the app.yaml file.
C. 
Create multiple Compute Engine instances with varying CPU and memory options. Install the Cloud Monitoring agent,
and deploy the third-party application on each of them. Run a load test with high traffic levels on the application, and use
the results to determine the optimal settings.
D. 
Create a Compute Engine instance with CPU and memory options similar to your application's current on-premises
virtual machine. Install the Cloud Monitoring agent, and deploy the third-party application. Run a load test with normal
traffic levels on the application, and follow the Rightsizing Recommendations in the Cloud Console. 
Most Voted
Correct Answer:
 
D 
Comments
pr2web
pr2web
 
Highly Voted
 
2 years, 10 months ago
Answer is D. 
https://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing?hl=en
"Rightsizing provides two types of recommendations:
1. Performance-based recommendations: Recommends Compute Engine instances based on the CPU and RAM currently
allocated to the on-premises VM. This recommendation is the default.
2. Cost-based recommendations: Recommends Compute Engine instances based on:
- The current CPU and RAM configuration of the on-premises VM.
Community vote distribution
D (69%)
A (22%)
B (9%)- The current CPU and RAM configuration of the on-premises VM.
- The average usage of this VM during a given period. To use this option, you must activate rightsizing monitoring with vSphere for
this group of VMs and allow time for Migrate for Compute Engine to analyze usage.
upvoted 
55 
times
melono
melono
 
1 year, 8 months ago
The point:
2. Cost-based recommendations: Recommends Compute Engine instances based on:
The current CPU and RAM configuration of the on-premises VM.
upvoted 
1 
times
cloudmon
cloudmon
 
Highly Voted
 
2 years, 2 months ago
Selected Answer: 
D
It's definitely D. See the reference at the following link that says "The recommendation algorithm is suited to workloads that follow
weekly patterns", which matches the part of the questions that says "consistent usage pattern over multiple weeks":
https://cloud.google.com/compute/docs/instances/apply-machine-type-recommendations-for-instances
Option A also has two problems;
1. It only focuses on CPU, but the question says "CPU and memory"
2. The question does not mention anything about horizontal scalability
upvoted 
9 
times
cloudmon
cloudmon
 
2 years, 2 months ago
Another (less obvious) reason for choosing D: I've noticed a pattern in these exams that the cloud provider wants to advertise and
promote anything that they consider to be a cool feature of their platform. In this case, they are promoting their recommendation
engine. If there's even an option that sounds like it's advertising a relevant managed service from the cloud provider, then that's
usually one to consider.
upvoted 
4 
times
cloudmon
cloudmon
 
2 years, 2 months ago
I also find the following wording in option A to be a bit iffy: "an image of the third-party application taken from a current on-
premises virtual machine". That seems a bit vague in terms of what the image format would be.
upvoted 
1 
times
e5019c6
e5019c6
 
Most Recent
 
6 months, 1 week ago
Selected Answer: 
D
I choose A at first, because I thought that the Rightsizing Recommendations took various days to offer the estimate stats. But
according to this article:
https://cloud.google.com/migrate/compute-engine/docs/4.11/concepts/planning-a-migration/cloud-instance-rightsizing
While it needs a week to give a proper estimate, it can give an estimate with less time too (But the accuracy decreases)
"For better recommendations, Migrate for Compute Engine recommends monitoring the migrated workloads for at least seven
consecutive days (or one typical business week). Migrate for Compute Engine warns you when the monitoring period is insufficient
for an adequate recommendation.
Even if the monitoring period is insufficient, Migrate for Compute Engine still offers a cost-optimized recommendation based on the
data available."
upvoted 
3 
times
e5019c6
e5019c6
 
6 months, 1 week ago
Also note that Migrate to Virtual Machines v4.11 (Which the info is from) is no longer the latest version. V5 is already out and,
strangely, lacks any article about rightsizing recommendations...
upvoted 
2 
times
tamj123
tamj123
 
8 months, 2 weeks ago
D make sense.
upvoted 
1 
times
RaviRS
RaviRS
 
10 months ago
Selected Answer: 
D
D is correct
upvoted 
1 
timesupvoted 
1 
times
WinSxS
WinSxS
 
1 year, 3 months ago
Selected Answer: 
D
Option D would be the best option to optimize resource usage for the lowest cost when migrating third-party applications from
optimized on-premises virtual machines to Google Cloud.
upvoted 
2 
times
AugustoKras011111
AugustoKras011111
 
1 year, 3 months ago
Selected Answer: 
D
Answer is D. Similar than third-party convince me...
upvoted 
1 
times
beehive
beehive
 
1 year, 6 months ago
why most of the answers selected by host is INCORRECT? Is it intentional to misguide the folks?
upvoted 
7 
times
thamaster
thamaster
 
1 year, 6 months ago
Selected Answer: 
D
i choose D as it's best practice create an instance with similar configuration as on premise and check metrics
upvoted 
1 
times
shefalia
shefalia
 
1 year, 6 months ago
Selected Answer: 
D
D is the right one because of Rightsizing option from GCP
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
D
I agree with D is the most accurate
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
D is correct
upvoted 
1 
times
SerGCP
SerGCP
 
1 year, 9 months ago
Selected Answer: 
D
A, application may not support horizontal scaling and may not run in instances whith small cpu
B, dockerize third-party applications is not a requirement....Complex and costly
C, too expensive
D, simple and works
upvoted 
3 
times
shekarcfc
shekarcfc
 
1 year, 10 months ago
Selected Answer: 
A
A, the benefit of moving to cloud is scaling based on load, start with min infra and scale-up based on usage.
upvoted 
4 
timesupvoted 
4 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
D
A - you cannot expect application to behavior similar in 2 different envior met without a test.
B - App Engine is costly
C- Varing cpu and memory cannot be doone.
D- correa.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #156
Your company has a Google Cloud project that uses BigQuery for data warehousing. They have a VPN tunnel between the on-
premises environment and Google 
Cloud that is configured with Cloud VPN. The security team wants to avoid data exfiltration by malicious insiders, compromised
code, and accidental oversharing. 
What should they do? 
A. 
Configure Private Google Access for on-premises only.
B. 
Perform the following tasks: 1. Create a service account. 2. Give the BigQuery JobUser role and Storage Reader role to
the service account. 3. Remove all other IAM access from the project.
C. 
Configure VPC Service Controls and configure Private Google Access. 
Most Voted
D. 
Configure Private Google Access.
Correct Answer:
 
C 
Comments
Craigenator
Craigenator
 
Highly Voted
 
2 years, 7 months ago
Without the discussion this site would be useless, many thanks to all that participate. 
Majority of answers are wrong...
upvoted 
72 
times
VarunGo
VarunGo
 
1 year, 1 month ago
you can used chatGPT now
upvoted 
3 
times
Murtuza
Murtuza
 
9 months ago
Then you are definitely bound to fail :-)
upvoted 
11 
times
diaga2
diaga2
 
Highly Voted
 
2 years, 10 months ago
C is the recommended one https://cloud.google.com/vpc-service-controls/docs/overview
upvoted 
31 
times
Community vote distribution
C (100%)upvoted 
31 
times
squishy_fishy
squishy_fishy
 
Most Recent
 
6 months, 3 weeks ago
Correct answer is C.
Security benefits of VPC Service Controls
Access from unauthorized networks using stolen credentials
Data exfiltration by malicious insiders or compromised code
https://cloud.google.com/vpc-service-controls/docs/overview#benefits
upvoted 
2 
times
thewalker
thewalker
 
7 months, 3 weeks ago
Selected Answer: 
C
VPC Service Controls is required to stop data exfiltration. Hence C
upvoted 
2 
times
tamj123
tamj123
 
8 months, 2 weeks ago
C, VPC Service controls is need for the solution
upvoted 
1 
times
Mrinalini19
Mrinalini19
 
1 year, 4 months ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
examch
examch
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer,
To secure data from exfiltration by malicious insiders, compromised code or accidental oversharing, we use VPC Service controls
https://cloud.google.com/vpc-service-controls/docs/overview
For private access options, connect to services in VPC networks we use private service endpoints or VPC network peering.
https://cloud.google.com/vpc/docs/private-access-options#connect-services
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
C
C is the right answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
I will go with C
upvoted 
1 
times
nkit
nkit
 
2 years, 2 months ago
Selected Answer: 
C
Going by definition- VPC Service Controls improves your ability to mitigate the risk of data exfiltration from Google Cloud servicesGoing by definition- VPC Service Controls improves your ability to mitigate the risk of data exfiltration from Google Cloud services
such as Cloud Storage and BigQuery. 
hence C is correct
upvoted 
7 
times
dangcpped
dangcpped
 
2 years, 2 months ago
Selected Answer: 
C
C is the recommended 
https://cloud.google.com/vpc-service-controls/docs/overview
upvoted 
2 
times
kimharsh
kimharsh
 
2 years, 4 months ago
I don't get it , C is correct because of the "VPC service Control", But Privet Google access is not for on On-premises, A is for On-
premises = https://cloud.google.com/vpc/docs/private-access-options
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
Selected Answer: 
C
I agree C.
The link that wroted in Reveral Solution means C.
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
C
C is the correct answer
https://cloud.google.com/vpc-service-controls/docs/overview
upvoted 
2 
times
sapsant
sapsant
 
2 years, 7 months ago
Selected Answer: 
C
https://cloud.google.com/vpc-service-controls/docs/overview
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #157
You are working at an institution that processes medical data. You are migrating several workloads onto Google Cloud.
Company policies require all workloads to run on physically separated hardware, and workloads from different clients must also
be separated. You created a sole-tenant node group and added a node for each client. You need to deploy the workloads on
these dedicated hosts. What should you do? 
A. 
Add the node group name as a network tag when creating Compute Engine instances in order to host each workload on
the correct node group.
B. 
Add the node name as a network tag when creating Compute Engine instances in order to host each workload on the
correct node.
C. 
Use node affinity labels based on the node group name when creating Compute Engine instances in order to host each
workload on the correct node group.
D. 
Use node affinity labels based on the node name when creating Compute Engine instances in order to host each
workload on the correct node. 
Most Voted
Correct Answer:
 
D 
Comments
pr2web
pr2web
 
Highly Voted
 
3 years, 3 months ago
Answer is D. 
Y'all not reading the fine details. The question is about aligning EACH client to their dedicated nodes (D), not to a node group (C). 
https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#default_affinity_labels
The above reference clearly articulates the default affinity label for node group and node name. Unless we're thinking about
growing each client to their own dedicated node groups (not in the current requirement), then the answer is not C, rather D. 
Compute Engine assigns two default affinity labels to each node:
A label for the node group name:
Key: compute.googleapis.com/node-group-name
Value: Name of the node group.
A label for the node name:
Community vote distribution
D (75%)
C (25%)A label for the node name:
Key: compute.googleapis.com/node-name
Value: Name of the individual node.
upvoted 
60 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
Except that sole tenant nodes can also be grouped, and wouldn't it be a best practice to design for scaling?
upvoted 
1 
times
Binoz
Binoz
 
Highly Voted
 
3 years, 4 months ago
D. Afinity should be set at node level, not node-group as every client has its own node in the group
upvoted 
18 
times
MikeB19
MikeB19
 
3 years, 3 months ago
That’s what 
i thought too
upvoted 
7 
times
exam400
exam400
 
Most Recent
 
2 weeks, 1 day ago
Selected Answer: 
D
you cannot use node affinity labels based on the node group name.
upvoted 
1 
times
192dcc7
192dcc7
 
3 months, 2 weeks ago
Selected Answer: 
C
Question already says node-group created "You created a sole-tenant node group and added a node for each client"
upvoted 
2 
times
Positron75
Positron75
 
2 months, 2 weeks ago
Sure, but the affinity must be set at the individual node level, not the node group level. Thus D is correct.
upvoted 
1 
times
192dcc7
192dcc7
 
3 months, 3 weeks ago
Selected Answer: 
C
KV Affinity Label > Node template > Node Group > CE
Question already said Node group is created. Then why tag it to node ?
upvoted 
1 
times
lucaluca1982
lucaluca1982
 
5 months ago
Selected Answer: 
C
Using node affinity labels based on the node group name when creating Compute Engine instances is the appropriate method to
ensure workloads are hosted on the correct node group. This approach aligns with Google Cloud's recommended practices for
provisioning sole-tenant VMs and provides the required isolation for client workloads.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
D
D
As per the documentation: https://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms#provision_a_sole-
tenant_vm
upvoted 
2 
times
tamj123
tamj123
 
1 year, 2 months ago
D, the question ask “You created a sole-tenant node group and added a node for each client.”
，
so node affinity labels based on
the node name is need it.
upvoted 
1 
times
Andras2k
Andras2k
 
1 year, 11 months ago
I had this question recently (end of jan 2023) and went with answer D. After doing some investigation, that seems to be the rightI had this question recently (end of jan 2023) and went with answer D. After doing some investigation, that seems to be the right
answer to me.
upvoted 
3 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 7 months ago
Preparing for exam and gone through the concept make sense the answer is D
upvoted 
1 
times
beehive
beehive
 
1 year, 12 months ago
Answer is D.
Ref: you can't specify node affinity labels on a node group.>> https://cloud.google.com/compute/docs/nodes/sole-tenant-
nodes#node_templates
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
D
D is the correct answer, VMs must be associated to a specific node within the node-group, so you must use the node name label
to provision the VM.
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
D is right, Node is right choice instead of node group
upvoted 
1 
times
deenee
deenee
 
2 years, 5 months ago
D : 
https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes
Node affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you:
Control how individual VM instances are assigned to nodes.
Control how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes.
Group sensitive VM instances on specific nodes or node groups, separate from other VMs.
upvoted 
4 
times
slars2k
slars2k
 
2 years, 9 months ago
I go with C as I believe single-tenant node group meant for only one client
upvoted 
3 
times
Skr6266
Skr6266
 
2 years, 9 months ago
Answer is D since it is clearly documented as 
When you create a VM, you request sole-tenancy by specifying node affinity or anti-affinity, referencing one or more node affinity
labels. You specify custom node affinity labels when you create a node template, and Compute Engine automatically includes
some default affinity labels on each node. By specifying affinity when you create a VM, you can schedule VMs together on a
specific node or nodes in a node group. By specifying anti-affinity when you create a VM, you can ensure that certain VMs are not
scheduled together on the same node or nodes in a node group.
Node affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you:
Control how individual VM instances are assigned to nodes.
Control how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes.
Group sensitive VM instances on specific nodes or node groups, separate from other VMs.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #158
Your company's test suite is a custom C++ application that runs tests throughout each day on Linux virtual machines. The full
test suite takes several hours to complete, running on a limited number of on-premises servers reserved for testing. Your
company wants to move the testing infrastructure to the cloud, to reduce the amount of time it takes to fully test a change to
the system, while changing the tests as little as possible. 
Which cloud infrastructure should you recommend? 
A. 
Google Compute Engine unmanaged instance groups and Network Load Balancer
B. 
Google Compute Engine managed instance groups with auto-scaling 
Most Voted
C. 
Google Cloud Dataproc to run Apache Hadoop jobs to process each test
D. 
Google App Engine with Google StackDriver for logging
Correct Answer:
 
B 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 7 months ago
B, https://cloud.google.com/compute/docs/autoscaler/
upvoted 
24 
times
tartar
tartar
 
3 years, 11 months ago
B is ok
upvoted 
7 
times
RVivek
RVivek
 
Most Recent
 
1 year, 5 months ago
Selected Answer: 
B
Changing the tests as little as possible rules out C & D.
Test takes several hours and you need to improve perfromace. 
Autocaling with MIG will do it
Unmanaged group cannot autosacle. 
Load balancer will not improve perfromance
upvoted 
3 
times
SerGCP
SerGCP
 
1 year, 8 months ago
Community vote distribution
B (100%)Why not A? the custom APP may be not supporto autoscaling....
upvoted 
2 
times
e5019c6
e5019c6
 
6 months, 1 week ago
I second the question. The App might not support horizontal scaling.
But I also admit that no other answer is valid. 
A: The Load Balancer offers no benefit.
C: Hadoop doesn't process C++
D: App Engine is for web apps.
upvoted 
1 
times
RVivek
RVivek
 
1 year, 5 months ago
Changing the tests as little as possible rules out C & D.
Test takes several hours and you need to improve perfromace. 
Autocaling with MIG will do it
Unmanaged group cannot autosacle. 
Load balancer will not improve perfromance
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
B
B is the right answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
B is right
upvoted 
2 
times
Pime13
Pime13
 
2 years, 5 months ago
Selected Answer: 
B
choose b
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
B
B is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
B
vote B
upvoted 
3 
times
Cloudguy123
Cloudguy123
 
2 years, 10 months ago
New Question 
Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the
application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that
the application cannot process the messages in real time. Most of them wait for minutes before being processed. You need to
scale the elaboration process that is 1/0-intensive. What should you do? 
A. Usekubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to 
configure Kubernetes autoscaling deployment. 
B. Configure a Kubemetes autoscaling deployment based on the 
subscription/push_request_latencies metric. 
C. Use the --enable-autoscaling flag when you create the Kubernetes cluster. 
D. Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric.
upvoted 
2 
times
PleeO
PleeO
 
2 years, 9 months ago
D is the correct answer 
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub
upvoted 
2 
timesupvoted 
2 
times
PeppaPig
PeppaPig
 
2 years, 10 months ago
D is the answer
upvoted 
4 
times
Cloudguy123
Cloudguy123
 
2 years, 10 months ago
New Question
Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to
rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow
Google-recommended practices for security What should you do? 
A. Create a key with Cloud Key Management Service (KMS) Encrypt the data using the encrypt method of Cloud KMS. 
B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key. 
C. Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket. 
D. Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature.
Answer Please
upvoted 
2 
times
rottzy
rottzy
 
2 years, 8 months ago
why are you repeating questions from this series as comments?
upvoted 
2 
times
fahad01hbti
fahad01hbti
 
2 years, 10 months ago
it is B
https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys
upvoted 
2 
times
GCP_New
GCP_New
 
2 years, 11 months ago
Are we getting questions from 1-100 in the exam?
upvoted 
2 
times
bishalsainju
bishalsainju
 
2 years, 8 months ago
Yeah I have the same question in mind.
upvoted 
2 
times
AnilKr
AnilKr
 
2 years, 11 months ago
Google Compute Engine and with MIG for auto-scaling
upvoted 
3 
times
bala786
bala786
 
2 years, 12 months ago
Agree with Option B.Google Compute Managed instance groups with auto-scaling
upvoted 
2 
times
[Removed]
[Removed]
 
3 years, 1 month ago
agree with B
upvoted 
3 
times
victory108
victory108
 
3 years, 1 month ago
B. Google Compute Engine managed instance groups with auto-scaling
upvoted 
2 
times
un
un
 
3 years, 2 months ago
B is 
correct
upvoted 
2 
times
sidbet
sidbet
 
3 years, 2 months ago
B should be the one
upvoted 
2 
timesupvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #159
A lead software engineer tells you that his new application design uses websockets and HTTP sessions that are not distributed
across the web servers. You want to help him ensure his application will run properly on Google Cloud Platform. 
What should you do? 
A. 
Help the engineer to convert his websocket code to use HTTP streaming
B. 
Review the encryption requirements for websocket connections with the security team
C. 
Meet with the cloud operations team and the engineer to discuss load balancer options 
Most Voted
D. 
Help the engineer redesign the application to use a distributed user session service that does not rely on websockets
and HTTP sessions.
Correct Answer:
 
C 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 7 months ago
I agree with C
upvoted 
16 
times
tartar
tartar
 
3 years, 11 months ago
C is ok
upvoted 
8 
times
[Removed]
[Removed]
 
2 years, 8 months ago
The key line from the link above: 
Session affinity for WebSockets works the same as for any other request. For information, see Session affinity.
upvoted 
4 
times
fraloca
fraloca
 
3 years, 5 months ago
https://cloud.google.com/load-balancing/docs/https#websocket_support
upvoted 
6 
times
Community vote distribution
C (85%)
D (15%)lynx256
lynx256
 
Highly Voted
 
3 years, 3 months ago
IMO C is ok.
Beside the reasons mentioned above regarding why A, B and D are wrong, there are also:
A and D are wrong because are abot changing the app - whereas in the task "You want to help him ensure his application will
run properly on GCP" (not REDESIGN/CHANGE).
B is wrong because you don't have to "Review the encryption requirements for websocket connections with the security team"...
upvoted 
7 
times
ashrafh
ashrafh
 
1 year, 7 months ago
thanks
upvoted 
1 
times
e5019c6
e5019c6
 
Most Recent
 
6 months, 1 week ago
Selected Answer: 
D
I think that, since the app is in design stage, it's totally valid to change its design to adapt to work better in the cloud.
Websockets and HTTP session, while supported, are not the optimal choice for apps in the cloud.
Some user said that the question asks for help ensure his app runs on GCP, so we shouldn't change it. But I don't think that's the
case. As architects we should oversee any design that developers and engineers are introducing to the organization's
architecture.
upvoted 
2 
times
examch
examch
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer,
Google Cloud HTTP(S)-based load balancers have native support for the WebSocket protocol when you use HTTP or HTTPS as
the protocol to the backend. The load balancer does not need any configuration to proxy WebSocket connections.
https://cloud.google.com/load-balancing/docs/https#websocket_support
upvoted 
3 
times
NodummyIQ
NodummyIQ
 
1 year, 6 months ago
The answer is D. C. is not the best answer because it does not address the issue of websockets and HTTP sessions not being
distributed across the web servers. While load balancer options may be relevant to the overall operation of the application,
they do not address the specific issue of ensuring that the websockets and HTTP sessions are properly distributed. A better
solution would be to help the engineer redesign the application to use a distributed user session service that does not rely on
websockets and HTTP sessions, as this would address the issue of session distribution. Alternatively, the engineer could consider
converting their websocket code to use HTTP streaming, which could potentially help with session distribution.
upvoted 
1 
times
e5019c6
e5019c6
 
6 months, 1 week ago
Totally agree. But you didn't vote to change the C hegemony :P
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
C is fine.
upvoted 
2 
times
ijazahmad722
ijazahmad722
 
1 year, 10 months ago
Selected Answer: 
C
I agree with C
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
CSelected Answer: 
C
vote C
upvoted 
4 
times
Cloudguy123
Cloudguy123
 
2 years, 10 months ago
New Case Study Question- TerramEarth
For this question, refer to the TerramEarth case study. 
You are building a microservice-based application for TerramEarth. 
The application is based on Docker containers. You want to follow Google-recommended practices to build the application
continuously and store the build artifacts. What should you do?
upvoted 
2 
times
Cloudguy123
Cloudguy123
 
2 years, 10 months ago
A) Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each
microservice, and tag them using the code commit hash. Push the images to the Container Registry. 
B)Configure a trigger in Cloud Build for new source changes. The trigger invokes build jobs and build container images for the
microservices. Tag the images with a version number, and push them to Cloud Storage. 
C) Create a Scheduler job to check the repo every minute. For any new change, invoke Cloud Build to build container images
for the microservices. Tag the images using the current timestamp, and push them to the Container Registry. 
D) Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build one container image, and tag the
image with the label 'latest' Push the image to the Container Registry
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A looks good to me.
upvoted 
2 
times
nickojul
nickojul
 
2 years, 10 months ago
A is ok
upvoted 
5 
times
DreamerK
DreamerK
 
2 years, 11 months ago
Why D is wrong is the wording "doesn't rely on". This means the application needs to use other protocols instead of http or
websocket. This is not realistic and requires too much application refactoring. Actually a distributed session service is possible
with http or websocket as long as the session information is stored in shared storage such as nosql database or redis that can
be accessed by all web servers. In this sense, D is wrong answer.
upvoted 
1 
times
e5019c6
e5019c6
 
6 months, 1 week ago
There is no refactor since the app is only in design stage. Read the question carefully:
''A lead software engineer tells you that his NEW APPLICATION DESIGN...''
upvoted 
1 
times
AnilKr
AnilKr
 
2 years, 11 months ago
C is fine. Global HTTP(S) load Balancer supports webSockets.
upvoted 
1 
times
kopper2019
kopper2019
 
2 years, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
3 
times
giovy_82
giovy_82
 
2 years, 11 months ago
I also agree with C. the answer D could be ok but the question says "his new application design " so it means that the app has
just been developed and deployed so there's no convenience to redesign it from scratch to avoid use of sessions and
websocket.
upvoted 
1 
timesupvoted 
1 
times
ashrafh
ashrafh
 
1 year, 7 months ago
from where you got the "new" word? ha ha
upvoted 
1 
times
e5019c6
e5019c6
 
6 months, 1 week ago
From the question, ninth word from the start.
Also, ''design'' is an important word. It means the app hasn't been built yet. Else it would have said ''his new application'' only.
upvoted 
1 
times
victory108
victory108
 
3 years, 1 month ago
C. Meet with the cloud operations team and the engineer to discuss load balancer options
upvoted 
1 
times
un
un
 
3 years, 2 months ago
C is correct
upvoted 
1 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is C
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #160
The application reliability team at your company this added a debug feature to their backend service to send all server events
to Google Cloud Storage for eventual analysis. The event records are at least 50 KB and at most 15 MB and are expected to
peak at 3,000 events per second. You want to minimize data loss. 
Which process should you implement? 
A. 
ג
€¢ Append metadata to file body 
ג
€¢ Compress individual files 
ג
€¢ Name files with serverName 
ג
 "€Timestamp 
ג
€¢
Create a new bucket if bucket is older than 1 hour and save individual files to the new bucket. Otherwise, save files to
existing bucket.
B. 
ג
€¢ Batch every 10,000 events with a single manifest file for metadata 
ג
€¢ Compress event files and manifest file into a
single archive file 
ג
€¢ Name files using serverName 
ג
 "€EventSequence 
ג
€¢ Create a new bucket if bucket is older than 1
day and save the single archive file to the new bucket. Otherwise, save the single archive file to existing bucket.
C. 
ג
€¢ Compress individual files 
ג
€¢ Name files with serverName 
ג
 "€EventSequence 
ג
€¢ Save files to one bucket 
ג
€¢ Set
custom metadata headers for each object after saving
D. 
ג
€¢ Append metadata to file body 
ג
€¢ Compress individual files 
ג
€¢ Name files with a random prefix pattern 
ג
€¢ Save
files to one bucket 
Most Voted
Correct Answer:
 
D 
Comments
rishab86
rishab86
 
Highly Voted
 
3 years, 7 months ago
answer is definitely D
https://cloud.google.com/storage/docs/request-rate#naming-convention
"A longer randomized prefix provides more effective auto-scaling when ramping to very high read and write rates. For example, a
1-character prefix using a random hex value provides effective auto-scaling from the initial 5000/1000 reads/writes per second up
to roughly 80000/16000 reads/writes per second, because the prefix has 16 potential values. If your use case does not need higher
rates than this, a 1-character randomized prefix is just as effective at ramping up request rates as a 2-character or longer
randomized prefix."
Example: 
my-bucket/2fa764-2016-05-10-12-00-00/file1 
my-bucket/5ca42c-2016-05-10-12-00-00/file2 
my-bucket/6e9b84-2016-05-10-12-00-01/file3
upvoted 
37 
times
Community vote distribution
D (77%)
B (23%)upvoted 
37 
times
kopper2019
kopper2019
 
Highly Voted
 
3 years, 6 months ago
- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 6
For this question, refer to the Helicopter Racing League (HRL) case study. A recent finance audit of cloud infrastructure noted an
exceptionally high number of Compute Engine instances are allocated to do video encoding and transcoding. You suspect that
these Virtual Machines are zombie machines that were not deleted after their workloads completed. You need to quickly get a list
of which VM instances are idle. What should you do?
A. Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis.
B. Use the gcloud compute instances list to list the virtual machine instances that have the idle: true label set.
C. Use the gcloud recommender command to list the idle virtual machine instances.
D. From the Google Console, identify which Compute Engine instances in the managed instance groups are no longer responding
to health check probes.
upvoted 
6 
times
kravenn
kravenn
 
3 years, 5 months ago
answer C
upvoted 
2 
times
juccjucc
juccjucc
 
3 years, 6 months ago
is it C?
upvoted 
1 
times
cloudstd
cloudstd
 
3 years, 6 months ago
this is not 100% accurate. you should investigate if you doubt if is incorrect
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations
upvoted 
4 
times
Papafel
Papafel
 
3 years, 5 months ago
The correct answer is A
upvoted 
1 
times
matmuh
matmuh
 
3 years, 1 month ago
Absulatly C
upvoted 
2 
times
squishy_fishy
squishy_fishy
 
1 year ago
The correct answer is C based on the URL you shared. 
gcloud recommender recommendations list \
--project=PROJECT_ID \
--location=ZONE \
--recommender=google.compute.instance.IdleResourceRecommender \
--format=yaml
upvoted 
1 
times
KS1911
KS1911
 
3 years, 5 months ago
I have my exam scheduled after 3 days. Would there be more questions coming on ExamTopics?
upvoted 
3 
times
cloudstd
cloudstd
 
3 years, 6 months ago
answer: C
upvoted 
8 
times
Sephethus
Sephethus
 
Most Recent
 
6 months, 2 weeks ago
This question is messed up. The formatting, the discussion, everything. I have no idea what to choose here. Chat GPT thinks the
answer is C but most think it is D and there's not much difference between the two answers.
upvoted 
1 
times
squishy_fishy
squishy_fishy
 
1 year agosquishy_fishy
squishy_fishy
 
1 year ago
The question is how to reduce the data loss, the answer should be something like separation of duty, data lost prevention, but
answer D is for reducing latency retrieving data. I'm baffled by this question.
upvoted 
2 
times
marcohol
marcohol
 
1 year, 2 months ago
I agree with D, but then, using a random prefix wouldn't it make more difficult the file retrieve?
upvoted 
2 
times
ptsironis
ptsironis
 
1 year, 7 months ago
Selected Answer: 
B
Why not option B??
upvoted 
3 
times
nunopires2001
nunopires2001
 
1 year, 11 months ago
I was thinking correct answer was A, because we should have some kind of bucket rotation in order to avoid hiting the max size of
a bucket.
However it seems there is no size limit for a GCP cloud bucket, so I will have to agree with community and stick to answer D.
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
D
D is the correct answer
https://cloud.google.com/storage/docs/request-rate#naming-convention
upvoted 
3 
times
Pime13
Pime13
 
2 years, 11 months ago
D: https://cloud.google.com/storage/docs/request-rate#naming-convention
upvoted 
2 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
D
D is the correct answer
upvoted 
2 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
D
vote D
upvoted 
5 
times
amxexam
amxexam
 
3 years, 3 months ago
Request admin to intervene and delete the hijacking of the question by 
kopper2019
upvoted 
4 
times
Examster1
Examster1
 
3 years, 3 months ago
Use the material for study dude! Hello? Anyone home?
upvoted 
5 
times
Arad
Arad
 
3 years, 1 month ago
it looks like this website does not have any admin
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 5
For this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking for a cost- effective approach for storingFor this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking for a cost- effective approach for storing
their race data such as telemetry. They want to keep all historical records, train models using only the previous season's data, and
plan for data growth in terms of volume and information collected. You need to propose a data solution. Considering HRL business
requirements and the goals expressed by CEO S. Hawke, what should you do?
A. Use Firestore for its scalable and flexible document-based database. Use collections to aggregate race data by season and
event.
B. Use Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as a
primary key.
C. Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.
D. Use Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate database
instances for each season.
upvoted 
3 
times
cloudstd
cloudstd
 
3 years, 6 months ago
answer: C
upvoted 
2 
times
Papafel
Papafel
 
3 years, 5 months ago
Yes answer is C
upvoted 
2 
times
juccjucc
juccjucc
 
3 years, 6 months ago
is it C?
all these questions are from the new exam? why they are here in the comments and not as questions in the list?
upvoted 
2 
times
kopper2019
kopper2019
 
3 years, 6 months ago
because exam was not updated so I added the Qs but they added this new Qs as normal now we have 218 Qs
upvoted 
4 
times
Roncy
Roncy
 
3 years, 3 months ago
Hey Kopper, when would you provide the new set of questions ?
upvoted 
1 
times
kravenn
kravenn
 
3 years, 5 months ago
answer: C
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 4
For this question, refer to the Helicopter Racing League (HRL) case study. HRL wants better prediction accuracy from their ML
prediction models. They want you to use Google’s AI Platform so HRL can understand and interpret the predictions. What should
you do?
A. Use Explainable AI.
B. Use Vision AI.
C. Use Google Cloud’s operations suite.
D. Use Jupyter Notebooks.
upvoted 
3 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
what does this have to do with the cloud storage question?
upvoted 
1 
times
cloudstd
cloudstd
 
3 years, 6 months ago
answer: A
upvoted 
4 
times
juccjucc
juccjucc
 
3 years, 6 months agojuccjucc
juccjucc
 
3 years, 6 months ago
is it A?
upvoted 
2 
times
Papafel
Papafel
 
3 years, 5 months ago
Yes answer is A
upvoted 
1 
times
kravenn
kravenn
 
3 years, 5 months ago
answer A
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 3
For this question, refer to the Helicopter Racing League (HRL) case study. The HRL development team releases a new version of
their predictive capability application every Tuesday evening at 3 a.m. UTC to a repository. The security team at HRL has
developed an in-house penetration test Cloud Function called Airwolf. The security team wants to run Airwolf against the predictive
capability application as soon as it is released every Tuesday. You need to set up Airwolf to run at the recurring weekly cadence.
What should you do?
A. Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.
B. Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function.
C. Configure the deployment job to notify a Pub/Sub queue that triggers a Cloud Function.
D. Set up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function.
upvoted 
2 
times
Amrit123
Amrit123
 
3 years, 2 months ago
C, is the right answer. The scheduler would run without a trigger even though the release has not been done. If you read
(application as soon as it is released ), the time is not certain. So, the answer is C. Check out the last 30 questions, would give a
better idea as there is a separate discussion
upvoted 
2 
times
esc
esc
 
3 years, 6 months ago
answer : A
upvoted 
5 
times
vchrist
vchrist
 
3 years, 1 month ago
why A? Does Cloud Storage make sense ?
upvoted 
1 
times
jask
jask
 
3 years, 3 months ago
in option A what is the use of Cloud storage bucket? In my opinion answer is C.
upvoted 
3 
times
Papafel
Papafel
 
3 years, 5 months ago
Answer is A
upvoted 
2 
times
cloudmon
cloudmon
 
2 years, 9 months ago
I would go with C
https://cloud.google.com/source-repositories/docs/code-change-notification
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
It's probably C due to pub sub on Cloud Deploy rather than source repos
https://cloud.google.com/deploy/docs/subscribe-deploy-notifications
upvoted 
1 
timesupvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 2
For this question, refer to the Helicopter Racing League (HRL) case study. Recently HRL started a new regional racing league in
Cape Town, South Africa. In an effort to give customers in Cape Town a better user experience, HRL has partnered with the
Content Delivery Network provider, Fastly. HRL needs to allow traffic coming from all of the Fastly IP address ranges into their
Virtual Private Cloud network (VPC network). You are a member of the HRL security team and you need to configure the update
that will allow only the Fastly IP address ranges through the External HTTP(S) load balancer. Which command should you use?
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
A. gcloud compute security-policies rules update 1000 \
--security-policy from-fastly \
--src-ip-ranges * \
--action “allow”
B. gcloud compute firewall rules update sourceiplist-fastly \
--priority 100 \
--allow tcp:443
C. gcloud compute firewall rules update hir-policy \
--priority 100 \
--target-tags=sourceiplist-fastly \
--allow tcp:443
D. gcloud compute security-policies rules update 1000 \
--security-policy hir-policy \
--expression “evaluatePreconfiguredExpr(‘sourceiplist-fastly’)” \
--action “allow”
upvoted 
1 
times
cloudstd
cloudstd
 
3 years, 6 months ago
answer: D
upvoted 
6 
times
Papafel
Papafel
 
3 years, 5 months ago
Answer is A
upvoted 
2 
times
matmuh
matmuh
 
3 years, 1 month ago
A is incorrect : To match all IPs specify *
https://cloud.google.com/sdk/gcloud/reference/compute/security-policies/rules/update
upvoted 
1 
times
kravenn
kravenn
 
3 years, 5 months ago
answer D
upvoted 
4 
times
xavi1
xavi1
 
3 years, 4 months ago
both A and D have correct syntax, but src-ip-ranges cannot be "*", correct is D
upvoted 
5 
times
cloudmon
cloudmon
 
2 years, 9 months ago
I agree
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
- New Q, 06/2021 
Helicopter Racing League Testlet 1
Company overviewCompany overview
Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world
championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL
offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.
Solution concept
HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race
predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their
content, both real-time and recorded, closer to their users.
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
Existing technical environment
HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider.
Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the
cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction
services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:
- Existing content is stored in an object storage service on their existing public cloud provider. 
Video encoding and transcoding is performed on VMs created for each job.
Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
Business requirements
HRL’s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their
requirements are:
Support ability to expose the predictive models to partners. Increase predictive capabilities during and before races:
○ Race results
○ Mechanical failures
○ Crowd sentiment
Increase telemetry and create additional insights. Measure fan engagement with new predictions. Enhance global availability and
quality of the broadcasts. Increase the number of concurrent viewers.
Minimize operational complexity. Ensure compliance with regulations.
Create a merchandising revenue stream.
Technical requirements
Maintain or increase prediction throughput and accuracy. Reduce viewer latency.
Increase transcoding performance.
Create real-time analytics of viewer consumption patterns and engagement. Create a data mart to enable processing of large
volumes of race data.
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
Executive statement
Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want
enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to
predict race outcomes but lacks the facility to support real- time predictions during races and the capacity to process season-
long results.
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 6 months ago
QUESTION 1
For this question, refer to the Helicopter Racing League (HRL) case study. Your team is in charge of creating a payment card
data vault for card numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders.
You need to implement a custom card tokenization service that meets the following requirements:
• It must provide low latency at minimal cost.
• It must be able to identify duplicate credit cards and must not store plaintext card numbers.
• It should support annual key rotation.
Which storage approach should you adopt for your tokenization service?A. Store the card data in Secret Manager after running a query to identify duplicates.
B. Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.
C. Encrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.
D. Use column-level encryption to store the data in Cloud SQL.
upvoted 
2 
times
SPNBLUE
SPNBLUE
 
3 years, 5 months ago
Why D ?
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #161
A recent audit revealed that a new network was created in your GCP project. In this network, a GCE instance has an SSH port
open to the world. You want to discover this network's origin. 
What should you do? 
A. 
Search for Create VM entry in the Stackdriver alerting console
B. 
Navigate to the Activity page in the Home section. Set category to Data Access and search for Create VM entry
C. 
In the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry
Most Voted
D. 
Connect to the GCE instance using project SSH keys. Identify previous logins in system logs, and match these with the
project owners list
Correct Answer:
 
C 
Comments
clouddude
clouddude
 
Highly Voted
 
3 years, 1 month ago
I am going to go with C. 
Answer A doesn't seem to fit because the matter of when a VM was created.
Answer B focuses on Data Access logs which doesn't seem to fit since the matter of creating a network firewall rule
is an Admin activity, not a data access activity.
D focuses on who logged in which is good to know but doesn't answer the question of how the network was created.
C focuses on logging, the selection of network events, and the Create/Insert entry.
upvoted 
17 
times
Eroc
Eroc
 
Highly Voted
 
3 years, 8 months ago
When you search for Create Insert, it displays a JSON code string that contains the creators e-mail
upvoted 
17 
times
tartar
tartar
 
2 years, 11 months ago
C is ok
upvoted 
14 
times
AugustoKras011111
AugustoKras011111
 
Most Recent
 
4 months, 1 week ago
Community vote distribution
C (100%)Selected Answer: 
C
C is ok to me!
upvoted 
1 
times
NodummyIQ
NodummyIQ
 
6 months ago
Option C is incorrect because the GCE Network logs are not the correct place to search for the creation of a VM instance. The
correct place to search for this information is the Activity page, as specified in option B.
upvoted 
1 
times
n_nana
n_nana
 
5 months, 3 weeks ago
Question is asking about network origin creation not VM creation. that's why is C
upvoted 
3 
times
megumin
megumin
 
7 months, 4 weeks ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
8 months, 2 weeks ago
Selected Answer: 
C
C is the right answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
C is right
upvoted 
2 
times
cloudmon
cloudmon
 
1 year, 2 months ago
Sorry to gripe again, but why on Earth would anybody need to remember this from the top of their mind. You will never be in a
situation in which you need to remember this without looking at the available options in the console (or simply Googling it, lol).
upvoted 
12 
times
vincy2202
vincy2202
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
Bobch
Bobch
 
1 year, 6 months ago
Selected Answer: 
C
Vote C
upvoted 
1 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
C
vote C
upvoted 
2 
times
muneebarshad
muneebarshad
 
1 year, 10 months ago
In Logs Explorer , Filter "resource.type="gce_firewall_rule" and Query insert Create
You would see below and email address
"methodName": "v1.compute.firewalls.insert",
"authorizationInfo": [
{
"permission": "compute.firewalls.create",
upvoted 
2 
times
bala786
bala786
 
1 year, 12 months agobala786
bala786
 
1 year, 12 months ago
Option C is correct, because logging section is the correct choice to get this details
upvoted 
2 
times
victory108
victory108
 
2 years, 1 month ago
C - In the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry
upvoted 
2 
times
un
un
 
2 years, 2 months ago
C is correct
upvoted 
2 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is C
upvoted 
2 
times
willan
willan
 
2 years, 5 months ago
Agree..C
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #162
You want to make a copy of a production Linux virtual machine in the US-Central region. You want to manage and replace the
copy easily if there are changes on the production virtual machine. You will deploy the copy as a new instance in a different
project in the US-East region. 
What steps must you take? 
A. 
Use the Linux dd and netcat commands to copy and stream the root disk contents to a new virtual machine instance in
the US-East region.
B. 
Create a snapshot of the root disk and select the snapshot as the root disk when you create a new virtual machine
instance in the US-East region.
C. 
Create an image file from the root disk with Linux dd command, create a new virtual machine instance in the US-East
region
D. 
Create a snapshot of the root disk, create an image file in Google Cloud Storage from the snapshot, and create a new
virtual machine instance in the US-East region using the image file the root disk. 
Most Voted
Correct Answer:
 
D 
Comments
Eroc
Eroc
 
Highly Voted
 
5 years, 2 months ago
D is correct. A and B are talking about appending the file system to a new VM, not setting it at the root in a new VM set. Option C is
not offered within the GCP because the image must be on the GCP platform to run the gcloud of Google Console instructions to
create a VM with the image.
upvoted 
30 
times
ccpmad
ccpmad
 
7 months ago
you are incorrect. It is D
upvoted 
1 
times
tartar
tartar
 
4 years, 5 months ago
D is ok
upvoted 
10 
times
Community vote distribution
D (71%)
B (29%)upvoted 
10 
times
Sudipta
Sudipta
 
Highly Voted
 
4 years, 11 months ago
Why Not B.
https://cloud.google.com/compute/docs/instances/create-start-instance#createsnapshot
This clearly tells we can use snapshot to create a VM instance, and only need a custom image if we need to create many
instances. Here we are creating only one.
upvoted 
15 
times
Jack_in_Large
Jack_in_Large
 
4 years, 4 months ago
You can't use the snapshot created by another project
upvoted 
13 
times
noussy
noussy
 
4 years, 3 months ago
According to the documentation we can now https://cloud.google.com/compute/docs/disks/create-snapshots
upvoted 
7 
times
JasminL
JasminL
 
4 years ago
I think the question has 2 different answers now as Google improve the snapshot function.
Quoted from the link:
'You can create snapshots from disks even while they are attached to running instances. Snapshots are global resources, so
you can use them to restore data to a new disk or instance within the same project. You can also share snapshots across
projects.'
upvoted 
11 
times
VSMu
VSMu
 
1 year, 11 months ago
B would have been the answer in the current context. But as I read carefully, it doesnt mention the step of sharing snapshot
across projects. It directly expects to use the snapshot. Hence D may be the right answer!
upvoted 
4 
times
ArthurL20
ArthurL20
 
3 years, 7 months ago
Only if its in the same zone: https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots
"Note: The disk must be in the same zone as the instance."
But this is not the case here, we have:
Different zones and different project hence, you must use a bucket.
upvoted 
16 
times
Toothpick
Toothpick
 
Most Recent
 
5 months, 1 week ago
In the current state of GCP ,
Both B and D can be done in 3 gcloud commands each, (and both require equal amount of snapshot handling). 
D is better if you have more than one instance to create but in this case , they are both equally valid.
upvoted 
2 
times
Toothpick
Toothpick
 
5 months, 1 week ago
Edit, 
B might even be easier , you can directly create a snapshot in a target project and when creating a new vm, simply choose it as a
root disk, just tried and verified it myself now
upvoted 
1 
times
ccpmad
ccpmad
 
7 months ago
Selected Answer: 
D
We neet to move from one region o another, so we need snapshot of the root disk and send it to Cloud Storage. In the scenario of
using in the same region, it is better tu use instance image, not snapshots of root disk.
upvoted 
1 
times
Toothpick
Toothpick
 
5 months, 1 week ago
U can directly create the snapshot in any target project
upvoted 
1 
timesadoyt
adoyt
 
1 year ago
B isn't correct because you have to create a disk first. You cannot create a VM from a snapshot directly.
gcloud compute disks create DISK_NAME \
--source-snapshot SNAPSHOT_NAME \
--project SOURCE_PROJECT_ID \ --zone ZONE
upvoted 
2 
times
odacir
odacir
 
1 year, 1 month ago
Selected Answer: 
D
https://cloud.google.com/compute/docs/instances/copy-vm-between-projects
upvoted 
2 
times
Prakzz
Prakzz
 
1 year, 2 months ago
Selected Answer: 
D
https://cloud.google.com/compute/docs/instances/copy-vm-between-projects
you have to create Image from Snapshot and share it to the destination project.
upvoted 
3 
times
didek1986
didek1986
 
1 year, 3 months ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
ananta93
ananta93
 
1 year, 5 months ago
B is the correct answer and It is straight forward.
upvoted 
1 
times
ccpmad
ccpmad
 
7 months ago
No, B is not correct, try your answer in GCP. Yo will not see your snapshot, because in different regions you don't have visibility of
snapshots, that's why we need to move it to Cloud Storage first.
upvoted 
1 
times
PST21
PST21
 
1 year, 9 months ago
B is incorrect as it doesnt create an image uses the snapshot and hence D is the only corret option
upvoted 
2 
times
AugustoKras011111
AugustoKras011111
 
1 year, 10 months ago
Selected Answer: 
D
D seems better, but B actually works too.
upvoted 
1 
times
romandrigo
romandrigo
 
1 year, 10 months ago
Selected Answer: 
D
https://cloud.google.com/compute/docs/instances/copy-vm-between-projects#zonal-boot-disk
upvoted 
4 
times
Clauther
Clauther
 
1 year, 11 months ago
Selected Answer: 
B
B is the right one as of 01/2023
upvoted 
2 
times
n_nana
n_nana
 
1 year, 11 months ago
Selected Answer: 
B
Currently , It is possible to create VM from snapshot within same project, different project or even different organisation. so answer
B is more straight forward.
upvoted 
2 
timesn_nana
n_nana
 
1 year, 11 months ago
https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots
https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots_across_orgs
upvoted 
2 
times
examch
examch
 
1 year, 12 months ago
Selected Answer: 
B
B is the correct answer,
We can create VM from snapshot across zones and regions, please read through the link,
https://cloud.google.com/compute/docs/instances/moving-instance-across-zones#moving-an-instance-manually
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
arpitshah20
arpitshah20
 
2 years, 1 month ago
Selected Answer: 
D
D is correct
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #163
Your company runs several databases on a single MySQL instance. They need to take backups of a specific database at regular
intervals. The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance. 
How should you configure the storage? 
A. 
Configure a cron job to use the gcloud tool to take regular backups using persistent disk snapshots.
B. 
Mount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the backup to
Google Cloud Storage. 
Most Voted
C. 
Use gcsfise to mount a Google Cloud Storage bucket as a volume directly on the instance and write backups to the
mounted location using mysqldump.
D. 
Mount additional persistent disk volumes onto each virtual machine (VM) instance in a RAID10 array and use LVM to
create snapshots to send to Cloud Storage
Correct Answer:
 
B 
Comments
hannibal1969
hannibal1969
 
Highly Voted
 
5 years, 1 month ago
I think it's B. If you use a tool like GCFUSE it will write immediatly to GCS which is a cost benefit because you don't need
intermediate storage. In this case however "Quickly as possible" key for understanding. GCFUSE will write to GCS which is much
slower than writing directly to an added SSD. During the write to GCS it would also execute reads for a longer period on the
production database. Therefor writing to the extra SSD would be my recommended solution. Offloading from the SSD to GCS
would not impact the running database because the data is already separated.
upvoted 
53 
times
heelhook_ambassador
heelhook_ambassador
 
3 years, 1 month ago
Thanks!
upvoted 
1 
times
kvenkatasudhakar
kvenkatasudhakar
 
3 years, 1 month ago
We cannot attach and mount a local SSD to a running instance. I think it's C (GCFUSE)
upvoted 
3 
times
 
3 years, 4 months ago
Community vote distribution
B (68%)
C (25%)
Other (7%)raf2121
raf2121
 
3 years, 4 months ago
Point for Discussion
Can local SSD be mounted in a running instance.
upvoted 
4 
times
JasonL_GCP
JasonL_GCP
 
3 years, 2 months ago
Good point, Because Local SSDs are located on the physical machine where your virtual machine instance is running, they can
be created only during the instance creation process
upvoted 
2 
times
pr2web
pr2web
 
3 years, 3 months ago
Yes they can. That's precisely why it makes Local SSD a good scratch / temp storage with 
very high IOPS. 
https://cloud.google.com/compute/docs/disks/local-ssd#formatandmount
upvoted 
2 
times
nymets
nymets
 
2 years, 11 months ago
No, you cannot attach a local SSD after the instance is created. 
"Because Local SSDs are located on the physical machine where your virtual machine instance is running, they can be
created only during the instance creation process." 
The above is from https://cloud.google.com/compute/docs/disks/local-ssd#formatandmount
upvoted 
3 
times
SerGCP
SerGCP
 
2 years, 3 months ago
The local SSD can be created only during the VM creation process.
After than you can mount disk for in the destination path for export mysqldump. gsutil is the supported tool that you may used to
migrate the dump to bucket.
upvoted 
2 
times
Rathish
Rathish
 
Highly Voted
 
4 years, 10 months ago
Ans: B
Persistent Disk snapshot not required: "They need to take backups of a specific database at regular intervals."
"The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance."
This can be achieved by using both Local SSD & GCS Fuse (mounting GCS as directory), but as the question stats needs to
complete as quickly as possible.
General Rule: Any addition of components introduce a latency. I could not get write throughput of GCS & Local SSD, even if we
consider both provides same throughput, streaming data through network to GCS Bucket introduce latency. Attached Local SSD
has advantage in this case, since there is no network involved.
From Local SSD to GCS bucket - copy job does not impact the mysql data disk.
upvoted 
16 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 1 week ago
Selected Answer: 
D
Local SSDs can only be attached when the VM is being created.
upvoted 
1 
times
46f094c
46f094c
 
6 months, 2 weeks ago
Selected Answer: 
A
"A" is the only one which gives an automated way to do it. All the rest involves a person action
upvoted 
2 
times
Saikatms
Saikatms
 
11 months, 2 weeks ago
I'll go with B
upvoted 
2 
times
Santhoshsunkari
Santhoshsunkari
 
11 months, 3 weeks agoB,https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up
upvoted 
2 
times
bandegg
bandegg
 
11 months, 4 weeks ago
Selected Answer: 
C
It says a specific database, not all of it. Otherwise, why not just use the snapshots? They are no cost.
upvoted 
1 
times
Tamim321
Tamim321
 
1 year ago
Selected Answer: 
C
You can only add local ssd to a VM during creation.Hence going with option C
upvoted 
1 
times
Tamim321
Tamim321
 
1 year ago
Refer to link - 
https://cloud.google.com/compute/docs/disks/local-
ssd#:~:text=You%20can%20only%20add%20Local,the%20types%20that%20do%20not.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
B
Option B would be the best choice for this scenario. Mounting a Local SSD volume as the backup location would ensure high
performance and minimal impact on disk performance, while also allowing for quick backups. After the backup is complete, using
gsutil to move the backup to Google Cloud Storage would provide a reliable and secure storage location for the backups. This
approach is also cost-effective, as Local SSD volumes are less expensive than persistent disks.
upvoted 
2 
times
Murtuza
Murtuza
 
1 year, 3 months ago
Local SSD are considered ephermeral and they are the most cost-effective and they are fast
upvoted 
1 
times
NodummyIQ
NodummyIQ
 
2 years ago
B is incorrect. The Local SSD volumes are only available on certain instance types, and they are not suitable for long-term storage
as they are ephemeral and are deleted when the instance is deleted or stopped. For long-term storage, it is recommended to use
persistent disks or Google Cloud Storage.
upvoted 
1 
times
RVivek
RVivek
 
1 year, 11 months ago
I guess umissed the second paryt of the answer B whic says "After the backup is complete, use gsutil to move the backup to
Google Cloud Storage"
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
Option B is the most appropriate solution in this case. Mounting a Local SSD volume as the backup location will allow the backups
to be taken quickly and efficiently, as Local SSDs have very high I/O performance and low latencies. Additionally, using gsutil to
move the backups to Google Cloud Storage after they have been taken will provide a secure and durable storage location for the
backups.
A, configuring a cron job to use the gcloud tool to take regular backups using persistent disk snapshots, may not be the most
efficient option because persistent disks have relatively lower I/O performance compared to Local SSDs.
C, using gcsfuse to mount a Google Cloud Storage bucket as a volume directly on the instance and writing the backups to the
mounted location using mysqldump, may not be the most efficient option because the backups would need to be transferred over
the network, which could impact the performance of the backups.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
D, mounting additional persistent disk volumes onto each VM instance in a RAID10 array and using LVM to create snapshots to
send to Cloud Storage, may not be the most efficient option because it would require additional disk space and setup, and LVM
snapshots may not be as fast as Local SSDs for taking backups.
upvoted 
1 
timesupvoted 
1 
times
minmin2020
minmin2020
 
2 years, 2 months ago
Selected Answer: 
C
Gcsfuse needs local storage for caching, usually local/non-persistent disks are used for this purpose. With gcsfuse you can have
the backend storage mounted as a filesystem on the server. Mysqldump allows for hot database backups.
Option C provides the automated solution needed to backup and store the database.
Option B is the manual version where you need to mount the local SSD, run the backup and then transfer it to a bucket manually.
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
I will go with B
upvoted 
3 
times
zellck
zellck
 
2 years, 3 months ago
Selected Answer: 
B
B is the answer.
https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up
When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage
your backups and then push them to a Cloud Storage bucket.
upvoted 
10 
times
ashrafh
ashrafh
 
2 years, 1 month ago
best answer thank you
upvoted 
1 
times
Pradeepkumar
Pradeepkumar
 
2 years, 4 months ago
https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up
When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage
your backups and then push them to a Cloud Storage bucket.
Though it is mentioned for SQL Server, the best practices are common for most of the databases. Also it is assumed that the
Local SSD are already mounted while creating the VM
upvoted 
4 
times
xman3
xman3
 
2 years, 5 months ago
Selected Answer: 
C
>backups of a specific database
upvoted 
2 
times
Ric350
Ric350
 
2 years, 5 months ago
B - I think this will clear things up. Local SSD is ATTACHED when CREATING the VM. The local SSDs are just LOCATED (on the
physical host) where the VM is running. See here. 
https://cloud.google.com/compute/docs/disks/add-local-ssd#create_local_ssd
You can have a VM with locally attached SSD in an unformatted and unmounted state or just not mounted! Maybe it was umounted
and now needs to be re-mounted? Answer B says to MOUNT the local SSD. MOUNTING the SSD is done when the VM is
RUNNING! We need to assume the VM was built with locally attached SSD but not formatted and mounted yet. See here.
https://cloud.google.com/compute/docs/disks/add-local-ssd#format_and_mount_a_local_ssd_device!
upvoted 
5 
times
n_nana
n_nana
 
1 year, 11 months ago
This clear confusion, Thank you.
upvoted 
1 
times
Ric350
Ric350
 
2 years, 5 months ago
Also, When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to
stage your backups and then push them to a Cloud Storage bucket. See here under "formatting secondary disks, backing up."
https://cloud.google.com/compute/docs/instances/sql-server/best-practices#formatting_secondary_diskshttps://cloud.google.com/compute/docs/instances/sql-server/best-practices#formatting_secondary_disks
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #164
You are helping the QA team to roll out a new load-testing tool to test the scalability of your primary cloud services that run on
Google Compute Engine with Cloud 
Bigtable. 
Which three requirements should they include? (Choose three.) 
A. 
Ensure that the load tests validate the performance of Cloud Bigtable
B. 
Create a separate Google Cloud project to use for the load-testing environment 
Most Voted
C. 
Schedule the load-testing tool to regularly run against the production environment
D. 
Ensure all third-party systems your services use is capable of handling high load 
Most Voted
E. 
Instrument the production services to record every transaction for replay by the load-testing tool
F. 
Instrument the load-testing tool and the target services with detailed logging and metrics collection 
Most Voted
Correct Answer:
 
BDF 
Comments
rishab86
rishab86
 
Highly Voted
 
3 years, 1 month ago
after reading link: 
https://cloud.google.com/bigtable/docs/performance
A:Run your typical workloads against Bigtable :Always run your own typical workloads against a Bigtable cluster when doing
capacity planning, so you can figure out the best resource allocation for your applications.
B. Create a separate Google Cloud project to use for the load-testing environment
F : The most important/standard factor of testing, you gather logs and metrics in TEST environment for further scaling.
upvoted 
32 
times
mikesp
mikesp
 
2 years, 8 months ago
I agree. It is important to verity that current BitTable cluster can deal with incoming traffic: 
A cluster must have enough nodes to support its current workload and the amount of data it stores. Otherwise, the cluster might
not be able to handle incoming requests, and latency could go up.
So although it is a managed service, it does not auto-scale.
upvoted 
2 
times
AK2020
AK2020
 
3 years ago
Community vote distribution
BDF (38%)
ABF (33%)
BEF (24%)
Other
(5%)There is no relevance to D here. So ABF
upvoted 
4 
times
PeppaPig
PeppaPig
 
Highly Voted
 
2 years, 10 months ago
AB&F 
Creating a separate project is highly recommended. It gives you total isolation from your product environment, and make sure it will
not share the resources with your product env such as service quota
upvoted 
9 
times
PeppaPig
PeppaPig
 
2 years, 10 months ago
You won't want load testing to consume the service quotas in your product project
https://cloud.google.com/docs/quota
upvoted 
1 
times
gcloud007
gcloud007
 
Most Recent
 
2 days, 17 hours ago
Selected Answer: 
BDF
The first line in the below doc reads ... "Bigtable delivers highly predictable performance that is linearly scalable. " then why do you
need to test Bigtable performance... https://cloud.google.com/bigtable/docs/performance
upvoted 
1 
times
Nad1122
Nad1122
 
4 months, 2 weeks ago
Selected Answer: 
ABF
It's ABF
upvoted 
1 
times
e5019c6
e5019c6
 
6 months, 1 week ago
Selected Answer: 
BDF
A: No. Not needed since it's a managed GCP product. It'll scale to satisfy demand.
B: Yes. You could leave it in the same project as the app, but it'll eventually be deployed to production and be a risk if anyone
accidentally runs it against prod.
C: No. You musn't run load testing against prod.
D: Yes. The capability of the third party systems should be tested. They are another link in the chain and if they are not up to the
task, they may be replaced.
E: No. There is no need to use real data in the requests, this is a load test, not a behavior one.
F: Yes. Having detailed logs and metrics helps diagnosing problems during the tests.
upvoted 
7 
times
guzmanelmalo
guzmanelmalo
 
7 months ago
Selected Answer: 
ABF
The quota impact of not using an isolated project in a region with higher quota make me think about ABF
upvoted 
1 
times
sampon279
sampon279
 
1 year ago
Don't think it would be this option: Ensure all third-party systems your services use is capable of handling high load. This is some
extra information and we are not sure if the application in this question is even using any third party tools.
upvoted 
1 
times
jlambdan
jlambdan
 
1 year, 3 months ago
Selected Answer: 
BEF
Here is my take, I respectfully disagree with ya all :)
A. Ensure that the load tests validate the performance of Cloud Bigtable Most Voted
=> not the requirement
B. Create a separate Google Cloud project to use for the load-testing environment Most Voted
=> yes, you don't want to use production quota.
C. Schedule the load-testing tool to regularly run against the production environment
=> yes please kill the prod ! 
D. Ensure all third-party systems your services use is capable of handling high load
=> well, that is what we shall test, so, it was more the task of the development team, not the QA team.
E. Instrument the production services to record every transaction for replay by the load-testing tool
=> yes, this way you can build your test dataset with realistic behavior.=> yes, this way you can build your test dataset with realistic behavior.
F. Instrument the load-testing tool and the target services with detailed logging and metrics collection Most Vo
=> yes, otherwise you test for nothing, you have no data at the end to evaluate the system's performance.
upvoted 
5 
times
ductrinh
ductrinh
 
9 months, 1 week ago
E. Instrument the production services to record every transaction for replay by the load-testing tool
=> yes, this way you can build your test dataset with realistic behavior.
lol you dont test on prd but still ned prd's records .... how can bro
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Answer ADF
A: It is important to ensure that the load-testing tool is able to accurately test the performance of Cloud Bigtable in order to ensure
that it can handle the expected load.
D: It is important to ensure that all third-party systems that your primary cloud services rely on are able to handle the expected load
in order to avoid any potential bottlenecks or failures.
F: Instrumenting the load-testing tool and the target services with detailed logging and metrics collection can provide valuable
insights into the performance and behavior of the system under test, allowing the QA team to identify any potential issues or
bottlenecks.
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Why not B, C and E:
B: creating a separate Google Cloud project to use for the load-testing environment, could also be a good idea by not necessary
in order to ensure that the load tests do not impact the performance of the production environment.
C: scheduling the load-testing tool to regularly run against the production environment, is not recommended, as this could
potentially impact the performance of the production environment and could lead to unexpected behavior or issues.
E: instrumenting the production services to record every transaction for replay by the load-testing tool, could also be a useful
requirement, as it would allow the QA team to accurately replay real-world workloads during the load tests in order to more
accurately simulate the expected production environment.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
ABF
ABF is the correct answer
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
ABF
A, B, F are the correct answer
upvoted 
1 
times
andras
andras
 
1 year, 8 months ago
why testing Bigtable... it's per definition of Google would absorb practically any load... don't you trust Google? :-)
upvoted 
5 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
ABF is right
upvoted 
2 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months ago
Selected Answer: 
ADF
There is no necessary reason for running it in a separate project.
A we have to test Bigtable.
F Important to record all the outputs and be able to review it.
D Important to stress test third party solutions or change it.
upvoted 
1 
timesupvoted 
1 
times
AMEJack
AMEJack
 
1 year, 9 months ago
It is Google best practice to create a separate project for testing
upvoted 
2 
times
kiappy81
kiappy81
 
1 year, 9 months ago
hi, in the sentence it's underline that you what to test the scalability of your primary CLOUD SERVICS, so I think that D is not
required. For me it's ABF
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
ABF
ABF is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
ABF
Vote ABF
upvoted 
2 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
BCF
1) B - you need to have a separate project for Load-Testing tool. That would at least separate role based access - dev and test.
Also, test will have their own code/project/config for testing, so no any chance of collision.
2) C - testing on production? because per Google recommendation, BigTable should be tested on production instances (not on
development) and for at least 10 min / 300 GB of data. Check "Testing Performance with Cloud Bigtable" here. 
I understand this as a requirement for integration test for projects using BigTable. Testing of BigTable on Dev instances won't give
proper results.
3) F - collecting of metrics would be useful anyway....
Why not A, D, E?
1) A - isolation testing of BigTable likely doesn't make sense, if anyway integration test will need to run. That's covered in C.
2) D - testing of 3rd party tools in isolated mode likely is a one time effort (only useful when upgrading these tools). No point to run
them regularly.
3) E - collecting metrics on production env just to replay them on load-testing tool? What's a point? We need test max load
anyway...
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #165
Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of
all projects in the organization. You provision the Google Cloud Resource Manager and set up yourself as the org admin. 
What Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team? 
A. 
Org viewer, project owner
B. 
Org viewer, project viewer 
Most Voted
C. 
Org admin, project browser
D. 
Project owner, network admin
Correct Answer:
 
B 
Comments
shandy
shandy
 
Highly Voted
 
4 years, 1 month ago
A is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.
B is correct because:-Org viewer grants the security team permissions to view the organization's display name.
-Project viewer grants the security team permissions to see the resources within projects.
C is not correct because Org admin is too broad. The security team does not need to be able to make changes to the
organization.
D is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.
upvoted 
31 
times
Pr44
Pr44
 
1 year ago
I agree.
upvoted 
1 
times
Eroc
Eroc
 
Highly Voted
 
4 years, 2 months ago
B is the best answer because according to Google documentation i is best to use predefined roles and give the every team the
least amount of access. (https://cloud.google.com/iam/docs/using-iam-securely) The question states the security must be able to
view things, and the viewer role allows just that.
upvoted 
12 
times
Community vote distribution
B (100%)upvoted 
12 
times
tartar
tartar
 
3 years, 5 months ago
B is ok
upvoted 
6 
times
tamj123
tamj123
 
Most Recent
 
2 months, 2 weeks ago
B, security team want to have visibility to all the project, so viewer to Org and Project is sufficiency.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year ago
Selected Answer: 
B
B is the correct answer
upvoted 
1 
times
allen_y_q_huang
allen_y_q_huang
 
1 year, 1 month ago
Agree B as security team does not need Project owner permission, but why need to grant project viewer after granting
organization viewer?
upvoted 
1 
times
Nirca
Nirca
 
1 year, 3 months ago
Selected Answer: 
B
B. Org viewer, project viewer!
upvoted 
1 
times
mahima123k
mahima123k
 
1 year, 5 months ago
Very similar question was presented on 15 July 2022 exam
upvoted 
3 
times
Bill76
Bill76
 
1 year, 5 months ago
Are the 260 exam topic questions enough to pass the exam?
upvoted 
3 
times
methamode
methamode
 
1 year, 8 months ago
Selected Answer: 
B
B is the answer!
upvoted 
1 
times
Surls
Surls
 
2 years ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
vincy2202
vincy2202
 
2 years ago
Selected Answer: 
B
B is the correct answer
upvoted 
1 
times
nqthien041292
nqthien041292
 
2 years, 1 month ago
Selected Answer: 
B
Vote B
upvoted 
1 
times
mudot
mudot
 
2 years, 1 month ago
Selected Answer: 
B
A is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.A is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.
B is correct because:
-Organization viewer grants the security team permissions to view the organization's display name.
-Project viewer grants the security team permissions to see the resources within projects.
C is not correct because Organization Administrator is too broad. The security team does not need to be able to make changes to
the organization.
D is not correct because Project Owner is too broad. The security team does not need to be able to make changes to projects.
upvoted 
2 
times
bala786
bala786
 
2 years, 6 months ago
Option B is correct as per Least Privilege
upvoted 
2 
times
victory108
victory108
 
2 years, 7 months ago
B. Org viewer, project viewer
upvoted 
2 
times
un
un
 
2 years, 8 months ago
B is correct
upvoted 
1 
times
lynx256
lynx256
 
2 years, 9 months ago
B is ok
upvoted 
1 
times
Ausias18
Ausias18
 
2 years, 9 months ago
Answer is B
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #166
Your company places a high value on being responsive and meeting customer needs quickly. Their primary business objectives
are release speed and agility. You want to reduce the chance of security errors being accidentally introduced. 
Which two actions can you take? (Choose two.) 
A. 
Ensure every code check-in is peer reviewed by a security SME
B. 
Use source code security analyzers as part of the CI/CD pipeline 
Most Voted
C. 
Ensure you have stubs to unit test all interfaces between components
D. 
Enable code signing and a trusted binary repository integrated with your CI/CD pipeline
E. 
Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline
Most Voted
Correct Answer:
 
BE 
Comments
PeppaPig
PeppaPig
 
Highly Voted
 
3 years, 4 months ago
B&E
Code signing only verifies the author. In other words it only check who you are, but not what have you done
upvoted 
50 
times
robotgeek
robotgeek
 
3 years, 1 month ago
I understand that would be a requirement for security
upvoted 
2 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 3 months ago
But when we select E , it might auto include B . SOme VA scanning tools also do SAST.
So why choose B and E in that case. 
D makes more sense with E .
Authorised repo will add an additional layer of security with verified images and artifacts in it.
upvoted 
2 
times
squishy_fishy
squishy_fishy
 
1 year ago
Community vote distribution
BE (48%)
DE (38%)
Other (14%)squishy_fishy
squishy_fishy
 
1 year ago
At work, we do B and E.
upvoted 
1 
times
rishab86
rishab86
 
Highly Voted
 
3 years, 7 months ago
I think answer is D & E.
upvoted 
36 
times
AK2020
AK2020
 
3 years, 6 months ago
Agree with this. https://cloud.google.com/container-registry/docs/container-analysis
upvoted 
3 
times
ravisar
ravisar
 
3 years, 1 month ago
Here the question is to provide solution for "Speed and Agility". The Binary authorization prevent unauthorized deployments in
production for GKE, Anthos Servicemesh and Cloud run, however will add delay in deployment process. So D may not be
suitable in this scenario. Answer is B&E.
upvoted 
10 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 3 months ago
Speed will nit get hampered if the images are verified and attested. Checks need to be there. If you argument would be true
than why to introduce VA scanner , as that will also induce delay in deployment.
when we select E , it might auto include B . Some VA scanning tools also do SAST.
So why choose B and E in that case.
D makes more sense with E .
Authorised repo will add an additional layer of security with verified images and artifacts in it.
Answer - D & E
upvoted 
2 
times
balajisreenivas
balajisreenivas
 
Most Recent
 
2 weeks ago
Selected Answer: 
BE
B. Source Code Security Analyzers:
Integrating source code security analyzers into the CI/CD pipeline helps identify vulnerabilities in the codebase early in the
development cycle. This ensures that security errors are caught and addressed before they make it into production.
E. Vulnerability Security Scanner:
Running a vulnerability scanner as part of the CI/CD pipeline identifies weaknesses in dependencies, configurations, and deployed
artifacts. This provides an additional layer of security by detecting risks that might not be evident in the source code alone.
upvoted 
2 
times
Qix
Qix
 
3 weeks, 2 days ago
The question clearly states that "primary business objective are release speed and agility". To achieve this, you should have good
unit tests in place (C).
For this reason I think BC is a more balanced choice.
upvoted 
1 
times
vjk1991
vjk1991
 
1 month ago
Selected Answer: 
BD
BD, D because it enables only validated trusted images to be deployed.
upvoted 
1 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
1 month, 1 week ago
Selected Answer: 
BE
SAST & DAST 
( @aAbdelhamid: our EA Work hahahahaha )
upvoted 
1 
times
wooyourdaddy
wooyourdaddy
 
3 months, 1 week ago
Selected Answer: 
BE
Option D does not directly address the primary concern of reducing the chance of security errors being accidentally introduced.
Here’s why:Focus on Integrity: Code signing and using a trusted binary repository primarily ensure that the code and binaries have not been
tampered with and are from a trusted source. While this is important for security, it doesn’t specifically target the detection and
prevention of security vulnerabilities within the code itself.
Indirect Impact on Security Errors: While code signing can help prevent the introduction of malicious code, it doesn’t directly scan
for or identify security vulnerabilities that might be accidentally introduced by developers.
upvoted 
2 
times
pico
pico
 
8 months ago
Selected Answer: 
BE
why the other options aren't as ideal:
A. Ensure every code check-in is peer reviewed by a security SME: Manual reviews can become a bottleneck in agile
environments and are less scalable than automated tools.
C. Ensure you have stubs to unit test all interfaces between components: Good practice, but primarily focuses on functional rather
than security testing.
D. Enable code signing and a trusted binary repository...: Integrity checks are essential but don't directly prevent the introduction of
the security errors themselves.
upvoted 
1 
times
phantomsg
phantomsg
 
9 months, 3 weeks ago
Selected Answer: 
BE
Cyber Sec professional here. Question asks to reduce chance of security errors accidentally introduced. This means to integrate
Static Application Security Tests (SAST) and Dynamic Application Security Tests (DAST) as part of CI/CD pipeline. Hence B and
E are the right match. D is to ensure only trusted code is deployed to production, not reduce 'security error accidentally
introduced'.
upvoted 
4 
times
OrangeTiger
OrangeTiger
 
11 months, 2 weeks ago
I guess A and C are both time consuming and labor intensive. Also, aren't C stubs supposed to be used for unit tests?
What remains is BDE.
B is source code inspection.
Doing D ensures that the repository is not contaminated.
E's vulnerability scan detects whether there are any CVEs.
I think all of them are correct. If you had to choose two, what would it be?
Isn't it really slow if you do B and E?
upvoted 
1 
times
02fc23a
02fc23a
 
1 year, 1 month ago
Selected Answer: 
DE
https://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services
upvoted 
5 
times
cchiaramelli
cchiaramelli
 
1 year, 2 months ago
Selected Answer: 
DE
The thing that makes me think D makes sense is that it ensures that only images that have passed though the configured CI/CD
pipeline (with vulnerability checks) will be able to be deployed. This is better explained here:
https://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-
vulnerability-scanning
upvoted 
2 
times
cchiaramelli
cchiaramelli
 
1 year, 2 months ago
Selected Answer: 
DE
https://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-
vulnerability-scanning
upvoted 
1 
times
steghe
steghe
 
1 year, 2 months ago
Selected Answer: 
BE
Code signing only verifies the author not content
upvoted 
1 
timesupvoted 
1 
times
someone2011
someone2011
 
1 year, 3 months ago
DE
https://cloud.google.com/blog/products/containers-kubernetes/guard-against-security-vulnerabilities-with-container-registry-
vulnerability-scanning
upvoted 
2 
times
sampon279
sampon279
 
1 year, 6 months ago
Selected Answer: 
BE
trusted binary repository option seems a static thing. For a release if we haven not used any new packages, trusted binary
repository would not add any extra value. So B&E which will are needed for every checking/release.
upvoted 
1 
times
red_panda
red_panda
 
1 year, 6 months ago
Selected Answer: 
BE
B and E is the answer for me also.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #167
You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes. 
What should you do? 
A. 
Add additional nodes to your Kubernetes Engine cluster using the following command: gcloud container clusters resize
CLUSTER_Name 
ג
- "€size 10
B. 
Add a tag to the instances in the cluster with the following command: gcloud compute instances add-tags INSTANCE - -
tags enable- autoscaling max-nodes-10
C. 
Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update
mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10 
Most Voted
D. 
Create a new Kubernetes Engine cluster with the following command: gcloud alpha container clusters create mycluster -
-enable- autoscaling - -min-nodes=1 - -max-nodes=10 and redeploy your application
Correct Answer:
 
C 
Comments
AWS56
AWS56
 
Highly Voted
 
3 years, 11 months ago
Agree C
upvoted 
24 
times
Eroc
Eroc
 
Highly Voted
 
4 years, 2 months ago
A is incorrect because there is supposed to be two hypens "--" not one before size
(https://cloud.google.com/sdk/gcloud/reference/container/clusters/resize). B is incorrect because it just adds a string to the
cluster (https://cloud.google.com/sdk/gcloud/reference/compute/instances/add-tags). "C" is just as wrong as "A" because the
documentation says it should be "--max-nodes" followed by "--min-nodes"
(https://cloud.google.com/sdk/gcloud/reference/alpha/container/clusters/update), also the alpha command no longer works
but it used to and is still up on google docs. This goes for "D" as well but D talks about making another, which doesn't have to
be done because one it already up. So the debate is between A and C, and C used to work so C was chosen, although C also has
spaces which never worked... So this question is an absolute thug tactic by a Google team to steal from the Google kingdom
preventing the establishment of their library by failing people that actually know the science behind the technology. When you
see this question at a test center I'd select C.
upvoted 
11 
times
tartar
tartar
 
3 years, 5 months ago
Community vote distribution
C (100%)tartar
tartar
 
3 years, 5 months ago
C is ok
upvoted 
9 
times
tartar
tartar
 
3 years, 4 months ago
To enable autoscaling for an existing node pool, run the following command:
gcloud container clusters update cluster-name --enable-autoscaling \
--min-nodes 1 --max-nodes 10 --zone compute-zone --node-pool default-pool
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler
upvoted 
9 
times
svjl
svjl
 
3 years ago
You didn't check the documentation.
upvoted 
4 
times
piyu1515
piyu1515
 
Most Recent
 
3 days, 23 hours ago
Selected Answer: 
C
Agreed with C because 
Explanation:
Autoscaling in GKE
Kubernetes Engine supports Cluster Autoscaler, which automatically adjusts the size of a node pool based on the resource
demands of the workloads.
Enabling autoscaling allows the cluster to add or remove nodes dynamically to handle increased load or scale down during low
activity.
Updating the Existing Cluster
The command in option C updates an existing cluster with autoscaling enabled, setting minimum (1) and maximum (10) node
limits.
This avoids downtime and does not require recreating the cluster or redeploying applications, ensuring a seamless transition to
autoscaling.
upvoted 
1 
times
anil23
anil23
 
4 months, 3 weeks ago
Agree C
upvoted 
1 
times
AugustoKras011111
AugustoKras011111
 
10 months, 1 week ago
Selected Answer: 
C
no need to create a new one, just update!
upvoted 
4 
times
zerg0
zerg0
 
11 months ago
Selected Answer: 
C
See the cli docs
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
gonlafer
gonlafer
 
1 year ago
Selected Answer: 
C
It's C
upvoted 
1 
times
megumin
megumin
 
1 year, 1 month agomegumin
megumin
 
1 year, 1 month ago
Selected Answer: 
C
ok for C
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 2 months ago
I agree with C
upvoted 
2 
times
vincy2202
vincy2202
 
2 years ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
Bobch
Bobch
 
2 years ago
Selected Answer: 
C
C looks OK
upvoted 
1 
times
TheCloudBoy77
TheCloudBoy77
 
2 years, 1 month ago
C - cluster is already running so use update instead of create new cluster.
upvoted 
6 
times
[Removed]
[Removed]
 
2 years, 2 months ago
Answer should 
be C. Now alpha command is not needed. seems question is older and now 
kubernets command is not with
alpha.
gcloud container clusters update cluster-name --enable-autoscaling 
....
upvoted 
5 
times
Examster1
Examster1
 
2 years, 3 months ago
This couldn’t be C, you shouldn’t use alpha commands in a production(app) workload.
upvoted 
1 
times
kopper2019
kopper2019
 
2 years, 6 months ago
C is the way to go min and max and done
upvoted 
2 
times
victory108
victory108
 
2 years, 7 months ago
C. Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update
mycluster - -enable- autoscaling - -min-nodes=1 - -max-nodes=10
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #168
Your marketing department wants to send out a promotional email campaign. The development team wants to minimize direct
operation management. They project a wide range of possible customer responses, from 100 to 500,000 click-through per day.
The link leads to a simple website that explains the promotion and collects user information and preferences. 
Which infrastructure should you recommend? (Choose two.) 
A. 
Use Google App Engine to serve the website and Google Cloud Datastore to store user data. 
Most Voted
B. 
Use a Google Container Engine cluster to serve the website and store data to persistent disk.
C. 
Use a managed instance group to serve the website and Google Cloud Bigtable to store user data. 
Most Voted
D. 
Use a single Compute Engine virtual machine (VM) to host a web server, backend by Google Cloud SQL.
Correct Answer:
 
AC 
Comments
rishab86
rishab86
 
Highly Voted
 
3 years, 7 months ago
A & C seems to be the correct answer.
upvoted 
27 
times
victory108
victory108
 
Highly Voted
 
3 years, 6 months ago
A. Use Google App Engine to serve the website and Google Cloud Datastore to store user data.
C. Use a managed instance group to serve the website and Google Cloud Bigtable to store user data.
upvoted 
9 
times
J19G
J19G
 
3 years, 2 months ago
Why not D?
upvoted 
1 
times
Bert_77
Bert_77
 
3 years ago
Because a single GCE instance might not be able to handle the unpredictable load
upvoted 
6 
times
Sephethus
Sephethus
 
Most Recent
 
6 months, 2 weeks ago
Community vote distribution
AC (100%)Sephethus
Sephethus
 
Most Recent
 
6 months, 2 weeks ago
Selected Answer: 
AC
Both bigtable and datastore seem like overkill solutions but A&C are the only options that make sense here. In the real world
use BigQuery and either App Engine or Cloud Run.
upvoted 
3 
times
anil23
anil23
 
1 year, 4 months ago
Why not B, GKE si best fit and preferred over MI
upvoted 
1 
times
piyu1515
piyu1515
 
3 days, 23 hours ago
Because they want to minimize direct operation management
upvoted 
1 
times
zerg0
zerg0
 
1 year, 11 months ago
Selected Answer: 
AC
Cloud Data store and Big Table are the only solutions that can handle 500000 clicks
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
A: Google App Engine is a fully managed platform for building and running web applications and APIs. It can automatically
scale to meet high traffic demands, making it a good choice for serving the website for the promotional email campaign.
Google Cloud Datastore can also scale automatically to meet high traffic demands, making it a good choice for storing user
data.
C: A managed instance group are managed as a single entity and can automatically scale up or down based on demand. This
makes it a good choice for serving the website for the promotional email campaign. Google Cloud Bigtable is a fully managed,
high-performance NoSQL database that can store and serve large amounts of structured data with low latency. It is designed to
scale horizontally and can handle high traffic demands, making it a good choice for storing user data.
upvoted 
6 
times
omermahgoub
omermahgoub
 
2 years ago
B, using a Google Container Engine cluster to serve the website and store data to persistent disk, could be a valid solution as
well. However, persistent disks may not be able to scale horizontally to meet high traffic demands, which could impact the
performance of the website.
D, using a single Compute Engine VM to host a web server, backed by Google Cloud SQL, would not be a good choice for this
scenario. A single VM would not be able to scale to meet the wide range of possible traffic levels for the promotional email
campaign, and Google Cloud SQL may not be able to scale horizontally to meet high traffic demands.
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A and C is right choice, D is saying single VM
upvoted 
3 
times
Nirca
Nirca
 
2 years, 3 months ago
Selected Answer: 
AC
AC (100%) !!!
upvoted 
2 
times
Nirca
Nirca
 
2 years, 3 months ago
Selected Answer: 
AC
A & C seems to be the correct answer.
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
AC
AC is the correct answer.
upvoted 
1 
timesupvoted 
1 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
A only, choose two - App Engine + Datastore
Use GAE to serve the website and Google Datastore to store user data.
GCE – is too complex solution with specific OS to maintain.
GKE – is for microservices apps, and Persistent Disk is not good solution for relational data storage;
GAE – is fast and reliable solution, you write just code and run it on fully managed service. DataStore also matches perfectly
since intended for storing user profiles, key-value pairs.
upvoted 
1 
times
alan9999
alan9999
 
3 years, 6 months ago
A & B with less operations management. Also Containers and App Engine as the clicks varies.
upvoted 
3 
times
poseidon24
poseidon24
 
3 years, 5 months ago
"Google Container Engine" does not exist, only "GKE", but operating a Kubernetes cluster is not easy, in that case, an option
could be Cloud Run.
upvoted 
1 
times
AK2020
AK2020
 
3 years, 6 months ago
But user data storing in persistent disks? Not correct to me. Seems A & C
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #169
Your company just finished a rapid lift and shift to Google Compute Engine for your compute needs. You have another 9
months to design and deploy a more cloud-native solution. Specifically, you want a system that is no-ops and auto-scaling. 
Which two compute products should you choose? (Choose two.) 
A. 
Compute Engine with containers
B. 
Google Kubernetes Engine with containers 
Most Voted
C. 
Google App Engine Standard Environment 
Most Voted
D. 
Compute Engine with custom instance types
E. 
Compute Engine with managed instance groups
Correct Answer:
 
BC 
Comments
PeppaPig
PeppaPig
 
Highly Voted
 
3 years, 5 months ago
I would go with B&C
Cloud-native, less-ops and auto-scaling all get addressed
upvoted 
18 
times
kinghin
kinghin
 
Highly Voted
 
2 years, 9 months ago
Why E is incorrect? can't MIG also perform autoscaling? Also it needs fewer administration as GKE
upvoted 
9 
times
AhmedH7793
AhmedH7793
 
2 years, 3 months ago
No ops = Serverless / Almost Serverless MIG is not.
upvoted 
8 
times
JaimeMS
JaimeMS
 
6 months, 3 weeks ago
No Ops -> Kubernetes?
This question is too generic to choose between MIG and GKE. In the context of this exam, I would choose B (GKE), specally
with current options as Autopilot. But in my day to day I would consider way more factors.
Community vote distribution
BC (66%)
CE (34%)with current options as Autopilot. But in my day to day I would consider way more factors.
upvoted 
2 
times
piyu1515
piyu1515
 
Most Recent
 
3 days, 23 hours ago
App Engine Standard is a Platform-as-a-Service (PaaS) offering that’s fully managed, allowing you to deploy applications with zero
infrastructure management.
It supports auto-scaling out-of-the-box and handles traffic spikes automatically.
It’s great for stateless applications, APIs, and web applications that need rapid deployment and minimal operations.
Best Use Case:
Ideal for teams looking for a serverless approach without worrying about infrastructure scaling or patching.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 8 months ago
Selected Answer: 
BC
Option B, Google Kubernetes Engine (GKE) with containers, is a managed Kubernetes service that automatically manages and
scales containerized applications. GKE handles cluster management tasks like scaling, upgrades, and security patches, allowing
you to focus on the application itself.
Option C, Google App Engine Standard Environment, is a fully managed platform for building and deploying applications. It
automatically scales applications based on demand and provides a no-ops experience. With App Engine Standard Environment,
you don't need to worry about infrastructure management, as Google handles it for you.
upvoted 
5 
times
jlambdan
jlambdan
 
1 year, 9 months ago
Selected Answer: 
BC
B: GKE with autopilot mode for workload not requiring ingress or egress. Otherwise you will need some ops work IMHO.
C: app engine for workload requiring ingress. It comes with autoscaling features and rolling update features without being as heavy
as gke.
upvoted 
2 
times
Deb2293
Deb2293
 
1 year, 9 months ago
Selected Answer: 
CE
I would still go for C & E. My take is GKE still requires some operational overhead for managing the Kubernetes cluster and
ensuring high availability of the workloads.
Hence C & E would be most suitable one.
upvoted 
5 
times
telp
telp
 
1 year, 10 months ago
Selected Answer: 
BC
No ops: use container or gcp product without mangement.
So not VM possible in the answer
upvoted 
1 
times
habros
habros
 
2 years, 1 month ago
Selected Answer: 
BC
App Engine standard = container based (can even go to zero)
App Engine flexible = VM based (minimum 1)
No ops: container > VM
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
BC
B & C seems right to me, E needs lots of Ops to build image, instance template and instance group, ... maintain your image
always
upvoted 
3 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
B. Google Kubernetes Engine with containers
C. Google App Engine Standard Environmen
upvoted 
2 
times
 
2 years, 3 months agoalexandercamachop
alexandercamachop
 
2 years, 3 months ago
Selected Answer: 
BC
No ops = Serverless / Almost Serverless, less operational management overhead.
Kubernetes and App Engine are the only one that gives us that flexibility, plus is modernizing apps
upvoted 
2 
times
6721sora
6721sora
 
2 years, 3 months ago
Selected Answer: 
CE
C and E
GKE is absolutely nor no-ops.
MIG can be closest to no-ops among the other options
upvoted 
5 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
2 years, 4 months ago
Selected Answer: 
BC
B&C seem to be right for this question. In reality, whoever really proposes B as an option never ran Kubernetes in production.
upvoted 
2 
times
JoeyCASD
JoeyCASD
 
2 years, 7 months ago
Vote A and B
However I think option B should address more specifically, like GKE - autopilot mode.
upvoted 
1 
times
JoeyCASD
JoeyCASD
 
2 years, 7 months ago
Correct the answer for B and C
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
BC
BC are the correct answers
upvoted 
1 
times
Bobch
Bobch
 
3 years ago
Selected Answer: 
BC
Agree B and C
upvoted 
1 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
Correct Answer: BC
B: With Container Engine, Google will automatically deploy your cluster for you, update, patch, secure the nodes.
Kubernetes Engine's cluster autoscaler automatically resizes clusters based on the demands of the workloads you want to run.
C: Solutions like Datastore, BigQuery, AppEngine, etc are truly NoOps.
App Engine by default scales the number of instances running up and down to match the load, thus providing consistent
performance for your app at all times while minimizing idle instances and thus reducing cost.
upvoted 
3 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
Note: At a high level, NoOps means that there is no infrastructure to build out and manage during usage of the platform. Typically,
the compromise you make with NoOps is that you lose control of the underlying infrastructure.
https://www.quora.com/How-well-does-Google-Container-Engine-support-Google-Cloud-Platform%E2%80%99s-NoOps-claim
B – Google Container Engine (autoscaling)
C – Google AppEngine Standard Environment (no ops)
You should understand this Q as following: after Lift-n-Shift parts of the monolith should be moved to managed services (e.g.
REST API) running on GAE; and other micro-services will run in containers / pods.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #170
One of your primary business objectives is being able to trust the data stored in your application. You want to log all changes
to the application data. 
How can you design your logging system to verify authenticity of your logs? 
A. 
Write the log concurrently in the cloud and on premises
B. 
Use a SQL database and limit who can modify the log table
C. 
Digitally sign each timestamp and log entry and store the signature 
Most Voted
D. 
Create a JSON dump of each log entry and store it in Google Cloud Storage
Correct Answer:
 
C 
Comments
get2dd
get2dd
 
Highly Voted
 
3 years, 4 months ago
Correct answer is C (verified from Question Bank in Whizlabs.com) 
Feedback
C (Correct answer) - Digitally sign each timestamp and log entry and store the signature.
Answer A, B, and D don’t have any added value to verify the authenticity of your logs. Besides, Logs are mostly suitable for
exporting to Cloud storage, BigQuery, and PubSub. SQL database is not the best way to be exported to nor store log data.
Simplified Explanation
To verify the authenticity of your logs if they are tampered with or forged, you can use a certain algorithm to generate digest by
hashing each timestamp or log entry and then digitally sign the digest with a private key to generate a signature. Anybody with your
public key can verify that signature to confirm that it was made with your private key and they can tell if the timestamp or log entry
was modified. You can put the signature files into a folder separate from the log files. This separation enables you to enforce
granular security policies.
upvoted 
31 
times
JoeShmoe
JoeShmoe
 
Highly Voted
 
4 years, 7 months ago
C is correct and common practice
upvoted 
24 
times
666Amitava666
666Amitava666
 
Most Recent
 
2 months, 1 week ago
Selected Answer: 
C
Community vote distribution
C (100%)Digitally sign each timestamp and log entry and store the signature
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
I would recommend option C, digitally signing each timestamp and log entry and storing the signature. Digitally signing a log entry
involves creating a cryptographic hash of the log entry and a timestamp, and then encrypting the hash using a private key. The
encrypted hash, known as the signature, can be stored along with the log entry in a secure manner. To verify the authenticity of the
log entry, you can use the public key associated with the private key used to create the signature to decrypt the signature and
recreate the hash. If the recreated hash matches the original hash, it indicates that the log entry has not been tampered with and is
authentic.
upvoted 
4 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Writing the log concurrently in the cloud and on premises, would not necessarily help to verify the authenticity of the logs, so A is
not an option
B, using a SQL database and limiting who can modify the log table, could help to prevent unauthorized modification of the logs,
but it would not necessarily provide a way to verify the authenticity of the logs if they are modified by an authorized user.
Option D, creating a JSON dump of each log entry and storing it in Google Cloud Storage, would not necessarily help to verify the
authenticity of the logs.
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
Digitally signing is correct. C is right option!
upvoted 
2 
times
GMats
GMats
 
2 years, 6 months ago
C is correct.You can use deterministic algorithm to validate hash values.
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
C
vote C
upvoted 
2 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
C – Digitally sign each timestamp and log entry and store the signature.
This is fun Q where all options are technically correct. But, the point is to find most efficient. Since, Q asks about verification of log
entry - then you don't need to dub it. Using of much shorter timestamp-hash pair will address the request. So, when reading log
from original source, you also read hash for this timestamp and then verify the entry's body. 
BTW, this is one of general purpose questions, which is not directly related to GCP. Just checks your attentiveness
A - is about duplication, can work, but redundant;
B / D - both have similar design, but don’t allow verification of entry. No cross-checking of entry. E.g. person having access to log
can change it in one place.
C - storing log in one place, and hash-code in another. So, even if "trusted" person has modified original log, then it will break
correspondence with hash code in other storage. That storage should be available only for authentication program (via service
account).
upvoted 
5 
times
Neo_ACE
Neo_ACE
 
2 years, 8 months ago
If you attended recently, Please update some new questions too. It would be great help
upvoted 
2 
times
Wonka
Wonka
 
2 years, 5 months ago
@MaxNRG, very clearly articulated elimination technique. BTW are these questions appearing in actual exam?@MaxNRG, very clearly articulated elimination technique. BTW are these questions appearing in actual exam?
upvoted 
1 
times
aviratna
aviratna
 
3 years ago
C is correct
upvoted 
1 
times
Amrit00009
Amrit00009
 
3 years, 1 month ago
C seems like the right answer
upvoted 
1 
times
victory108
victory108
 
3 years, 1 month ago
C. Digitally sign each timestamp and log entry and store the signature
upvoted 
1 
times
Amber25
Amber25
 
3 years, 1 month ago
C (Correct answer) - Digitally sign each timestamp and log entry and store the signature.
Other options are possible to export logs but won't be able to verify authenticity of logs
upvoted 
2 
times
un
un
 
3 years, 1 month ago
C is correct
upvoted 
1 
times
mrhege
mrhege
 
3 years, 2 months ago
I'm on the fence between C and D. C is a good practice but D can do the job as well as versioned objects might be able to do job
at some level... Now, C tells that only the signature would be stored which is obviously not enough, but the owner of versioned
objects might be tampered too... IDK
upvoted 
1 
times
lynx256
lynx256
 
3 years, 3 months ago
IMO - C is ok
upvoted 
1 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answers is C
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #171
Your company has a Google Workspace account and Google Cloud Organization. Some developers in the company have created
Google Cloud projects outside of the Google Cloud Organization. 
You want to create an Organization structure that allows developers to create projects, but prevents them from modifying
production projects. You want to manage policies for all projects centrally and be able to set more restrictive policies for
production projects. 
You want to minimize disruption to users and developers when business needs change in the future. You want to follow Google-
recommended practices. Now should you design the Organization structure? 
A. 
1. Create a second Google Workspace account and Organization. 2. Grant all developers the Project Creator IAM role on
the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on both
Organizations. 5. Additionally, set the production policies on the original Organization.
B. 
1. Create a folder under the Organization resource named 
ג
€Production.
2
€ 
ג
 .Grant all developers the Project Creator
IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all
projects on the Organization. 5. Additionally, set the production policies on the 
ג
€Production
ג
 €folder.
C. 
1. Create folders under the Organization resource named 
ג
€Development
ג
 €and 
ג
€Production.
2
€ 
ג
 .Grant all developers
the Project Creator IAM role on the 
ג
€Development
ג
 €folder. 3. Move the developer projects into the 
ג
€Development
ג
€
folder. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the
ג
€Production
ג
 €folder. 
Most Voted
D. 
1. Designate the Organization for production projects only. 2. Ensure that developers do not have the Project Creator
IAM role on the Organization. 3. Create development projects outside of the Organization using the developer Google
Workspace accounts. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on
the individual production projects.
Correct Answer:
 
C 
Comments
cloudmon
cloudmon
 
Highly Voted
 
1 year, 9 months ago
Selected Answer: 
C
C, because managing multiple organizations is not a Google best practice
Community vote distribution
C (97%)
D
(3%)C, because managing multiple organizations is not a Google best practice
upvoted 
14 
times
devnul
devnul
 
Most Recent
 
4 months ago
the requirement is "...You want to manage policies for all projects centrally..." With multiple organizations that wont be possible
as you would have to set policies on multiple organizations. Therefore I opt for "C".
upvoted 
1 
times
aaa7
aaa7
 
6 months ago
for everyone commenting that multiple organizations is bad practice according to google check
https://cloud.google.com/architecture/identity/best-practices-for-planning
upvoted 
3 
times
cerveza7
cerveza7
 
4 weeks, 1 day ago
"The right number of organizations to use depends on the number of independent groups of administrative users in your
company:
- If your company is organized by function, you might have a single department that's in charge of overseeing all Google
Cloud deployments.
- If your company is organized by division or owns a number of autonomously-run subsidiaries, then there might not be a
single department that's in charge.
No divisions is mentioned in the questions. Developer is a function.
upvoted 
1 
times
AugustoKras011111
AugustoKras011111
 
10 months, 1 week ago
Selected Answer: 
C
C, Bcuz manage multiple organizations is not a Google best practice
upvoted 
1 
times
OttomanSheikhIran
OttomanSheikhIran
 
11 months, 3 weeks ago
clearly C. Two orgs is a BAD practice
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year ago
I would recommend option C, creating two folders under the Organization resource named "Development" and "Production"
and placing developer and production projects in the respective folders. This approach would allow you to centrally manage
policies for all projects, while also being able to set more restrictive policies for production projects. It would also allow you to
easily move projects between the Development and Production folders as business needs change, without disrupting users or
developers.
Option D, designating the Organization for production projects only, would not allow developers to create projects within the
Organization and could lead to confusion around project ownership and management. It would also make it more difficult to
move projects between development and production environments.
upvoted 
3 
times
omermahgoub
omermahgoub
 
1 year ago
Option A, creating a second Google Workspace account and Organization, would not be a recommended practice as it would
create unnecessary complexity and make it more difficult to manage policies and move projects between environments.
Option B, creating a single folder under the Organization resource and placing all projects in that folder, would not allow you
to set different policies for development and production projects.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year ago
Selected Answer: 
C
C Is the Correct Answer
upvoted 
1 
times
ashrafh
ashrafh
 
1 year, 1 month ago
all 4 answers seems stupid
upvoted 
2 
timesupvoted 
2 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 2 months ago
C is the best option
upvoted 
2 
times
6721sora
6721sora
 
1 year, 3 months ago
Selected Answer: 
C
C is Ok
upvoted 
2 
times
cloudinit
cloudinit
 
1 year, 4 months ago
Selected Answer: 
C
I don't think anyone can create projects outside the organization using the workspace account as it redirects the users into the
organization.
upvoted 
2 
times
gardislan18
gardislan18
 
1 year, 5 months ago
Answer is C
A - you only want to create and Organization structure not Google Workspace
B - best practice is to move your projects to a folders
D - developers are allowed to create projects
upvoted 
1 
times
szefco
szefco
 
1 year, 5 months ago
Selected Answer: 
C
C makes most sense in this scenario
upvoted 
1 
times
amxexam
amxexam
 
1 year, 7 months ago
Selected Answer: 
D
D is better than C.
upvoted 
1 
times
sjmsummer
sjmsummer
 
1 year, 11 months ago
Selected Answer: 
C
C seems to be more organized solution than D.
upvoted 
3 
times
technodev
technodev
 
1 year, 12 months ago
Selected Answer: 
C
I would go with C
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #172
Your company has an application running on Compute Engine that allows users to play their favorite music. There are a fixed
number of instances. Files are stored in Cloud Storage, and data is streamed directly to users. Users are reporting that they
sometimes need to attempt to play popular songs multiple times before they are successful. You need to improve the
performance of the application. What should you do? 
A. 
1. Mount the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances. 2. Serve music files directly
from the backend Compute Engine instance.
B. 
1. Create a Cloud Filestore NFS volume and attach it to the backend Compute Engine instances. 2. Download popular
songs in Cloud Filestore. 3. Serve music files directly from the backend Compute Engine instance.
C. 
1. Copy popular songs into CloudSQL as a blob. 2. Update application code to retrieve data from CloudSQL when Cloud
Storage is overloaded.
D. 
1. Create a managed instance group with Compute Engine instances. 2. Create a global load balancer and configure it
with two backends: 
ג
 ‹—Managed instance group 
ג
 ‹—Cloud Storage bucket 3. Enable Cloud CDN on the bucket backend.
Most Voted
Correct Answer:
 
D 
Comments
CGS22
CGS22
 
Highly Voted
 
10 months ago
Selected Answer: 
D
The correct answer is: D. Create a managed instance group with Compute Engine instances. Create a global load balancer and
configure it with two backends: Managed instance group, Cloud Storage bucket. Enable Cloud CDN on the bucket backend.
This solution will improve the performance of the application by:
Automatically scaling the number of Compute Engine instances to meet demand.
Distributing traffic across multiple instances to reduce load on each instance.
Caching popular songs in memory to reduce the number of times that they need to be loaded from Cloud Storage.
Using a global load balancer to distribute traffic evenly across all regions.
Using Cloud CDN to deliver files to users from a location that is closer to them.
This solution is the most efficient and cost-effective way to improve the performance of the application.
upvoted 
10 
times
Community vote distribution
D (97%)
A
(3%)upvoted 
10 
times
Jconnor
Jconnor
 
Highly Voted
 
1 month ago
A is Ridiculous.
upvoted 
6 
times
MikeH20
MikeH20
 
3 weeks, 6 days ago
Most of the "official" answers are, unfortunately. 
I've pretty much defaulted to the community answer distributions exclusively.
upvoted 
3 
times
nocrush
nocrush
 
Most Recent
 
3 months, 3 weeks ago
Selected Answer: 
D
The correct answer is D.
https://cloud.google.com/cdn?hl=en#static-content
upvoted 
2 
times
nocrush
nocrush
 
3 months, 3 weeks ago
D. Absolutely
upvoted 
1 
times
ranbatrekker
ranbatrekker
 
3 months, 3 weeks ago
Not agree on A , why pay unnecessary for same data for VM disk size.
upvoted 
1 
times
zerg0
zerg0
 
11 months ago
Selected Answer: 
D
The Cloud CDN is the best practice for the content caching.
upvoted 
3 
times
omermahgoub
omermahgoub
 
1 year ago
I would recommend option D, creating a managed instance group with Compute Engine instances and a global load balancer with
two backends: the managed instance group and the Cloud Storage bucket, and enabling Cloud CDN on the bucket backend. This
approach would allow you to scale the number of instances in the managed instance group as needed to handle the demand for
the application, and would also use the Cloud CDN to improve the performance of the application by caching the music files closer
to the users.
Option A, mounting the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances, would not provide a way
to scale the number of instances to handle increased demand for the application.
upvoted 
3 
times
omermahgoub
omermahgoub
 
1 year ago
Option B, creating a Cloud Filestore NFS volume and attaching it to the backend Compute Engine instances, would not provide a
way to scale the number of instances to handle increased demand for the application.
Option C, copying popular songs into CloudSQL as a blob and updating the application code to retrieve data from CloudSQL
when Cloud Storage is overloaded, would not provide a way to scale the number of instances to handle increased demand for the
application. Additionally, using CloudSQL to store and serve music files may not be the most appropriate use case for the
service, as it is designed for storing and querying structured data, rather than serving large files.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year ago
Selected Answer: 
D
D 
Is the Correct Answer
upvoted 
2 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
timesupvoted 
1 
times
adelynllllllllll
adelynllllllllll
 
1 year, 2 months ago
Selected Answer: 
A
I think it should be A,
upvoted 
1 
times
adelynllllllllll
adelynllllllllll
 
1 year, 2 months ago
Why not A, I think it should be A, since it mentioned popular songs and file store is faster then the cloud storage, I vote for A
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 2 months ago
I will go with D
upvoted 
2 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
1 year, 4 months ago
Selected Answer: 
D
Do not trust the official answers here, D is correct. In special for this question, never use gcsfuse in production. Performance is
bad and reliability is trashy - Google states it themselves.
upvoted 
4 
times
szefco
szefco
 
1 year, 5 months ago
D is correct
upvoted 
2 
times
JoeyCASD
JoeyCASD
 
1 year, 7 months ago
Remember Gcsfuse performance is not good, reference
https://cloud.google.com/storage/docs/gcs-fuse#notes
upvoted 
3 
times
nkit
nkit
 
1 year, 8 months ago
Selected Answer: 
D
A is wrong because you can't be serving files directly from Compute Engine instance.
GCS + CDN is best option
upvoted 
4 
times
cloudmon
cloudmon
 
1 year, 9 months ago
Selected Answer: 
D
D for sure
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #173
The operations team in your company wants to save Cloud VPN log events for one year. You need to configure the cloud
infrastructure to save the logs. What should you do? 
A. 
Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save. 
Most Voted
B. 
Enable the Compute Engine API, and then enable logging on the firewall rules that match the traffic you want to save.
C. 
Set up a Cloud Logging Dashboard titled Cloud VPN Logs, and then add a chart that queries for the VPN metrics over a
one-year time period.
D. 
Set up a filter in Cloud Logging and a topic in Pub/Sub to publish the logs.
Correct Answer:
 
A 
Comments
gregorgrinc
gregorgrinc
 
2 months, 1 week ago
Selected Answer: 
A
It is A
upvoted 
1 
times
theBestStudent
theBestStudent
 
7 months, 1 week ago
Selected Answer: 
A
I guess a similar approach to this shuld be followed https://cloud.google.com/architecture/exporting-stackdriver-logging-for-
compliance-requirements
upvoted 
1 
times
Deb2293
Deb2293
 
1 year, 3 months ago
Selected Answer: 
A
Archival storage: Cloud Storage is the best
upvoted 
4 
times
tdotcat
tdotcat
 
1 year, 5 months ago
Selected Answer: 
A
Community vote distribution
A (100%)A is ok
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
I would like to go with Option A
upvoted 
2 
times
6721sora
6721sora
 
1 year, 10 months ago
Selected Answer: 
A
Logs needed for a year. Coldline or Archive storage classes available.
A seems fine
upvoted 
3 
times
exam9391
exam9391
 
1 year, 11 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 3 months ago
A should be right.
upvoted 
1 
times
technodev
technodev
 
2 years, 6 months ago
It should be D, CDN serves the purpose.
upvoted 
1 
times
technodev
technodev
 
2 years, 6 months ago
Ignore A is correct. CDN was an answer to a diff question.
upvoted 
1 
times
victory108
victory108
 
2 years, 6 months ago
A. Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.
upvoted 
1 
times
edilramos
edilramos
 
2 years, 6 months ago
Selected Answer: 
A
A Is correct.
Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.
Filter in Cloud Loggin for specific content, and Cloud Storage for storage.
upvoted 
4 
times
StelSen
StelSen
 
2 years, 6 months ago
Option-A is correct. Need cloud storage bucket for long time storage.
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #174
You are working with a data warehousing team that performs data analysis. The team needs to process data from external
partners, but the data contains personally identifiable information (PII). You need to process and store the data without storing
any of the PIIE data. What should you do? 
A. 
Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, use the Cloud Data
Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery. 
Most Voted
B. 
Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, store all non-PII data in
BigQuery and store all PII data in a Cloud Storage bucket that has a retention policy set.
C. 
Ask the external partners to upload all data on Cloud Storage. Configure Bucket Lock for the bucket. Create a Dataflow
pipeline to read the data from the bucket. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to
remove any PII data. Store the result in BigQuery.
D. 
Ask the external partners to import all data in your BigQuery dataset. Create a dataflow pipeline to copy the data into a
new table. As part of the Dataflow bucket, skip all data in columns that have PII data
Correct Answer:
 
A 
Comments
StelSen
StelSen
 
Highly Voted
 
2 years, 6 months ago
Option-A is correct. Although Option-C sounds good, ultimately we should not store PI data at all as per question says.
upvoted 
50 
times
edilramos
edilramos
 
Highly Voted
 
2 years, 6 months ago
Selected Answer: 
A
The correct answer is A.
Option C seems to be an option, but there are two non-conformities there. In addition to storing personal data in the GCS, it is
being improperly retained.
upvoted 
14 
times
dija123
dija123
 
Most Recent
 
2 months, 3 weeks ago
Selected Answer: 
A
Community vote distribution
A (75%)
C (25%)Agree with A
upvoted 
1 
times
ptapia_el
ptapia_el
 
3 months, 3 weeks ago
Selected Answer: 
A
This best option.
upvoted 
1 
times
tamj123
tamj123
 
8 months, 2 weeks ago
A is best answer, C seems be an extract step and security risk to upload to a bucket first
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
The problem with C is the data is stored in the bucket with the PII data even though the BigQuery data has it removed?
upvoted 
3 
times
johny_doe
johny_doe
 
10 months ago
exactly
upvoted 
1 
times
AugustoKras011111
AugustoKras011111
 
1 year, 4 months ago
Selected Answer: 
A
option A, the question say dont store data...
upvoted 
1 
times
someCloudUser
someCloudUser
 
1 year, 4 months ago
Selected Answer: 
A
A is correct.
upvoted 
1 
times
telp
telp
 
1 year, 4 months ago
Selected Answer: 
A
Answer A. The question say do not store PII data so need to remove it before storing.
upvoted 
1 
times
rotorclear
rotorclear
 
1 year, 4 months ago
Selected Answer: 
A
Answer should be A because the question emphasises on processing the data without storing it. That rules out C.
upvoted 
1 
times
RVivek
RVivek
 
1 year, 4 months ago
Selected Answer: 
A
C -- is wrong because PII data is uploaded and the bucket is locked which means the data cannot be deleted
B and D are wron as they do not use Data loss prevention to protect data
upvoted 
2 
times
dataqueen_3110
dataqueen_3110
 
1 year, 5 months ago
PII --> Cloud DLP. 
So that narrows the choices down to A or C. 
C says "Ask the external partners to upload all data on Cloud
Storage" which is not generally a feasible or recommended practice. Also, we cannot store PII anywhere, including in GCS.
Answer is A.
upvoted 
1 
times
Wael216
Wael216
 
1 year, 6 months ago
Selected Answer: 
A
A i s correct, C sounds good but storing the data in GCS is already a violation of the PII requirements
upvoted 
1 
timesomermahgoub
omermahgoub
 
1 year, 6 months ago
I would recommend option A, creating a Dataflow pipeline to retrieve the data from the external sources and using the Cloud Data
Loss Prevention (Cloud DLP) API to remove any PII data. Storing the result in BigQuery would allow the data warehousing team to
easily perform analysis on the data.
Option C, using Bucket Lock to protect the data and using the Cloud DLP API to remove PII data, would protect the data from
unauthorized access, but would not allow the data warehousing team to easily perform analysis on the data.
upvoted 
5 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option B, storing non-PII data in BigQuery and PII data in a Cloud Storage bucket with a retention policy set, would not fully
protect the PII data and could potentially lead to data breaches.
Option D, copying the data into a new table and skipping columns with PII data, would not fully protect the PII data and could
potentially lead to data breaches. It would also require the data warehousing team to manually skip certain columns when
performing analysis, which could be time-consuming and error-prone.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A 
Is the Correct Answer
upvoted 
1 
times
ardit
ardit
 
1 year, 6 months ago
Selected Answer: 
A
A is the right one.
upvoted 
1 
times
jaxclain
jaxclain
 
1 year, 7 months ago
Selected Answer: 
A
Of course the correct answer is A, not sure how some people think C is valid, probably trolling trying to confuse some here.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #175
You want to allow your operations team to store logs from all the production projects in your Organization, without including
logs from other projects. All of the production projects are contained in a folder. You want to ensure that all logs for existing
and new production projects are captured automatically. What should you do? 
A. 
Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations
project. 
Most Voted
B. 
Create an aggregated export on the Organization resource. Set the log sink to be a Cloud Storage bucket in an
operations project.
C. 
Create log exports in the production projects. Set the log sinks to be a Cloud Storage bucket in an operations project.
D. 
Create log exports in the production projects. Set the log sinks to be BigQuery datasets in the production projects, and
grant IAM access to the operations team to run queries on the datasets.
Correct Answer:
 
A 
Comments
simbu1299
simbu1299
 
Highly Voted
 
3 years ago
The correct answer is A
upvoted 
16 
times
Atanu
Atanu
 
Highly Voted
 
1 year, 7 months ago
The admin must have failed this exam multiple times. How can one select option B here.
upvoted 
9 
times
lucaluca1982
lucaluca1982
 
Most Recent
 
5 months ago
Selected Answer: 
B
Creating an aggregated export at the organization level and setting the log sink to a Cloud Storage bucket in an operations project
ensures comprehensive, automatic, and centralized log capture for all existing and new production projects. Therefore, Option B is
the correct choice.
upvoted 
1 
times
Namshru
Namshru
 
10 months, 4 weeks ago
Community vote distribution
A (95%)
B (5%)Namshru
Namshru
 
10 months, 4 weeks ago
Correct Answer is A.Looks like most of tje Admin answers are all incorrect to avoid sharing PDF to others.
upvoted 
2 
times
CGS22
CGS22
 
1 year, 10 months ago
Selected Answer: 
A
The correct answer is: A. Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in
an operations project.
This solution will allow the operations team to store logs from all the production projects in your Organization, without including
logs from other projects. All of the production projects are contained in a folder, so you can create an aggregated export on the
Production folder. You can then set the log sink to be a Cloud Storage bucket in an operations project. This will allow the
operations team to store all of the logs from the production projects in one place.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
Option B is not the correct solution because it creates an aggregated export on the Organization resource, which will capture logs
from all projects in the Organization, including those outside the Production folder.
The best option to achieve the desired result is to create an aggregated export on the Production folder. Set the log sink to be a
Cloud Storage bucket in an operations project. This will allow the operations team to store logs from all the production projects in
the Organization, without including logs from other projects. Additionally, this setup will automatically capture logs for existing and
new production projects.
Option A is the correct solution because it allows you to create an aggregated export on the Production folder, which will capture
logs from all the production projects contained in the folder. Setting the log sink to a Cloud Storage bucket in an operations project
will allow the operations team to store the logs in a central location.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
Option C is not the correct solution because it requires you to create log exports in each production project, which can be time-
consuming and error-prone. Additionally, setting the log sink to a Cloud Storage bucket in an operations project will not
automatically capture logs for new production projects.
Option D is not the correct solution because it requires you to create log exports in each production project, which can be time-
consuming and error-prone. Additionally, storing the logs in BigQuery datasets in the production projects will not allow the
operations team to easily access the logs. Instead, they would need to be granted IAM access to run queries on the datasets.
upvoted 
3 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A is the right answer https://cloud.google.com/logging/docs/export/aggregated_sinks
upvoted 
6 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
I will choose A as right answer
upvoted 
1 
times
zellck
zellck
 
2 years, 3 months ago
Selected Answer: 
A
A is the answer.
https://cloud.google.com/logging/docs/export/aggregated_sinks
Aggregated sinks combine and route log entries from the Google Cloud resources contained by an organization or folder.
upvoted 
3 
times
alexandercamachop
alexandercamachop
 
2 years, 3 months agoSelected Answer: 
A
A, is the only one that creates the policy in the Production Folder.
Hence doing what the question says "make sure existing / all future projects gets automatically logs sabed"
upvoted 
3 
times
exam9391
exam9391
 
2 years, 5 months ago
Selected Answer: 
A
A is ok
upvoted 
3 
times
cloudmon
cloudmon
 
2 years, 9 months ago
Selected Answer: 
A
The correct answer is A
upvoted 
3 
times
kinghin
kinghin
 
2 years, 9 months ago
Anyone can explain the difference between A and C? Both options look similar...
upvoted 
1 
times
9xnine
9xnine
 
2 years, 7 months ago
Projects vs. folder. IF you create the export on the folder it will apply to all new projects under that folder.
upvoted 
10 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
2 years, 4 months ago
Now this is a really cool feature, thanks for the explanation!
upvoted 
2 
times
AsadZaidi
AsadZaidi
 
2 years, 11 months ago
"without including logs from other projects. " << 
Chose A as answer
upvoted 
2 
times
kongae
kongae
 
2 years, 11 months ago
Selected Answer: 
A
Don't understand why we need to choose at organization level, Please explain if B is correct
upvoted 
5 
times
technodev
technodev
 
2 years, 11 months ago
Selected Answer: 
A
A is the right answer.
upvoted 
9 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #176
Your company has an application that is running on multiple instances of Compute Engine. It generates 1 TB per day of logs.
For compliance reasons, the logs need to be kept for at least two years. The logs need to be available for active query for 30
days. After that, they just need to be retained for audit purposes. You want to implement a storage solution that is compliant,
minimizes costs, and follows Google-recommended practices. What should you do? 
A. 
1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3.
Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention
policy at the bucket level using bucket lock. 
Most Voted
B. 
1. Write a daily cron job, running on all instances, that uploads logs into a Cloud Storage bucket. 2. Create a sink to
export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud
Storage bucket after one month.
C. 
1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a partitioned BigQuery table. 3. Set
a time_partitioning_expiration of 30 days.
D. 
1. Create a daily cron job, running on all instances, that uploads logs into a partitioned BigQuery table. 2. Set a
time_partitioning_expiration of 30 days.
Correct Answer:
 
A 
Comments
gggsrs
gggsrs
 
Highly Voted
 
2 years, 6 months ago
The answer is A.
The practice for managing logs generated on Compute Engine on Google Cloud is to install the Cloud Logging agent and send
them to Cloud Logging.
The sent logs will be aggregated into a Cloud Logging sink and exported to Cloud Storage.
The reason for using Cloud Storage as the destination for the logs is that the requirement in question requires setting up a
lifecycle based on the storage period.
In this case, the log will be used for active queries for 30 days after it is saved, but after that, it needs to be stored for a longer
period of time for auditing purposes.
If the data is to be used for active queries, we can use BigQuery's Cloud Storage data query feature and move the data past 30
Community vote distribution
A (97%)
B
(3%)If the data is to be used for active queries, we can use BigQuery's Cloud Storage data query feature and move the data past 30
days to Coldline to build a cost-optimal solution.
Therefore, the correct answer is as follows
1. Install the Cloud Logging agent on all instances.
Create a sync that exports the logs to the region's Cloud Storage bucket.
3. Create an Object Lifecycle rule to move the files to the Coldline Cloud Storage bucket after one month. 4.
4. set up a bucket-level retention policy using bucket locking."
upvoted 
24 
times
gggsrs
gggsrs
 
2 years, 6 months ago
https://cloud.google.com/logging/docs/agent/logging/installation
https://cloud.google.com/logging/docs/export/configure_export_v2
https://cloud.google.com/bigquery/external-data-cloud-storage
upvoted 
4 
times
SANTHEDAN
SANTHEDAN
 
Highly Voted
 
8 months, 2 weeks ago
None of the options are correct:
A - It should be archive (>= 365 days) and not coldline ( >= 90 days) - Proposed solution is more expensive that what is
possible. Also no way to query unless you use BigQuery external tables.
B & C - Wrong because CRON is not the way to do this.
D - Wrong because data is deleted after 30 days and not retained for 2 years.
upvoted 
5 
times
Gino17m
Gino17m
 
2 months ago
The question comes from the times when there was no Archive storage class in GCS yet
upvoted 
1 
times
Tirthankar17
Tirthankar17
 
Most Recent
 
4 months, 2 weeks ago
Selected Answer: 
A
No need to look at any other options.
upvoted 
2 
times
devnul
devnul
 
10 months, 1 week ago
Answer A is misleading/confusing: "3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after
one month." 
A lifecycle rule will NOT move files in another bucket (coldline bucket etc). It will just change the storage class of the file.
upvoted 
1 
times
someCloudUser
someCloudUser
 
1 year, 4 months ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
someCloudUser
someCloudUser
 
1 year, 4 months ago
Selected Answer: 
A
A is correct.
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
A. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3.
Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention
policy at the bucket level using bucket lock.
This approach would allow you to use Cloud Logging to collect and export the logs from the Compute Engine instances into a
Cloud Storage bucket. You can then use an Object Lifecycle rule to automatically move the logs from the regional bucket to a
Coldline bucket after one month, which will reduce storage costs for logs that are not actively being queried. By configuring a
retention policy using bucket lock, you can ensure that the logs are retained for at least two years for audit purposes. This
approach follows Google-recommended practices for storing logs and minimizing costs.
upvoted 
2 
timessurajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A 
Is the Correct Answer
upvoted 
1 
times
habros
habros
 
1 year, 7 months ago
Selected Answer: 
A
A is perfect answer… the rest doesn’t sound rational
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
I agree with A, There is no need of BigQuery.
upvoted 
2 
times
manis68
manis68
 
1 year, 8 months ago
Selected Answer: 
A
A is fine
upvoted 
1 
times
Imran109
Imran109
 
1 year, 10 months ago
For compliance reasons, the logs need to be kept for at least two years... In Bigquery time partitioned after 30 days ..how the
logs be present for 2 years ..Hence going with A
upvoted 
1 
times
harutheorochimaru
harutheorochimaru
 
1 year, 11 months ago
Selected Answer: 
A
A is a no-brainer
upvoted 
1 
times
exam9391
exam9391
 
1 year, 11 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
H_S
H_S
 
2 years ago
Selected Answer: 
A
when a partition expires, the data in the partition is no longer available
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #177
Your company has just recently activated Cloud Identity to manage users. The Google Cloud Organization has been configured
as well. The security team needs to secure projects that will be part of the Organization. They want to prohibit IAM users
outside the domain from gaining permissions from now on. What should they do? 
A. 
Configure an organization policy to restrict identities by domain. 
Most Voted
B. 
Configure an organization policy to block creation of service accounts.
C. 
Configure Cloud Scheduler to trigger a Cloud Function every hour that removes all users that don't belong to the Cloud
Identity domain from all projects.
D. 
Create a technical user (e.g., 
crawler@yourdomain.com
 
), and give it the project owner role at root organization level.
Write a bash script that: 
ג
€¢ Lists all the IAM rules of all projects within the organization. 
ג
€¢ Deletes all users that do not
belong to the company domain. Create a Compute Engine instance in a project within the Organization and configure
gcloud to be executed with technical user credentials. Configure a cron job that executes the bash script every hour.
Correct Answer:
 
A 
Comments
kimharsh
kimharsh
 
Highly Voted
 
2 years, 2 months ago
LOL , if we give this question to someone who know nothing about GCP they will select A
upvoted 
16 
times
Fotofilico
Fotofilico
 
Highly Voted
 
2 years, 6 months ago
Selected Answer: 
A
https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains
upvoted 
16 
times
Tirthankar17
Tirthankar17
 
Most Recent
 
4 months, 2 weeks ago
Whoever wrote option D was high af.
upvoted 
3 
times
Jconnor
Jconnor
 
7 months ago
Community vote distribution
A (100%)D is Ridiculous.
upvoted 
1 
times
Atanu
Atanu
 
1 year, 1 month ago
Selected Answer: 
A
Option D is just to create confusion only.
upvoted 
1 
times
CGS22
CGS22
 
1 year, 3 months ago
Selected Answer: 
A
The correct answer is: A. Configure an organization policy to restrict identities by domain.
This solution will allow the security team to secure projects that will be part of the Organization by prohibiting IAM users
outside the domain from gaining permissions.
The other options are not as efficient or effective. Option B would not be efficient, as it would block the creation of all service
accounts, which are necessary for some applications. Option C would not be effective, as it would not prevent IAM users from
gaining permissions, as it would only remove users that do not belong to the Cloud Identity domain from all projects. Option D
would not be efficient, as it would require a Compute Engine instance to be created and a cron job to be configured, which
would add complexity and cost to the solution.
upvoted 
2 
times
someCloudUser
someCloudUser
 
1 year, 4 months ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The security team should configure an organization policy to restrict identities by domain. This will allow them to specify a list
of allowed domains, and prevent users from outside those domains from gaining permissions in the Organization.
Alternatively, the security team could configure an organization policy to block creation of service accounts. This would prevent
the creation of new service accounts, which could be used to grant permissions to users outside the domain.
The other options are not recommended. Option C involves manually removing users every hour, which could be time-
consuming and error-prone. Option D involves creating a technical user and writing a bash script to delete users, which is not a
recommended approach. It would be more secure and efficient to use an organization policy to restrict identities by domain.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A 
Is the Correct Answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is the correct answer
https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A is right
upvoted 
1 
times
manis68
manis68
 
1 year, 8 months agoSelected Answer: 
A
A is OK
upvoted 
1 
times
exam9391
exam9391
 
1 year, 11 months ago
Selected Answer: 
A
A is ok
upvoted 
3 
times
azureaspirant
azureaspirant
 
2 years, 4 months ago
2/15/21 exam
upvoted 
5 
times
blk_rook
blk_rook
 
2 years, 6 months ago
Selected Answer: 
A
must restrict the access, not clean up every hour. see reference from Fotofilico
upvoted 
3 
times
AJapieGuru
AJapieGuru
 
2 years, 6 months ago
Go for A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #178
Your company has an application running on Google Cloud that is collecting data from thousands of physical devices that are
globally distributed. Data is published to Pub/Sub and streamed in real time into an SSD Cloud Bigtable cluster via a Dataflow
pipeline. The operations team informs you that your Cloud 
Bigtable cluster has a hotspot, and queries are taking longer than expected. You need to resolve the problem and prevent it
from happening in the future. What should you do? 
A. 
Advise your clients to use HBase APIs instead of NodeJS APIs.
B. 
Delete records older than 30 days.
C. 
Review your RowKey strategy and ensure that keys are evenly spread across the alphabet. 
Most Voted
D. 
Double the number of nodes you currently have.
Correct Answer:
 
C 
Comments
StelSen
StelSen
 
Highly Voted
 
1 year, 6 months ago
Option-C is correct: https://cloud.google.com/bigtable/docs/schema-design#row-keys
upvoted 
13 
times
omermahgoub
omermahgoub
 
Highly Voted
 
6 months, 1 week ago
C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.
The RowKey is used to sort data within a Cloud Bigtable cluster. If the keys are not evenly spread across the alphabet, it can
result in a hotspot and slow down queries. To prevent this from happening in the future, you should review your RowKey
strategy and ensure that keys are evenly spread across the alphabet. This will help to distribute the data evenly across the
cluster and improve query performance. Other potential solutions to consider include adding more nodes to the cluster or
optimizing your query patterns. However, deleting records older than 30 days or advising clients to use HBase APIs instead of
NodeJS APIs would not address the issue of a hotspot in the cluster.
upvoted 
9 
times
AugustoKras011111
AugustoKras011111
 
Most Recent
 
4 months ago
Selected Answer: 
C
C in the only answer that make sense.
Community vote distribution
C (100%)C in the only answer that make sense.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
6 months, 3 weeks ago
Selected Answer: 
C
C 
Is the Correct Answer
upvoted 
1 
times
gonlafer
gonlafer
 
6 months, 3 weeks ago
Selected Answer: 
C
C
https://cloud.google.com/bigtable/docs/overview#load-balancing
upvoted 
1 
times
megumin
megumin
 
7 months, 2 weeks ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
8 months, 2 weeks ago
Selected Answer: 
C
C is the right answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
C is better option
upvoted 
1 
times
Jeyakumar
Jeyakumar
 
11 months ago
Option-C is the better one.
upvoted 
1 
times
Nirca
Nirca
 
12 months ago
The issue described is with "querying" 
meaning reading. Not writing. C: distributing across the Alphabet is good for Writing.
upvoted 
2 
times
JoeyCASD
JoeyCASD
 
1 year, 1 month ago
Suggest to study the following reference, it's important to design the row key pattern in Bigtable.
https://cloud.google.com/bigtable/docs/overview#architecture
https://cloud.google.com/bigtable/docs/overview#load-balancing
upvoted 
8 
times
AjayPrajapati
AjayPrajapati
 
1 year, 5 months ago
C looks good, I dont think we have to control number of nodes in Big table
upvoted 
3 
times
AJapieGuru
AJapieGuru
 
1 year, 6 months ago
Vote C
upvoted 
2 
times
victory108
victory108
 
1 year, 6 months ago
C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.
upvoted 
1 
times
spoxman
spoxman
 
1 year, 6 months ago
Selected Answer: 
C
correct row-key strategy improves performance
upvoted 
1 
timesupvoted 
1 
times
GMats
GMats
 
1 year, 6 months ago
C is answer.Hot key/partitions are created due to improper row key design.
upvoted 
2 
times
ACasper
ACasper
 
1 year, 6 months ago
I think the ans is D
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #179
Your company has a Google Cloud project that uses BigQuery for data warehousing. There are some tables that contain
personally identifiable information (PII). 
Only the compliance team may access the PII. The other information in the tables must be available to the data science team.
You want to minimize cost and the time it takes to assign appropriate access to the tables. What should you do? 
A. 
1. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2.
Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the
dataset that contains the view.
B. 
1. From the dataset where you have the source data, create materialized views of tables that you want to share,
excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access
controls to the dataset that contains the view.
C. 
1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign
an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset
that contains the view. 5. Authorize the view to access the source dataset. 
Most Voted
D. 
1. Create a dataset for the data science team. 2. Create materialized views of tables that you want to share, excluding
PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to
the dataset that contains the view. 5. Authorize the view to access the source dataset.
Correct Answer:
 
C 
Comments
[Removed]
[Removed]
 
Highly Voted
 
2 years, 7 months ago
Selected Answer: 
A
Materialized view is too costly for the requirement. So B & D is out.
To protect PII, there is no need to create another dataset. Creating a view on the original dataset should be sufficient. In
addition, according to https://cloud.google.com/bigquery/docs/view-access-controls, view access can be granted at the
'dataset' level.
upvoted 
17 
times
ashrafh
ashrafh
 
2 years, 1 month ago
Community vote distribution
C (59%)
A (35%)
B (6%)ashrafh
ashrafh
 
2 years, 1 month ago
Sorry I mean C
upvoted 
4 
times
zellck
zellck
 
2 years, 3 months ago
Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to
the authorized view without simultaneously granting access to the underlying data.
upvoted 
8 
times
ashrafh
ashrafh
 
2 years, 1 month ago
A is correct 
Giving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share
query results with particular users and groups without giving them access to the underlying tables. You can also use the view's
SQL query to restrict the columns (fields) the users are able to query. In this tutorial, you create an authorized view.
https://cloud.google.com/bigquery/docs/share-access-views
upvoted 
4 
times
[Removed]
[Removed]
 
Highly Voted
 
3 years ago
Selected Answer: 
C
C is correct here. You need view to avoid PII data. So materialized view is not needed.
upvoted 
13 
times
melono
melono
 
2 years, 2 months ago
also can't query data from a view, so A not.
upvoted 
1 
times
melono
melono
 
2 years, 2 months ago
https://cloud.google.com/bigquery/docs/share-access-views
upvoted 
1 
times
192dcc7
192dcc7
 
Most Recent
 
3 months, 2 weeks ago
Devil is in the details. Question mentioned table"s". Which mean we have more than 1 table which will result in more than 1
view. You can authorize each view on "same" dataset or group all view"s" in a dataset then authorize the dataset. This does not
answer the question (may be wording) but this is the concept behind the question.
upvoted 
1 
times
someone2011
someone2011
 
1 year, 4 months ago
C: view in a different dataset (https://cloud.google.com/bigquery/docs/share-access-views: "Authorized views should be
created in a different dataset from the source data. That way, data owners can give users access to the authorized view without
simultaneously granting access to the underlying data.")
upvoted 
7 
times
cchiaramelli
cchiaramelli
 
1 year, 2 months ago
Yes, good source
upvoted 
1 
times
Mournes
Mournes
 
1 year, 5 months ago
Selected Answer: 
C
I went with C. A will prevent data scientists from viewing PII in the view, it doesn't stop them from viewing it in the table
however.
upvoted 
6 
times
gary_cooper
gary_cooper
 
1 year, 5 months ago
Selected Answer: 
C
https://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view
upvoted 
1 
timesred_panda
red_panda
 
1 year, 5 months ago
Selected Answer: 
A
It's no needed to create a new dataset and for sure is not cost-effective. A is best option
upvoted 
1 
times
Umesh09
Umesh09
 
1 year, 6 months ago
It should be option A the question states minimize cost and time, option C though has better security requires additional step.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
C
Option A is not the best choice because it doesn't involve creating a separate dataset for the data science team. Creating a
separate dataset provides better organization and access control management for different teams.
In option C, you create a separate dataset specifically for the data science team and then create views that exclude PII. This
allows for more granular access controls and a better separation of concerns. By authorizing the view to access the source
dataset, you ensure that the data science team can only access the non-PII data through the views, maintaining privacy and
compliance.
upvoted 
2 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
A
Option C 
are not appropriate because creating a new dataset is not necessary in this scenario. Creating views of the tables that
exclude PII is a simpler and more cost-effective solution. Additionally, authorizing the view to access the source dataset is not
necessary because the view already contains the relevant data.
upvoted 
1 
times
Toothpick
Toothpick
 
5 months, 1 week ago
Wrong , because you then have to give access at view level to prevent spillover , C is better.
Also, creating another dataset costs nothing, BQ charges for data stored and processed only, you can have a thousand
datasets, it won't matter
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 9 months ago
C = A + One additional step to create the dataset which is not necessary so the answer is A
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 9 months ago
Selected Answer: 
A
No need for materialized view which is an operational overhead.
upvoted 
1 
times
CGS22
CGS22
 
1 year, 10 months ago
Selected Answer: 
A
A. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an
appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that
contains the view. This solution will minimize cost and the time it takes to assign appropriate access to the tables. The other
options are not as efficient or effective.
upvoted 
1 
times
AugustoKras011111
AugustoKras011111
 
1 year, 10 months ago
Selected Answer: 
C
I vote for C. Option C provides better security option.
upvoted 
2 
times
szagarella
szagarella
 
1 year, 10 months ago
Selected Answer: 
A
A is my answer.A is my answer.
upvoted 
1 
times
telp
telp
 
1 year, 10 months ago
Selected Answer: 
C
Agree with C from the link with google best practice
https://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view
Create a dataset where you can store your view
After creating your source dataset, you create a new, separate dataset to store the authorized view that you share with your
data analysts. In a later step, you grant the authorized view access to the data in the source dataset. Your data analysts then
have access to the authorized view, but not direct access to the source data.
upvoted 
4 
times
Jeena345
Jeena345
 
1 year, 10 months ago
Selected Answer: 
A
Materialized views costs more than normal ones. Creating a new dataset is not cost-effective. You can use authorized views to
restrict data access.
From Google doc:
Giving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share
query results with particular users and groups without giving them access to the underlying tables. You can also use the view's
SQL query to restrict the columns (fields) the users are able to query.
Users need the bigquery.tables.getData permission on all tables and views that their query references. In addition, when
querying a view users need this permission on all underlying tables and views. However, if you are using authorized views or
authorized datasets, you don't need to give users access to the underlying source data.
Reference:
https://cloud.google.com/bigquery/docs/share-access-views
https://cloud.google.com/bigquery/docs/table-access-controls#required_permission_to_query_tables_and_views
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #180
Your operations team currently stores 10 TB of data in an object storage service from a third-party provider. They want to move
this data to a Cloud Storage bucket as quickly as possible, following Google-recommended practices. They want to minimize
the cost of this data migration. Which approach should they use? 
A. 
Use the gsutil mv command to move the data.
B. 
Use the Storage Transfer Service to move the data. 
Most Voted
C. 
Download the data to a Transfer Appliance, and ship it to Google.
D. 
Download the data to the on-premises data center, and upload it to the Cloud Storage bucket.
Correct Answer:
 
B 
Comments
amxexam
amxexam
 
Highly Voted
 
2 years, 1 month ago
Selected Answer: 
B
I would have voted C looking at 10 TB at first glance. But the senario other cloud storge changes the look out. You cannot wre
master applica here. As you wont be able to remove and store data like on prem. Hence is or Starge Transfer service.
upvoted 
10 
times
cloudmon
cloudmon
 
Highly Voted
 
2 years, 2 months ago
B
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options
upvoted 
8 
times
OrangeTiger
OrangeTiger
 
Most Recent
 
4 months, 4 weeks ago
Selected Answer: 
B
I chose B.
Bandwidth specification is not included in the question.If the bandwidth is 10G or more, it is A.
The guideline based on the gcloud command is 1TB. Since the data is in an online service, you can use TransferService.
upvoted 
1 
times
someCloudUser
someCloudUser
 
1 year, 4 months ago
Selected Answer: 
B
Community vote distribution
B (100%)Selected Answer: 
B
Correct answer is B
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
B is right, please refer this link
https://cloud.google.com/storage-transfer-service
upvoted 
2 
times
SAMBIT
SAMBIT
 
2 years, 3 months ago
Current storage is object store so mostly a cloud provider… thus storage transfer service
upvoted 
3 
times
technodev
technodev
 
2 years, 5 months ago
Got this question in my exam, answered B
upvoted 
5 
times
AjayPrajapati
AjayPrajapati
 
2 years, 5 months ago
B 
https://cloud.google.com/storage-transfer-service
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
I think B or C.
A&D are out. Becaus tha data size is 10TB.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
Can the third-party provider use B StrageTransferService? 
If can,B is correct.
There is not enough information.
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 6 months ago
B 
Storage Transfer Service is correct.
upvoted 
1 
times
victory108
victory108
 
2 years, 6 months ago
B. Use the Storage Transfer Service to move the data.
upvoted 
1 
times
spoxman
spoxman
 
2 years, 6 months ago
@StelSen: re. B. Being a cloud provider is not a requirement for using STS
upvoted 
1 
times
edilramos
edilramos
 
2 years, 6 months ago
Selected Answer: 
B
B: Storage Transfer Service
https://cloud.google.com/storage-transfer-service
upvoted 
1 
times
StelSen
StelSen
 
2 years, 6 months ago
Very Tricky and don't have enough details to answer the question. Use this GuideVery Tricky and don't have enough details to answer the question. Use this Guide
(https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options) and let's
try to eliminate options.
A. Use the gsutil mv command to move the data. (We have 10TB, hence rejected)
B. Use the Storage Transfer Service to move the data. (Source might not be Cloud provider. Hence rejecting it. If source is
AWS/Azure then this is the answer)
C. Download the data to a Transfer Appliance, and ship it to Google. (I don't think we can use Transfer Appliance at Third party
service providers DC. Assuming this 3rd party is not a cloud provider)
D. Download the data to the on-premises data center, and upload it to the Cloud Storage bucket. (This seems better assuming
DC has good bandwidth such as 1 Gbbs)
upvoted 
5 
times
Dhiraj03
Dhiraj03
 
2 years ago
B is the answer
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #181
You have a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in
response to the load on your application. The instances have a shutdown script that removes REDIS database entries
associated with the instance. You see that many database entries have not been removed, and you suspect that the shutdown
script is the problem. You need to ensure that the commands in the shutdown script are run reliably every time an instance is
shut down. You create a Cloud Function to remove the database entries. What should you do next? 
A. 
Modify the shutdown script to wait for 30 seconds before triggering the Cloud Function.
B. 
Do not use the Cloud Function. Modify the shutdown script to restart if it has not completed in 30 seconds.
C. 
Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud
Logging. 
Most Voted
D. 
Modify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue.
Correct Answer:
 
C 
Comments
Tesla
Tesla
 
Highly Voted
 
2 years, 2 months ago
Actually C is correct but Wrong also in a way .. Sink cannot trigger a cloud function directly. It need Pub/Sub which then will trigger
Cloud Function.
upvoted 
11 
times
omermahgoub
omermahgoub
 
Highly Voted
 
2 years ago
The correct answer is C: Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message
arrives in Cloud Logging.
In this scenario, you want to ensure that the commands in the shutdown script are run reliably every time an instance is shut
down. One way to do this is by setting up a Cloud Monitoring sink that triggers a Cloud Function after an instance removal log
message arrives in Cloud Logging. This will allow you to use the Cloud Function to perform the necessary tasks (such as
removing database entries) when an instance is shut down, and it will ensure that these tasks are performed reliably and
consistently.
Option A: Modifying the shutdown script to wait for 30 seconds before triggering the Cloud Function is not a reliable solution, as it
relies on the shutdown script being able to run for at least 30 seconds before the instance is shut down.
upvoted 
8 
times
Community vote distribution
C (71%)
D (14%)
A (14%)upvoted 
8 
times
omermahgoub
omermahgoub
 
2 years ago
Option B: Modifying the shutdown script to restart if it has not completed in 30 seconds is also not a reliable solution, as it may
not be feasible to restart the script if the instance has already been shut down.
Option D: Modifying the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue is not a reliable
solution, as it relies on the shutdown script being able to run for at least 30 seconds before the instance is shut down, and it also
requires additional infrastructure (a Pub/Sub queue) to be set up and maintained.
upvoted 
3 
times
pcamaster
pcamaster
 
Most Recent
 
3 months ago
Selected Answer: 
C
I'll go for C, even though this scenario should be handled in another way.
The reason why REDIS entries are still persisted is possibly caused by the application shutdown being not handled correctly. The
shutdown script should first wait for the APP shutdown and only after that trigger the REDIS removal (cloud function). 
Using cloud monitoring to intercept this situation is an option, but a bit of overkill in my opinion. I would better modify the shutdown
script and the app shutdown procedure and only then triggering a CF. Too bad there is no option for this :)
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 2 weeks ago
You cannot trigger a Cloud Function directly from a Cloud Monitoring sink. Instead, you can set up a Cloud Monitoring alert that
sends notifications to a Pub/Sub topic, and then trigger the Cloud Function from that Pub/Sub topic.
upvoted 
2 
times
bruh_1
bruh_1
 
1 year, 3 months ago
Option C is not an efficient way as it causes delay and complexity. on top of that, you wont be able to trigger a cloud function.
Correct answer is option D.
upvoted 
1 
times
[Removed]
[Removed]
 
1 year, 11 months ago
Selected Answer: 
C
cCorrect Ans is C
upvoted 
1 
times
samirzubair
samirzubair
 
2 years ago
Correct Ans is C
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
C
C 
Is the Correct Answer
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
C is the right answer
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
c is correct
upvoted 
1 
timesupvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is right
upvoted 
3 
times
charlie_lee
charlie_lee
 
2 years, 3 months ago
Selected Answer: 
D
use pub/sub trigger cloud function
upvoted 
4 
times
zellck
zellck
 
2 years, 3 months ago
Selected Answer: 
C
C is the answer as shutdown script is run based on best effort and not a reliable method.
https://cloud.google.com/compute/docs/shutdownscript#limitations
Compute Engine executes shutdown scripts only on a best-effort basis. In rare cases, Compute Engine cannot guarantee that the
shutdown script will complete.
upvoted 
5 
times
kuboraam
kuboraam
 
2 years, 3 months ago
Selected Answer: 
C
C would be the cleanest solution. Although at this time, Cloud Monitoring sink cannot trigger a cloud function directly, it can be
done via Pub/Sub. Still better than solution D.
upvoted 
4 
times
ramzez4815
ramzez4815
 
2 years, 4 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 4 months ago
Selected Answer: 
C
C looks not a professional way, but can make sure the work being done.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #182
You are managing several projects on Google Cloud and need to interact on a daily basis with BigQuery, Bigtable, and
Kubernetes Engine using the gcloud CL tool. You are travelling a lot and work on different workstations during the week. You
want to avoid having to manage the gcloud CLI manually. What should you do? 
A. 
Use Google Cloud Shell in the Google Cloud Console to interact with Google Cloud. 
Most Voted
B. 
Create a Compute Engine instance and install gcloud on the instance. Connect to this instance via SSH to always use the
same gcloud installation when interacting with Google Cloud.
C. 
Install gcloud on all of your workstations. Run the command gcloud components auto-update on each workstation
D. 
Use a package manager to install gcloud on your workstations instead of installing it manually.
Correct Answer:
 
A 
Comments
alexandercamachop
alexandercamachop
 
Highly Voted
 
2 years, 3 months ago
Selected Answer: 
A
First discard: 
C / D are totally not it. Since they will do so much work and it says "do not want to manage gcoud CLI manually"
Then B is not a good cost option, besides at the end you are managing your gcloud cli manually. So A is the only left correct
answer. 
It even saves your $HOME files.
upvoted 
8 
times
kiappy81
kiappy81
 
Highly Voted
 
2 years, 4 months ago
Selected Answer: 
A
If you're using Cloud Shell, the gcloud CLI is available automatically and you don't need to install it. commadn gcloud
components auto-update doesn't exist but the command gcloud components update ensure that you are using updated
version of your components.
upvoted 
5 
times
moiradavis
moiradavis
 
Most Recent
 
5 months, 2 weeks ago
Selected Answer: 
A
Community vote distribution
A (100%)It is the simplest way!
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
11 months, 2 weeks ago
With B, you can keep your terminal working from any workstation. Why not?
Okay, so the question is saying that you don't want to manage gcloud cli itself.
upvoted 
1 
times
odacir
odacir
 
1 year, 1 month ago
Selected Answer: 
A
I will say A, except if some VPC restriction prevent using cloud shell..
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
A
A 
Is the Correct Answer
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A is right
upvoted 
1 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
2 years, 4 months ago
Selected Answer: 
A
A is the correct solution, as the only requirement for your workstation is to have a browser installed.
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #183
Your company recently acquired a company that has infrastructure in Google Cloud. Each company has its own Google Cloud
organization. Each company is using a Shared Virtual Private Cloud (VPC) to provide network connectivity for its applications.
Some of the subnets used by both companies overlap. In order for both businesses to integrate, the applications need to have
private network connectivity. These applications are not on overlapping subnets. You want to provide connectivity with minimal
re-engineering. What should you do? 
A. 
Set up VPC peering and peer each Shared VPC together.
B. 
Migrate the projects from the acquired company into your company's Google Cloud organization. Re-launch the
instances in your companies Shared VPC.
C. 
Set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs. 
Most Voted
D. 
Configure SSH port forwarding on each application to provide connectivity between applications in the different Shared
VPCs.
Correct Answer:
 
C 
Comments
6721sora
6721sora
 
Highly Voted
 
1 year, 10 months ago
Selected Answer: 
C
VPC peering cannot be established between VPCs if there is IP range overlap. C is ok since you can establish VPN across these
VPCs and only include the applications required IP ranges as its mentioned that they do not overlap
upvoted 
28 
times
omermahgoub
omermahgoub
 
Highly Voted
 
1 year, 6 months ago
VPC peering is generally possible even if there are overlapping subnets between the two VPCs. However, there are some
considerations to keep in mind if there's overlapping subnets:
1. You will not be able to route traffic between the overlapping subnets. If needed, you will have to use a different method
(such as a Cloud VPN connection or a Cloud Router) to connect the VPCs.
2. You will need to ensure that the overlapping subnets are not used by any resources in either VPC. This means that you will
need to either modify the existing network configuration to avoid using the overlapping subnets, or you will need to create new
subnets that do not overlap.
3. You may need to update any existing firewall rules or routes that refer to the overlapping subnets to ensure that they are still
valid after the VPCs are peered.
Community vote distribution
C (86%)
Other (14%)valid after the VPCs are peered.
In the question, you want to provide private network connectivity between the two companies' applications, which are not on
overlapping subnets. However, there is overlap in the subnets used by both companies, which means that you will not be able
to use VPC peering to connect the two VPCs.
upvoted 
8 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
One solution in this case would be to set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs. This will allow
you to create a secure, private network connection between the two VPCs, and it will allow the applications in each company's
Shared VPC to communicate with each other over the private connection.
The correct answer is C: Set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs.
Option A: Setting up VPC peering and peering each Shared VPC together would not be a viable solution in this case, because
the subnets used by both companies overlap, and VPC peering does not support overlapping subnets.
upvoted 
10 
times
Pime13
Pime13
 
Most Recent
 
5 months ago
Selected Answer: 
C
c -> https://cloud.google.com/vpc/docs/using-vpc-peering#no_subnet_ip_range_overlap_across_peered_networks
upvoted 
2 
times
Ahmed_Safwat
Ahmed_Safwat
 
7 months, 1 week ago
Selected Answer: 
C
No subnet IP range overlap across peered VPC networks
https://cloud.google.com/vpc/docs/using-vpc-peering
upvoted 
3 
times
jlambdan
jlambdan
 
1 year, 1 month ago
Selected Answer: 
C
looks like the following best practice: https://cloud.google.com/architecture/best-practices-vpc-design#shared-service
Cloud VPN is another alternative. Because Cloud VPN establishes reachability through managed IPsec tunnels, it doesn't have
the aggregate limits of VPC Network Peering. Cloud VPN uses a VPN Gateway for connectivity and doesn't consider the
aggregate resource use of the IPsec peer. The drawbacks of Cloud VPN include increased costs (VPN tunnels and traffic egress),
management overhead required to maintain tunnels, and the performance overhead of IPsec.
upvoted 
8 
times
kratosmat
kratosmat
 
1 year, 2 months ago
Selected Answer: 
C
It seems to be C because VPN peering use BGP protocol that manages the overlaps.
https://cloud.google.com/network-connectivity/docs/vpn/how-to/configuring-peer-gateway
upvoted 
3 
times
HD2023
HD2023
 
1 year, 3 months ago
Selected Answer: 
C
omermahgoub said it best. C
upvoted 
1 
times
rr4444
rr4444
 
1 year, 3 months ago
Selected Answer: 
B
B to reorg it all under one org cos it's a mess. 
You cant have shared RFC1918 ranges between peered networks OR VPNs... Don't know why everyone thinks VPNs avoid that
problem. https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-ha-vpn2 "You can connect two VPC
networks together as long as the primary and secondary subnet IP address ranges in each network don't overlap."
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
This applies to static routes only. 
"A dynamic route can overlap with a subnet route in a peer network. For dynamic routes, theThis applies to static routes only. 
"A dynamic route can overlap with a subnet route in a peer network. For dynamic routes, the
destination ranges that overlap with a subnet route from the peer network are silently dropped. Google Cloud uses the subnet
route.". 
https://cloud.google.com/vpc/docs/vpc-peering
upvoted 
1 
times
RVivek
RVivek
 
1 year, 4 months ago
Selected Answer: 
C
Please check answer from 
BalaGCPArch 
"https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering
Overlapping subnets at time of peering
At the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC
networks or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is
created between VM instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause
routing issues."
upvoted 
1 
times
colombrican
colombrican
 
1 year, 6 months ago
Selected Answer: 
C
Answer is C
A is wrong because you cannot peer VPCs with overlapping subnets:
https://cloud.google.com/vpc/docs/vpc-peering#interaction-subnet-subnet
IPv4 subnet routes in peered VPC networks can't overlap:
- Peering prohibits identical IPv4 subnet routes. For example, two peered VPC networks can't both have an IPv4 subnet route
whose destination is 100.64.0.0/10.
- Peering prohibits a subnet route from being contained within a peering subnet route. For example, if the local VPC network
has a subnet route whose destination is 100.64.0.0/24, then none of the peered VPC networks can have a subnet route whose
destination is 100.64.0.0/10.
B and D are ruled out because it breaks the requirement "with minimal re-engineering" to the applications
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok: The applications are not on overlapping subnets. So use VPC peering.
You want to provide connectivity with minimal re-engineering. VPC Network Peering
accomplishes this.
https://cloud.google.com/vpc/docs/vpc-peering
upvoted 
4 
times
BalaGCPArch
BalaGCPArch
 
1 year, 7 months ago
C should be the Answer : same explaination goes here 
"https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering
Overlapping subnets at time of peering
At the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC
networks or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is
created between VM instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause
routing issues."
upvoted 
6 
times
enado
enado
 
12 months ago
Thanks for this
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
C
C is correct answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
C is best optionC is best option
upvoted 
1 
times
charlie_lee
charlie_lee
 
1 year, 9 months ago
Selected Answer: 
A
These applications ARE NOT on overlapping subnets
upvoted 
2 
times
SerGCP
SerGCP
 
1 year, 9 months ago
https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering
Overlapping subnets at time of peering
At the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC
networks or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is
created between VM instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause
routing issues.
upvoted 
3 
times
sTree100
sTree100
 
1 year, 9 months ago
the answer is A
upvoted 
1 
times
kiappy81
kiappy81
 
1 year, 9 months ago
Selected Answer: 
B
Can someone explain my why not B?
It's fine to eliminate A due to overlapping and also D because is out of discussion, but why C is better than B?
upvoted 
1 
times
zellck
zellck
 
1 year, 9 months ago
you need minimal re-engineering. migrating projects and relaunching instances will be a significant effort.
upvoted 
5 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months ago
Selected Answer: 
C
Google Documentation " When a VPC subnet is created or a subnet IP range is expanded, Google Cloud performs a check to
make sure the new subnet range does not overlap with IP ranges " 
I know the subnets where the application is hosted, does not overlap, however it will not allow a VPC peering because of that
overlap, so the only possible answer is C.
https://cloud.google.com/vpc/docs/vpc-peering
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #184
You are managing several internal applications that are deployed on Compute Engine. Business users inform you that an
application has become very slow over the past few days. You want to find the underlying cause in order to solve the problem.
What should you do first? 
A. 
Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring. 
Most Voted
B. 
Change the Compute Engine Instances behind the application to a machine type with more CPU and memory.
C. 
Restore a backup of the application database from a time before the application became slow.
D. 
Deploy the applications on a managed instance group with autoscaling enabled. Add a load balancer in front of the
managed instance group, and have the users connect to the IP of the load balancer.
Correct Answer:
 
A 
Comments
aut0pil0t
aut0pil0t
 
Highly Voted
 
1 year, 10 months ago
Selected Answer: 
A
First thing to do is to inspect logs and monitoring to see what is happening
upvoted 
23 
times
omermahgoub
omermahgoub
 
Highly Voted
 
1 year, 6 months ago
When an application becomes slow, the first step you should take is to gather information about the underlying cause of the
problem. One way to do this is by inspecting the logs and metrics from the instances where the application is deployed. Google
Cloud Platform (GCP) provides tools such as Cloud Logging and Cloud Monitoring that can help you to collect and analyze this
information.
By reviewing the logs and metrics from the instances, you may be able to identify issues such as resource shortages (e.g. CPU,
memory, or disk), network problems, or application errors that are causing the performance issues. Once you have identified the
underlying cause of the problem, you can take steps to resolve it.
The correct answer is A: Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring.
upvoted 
7 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Community vote distribution
A (100%)Option D: Deploying the applications on a managed instance group with autoscaling enabled and adding a load balancer in
front of the managed instance group may help to improve the performance of the application, but it is not necessarily the first
step you should take. You should first try to understand the underlying cause of the performance issues before making
changes to the deployment architecture.
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option B: Changing the Compute Engine instances behind the application to a machine type with more CPU and memory
may help to improve the performance of the application, but it is not necessarily the first step you should take. You should
first try to understand the underlying cause of the performance issues before making changes to the instances.
Option C: Restoring a backup of the application database from a time before the application became slow may help to
resolve the performance issues if the problem is related to the database. However, it is not necessarily the first step you
should take, as there may be other issues causing the performance problems.
upvoted 
2 
times
piyu1515
piyu1515
 
Most Recent
 
3 days, 20 hours ago
Selected Answer: 
A
Logging Agent can be used
upvoted 
1 
times
[Removed]
[Removed]
 
6 months, 1 week ago
Selected Answer: 
A
A is the only inpection. 
You want to inspect and find the underlying cause in order to solve the problem.
B & D are possible solutions, not inspection. 
C is neither solution nor inspection. C will just lead to the issue again.
upvoted 
1 
times
Exyzxz
Exyzxz
 
7 months, 3 weeks ago
The admin has lost it
upvoted 
2 
times
AugustoKras011111
AugustoKras011111
 
1 year, 4 months ago
Selected Answer: 
A
Key Word "find the underlying cause", so the answer is A.
upvoted 
4 
times
RVivek
RVivek
 
1 year, 4 months ago
Selected Answer: 
A
Question mentions "You want to find the underlying cause in order to solve the problem"
B, C and D 
are 
attempt to solve the problem without finding the cause
upvoted 
5 
times
Wael216
Wael216
 
1 year, 6 months ago
this has nothing to do with "gcp" in real, this is SRE instinct
upvoted 
4 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A 
Is the Correct Answer
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
This is no brainer question, I would choose A
upvoted 
3 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months agoSelected Answer: 
A
Agree with A.
First remove any non possible answers: B / C.
Then we have A or D left. 
But D does a good action / recommended action but it says "what do we do first" which is always troubleshoot.
upvoted 
3 
times
rorz
rorz
 
1 year, 10 months ago
Selected Answer: 
A
First thing would be to inspect the logs
upvoted 
1 
times
EricG77
EricG77
 
1 year, 10 months ago
Answe is A. 
I would agree the question is stating "You want to find the underlying cause in order to solve the problem." 
Everything else is making changes without understanding the issues at hand
upvoted 
1 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
1 year, 10 months ago
A is the only answer that is really caring about **analyzing** the underlying problem before **touching** anything.
upvoted 
2 
times
rhage_56
rhage_56
 
1 year, 10 months ago
Selected Answer: 
A
key word is "You want to find the underlying cause in order to solve the problem"
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #185
Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. When releasing new
versions of the application via a rolling deployment, the team has been causing outages. The root cause of the outages is
misconfigurations with parameters that are only used in production. You want to put preventive measures for this in the
platform to prevent outages. What should you do? 
A. 
Configure liveness and readiness probes in the Pod specification. 
Most Voted
B. 
Configure health checks on the managed instance group.
C. 
Create a Scheduled Task to check whether the application is available.
D. 
Configure an uptime alert in Cloud Monitoring.
Correct Answer:
 
A 
Comments
jabrrJ68w02ond1
jabrrJ68w02ond1
 
Highly Voted
 
2 years, 4 months ago
Selected Answer: 
A
A: Configuring the right liveness and readiness probes prevents outages when rolling out a new ReplicaSet of a Deployment,
because Pods are only getting traffic when they are considered ready.
B: With GKE, you do not deal with MIGs.
C: Does not use GKE tools and is therefore not the best option.
D: Does alert you but does not prevent the outage.
upvoted 
20 
times
khadar
khadar
 
2 years, 3 months ago
more explanation in the below link..https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-
practices-setting-up-health-checks-with-readiness-and-liveness-probes
upvoted 
8 
times
MarcoPellegrino
MarcoPellegrino
 
Most Recent
 
2 months, 2 weeks ago
B. We are talking about GKE, not Compute Engine instances
C. The task will not check in real-time
D. Uptime alerts do not apply to GKE pods
upvoted 
1 
times
Community vote distribution
A (97%)
D
(3%)upvoted 
1 
times
a53fd2c
a53fd2c
 
8 months, 4 weeks ago
There is not such a thing as Managed compute instances in GKE
https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-setting-up-health-checks-with-
readiness-and-liveness-probes
upvoted 
2 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
D
Right answer is D. Liveness and readiness probes (option A) are essential for overall application health but might not directly
detect misconfigurations during deployments. They focus on ensuring pods are healthy and responsive, not necessarily catching
configuration issues.
upvoted 
1 
times
tlopsm
tlopsm
 
6 months, 4 weeks ago
uptime and monitoring will not stop outages in application, howver you will be informed on time to respond to the issue.
A. Configuring liveness and readiness probe in each pod will stop starting pods from receiving traffic before they are declared
ready and available. hence before taking down a working pod.
upvoted 
1 
times
massacare
massacare
 
1 year, 4 months ago
Selected Answer: 
A
Who answered aside from A never read/implementes kubernetes best practices. Link
https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster
upvoted 
2 
times
red_panda
red_panda
 
1 year, 6 months ago
Selected Answer: 
A
A without any doubts
upvoted 
2 
times
mimicha1
mimicha1
 
1 year, 6 months ago
D. Configure an uptime alert in Cloud Monitoring.
Configuring an uptime alert in Cloud Monitoring will notify the team when the application becomes unavailable. This will help
in detecting outages before they occur and mitigate the risks of releasing new versions with misconfigurations.
While configuring liveness and readiness probes in the Pod specification and configuring health checks on the managed
instance group are important for ensuring that the application is running, they do not prevent outages caused by
misconfigurations with production parameters.
Creating a Scheduled Task to check whether the application is available is also useful, but it is not preventive in nature. By the
time a scheduled task detects an outage, the damage may have already been done.
upvoted 
2 
times
mimicha1
mimicha1
 
1 year, 6 months ago
why not A ? 
* Configuring liveness and readiness probes in the Pod specification is important to detect when a container in a Pod becomes
unresponsive or starts experiencing problems. However, it does not directly prevent outages caused by misconfigurations with
parameters that are only used in production.
Liveness and readiness probes can help to detect issues with the application, but they do not provide information about the
health of the underlying infrastructure. Misconfigurations with parameters that are only used in production can cause
problems with the infrastructure itself, which may not be detected by liveness and readiness probes.
In summary, while configuring liveness and readiness probes is important, it should be done in addition to other preventive
measures such as configuring an uptime alert in Cloud Monitoring to ensure timely detection of outages and reduce their
impact on the application.
upvoted 
1 
times
CGS22
CGS22
 
1 year, 10 months ago
Selected Answer: 
A
Configure liveness and readiness probes in the Pod specification.Configure liveness and readiness probes in the Pod specification.
This will help to prevent outages by ensuring that only healthy Pods are serving traffic. The liveness probe will check that the
Pod is running and responding to requests. The readiness probe will check that the Pod is ready to serve traffic, such as by
checking that the application is installed and configured.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
Liveness and readiness probes are used to determine the health of a Pod. Liveness probes are used to determine whether a Pod
is running, and readiness probes are used to determine whether a Pod is able to receive traffic.
By configuring liveness and readiness probes in the Pod specification, you can help to prevent outages when releasing new
versions of the application via a rolling deployment. If a Pod fails a liveness or readiness probe, it will be restarted, which can
help to prevent issues caused by misconfigured parameters or other problems.
The correct answer is A: Configure liveness and readiness probes in the Pod specification.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
Option B: Configuring health checks on the managed instance group is not relevant in this scenario, as the application is
running in a GKE cluster, not on a managed instance group.
Option C: Creating a Scheduled Task to check whether the application is available may help to detect outages, but it will not
prevent them from occurring. To prevent outages, you should focus on identifying and addressing the root cause of the
problem.
Option D: Configuring an uptime alert in Cloud Monitoring may help to detect outages, but it will not prevent them from
occurring. To prevent outages, you should focus on identifying and addressing the root cause of the problem.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
A
A 
Is the Correct Answer
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A is best answer
upvoted 
1 
times
zellck
zellck
 
2 years, 3 months ago
Selected Answer: 
A
A is the answer.
Kubernetes Health Checks with Readiness and Liveness Probes
https://www.youtube.com/watch?v=mxEvAPQRwhw
upvoted 
2 
times
rhage_56
rhage_56
 
2 years, 4 months ago
Selected Answer: 
A
B is out since MIGs relate to compute engine. D and C are both not preventive measures.
upvoted 
1 
times
spET_1024
spET_1024
 
2 years, 4 months ago
Option A is correct. Since it is regarding GKE and the application deployed in GKE cluster. Therefore, managed instance group
does not have anything to do.
So, right answer is:
A. Configure liveness and readiness probes in the Pod specification.
upvoted 
2 
timesupvoted 
2 
times
aut0pil0t
aut0pil0t
 
2 years, 4 months ago
Selected Answer: 
A
A. 
There are no MIGs in GKE. Only thing that makes sense is to have good readiness probes
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #186
Your company uses Google Kubernetes Engine (GKE) as a platform for all workloads. Your company has a single large GKE
cluster that contains batch, stateful, and stateless workloads. The GKE cluster is configured with a single node pool with 200
nodes. Your company needs to reduce the cost of this cluster but does not want to compromise availability. What should you
do? 
A. 
Create a second GKE cluster for the batch workloads only. Allocate the 200 original nodes across both clusters.
B. 
Configure CPU and memory limits on the namespaces in the cluster. Configure all Pods to have a CPU and memory
limits.
C. 
Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the
cluster to use node auto scaling. 
Most Voted
D. 
Change the node pool to use preemptible VMs.
Correct Answer:
 
C 
Comments
jabrrJ68w02ond1
jabrrJ68w02ond1
 
Highly Voted
 
2 years, 4 months ago
Selected Answer: 
C
A: Is not necessary because you can have multiple node pools with different configurations.
B: Optimizes resource usage of CPU/memory in your existing node pool but does not necessarily improve cost - still an option
that should be considered.
C: This looks really good. Autoscaling workloads and the node pools makes your whole infrastructure more elastic and gives
you the option to rely on the same node pool.
D: This might not be a good option for every type of workload. Batch and stateless workloads can often handle this quite well,
but stateful workloads are not well-suited for operation on preemptible VMs.
Since only one answer is accepted, I'll choose C.
upvoted 
15 
times
ramzez4815
ramzez4815
 
Highly Voted
 
2 years, 4 months ago
Selected Answer: 
C
C is the correct answer as it doesn't involve major changes to the current Kubernetes configuration
upvoted 
8 
times
Community vote distribution
C (100%)upvoted 
8 
times
MarcoPellegrino
MarcoPellegrino
 
Most Recent
 
2 months, 2 weeks ago
It uses Google features
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 1 week ago
Selected Answer: 
C
Vote for C. In B limits could compromise availability.
upvoted 
1 
times
AugustoKras011111
AugustoKras011111
 
1 year, 10 months ago
Selected Answer: 
C
Answer C. Use HorizontalPodAutoscaler.
upvoted 
1 
times
zerg0
zerg0
 
1 year, 11 months ago
Selected Answer: 
C
HorizontalPodAutoscaler is the way
upvoted 
1 
times
tdotcat
tdotcat
 
1 year, 11 months ago
Selected Answer: 
C
c is good
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is C: Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful
workloads. Configure the cluster to use node auto scaling.
One way to reduce the cost of a Google Kubernetes Engine (GKE) cluster without compromising availability is to use horizontal
pod autoscalers (HPA) and node auto scaling.
HPA allows you to automatically scale the number of Pods in a deployment based on the resource usage of the Pods. By
configuring HPA for stateless workloads and for compatible stateful workloads, you can ensure that the number of Pods is
automatically adjusted based on the actual resource usage, which can help to reduce costs.
Node auto scaling allows you to automatically add or remove nodes from the node pool based on the resource usage of the
cluster. By configuring node auto scaling, you can ensure that the cluster has the minimum number of nodes needed to meet
the resource requirements of the workloads, which can also help to reduce costs.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
A: Creating a second GKE cluster for the batch workloads only and allocating the 200 original nodes across both clusters
would not necessarily help to reduce costs, as the total number of nodes in the clusters would remain the same.
B: Configuring CPU and memory limits on the namespaces in the cluster and configuring all Pods to have CPU and memory
limits may help to reduce costs, but it is not sufficient on its own. You should also use HPA and node auto scaling to ensure
that the cluster is properly sized based on the actual resource usage.
D: Changing the node pool to use preemptible VMs may help to reduce costs, but it is not sufficient on its own. Preemptible
VMs can be terminated at any time, which may not be suitable for all workloads. You should also use HPA and node auto
scaling to ensure that the cluster is properly sized based on the actual resource usage.
upvoted 
2 
times
ale_brd_111
ale_brd_111
 
2 years ago
Selected Answer: 
C
C is the correct answer as it doesn't involve major changes to the current Kubernetes configuration
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
CSelected Answer: 
C
C is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is correct
upvoted 
1 
times
rorz
rorz
 
2 years, 4 months ago
Selected Answer: 
C
C is correc
upvoted 
1 
times
aswani
aswani
 
2 years, 4 months ago
Selected Answer: 
C
Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the
cluster to use node auto scaling
upvoted 
2 
times
spET_1024
spET_1024
 
2 years, 4 months ago
Option C is correct. Since, the company does not want to compromise availability of the application so, HPA is suitable option
for autoscaling pods. Keeping the cost optimization in mind, nodes of the GKE cluster also needs to be autoscaled. Therefore,
the correct option is, 
C. Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the
cluster to use node auto scaling.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #187
Your company has a Google Cloud project that uses BigQuery for data warehousing on a pay-per-use basis. You want to monitor
queries in real time to discover the most costly queries and which users spend the most. What should you do? 
A. 
1. In the BigQuery dataset that contains all the tables to be queried, add a label for each user that can launch a query. 2.
Open the Billing page of the project. 3. Select Reports. 4. Select BigQuery as the product and filter by the user you want to
check.
B. 
1. Create a Cloud Logging sink to export BigQuery data access logs to BigQuery. 2. Perform a BigQuery query on the
generated table to extract the information you need. 
Most Voted
C. 
1. Create a Cloud Logging sink to export BigQuery data access logs to Cloud Storage. 2. Develop a Dataflow pipeline to
compute the cost of queries split by users.
D. 
1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information you
need.
Correct Answer:
 
B 
Comments
kuboraam
kuboraam
 
Highly Voted
 
2 years, 3 months ago
Selected Answer: 
B
I choose B because of "real-time". Otherwise, D seems to be the most relevant and flexible.
upvoted 
13 
times
VSMu
VSMu
 
1 year, 11 months ago
D also can be continuous https://cloud.google.com/billing/docs/how-to/export-data-bigquery#setup. I think D is the right
answer.
upvoted 
9 
times
Mahmoud_E
Mahmoud_E
 
Highly Voted
 
2 years, 2 months ago
Selected Answer: 
B
B is the correct answer https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-
monitoring
Community vote distribution
B (74%)
D (21%)
A (5%)monitoring
A is incorrect as there is not billing page for a project, its billing account that handles all org billing.
upvoted 
8 
times
jlambdan
jlambdan
 
1 year, 7 months ago
"details about the query that was executed, like the SQL code, the job ID and, most important, the user who executed the
query and the amount of data that was processed. With that information, you can compute the total cost of the query using a
simple multiplication equation: cost per TB processed * numbers of TB processed" means it will be an estimation, not the real
numbers.
upvoted 
1 
times
[Removed]
[Removed]
 
1 year, 4 months ago
That blog page says it shows DAILY results, not real time...
upvoted 
1 
times
JamesKarianis
JamesKarianis
 
Most Recent
 
4 months, 1 week ago
Selected Answer: 
D
Google recommends to export cloud billing data to bigquery to control cost in real-time:
https://cloud.google.com/billing/docs/how-to/export-data-bigquery
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
1 month ago
Read again... the question relates to BQ queries cost. The link states:
Use the Detailed usage export to analyze costs at the resource level, and identify specific resources that might be driving up
costs. The detailed export includes resource-level information for the following products:
Compute Engine
Google Kubernetes Engine (GKE)
Cloud Run functions
Cloud Run.... 
Big Query is not listed
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
1 month ago
please ignore my comment James. I miss read the answers and the usage of the export to BQ ;)
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
1 month ago
Answer is D.
upvoted 
1 
times
dija123
dija123
 
8 months, 4 weeks ago
Selected Answer: 
B
B is correct!
upvoted 
1 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
B
B is the correct option. Why not A: 
While the Billing page offers reports with user-level cost breakdowns, it doesn't provide real-
time information or detailed query data. Why not D: Billing export can provide cost data in BigQuery, but it doesn't capture
details about individual queries or users, making it insufficient for the specific needs of identifying costly queries and high-
spending users.
upvoted 
1 
times
Pime13
Pime13
 
11 months ago
Selected Answer: 
B
b -> real time
upvoted 
1 
timesammonia_free
ammonia_free
 
11 months, 2 weeks ago
Selected Answer: 
B
I tend to agree with the GPT4 summary:
In summary, 
Option B is more focused on analyzing specific BigQuery usage patterns and costs down to the level of individual queries and
users. It's better for real-time analysis of query activities. 
Option D, on the other hand, provides a broader overview of all costs associated with the Google Cloud project, which is
beneficial for general cost management but less so for in-depth analysis of specific BigQuery queries and user activities. 
For the specific need to discover the most costly queries and which users are responsible, Option B is more targeted and
appropriate
upvoted 
1 
times
91d8ca7
91d8ca7
 
12 months ago
I took PCA exam today and 
I passed PCA exam. In my test, I choose D on this question.
upvoted 
1 
times
ammonia_free
ammonia_free
 
11 months, 2 weeks ago
My congrats! Did the exam results show you that you answered correctly to this particular 
question?
upvoted 
2 
times
theBestStudent
theBestStudent
 
1 year, 1 month ago
Selected Answer: 
B
It can't be better explained https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-
cost-monitoring
upvoted 
1 
times
someone2011
someone2011
 
1 year, 3 months ago
B: because of" https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-
monitoring
And because https://cloud.google.com/billing/docs/how-to/export-data-bigquery#example-queries. is not mentioning
anything about query per user.
B
upvoted 
2 
times
dsyouness
dsyouness
 
1 year, 3 months ago
Selected Answer: 
D
B don't provide the cost
upvoted 
2 
times
someone2011
someone2011
 
1 year, 3 months ago
Yes it can:
https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring
upvoted 
2 
times
daidaidai
daidaidai
 
1 year, 4 months ago
The answer is D. 1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the
information you need.
Explanation:
A. This option is not correct because adding a label for each user in the BigQuery dataset will not allow you to monitor the cost
of queries or find out which users spend the most.
B. This option is not correct because BigQuery data access logs do not include billing information or query costs.
C. This option is not correct because BigQuery data access logs stored in Cloud Storage do not include billing information or
query costs, and developing a Dataflow pipeline to compute the cost of queries would be unnecessarily complex.
D. This is the correct option because activating billing export into BigQuery will allow you to query the billing data in real-timeD. This is the correct option because activating billing export into BigQuery will allow you to query the billing data in real-time
to discover the most costly queries and which users spend the most.
upvoted 
2 
times
didek1986
didek1986
 
1 year, 5 months ago
Selected Answer: 
D
D is the easiest
upvoted 
2 
times
gary_cooper
gary_cooper
 
1 year, 5 months ago
Selected Answer: 
D
1. Activate billing export into BigQuery. 
2. Perform a BigQuery query on the billing table to extract the information you need.
upvoted 
1 
times
Umesh09
Umesh09
 
1 year, 6 months ago
both B and D do not meet the exact requirements while B does not give the cost D does not provide user level details. Answer
A seems to better suited though complicated
upvoted 
1 
times
red_panda
red_panda
 
1 year, 6 months ago
Selected Answer: 
D
For me, most simple and also available in real time is D
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
B
Option D might seem like a reasonable choice, but it doesn't provide real-time monitoring of queries, which is the requirement
mentioned in the question.
Activating billing export to BigQuery provides detailed billing information for your Google Cloud project. However, this
method doesn't provide real-time insights into query costs and user expenditures, as billing data is typically updated once per
day.
On the other hand, option B allows you to monitor queries in real-time by exporting BigQuery data access logs directly to
another BigQuery table, enabling you to analyze the most costly queries and user expenses as they happen.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #188
Your company and one of its partners each have a Google Cloud project in separate organizations. Your company's project (prj-
a) runs in Virtual Private Cloud 
(vpc-a). The partner's project (prj-b) runs in vpc-b. There are two instances running on vpc-a and one instance running on vpc-b.
Subnets defined in both VPCs are not overlapping. You need to ensure that all instances communicate with each other via
internal IPs, minimizing latency and maximizing throughput. What should you do? 
A. 
Set up a network peering between vpc-a and vpc-b. 
Most Voted
B. 
Set up a VPN between vpc-a and vpc-b using Cloud VPN.
C. 
Configure IAP TCP forwarding on the instance in vpc-b, and then launch the following gcloud command from one of the
instances in vpc-a gcloud: gcloud compute start-iap-tunnel INSTANCE_NAME_IN_VPC_8 22 \ --local-host-port=localhost:22
D. 
1. Create an additional instance in vpc-a. 2. Create an additional instance in vpc-b. 3. Install OpenVPN in newly created
instances. 4. Configure a VPN tunnel between vpc-a and vpc-b with the help of OpenVPN.
Correct Answer:
 
A 
Comments
zellck
zellck
 
Highly Voted
 
1 year, 9 months ago
Selected Answer: 
A
definitely A.
https://cloud.google.com/vpc/docs/vpc-peering
Google Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks
regardless of whether they belong to the same project or the same organization.
upvoted 
12 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
A
VPC peering: https://cloud.google.com/vpc/docs/vpc-peering
Peered VPC networks can be in the same project, different projects of the same organization, or different projects of different
organizations.
IPv4 subnet routes in peered VPC networks can't overlap
Community vote distribution
A (100%)IPv4 subnet routes in peered VPC networks can't overlap
upvoted 
3 
times
OrangeTiger
OrangeTiger
 
5 months, 2 weeks ago
Selected Answer: 
A
A is ok!
upvoted 
1 
times
[Removed]
[Removed]
 
6 months, 1 week ago
Selected Answer: 
A
Since it's mentioned that the subnets do not overlap, A is the best way to go. 
If the subnets overlapped, you would go with B. 
https://cloud.google.com/vpc/docs/vpc-peering#interaction-subnet-subnet
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
2 
times
jake_edman
jake_edman
 
1 year, 7 months ago
Selected Answer: 
A
Clearly A as the IPs do not overlap
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is the correct answer as per https://cloud.google.com/vpc/docs/vpc-peering
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A is right answer
upvoted 
2 
times
6721sora
6721sora
 
1 year, 10 months ago
Selected Answer: 
A
Clearly A
upvoted 
2 
times
rorz
rorz
 
1 year, 10 months ago
Selected Answer: 
A
A - VPC peering should be good
upvoted 
1 
times
aswani
aswani
 
1 year, 10 months ago
Selected Answer: 
A
It should be A
upvoted 
1 
times
kiappy81
kiappy81
 
1 year, 10 months ago
Selected Answer: 
A
network peering is fine
upvoted 
1 
times
ilcasta73
ilcasta73
 
1 year, 10 months ago
It should be A
upvoted 
1 
timesupvoted 
1 
times
rhage_56
rhage_56
 
1 year, 10 months ago
Selected Answer: 
A
peering is better as both orgs are in GCP
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #189
You want to store critical business information in Cloud Storage buckets. The information is regularly changed, but previous
versions need to be referenced on a regular basis. You want to ensure that there is a record of all changes to any information
in these buckets. You want to ensure that accidental edits or deletions can be easily rolled back. Which feature should you
enable? 
A. 
Bucket Lock
B. 
Object Versioning 
Most Voted
C. 
Object change notification
D. 
Object Lifecycle Management
Correct Answer:
 
B 
Comments
namesgeo
namesgeo
 
Highly Voted
 
3 months, 3 weeks ago
Object Versioning is a feature that allows you to store multiple versions of an object in Cloud Storage. Hence, answer should be
B
upvoted 
6 
times
megumin
megumin
 
Most Recent
 
7 months, 2 weeks ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
samsonakala
samsonakala
 
8 months, 1 week ago
Most definitely B
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
8 months, 2 weeks ago
Selected Answer: 
B
B is the correct answer as per https://cloud.google.com/storage/docs/object-versioning
upvoted 
1 
times
Community vote distribution
B (100%)upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
B - Object versioning
upvoted 
2 
times
alexandercamachop
alexandercamachop
 
9 months, 3 weeks ago
Selected Answer: 
B
Object versioning, super important to be able to rollback in case of any deletion.
upvoted 
4 
times
kiappy81
kiappy81
 
10 months ago
Selected Answer: 
B
https://cloud.google.com/storage/docs/object-versioning
upvoted 
3 
times
khadar
khadar
 
9 months, 4 weeks ago
I too got this question in 10-09-22 exam with similar option and result is pass
upvoted 
7 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #190
You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed
the Cloud Monitoring agent and configured the autoscaling policy as follows: 
  Metric identifier: agent.googleapis.com/memory/percent_used 
  Filter: metric.label.state = 'used' 
  Target utilization level: 80 
  Target type: GAUGE 
You observe that the application does not scale under high load. You want to resolve this. What should you do? 
A. 
Change the Target type to DELTA_PER_MINUTE.
B. 
Change the Metric identifier to agent.googleapis.com/memory/bytes_used.
C. 
Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND
metric.label.state = 'slab'. 
Most Voted
D. 
Change the filter to metric.label.state = 'free' and the Target utilization to 20.
Correct Answer:
 
C 
Comments
Mahmoud_E
Mahmoud_E
 
Highly Voted
 
2 years, 2 months ago
Selected Answer: 
C
C is correct answer:
A. Change the Target type to DELTA_PER_MINUTE. (in this case the utlization tagret need to be in minutes which is not the case
its percentage % and not time based.
B. Change the Metric identifier to agent.googleapis.com/memory/bytes_used. (not applicable) 
C. Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND
metric.label.state = 'slab'. (this gives total memory used)
D. Change the filter to metric.label.state = 'free' and the Target utilization to 20. (you would still need to change the the
percent_used to percent_free)
https://stackoverflow.com/questions/69267526/what-is-disk-data-cached-in-the-memory-usage-chart-metrics-of-gcp-
compute-in
https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics
upvoted 
34 
times
Community vote distribution
C (60%)
A (27%)
D (13%)upvoted 
34 
times
chickennuggets
chickennuggets
 
2 months, 1 week ago
C is correct - the link provided actually specs exactly what needs to be done
https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage
upvoted 
2 
times
kiappy81
kiappy81
 
Highly Voted
 
2 years, 4 months ago
Selected Answer: 
A
TARGET_TYPE: the value type for the metric.
gauge: the autoscaler computes the average value of the data collected in the last couple of minutes and compares that to the
utilization target.
delta-per-minute: the autoscaler calculates the average rate of growth per minute and compares that to the utilization target.
delta-per-second: the autoscaler calculates the average rate of growth per second and compares that to the utilization target.
For accurate comparisons, if you set the utilization target in seconds, use delta-per-second as the target type. Likewise, use
delta-per-minute for a utilization target in minutes.
upvoted 
16 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month ago
Selected Answer: 
C
This question is wrong and contradict with question 199, 
Q199 is the right one
upvoted 
1 
times
MarcoPellegrino
MarcoPellegrino
 
2 months, 2 weeks ago
There should always be a time reference when having a metric
upvoted 
1 
times
pcamaster
pcamaster
 
5 months, 1 week ago
exam done today, This question has been changed in the exam and the filter in the text of the question is actually
"metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state =
'slab'."
upvoted 
3 
times
Gino17m
Gino17m
 
8 months, 1 week ago
Selected Answer: 
C
...but....propbably thwre is a mistake in the question. I assume that metric.label.state can't have many values in the same time so
instead AND operator OR should be used ??????
upvoted 
3 
times
huuthanhdlv
huuthanhdlv
 
7 months, 1 week ago
Have same thought as you. Misunderstood this point thus selected wrong answer. All answers seem wrong...
upvoted 
1 
times
[Removed]
[Removed]
 
8 months, 3 weeks ago
To configure autoscaling based on the percent of used memory, specify the percent_used metric provided by the memory Ops
Agent metrics. You should filter the metric by state to use only the used memory state. If you do not specify the filter, then the
autoscaler takes the sum of memory usage by all memory states labeled as buffered, cached, free, slab, and used.
upvoted 
2 
times
kalyan_krishna742020
kalyan_krishna742020
 
8 months, 3 weeks ago
I'm preparing for a test and see that questions from 115 onwards are considered valid. Can anyone who's taken the test offer
any insights or advice? Thank you!
upvoted 
1 
times
a53fd2c
a53fd2c
 
8 months, 4 weeks ago
C is the right option ( use used and gauge as options as in the guide listed here
https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage
upvoted 
1 
times
nuts_bee
nuts_bee
 
9 months, 3 weeks agonuts_bee
nuts_bee
 
9 months, 3 weeks ago
In the real exam 
"Filter: metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state =
'slab'" is in the QUESTION.
The answer should be:
C. Change the filter to metric.label.state = 'used'
upvoted 
2 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
C
C is the correct approach. The current filter only considers memory in the "used" state. However, the operating system also uses
memory for caching, buffering, and other purposes. By modifying the filter we ensure the autoscaling policy considers all
memory states, providing a more accurate representation of total memory usage.
upvoted 
2 
times
agadd2
agadd2
 
10 months, 1 week ago
This question came in recent exam and default state already have all metric.label.state . Went with DELTE per minute option A
upvoted 
1 
times
alpha_canary
alpha_canary
 
10 months, 3 weeks ago
Selected Answer: 
C
Question has a mistake
"Filter: metric.label.state = 'used'" is in option C
"Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND
metric.label.state = 'slab'."
is actually in the queston.
https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage
You should use: Filter: metric.label.state = 'used'
upvoted 
2 
times
[Removed]
[Removed]
 
11 months, 1 week ago
Selected Answer: 
C
The question in actual exam is reverse. The filter in the question is metric.label.state = 'used' AND metric.label.state = 'buffered'
AND metric.label.state = 'cached' AND metric.label.state = 'slab'.
and the option C is:
Filter: metric.label.state = 'used'
C is the correct answer in that case
https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage
upvoted 
4 
times
e5019c6
e5019c6
 
1 year ago
Selected Answer: 
D
Actually this question is kinda weird
We can discard A & B right away:
A: If you change to DELTA_PER_MINUTE it'll calculate the difference in memory used from one minute to the other, and if that
difference is bigger* than 80%, it'll trigger. Not what we want.
B: If we change the metric to bytes_used, we must change the value of the gauge too. Not an option.
Now comes the messy part.
Following what is said in this page: https://cloud.google.com/monitoring/api/metrics_opsagent#agent-memory
The metric.label.state should be ONE of these: [buffered, cached, free, slab, used]
And it also states that: 'Summing the values of all states yields the total memory on the machine'. So, using a simple equation, if
we remove the 'free' one from them, then that would give us the total memory that is being used. But remember, it said ONE of
them, so that would discard it.
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
Now D, for me is the closest one to being true. If you ask only for the free percentage_used and change the target to 20, you
should be done.should be done.
But a question I read here was very interesting, and connects with the * used above...
How does it know that it should scale when the metric is above or below? We don't set that filter. We can hope that the
autoscaling is smart enough to know that when we use 'used' we mean more than and when using 'free' we mean less than.
I couldn't find any information about that, so if anyone gets any additional info, please share it.
upvoted 
1 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
C
C is the correct one
upvoted 
1 
times
rsvd
rsvd
 
1 year, 2 months ago
Selected Answer: 
C
In the real exam, questions metric label state was mentioned as 
"metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'"
and "metric.label.state = 'used'" was given in answer C.
upvoted 
7 
times
ammonia_free
ammonia_free
 
11 months, 2 weeks ago
How does it work for autoscaling? 
"AND" is considered as "OR"? 
When you already have metric.label.state = 'used' in the problem statement and have an issue, then you trying to add more
conditions and hope that this will solve the problem?!?
Strange....
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #191
You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must
communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is
200 kbps. The business requires: 
  as close to 100% system availability as possible 
  cost optimization 
You need to design the connectivity between the locations to meet the business requirements. What should you provision? 
A. 
An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway 
Most Voted
B. 
Two Classic Cloud VPN gateways connected to two on-premises VPN gateways Configure each Classic Cloud VPN
gateway to have two tunnels, each connected to different on-premises VPN gateways
C. 
Two HA Cloud VPN gateways connected to two on-premises VPN gateways Configure each HA Cloud VPN gateway to
have two tunnels, each connected to different on-premises VPN gateways
D. 
A single Cloud VPN gateway connected to an on-premises VPN gateway
Correct Answer:
 
A 
Comments
minmin2020
minmin2020
 
Highly Voted
 
1 year, 8 months ago
Selected Answer: 
A
A is true only if the on-prem (peer) gateway has two separate external P addresses. The HA VPN gateway uses two tunnels, one
tunnel to each external IP address on the peer device as described in https://cloud.google.com/network-
connectivity/docs/vpn/concepts/topologies#configurations_that_support_9999_availability
C is a complete solution that provides full redundancy of the on-prem gateway. This is probably more expensive and having
two HA VPN Gateways is an unusual configuration as the online documentation only describes using one HA VPN Gateway
A appears to be correct with assumptions...!
upvoted 
7 
times
Brainstorm
Brainstorm
 
Most Recent
 
2 months, 1 week ago
Selected Answer: 
D
Community vote distribution
A (81%)
Other (19%)D is correct. 
A. Single HA Cloud VPN with Two Tunnels: This offers redundancy, but a single point of failure exists at the Cloud VPN gateway
itself. A second HA Cloud VPN gateway provides additional fault tolerance.
B. Two Classic Cloud VPNs with Multiple Tunnels: Classic Cloud VPNs are being phased out and might be less cost-effective
than HA Cloud VPNs. Additionally, managing multiple Classic Cloud VPN gateways can be more complex.
D. Single Cloud VPN Gateway: This offers a single point of failure and wouldn't achieve the desired level of high availability.
upvoted 
1 
times
Brainstorm
Brainstorm
 
2 months, 1 week ago
Sorry, I meant C is correct.
upvoted 
1 
times
devnul
devnul
 
8 months, 1 week ago
why not B? See https://cloud.google.com/network-connectivity/docs/vpn/concepts/classic-topologies#option-3
Don't you need redundant VPN gateways on the other side (on-prem) to reach close to 100% availability? A) has a single VPN
gateway on-prem.
upvoted 
1 
times
Gino17m
Gino17m
 
2 months ago
Because B i about increasing throughput and load balancing not about availability ??? Expected avarage throughput is "only"
200 kbps.
A) has a single VPN...right but I think the clue is in "You are deploying an application to Google Cloud. The application is part
of a system."....so probably you have no leverage on on-premises solutions ?????? ....but in fact I'm not sure wchich answer is
right :(
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
A: looks an exact match for the requirements, 99.99% availability
B: Is a manual implementation of HA, not optimizing cost
C: Is behoynd HA, no longer optimizing cost.
D: Does not provide close to 100% as possible
upvoted 
3 
times
AugustoKras011111
AugustoKras011111
 
1 year, 3 months ago
Selected Answer: 
A
Use HA (High Availability) VPN as required in the question. A is better aswer.
upvoted 
3 
times
ale_brd_111
ale_brd_111
 
1 year, 6 months ago
Selected Answer: 
A
both A and C are possible solutions but A is cheaper.
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
2 
times
winter0w
winter0w
 
1 year, 7 months ago
Selected Answer: 
D
Correct Answer is D,
You cannot migrate an existing Cloud VPN tunnel or tunnels on a Classic VPN gateway to an HA VPN gateway. Instead, you
need to create new tunnels and delete the old ones.
https://cloud.google.com/network-connectivity/docs/vpn/how-to/moving-to-ha-vpn#general_guidelines
upvoted 
2 
times
Gino17m
Gino17m
 
2 months agoGino17m
Gino17m
 
2 months ago
You cannot migrate.....but...."You are deploying an application to Google Cloud"....nothing to migrate....
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A satisfty both requriements
upvoted 
3 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Satisfy both requirements for close to 100% availability and cost containment
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A is right
upvoted 
1 
times
ForkMeSoftly
ForkMeSoftly
 
1 year, 9 months ago
Selected Answer: 
A
best explained in https://jayendrapatil.com/tag/classic-vpn-vs-ha-vpn/
HA VPN provides an SLA of 99.99% service availability, when configured with two interfaces and two external IP addresses.
upvoted 
2 
times
alvinlxw
alvinlxw
 
1 year, 9 months ago
Selected Answer: 
A
To meet the 99.99% SLA on the Google Cloud side, there must be a tunnel from each of the two interfaces on the HA VPN
gateway to the corresponding interfaces on the peer gateway.
upvoted 
1 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months ago
Selected Answer: 
A
A can provide 99.99% availability as well, and no need for C which will be more expensive.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#1-peer-1-address
upvoted 
4 
times
zellck
zellck
 
1 year, 9 months ago
Selected Answer: 
A
A can provide 99.99% availability as well, and no need for C which will be more expensive.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#1-peer-1-address
upvoted 
3 
times
Nirca
Nirca
 
1 year, 10 months ago
Selected Answer: 
C
C 
is full mash. real HR with 
redundancy on the on premises site
upvoted 
1 
times
kuboraam
kuboraam
 
1 year, 10 months ago
Selected Answer: 
A
I choose A. Gives you 99.99% availability, and is certainly cheaper than B, C and is more reliable than D.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies
upvoted 
3 
times
aswani
aswani
 
1 year, 10 months ago
Selected Answer: 
Chttps://cloud.google.com/static/network-connectivity/docs/vpn/images/ha-vpn-gcp-to-on-prem-2-a.svg
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #192
Your company has an application running on App Engine that allows users to upload music files and share them with other
people. You want to allow users to upload files directly into Cloud Storage from their browser session. The payload should not
be passed through the backend. What should you do?
A. 
1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an
allowed origin. 
2. Use the Cloud Storage Signed URL feature to generate a POST URL. 
Most Voted
B. 
1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an
allowed origin. 
2. Assign the Cloud Storage WRITER role to users who upload files.
C. 
1. Use the Cloud Storage Signed URL feature to generate a POST URL. 
2. Use App Engine default credentials to sign requests against Cloud Storage.
D. 
1. Assign the Cloud Storage WRITER role to users who upload files. 
2. Use App Engine default credentials to sign requests against Cloud Storage.
Correct Answer:
 
A 
Comments
mmathiou
mmathiou
 
Highly Voted
 
1 year, 3 months ago
Selected Answer: 
A
It should be A. Since it is stated that the payload should not passed from the backend and be send directly to the bucket, then a
CORS configuration should be set to the bucket.
upvoted 
10 
times
cruel_sun
cruel_sun
 
Most Recent
 
6 months, 1 week ago
Selected Answer: 
A
A. 1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an
allowed origin. 2. Use the Cloud Storage Signed URL feature to generate a POST URL.
Here's why this approach is most suitable:
• CORS configuration: This allows cross-origin requests from your App Engine application to access the Cloud Storage bucket
for uploads. Setting the App Engine base URL as an allowed origin ensures secure communication.
Community vote distribution
A (83%)
C (17%)for uploads. Setting the App Engine base URL as an allowed origin ensures secure communication.
• Cloud Storage Signed URL: This feature generates a temporary URL with specific permissions and expiration time. You can
provide this signed URL to the user's browser for uploading files directly to Cloud Storage. The payload (music file) doesn't
pass through your backend, reducing server load.
upvoted 
4 
times
madcloud32
madcloud32
 
10 months ago
Selected Answer: 
A
I would go for A as per its definition and it works good that way
upvoted 
1 
times
Amrita2012
Amrita2012
 
10 months, 4 weeks ago
It's between A and C.
But if you select C then you have to justify the use of "Use App Engine default credentials to sign requests against Cloud
Storage. " hence go with option A.
upvoted 
1 
times
Namanjain7206
Namanjain7206
 
11 months, 1 week ago
Selected Answer: 
C
https://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-
url
upvoted 
2 
times
91d8ca7
91d8ca7
 
12 months ago
Selected Answer: 
C
I'm also not sure either A or C. But in my PCA exam today, I choose C. And I have passed.
upvoted 
1 
times
Anudeep58
Anudeep58
 
1 year ago
C is the Answer. https://cloud.google.com/storage/docs/cross-origin
upvoted 
2 
times
Patrick2708
Patrick2708
 
1 year ago
if its cross-origin. Then why C is answer? Shouldn't it be A
upvoted 
2 
times
Prakzz
Prakzz
 
1 year, 3 months ago
Signed URL is for TIme-Based access. This needs access all the time.
upvoted 
3 
times
someone2011
someone2011
 
1 year, 3 months ago
A:
https://cloud.google.com/storage/docs/cross-origin#server-side-support
"Cloud Storage supports this specification by allowing you to configure your buckets to support CORS. Continuing the above
example, you can configure the example.storage.googleapis.com bucket so that a browser can share its resources with scripts
from example.appspot.com."
upvoted 
3 
times
Murtuza
Murtuza
 
1 year, 3 months ago
The correct answer is A
upvoted 
1 
times
sheucm89
sheucm89
 
1 year, 3 months ago
Not sure is A or C. I will go with C.
https://cloud.google.com/blog/products/storage-data-transfer/uploading-images-directly-to-cloud-storage-by-using-signed-
url
upvoted 
3 
timesxaqanik
xaqanik
 
1 year, 2 months ago
There is no any relationship between App engine application and cloud storage. 
You need bind them.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #193
You are configuring the cloud network architecture for a newly created project in Google Cloud that will host applications in
Compute Engine. Compute Engine virtual machine instances will be created in two different subnets (sub-a and sub-b) within a
single region: 
• Instances in sub-a will have public IP addresses. 
• Instances in sub-b will have only private IP addresses. 
To download updated packages, instances must connect to a public repository outside the boundaries of Google Cloud. You
need to allow sub-b to access the external repository. What should you do?
A. 
Enable Private Google Access on sub-b.
B. 
Configure Cloud NAT and select sub-b in the NAT mapping section. 
Most Voted
C. 
Configure a bastion host instance in sub-a to connect to instances in sub-b.
D. 
Enable Identity-Aware Proxy for TCP forwarding for instances in sub-b.
Correct Answer:
 
B 
Comments
piyu1515
piyu1515
 
3 days, 18 hours ago
Selected Answer: 
C
Answer is B. Cloud Nat is the right service
upvoted 
1 
times
Amrita2012
Amrita2012
 
4 months, 3 weeks ago
Selected Answer: 
B
https://www.youtube.com/watch?v=4uskhIk7LdM
upvoted 
1 
times
cchiaramelli
cchiaramelli
 
8 months, 1 week ago
Selected Answer: 
B
IMHO
Community vote distribution
B (90%)
C (10%)IMHO
A -> It doesn't make sense, Public Google Access allows you to access Google APIs without an external IP, which doesnt solve
the problem
C -> Bastion host is for the opposite purpose; accessing a machine administratively from the outside without an external IP, not
a machine without an external IP accessing the outside.
D -> It doesn't make sense.
B -> It's the recommended solution for GCP
upvoted 
3 
times
ampmusic
ampmusic
 
8 months, 1 week ago
Selected Answer: 
B
Answer is B. Cloud Nat is the right service to use when you want to connect to reach services on internet without exposing the
vm with an external IP
upvoted 
2 
times
RKS_2021
RKS_2021
 
9 months, 1 week ago
I will Select C. As there will many Instances will require internet access to update the OS.
upvoted 
1 
times
RKS_2021
RKS_2021
 
9 months, 1 week ago
Changed answer to B, Cloud NAT
upvoted 
1 
times
ductrinh
ductrinh
 
9 months, 1 week ago
Selected Answer: 
B
nat is what you need for non-external vm can reach the internet
B is the only 1
upvoted 
1 
times
dsyouness
dsyouness
 
9 months, 1 week ago
Selected Answer: 
B
Cloud NAT allows the resources in a private subnet to access the internet—for updates, patching, config management, and
more—in a controlled and efficient manner.
upvoted 
2 
times
Murtuza
Murtuza
 
9 months, 2 weeks ago
Correct answer is B you will need NAT to access repositories hosted on the public internet
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #194
Your company is planning to migrate their Windows Server 2022 from their on-premises data center to Google Cloud. You need
to bring the licenses that are currently in use in on-premises virtual machines into the target cloud environment. What should
you do?
A. 
1. Create an image of the on-premises virtual machines and upload into Cloud Storage. 
2. Import the image as a virtual disk on Compute Engine.
B. 
1. Create standard instances on Compute Engine. 
2. Select as the OS the same Microsoft Windows version that is currently in use in the on-premises environment.
C. 
1. Create an image of the on-premises virtual machine. 
2. Import the image as a virtual disk on Compute Engine. 
3. Create a standard instance on Compute Engine, selecting as the OS the same Microsoft Windows version that is
currently in use in the on-premises environment. 
4. Attach a data disk that includes data that matches the created image.
D. 
1. Create an image of the on-premises virtual machines. 
2. Import the image as a virtual disk on Compute Engine using --os=windows-2022-dc-v 
. 
3. Create a sole-tenancy instance on Compute Engine that uses the imported disk as a boot disk. 
Most Voted
Correct Answer:
 
D 
Comments
e5019c6
e5019c6
 
Highly Voted
 
1 year ago
Selected Answer: 
D
Well, yes, you actually need a sole-tenant instance to install a windows server with your licence.
A lot of links were pasted here, but after reading a lot of them, I reached this one who explicitly states:
'To create a VM instance that uses the custom BYOL image, you must provision the VM instance on a sole-tenant node.'
https://cloud.google.com/compute/docs/images/creating-custom-windows-byol-images#use_the_custom_image
upvoted 
19 
times
Ekramy_Elnaggar
Ekramy_Elnaggar
 
Most Recent
 
1 month, 1 week ago
Selected Answer: 
D
Community vote distribution
D (80%)
A (15%)
C (5%)To create a VM instance that uses the custom BYOL image, you must provision the VM instance on a sole-tenant node.
upvoted 
2 
times
dfizban
dfizban
 
2 months, 3 weeks ago
Selected Answer: 
A
No need to have a sole-tenancy instance
upvoted 
1 
times
madcloud32
madcloud32
 
10 months ago
Selected Answer: 
D
D is answer due to BYOL conditions of MS Mobility Licenses
upvoted 
1 
times
AGHPE
AGHPE
 
10 months, 1 week ago
Selected Answer: 
A
A. is a valid and used strategy to migrate Windows Server virtual machines from an on-premises environment to Google Cloud
Platform (GCP), especially when you want to carry over existing licenses using the Bring Your Own License (BYOL) approach.
upvoted 
1 
times
Amrita2012
Amrita2012
 
10 months, 4 weeks ago
Selected Answer: 
D
To create a VM instance that uses the custom BYOL image, you must provision the VM instance on a sole-tenant node.
upvoted 
1 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Selected Answer: 
C
It shouod be c
upvoted 
1 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
D
Sole tenant purpose is to facilitate importing licenses BYOL
Sole-tenant nodes can help you meet dedicated hardware requirements for bring your own license (BYOL) scenarios that
require per-core or per-processor licenses. When you use sole-tenant nodes, you have some visibility into the underlying
hardware, which lets you track core and processor usage. To track this usage, Compute Engine reports the ID of the physical
server on which a VM is scheduled. Then, by using Cloud Logging, you can view the historical server usage of a VM. To optimize
the use of the host hardware, you can overcommit sole-tenant VM CPUs, share sole-tenant node groups and manually live
migrate VMs.
upvoted 
4 
times
squishy_fishy
squishy_fishy
 
1 year, 1 month ago
Answer is A: 
Steps:
1 - Export an existing VM in .OVA format
2 - Install and Authorize the gCloud SDK
3 - Copy the .OVA file to a Google Storage Bucket
4 - Import the .OVA file to Google Cloud from the bucket. 
https://www.youtube.com/watch?v=NG38am3Y8hM 
gcloud compute instances import gcpinstanename --os=windows-10-x64-byol
upvoted 
1 
times
Jconnor
Jconnor
 
1 year, 1 month ago
Eligible Products: License Mobility typically applies to Microsoft software such as SQL Server and other Microsoft applications,
but it's important to note that Windows Server licenses are generally not eligible for License Mobility. So, D.
upvoted 
1 
times
techtitan
techtitan
 
1 year, 1 month ago
Selected Answer: 
ALicensing scenarios such as licenses related to Linux BYOS with RHEL or SLES, as well as Microsoft applications don't require
sole-tenant nodes. If you are considering bringing licenses from Microsoft applications such as SharePoint Server and SQL
Server, use Microsoft License Mobility.
https://cloud.google.com/compute/docs/nodes/bringing-your-own-
licenses#importing_and_creating_an_image_from_an_offline_virtual_disk
upvoted 
2 
times
issaprank
issaprank
 
1 year, 1 month ago
Selected Answer: 
C
either c or d but c seems right too
upvoted 
1 
times
Ahmed_Safwat
Ahmed_Safwat
 
1 year, 1 month ago
Selected Answer: 
A
No need to have a sole-tenancy instance
upvoted 
2 
times
Prakzz
Prakzz
 
1 year, 2 months ago
Selected Answer: 
D
https://cloud.google.com/compute/docs/import/importing-virtual-disks
upvoted 
1 
times
ductrinh
ductrinh
 
1 year, 3 months ago
Selected Answer: 
D
D for sure with BOYL
upvoted 
2 
times
Murtuza
Murtuza
 
1 year, 3 months ago
If the customer truly wants to BYOL, then sole-tenant nodes are required which is a requirement for this question 
Yes it is required for BYOL because that's a requirement from Microsoft themselves
https://www.microsoft.com/en-us/licensing/news/updated-licensing-rights-for-dedicated-cloud
upvoted 
1 
times
Murtuza
Murtuza
 
1 year, 3 months ago
Correct answer is D
Sole-tenant nodes can help you meet dedicated hardware requirements for bring your own license (BYOL) scenarios that
require per-core or per-processor licenses. When you use sole-tenant nodes, you have some visibility into the underlying
hardware, which lets you track core and processor usage.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #195
You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must
communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is
200 kbps. The business requires: 
• 99.99% system availability 
• cost optimization 
You need to design the connectivity between the locations to meet the business requirements. What should you provision?
A. 
An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway. 
Most Voted
B. 
A Classic Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.
C. 
Two HA Cloud VPN gateways connected to two on-premises VPN gateways. Configure each HA Cloud VPN gateway to
have two tunnels, each connected to different on-premises VPN gateways.
D. 
A Classic Cloud VPN gateway connected with one tunnel to an on-premises VPN gateway.
Correct Answer:
 
A 
Comments
e5019c6
e5019c6
 
Highly Voted
 
6 months, 1 week ago
Yep, this question is a duplicate of #191.
upvoted 
5 
times
sheucm89
sheucm89
 
Most Recent
 
9 months, 2 weeks ago
Selected Answer: 
A
Duplicated question in somewhere. Answer is A.
upvoted 
4 
times
TopTalk
TopTalk
 
9 months, 2 weeks ago
Selected Answer: 
A
HA VPN supports two tunnels to achieve 99.99%. Classic VPN does not. Any more than 2 tunnels is excessive cost.
Community vote distribution
A (100%)HA VPN supports two tunnels to achieve 99.99%. Classic VPN does not. Any more than 2 tunnels is excessive cost.
upvoted 
4 
times
Murtuza
Murtuza
 
9 months, 2 weeks ago
Correct answer is A
upvoted 
1 
times
thisisbob
thisisbob
 
9 months, 2 weeks ago
A, I think the question is duplicated
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #196
Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it
takes to complete this activity and the overall cost. The bandwidth between the on-premises environment and Google Cloud is
1 Gbps. You want to follow Google-recommended practices. What should you do?
A. 
Develop a Dataflow job to read data directly from the database and write it into Cloud Storage.
B. 
Use the Data Transfer appliance to perform an offline migration. 
Most Voted
C. 
Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud
Storage.
D. 
Upload the data with gcloud storage cp.
Correct Answer:
 
B 
Comments
cchiaramelli
cchiaramelli
 
Highly Voted
 
1 year, 2 months ago
Selected Answer: 
D
This Transfer Appliance docs says it is suitable when "It would take more than one week to upload your data over the network"
Since 10TB would take way less than a week for that bandwidth, I would go for D
https://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability
upvoted 
11 
times
exam4c3
exam4c3
 
1 month, 4 weeks ago
your link says: Transfer Appliance is a good fit for your data transfer needs if It would take more than one week to upload your
data over the network, but this workload takes 1 day to complete by CLi
upvoted 
1 
times
mgm7
mgm7
 
Highly Voted
 
1 year ago
Selected Answer: 
B
maximum object size in GCS is 5TB
upvoted 
6 
times
mstaicu
mstaicu
 
6 months ago
Community vote distribution
B (51%)
D (49%)mstaicu
mstaicu
 
6 months ago
So how would it work with B ? The data needs to still end up in GCS. Also, who says the export is 1 large file ?
upvoted 
2 
times
klefevre08
klefevre08
 
11 months, 3 weeks ago
so no answer is possible according to you ?...
upvoted 
2 
times
spuyol
spuyol
 
10 months, 4 weeks ago
you could solve that using split command
upvoted 
1 
times
rrope
rrope
 
Most Recent
 
1 week, 1 day ago
Selected Answer: 
B
The best option is B. Use the Data Transfer appliance to perform an offline migration.
upvoted 
1 
times
VishalMoon
VishalMoon
 
3 weeks, 1 day ago
Selected Answer: 
B
I think the key factor here is "Google Recommended Practices". Based on this alone we have to select B. If this was not there
and given the 1 GBPS speed, D would have been feasible.
upvoted 
1 
times
Zonci
Zonci
 
6 months, 3 weeks ago
Selected Answer: 
B
B. Use the Data Transfer Appliance to perform an offline migration.
using the Data Transfer Appliance aligns with Google's recommended practice for large-scale migrations where bandwidth
limitations are a concern, ensuring efficient, secure, and cost-effective transfer of your on-premises database export into
Google Cloud Storage.
upvoted 
3 
times
desertlotus1211
desertlotus1211
 
1 month, 1 week ago
What about cost? And time to get to GCP Region/Zone? And chain of custody of the data as well as upload and available?
What is the of cost of that? How ling will it take?
upvoted 
1 
times
Wasamela
Wasamela
 
7 months, 4 weeks ago
The current maximum object size supported by GCS is 5 TB, so it should be B
upvoted 
1 
times
smithloo
smithloo
 
8 months, 3 weeks ago
thanks for sharing it <a href="https://www.qualitybacklink.net">Link building SEO</a>
upvoted 
1 
times
a53fd2c
a53fd2c
 
8 months, 4 weeks ago
Option B. 
Cloud Storage object limit is 5 TB. 
https://cloud.google.com/storage/quotas?hl=en#objects
https://cloud.google.com/storage/quotas?hl=en#objects
upvoted 
2 
times
madcloud32
madcloud32
 
10 months, 1 week ago
Selected Answer: 
B
Answer is B. Max cp limit of file is 5 TB
upvoted 
4 
times
JohnDohertyDoe
JohnDohertyDoe
 
11 months, 3 weeks ago
Selected Answer: 
DSelected Answer: 
D
Since we would want to do it in the shortest time possible, using gsutil cp would take only 30 hours to move 10TB. So answer is
D.
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets?
hl=en#online_versus_offline_transfer
upvoted 
4 
times
Prudvi3266
Prudvi3266
 
1 year ago
Selected Answer: 
D
As we have 1Ggigabit network we can transfer this with CLI command quicker than Transfer appliance. which takes time like
more than week. Incase of lesser band width may be we use transfer appliance
upvoted 
3 
times
anjanc
anjanc
 
1 year ago
Selected Answer: 
B
ill go for b
upvoted 
1 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
B
Because there is 10 TB of data, it's B
upvoted 
3 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
D
Appliance is overkilling for 30 hours cli command, the appliance could take more than a week with shipping, how many times
you run jobs which take hours to completed? Many times I would go with D
upvoted 
1 
times
Nora9
Nora9
 
1 year, 1 month ago
Selected Answer: 
B
option B (Use the Data Transfer appliance to perform an offline migration) seems to be the most appropriate. It addresses the
need for a speedy transfer of a large amount of data and is a cost-effective solution recommended by Google for large-scale
data migrations. This option circumvents potential network bandwidth limitations and provides a reliable way to transfer large
datasets.
Why option D is not a good choice : 
D. Upload the data with gcloud storage cp: This method uses the gcloud command-line tool to copy files to Cloud Storage.
While simple, it might not be the most efficient for a 10-TB migration given the 1 Gbps bandwidth. The process could be slow
and may require additional handling for potential interruptions and resuming uploads.
upvoted 
5 
times
MikeH20
MikeH20
 
1 year ago
Respectfully, I feel option D is the better choice. 
According to https://cloud.google.com/transfer-
appliance/docs/4.0/overview#suitability, Google recommends using a transfer appliance if the data transfer would take
longer than 1 week. 
At 1 Gbps and 10TB of data, the transfer would take 30 hours
(https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-
datasets#online_versus_offline_transfer). 
We also need to take into account shipping time for the appliance itself, to and from.
That would take a couple weeks.
The question mentions to minimize time and cost, and follow Google best practices. 
In this case, D checks those boxes. The
question does not mention a customer concern about network interruptions. If they did, then B could be argued as the more
appropriate answer.
upvoted 
2 
times
Murtuza
Murtuza
 
1 year, 3 months ago
Duplicate so review question #146 and the choice over there is offline transfer appliance.This is a tricky question
upvoted 
3 
times
Gino17m
Gino17m
 
8 months, 1 week agoGino17m
Gino17m
 
8 months, 1 week ago
No exactly duplicate. In #146 
choice betwee Data Transfer Appliance and gsutil -m. here between DTA and gcloud storage
cp....but stilll a tricky question
upvoted 
1 
times
LifeWins
LifeWins
 
1 year, 1 month ago
Answer has to be D.
upvoted 
1 
times
RJAY123
RJAY123
 
1 year, 3 months ago
Selected Answer: 
D
D is the answer
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #197
You are working at a financial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these
approval documents must be uploaded as a separate approval file. You need to ensure that these documents cannot be
deleted or overwritten for the next 5 years. What should you do?
A. 
Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy. 
Most Voted
B. 
Create a retention policy organizational constraint constraints/storage.retentionPolicySeconds at the organization level.
Set the duration to 5 years.
C. 
Use a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.
D. 
Create a retention policy organizational constraint constraints/storage.retentionPolicySeconds at the project level. Set
the duration to 5 years.
Correct Answer:
 
A 
Comments
japij10711
japij10711
 
1 month, 3 weeks ago
Selected Answer: 
A
Yes, its A - same question as 125 :)
upvoted 
2 
times
Gino17m
Gino17m
 
2 months ago
Selected Answer: 
A
A is correct.
- retention policy must be locked
- the is now need for retention policy for all buckets in organization or all buckets in the project
upvoted 
2 
times
666Amitava666
666Amitava666
 
2 months, 1 week ago
Selected Answer: 
A
Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy
upvoted 
2 
times
Community vote distribution
A (100%)MoeHaydar
MoeHaydar
 
2 months, 3 weeks ago
Selected Answer: 
A
A. Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy
upvoted 
3 
times
201b6fa
201b6fa
 
2 months, 3 weeks ago
Selected Answer: 
A
I think a ist correct
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #198
Your company has decided to make a major revision of their API in order to create better experiences for their developers.
They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the
new API. They want to keep the same SSL and DNS records in place to serve both APIs. 
What should they do?
A. 
Configure a new load balancer for the new version of the API
B. 
Reconfigure old clients to use a new endpoint for the new API
C. 
Have the old API forward traffic to the new API based on the path
D. 
Use separate backend pools for each API path behind the load balancer 
Most Voted
Correct Answer:
 
D 
Comments
f9bc58e
f9bc58e
 
3 months, 3 weeks ago
Selected Answer: 
D
D is the correct answer as two versions (Old and New) have to be maintained with a single end point exposed to the
developers.
upvoted 
2 
times
JamesKarianis
JamesKarianis
 
4 months, 1 week ago
Selected Answer: 
D
This solution allows you to meet all the requirements in a clean and maintainable way
upvoted 
3 
times
Community vote distribution
D (100%) 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 1
Question #199
You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed
the Cloud Monitoring agent and configured the autoscaling policy as follows: 
 
You observe that the application does not scale under high load. You want to resolve this. What should you do?
A. 
Change the Target type to DELTA_PER_MINUTE.
B. 
Change the Metric identifier to agent.googleapis.com/memory/bytes_used.
C. 
Change the filter to metric.label.state = ‘used’. 
Most Voted
D. 
Change the filter to metric.label.state = ‘free’ and the Target utilization to 20.
Correct Answer:
 
C 
Comments
Nick89GR
Nick89GR
 
Highly Voted
 
2 months, 1 week ago
I am confused since this question comes in contrast with the answer in question 190. Anyone knows what is the real answer? I
would expected used+buffered+cached+slab Which means the sum of all these.
upvoted 
8 
times
JamesKarianis
JamesKarianis
 
Highly Voted
 
4 months, 1 week ago
C. Change the filter to metric.label.state = ‘used’. The current filter is set up with multiple AND conditions, which means it's
looking for a metric that simultaneously has all these states: 'used', 'buffered', 'cached', and 'slab'. This is logically impossible, as
a memory location can't be in multiple states at once. Therefore, the filter will never match any metrics, and the autoscaling
Community vote distribution
C (100%)a memory location can't be in multiple states at once. Therefore, the filter will never match any metrics, and the autoscaling
policy won't trigger.
upvoted 
7 
times
JaquiMB
JaquiMB
 
Most Recent
 
6 days, 3 hours ago
Selected Answer: 
C
Autoscale based on memory usage
To configure autoscaling based on the percent of used memory, specify the percent_used metric provided by the memory Ops
Agent metrics. You should filter the metric by state to use only the used memory state. If you do not specify the filter, then the
autoscaler takes the sum of memory usage by all memory states labeled as buffered, cached, free, slab, and used.
upvoted 
1 
times
tangac
tangac
 
4 months, 1 week ago
Selected Answer: 
C
as it is clearly indicated in the public documentation https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-
monitoring-metrics#gcloud_5
you have change the filter to metric.label.state="used"
upvoted 
6 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 2
Question #1
The JencoMart security team requires that all Google Cloud Platform infrastructure is deployed using a least privilege model
with separation of duties for administration between production and development resources. 
What Google domain and project structure should you recommend? 
A. 
Create two G Suite accounts to manage users: one for development/test/staging and one for production. Each account
should contain one project for every application
B. 
Create two G Suite accounts to manage users: one with a single project for all development applications and one with a
single project for all production applications
C. 
Create a single G Suite account to manage users with each stage of each application in its own project 
Most Voted
D. 
Create a single G Suite account to manage users with one project for the development/test/staging environment and
one project for the production environment
Correct Answer:
 
C 
Comments
Anjoy
Anjoy
 
Highly Voted
 
3 years, 8 months ago
Here are the correct answers: 
https://cloud.google.com/resource-manager/docs/creating-managing-folders
Refer to the diagram on top, different envs are created at the project level.
https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
"A general recommendation is to have one project per application per environment. For example, if you have two applications,
"app1" and "app2", each with a development and production environment, you would have four projects: app1-dev, app1-
prod, app2-dev, app2-prod. This isolates the environments from each other, so changes to the development project do not
accidentally impact production, and gives you better access control, since you can (for example) grant all developers access to
development projects but restrict production access to your CI/CD pipeline."
The answer is C.
upvoted 
57 
times
JohnWick2020
JohnWick2020
 
3 years, 2 months ago
Agreed, definitely C!
Community vote distribution
C (91%)
D (9%)Agreed, definitely C!
upvoted 
2 
times
pepYash
pepYash
 
3 years, 7 months ago
the most logical link and explanation. Thank you!. It is 'C' indeed
upvoted 
2 
times
francisco_guerra
francisco_guerra
 
3 years, 8 months ago
Yes its right, its C for me.
upvoted 
3 
times
jaxclain
jaxclain
 
Highly Voted
 
1 year, 7 months ago
Selected Answer: 
C
Ok, just to help all of use lol vote so everyone who sees this comment, will not reply or comment on this questions, why?
because the case study is very old so 100% guaranteed this question will not appear in the exam, any question from JencoMart
and Dress4Win, maybe they can place similar question without involving those 2 case studies but as for now, nobody has
reported seeing those questions again so stop replying or posting on this thread so it will no longer show at the top of the
forum lol please..
upvoted 
17 
times
dija123
dija123
 
Most Recent
 
2 months, 3 weeks ago
Selected Answer: 
C
Case has been deprecated:
https://cloud.google.com/learn/certification/guides/professional-cloud-architect
upvoted 
5 
times
sheucm89
sheucm89
 
9 months, 2 weeks ago
I voted C
upvoted 
1 
times
TheCloudGuruu
TheCloudGuruu
 
1 year, 1 month ago
Selected Answer: 
C
C is best practice
upvoted 
1 
times
gonlafer
gonlafer
 
1 year, 7 months ago
Selected Answer: 
C
https://cloud.google.com/kms/docs/separation-of-duties
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
C
C is best practice as recommended by google
https://cloud.google.com/resource-manager/docs/creating-managing-folders
upvoted 
1 
times
6721sora
6721sora
 
1 year, 9 months ago
Selected Answer: 
C
For segregation of applications and environments, C is the best reference architecture model
upvoted 
1 
times
riyer1
riyer1
 
1 year, 10 months ago
The case study should be removed as it is deprecated
upvoted 
6 
times
JohnnyBG
JohnnyBG
 
1 year, 11 months ago
This case study is deprecated, you (admin) should remove this content.
upvoted 
12 
times
 
2 years agoH_S
H_S
 
2 years ago
case study deprecated
upvoted 
6 
times
1289dev
1289dev
 
2 years, 1 month ago
Selected Answer: 
D
D is Right Answer
upvoted 
1 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
C
multi account is 
a overkill.One project per environment will ensure the segregation in enviorment henc C.
upvoted 
2 
times
Rajj98
Rajj98
 
2 years, 3 months ago
Selected Answer: 
C
Its C - Google's best practice
upvoted 
4 
times
sjmsummer
sjmsummer
 
2 years, 5 months ago
Selected Answer: 
D
D is mostly seen in real world. I will choose it.
upvoted 
3 
times
sjmsummer
sjmsummer
 
2 years, 5 months ago
D. dev/test normally is in one function.
upvoted 
1 
times
Sekierer
Sekierer
 
2 years, 5 months ago
Selected Answer: 
C
Vote C
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 2
Question #2
A few days after JencoMart migrates the user credentials database to Google Cloud Platform and shuts down the old server,
the new database server stops responding to SSH connections. It is still serving database requests to the application servers
correctly. 
What three steps should you take to diagnose the problem? (Choose three.) 
A. 
Delete the virtual machine (VM) and disks and create a new one
B. 
Delete the instance, attach the disk to a new VM, and investigate
C. 
Take a snapshot of the disk and connect to a new machine to investigate 
Most Voted
D. 
Check inbound firewall rules for the network the machine is connected to 
Most Voted
E. 
Connect the machine to another network with very simple firewall rules and investigate
F. 
Print the Serial Console output for the instance for troubleshooting, activate the interactive console, and investigate
Most Voted
Correct Answer:
 
CDF 
Comments
Paul_DSouza
Paul_DSouza
 
Highly Voted
 
2 years, 6 months ago
Assumption:- VM in production environment, cannot be taken down.
based on above, A/B are out. E causes downtime to remove the VM from the old network and connect it to a new network for
testing.
Only leaves CDF
C - Snapshotting is fine, it will reduce the performance for a short duration, but the database will still be up
D - Obvious place to check for firewall rules (if ssh ports are open or not)
F - Easy to see server messages on console (without downtime)
upvoted 
39 
times
H_S
H_S
 
Highly Voted
 
6 months, 3 weeks ago
CASE STUDY DEPRECATED PLEASE REMOVE
Community vote distribution
CDF (100%)https://cloud.google.com/certification/guides/professional-cloud-architect
Review the case studies that may be used in the exam.
EHR Healthcare
Helicopter Racing League
Mountkirk Games
TerramEarth
upvoted 
20 
times
Mahmoud_E
Mahmoud_E
 
Most Recent
 
2 months, 2 weeks ago
Selected Answer: 
CDF
CDF best answers
upvoted 
1 
times
6721sora
6721sora
 
3 months, 3 weeks ago
Selected Answer: 
CDF
D and F are ok and easy to conclude on.
Between E and C - C may cause disruptions to a running production VM.
E allows us to snapshot to a different VM and you can check the sshd configuration there. Also connect the snapshotted VM to
the same networks to troubleshoot in real time
upvoted 
1 
times
extopic01
extopic01
 
7 months, 2 weeks ago
CDF is correct.
Dcoumentation with regards to option C : https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-
ssh#inspect_vm
upvoted 
1 
times
exam_war
exam_war
 
1 year, 1 month ago
go with DEF. C doesn't make sense, taking a snapshot for OS and connect to new machine, it won't tell you what's going on with
ssh session.
upvoted 
4 
times
joe2211
joe2211
 
1 year, 1 month ago
Selected Answer: 
CDF
vote CDF
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
1 year, 6 months ago
Answer is CDF
upvoted 
2 
times
victory108
victory108
 
1 year, 7 months ago
C. Take a snapshot of the disk and connect to a new machine to investigate
D. Check inbound firewall rules for the network the machine is connected to
F. Print the Serial Console output for the instance for troubleshooting, activate the interactive console, and investigate
upvoted 
1 
times
Ausias18
Ausias18
 
1 year, 9 months ago
Answer are C,D, F
upvoted 
1 
times
Rathul
Rathul
 
1 year, 9 months ago
I will vote for CEF
upvoted 
2 
times
bnlcnd
bnlcnd
 
1 year, 11 months ago
You need to select 1 from A/B/C before you can have something to troubleshoot. You cannot troubleshoot on a running PORD
server.server.
C is best.
D/E/F get 2, rule out E.
upvoted 
3 
times
sealvarezmx
sealvarezmx
 
1 year, 12 months ago
C. SSH is related to the VM, not to the database.
Why would you create a snapshot of the disk and attach it to a NEW VMs? by creating a new VM you could fix the SSH problem
without even attaching the disk.
D. Doesnt make sense to print the output, but it does make sense to log into the console and troubleshoot SSH.
I'll go with DEF.
upvoted 
3 
times
_CloudTech_
_CloudTech_
 
2 years, 1 month ago
CDF is ok
upvoted 
3 
times
shaun_ko
shaun_ko
 
2 years, 2 months ago
I think DEF.. need to Snapshot? why?
upvoted 
3 
times
shaun_ko
shaun_ko
 
2 years, 2 months ago
sorry. I misunderstood. go CDF
upvoted 
3 
times
AshokC
AshokC
 
2 years, 3 months ago
Agree with CDF
upvoted 
3 
times
rehma017
rehma017
 
2 years, 6 months ago
I like DEF - the problem with C is you are snapshotting an instance which is currently being queried by the application, you are
definitely going to cause an IOPS spike..
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 2
Question #3
JencoMart has decided to migrate user profile storage to Google Cloud Datastore and the application servers to Google
Compute Engine (GCE). During the migration, the existing infrastructure will need access to Datastore to upload the data. 
What service account key-management strategy should you recommend? 
A. 
Provision service account keys for the on-premises infrastructure and for the GCE virtual machines (VMs)
B. 
Authenticate the on-premises infrastructure with a user account and provision service account keys for the VMs
C. 
Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys
for the VMs 
Most Voted
D. 
Deploy a custom authentication service on GCE/Google Kubernetes Engine (GKE) for the on-premises infrastructure and
use GCP managed keys for the VMs
Correct Answer:
 
C 
Comments
Zarmi
Zarmi
 
Highly Voted
 
3 years, 8 months ago
Answer: C.
https://cloud.google.com/iam/docs/understanding-service-accounts#migrating_data_to_google_cloud_platform
There are two types of service account keys:
GCP-managed keys. These keys are used by Cloud Platform services such as App Engine and Compute Engine. They cannot be
downloaded, and are automatically rotated and used for signing for a maximum of two weeks. The rotation process is
probabilistic; usage of the new key will gradually ramp up and down over the key's lifetime. We recommend caching the public
key set for a service account for at most 24 hours to ensure that you always have access to the current key set.
User-managed keys. These keys are created, downloadable, and managed by users. They expire 10 years from creation, and
cease authenticating successfully when they are deleted from the service account.
upvoted 
28 
times
Carsonza
Carsonza
 
3 years, 4 months ago
while that heading doesn't exist anymore the graphic that it is that doc speaks for itself.
upvoted 
1 
times
Community vote distribution
C (100%)shashu07
shashu07
 
Highly Voted
 
3 years, 6 months ago
Correct Answer : C
Where will the code that assumes the identity of the service account be running: on Google Cloud Platform or on-premises?
https://cloud.google.com/iam/docs/understanding-service-accounts
upvoted 
8 
times
pakilodi
pakilodi
 
Most Recent
 
1 month ago
Selected Answer: 
C
Answer: C
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
1 year, 2 months ago
Selected Answer: 
C
C is the right answer https://cloud.google.com/iam/docs/understanding-service-
accounts#migrating_data_to_google_cloud_platfor
upvoted 
2 
times
joe2211
joe2211
 
2 years, 1 month ago
Selected Answer: 
C
vote C
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
2 years, 6 months ago
Answer is C
upvoted 
1 
times
Yogikant
Yogikant
 
2 years, 7 months ago
Answer C.
Refer to first figure in https://cloud.google.com/iam/docs/understanding-service-
accounts#migrating_data_to_google_cloud_platfor. It mentions using User Managed Keys for on-premises usage of services
accounts and GCP managed keys for code running in GCP.
Also in case study it emphasise using "managed services as much as possible". So this rules out A.
upvoted 
4 
times
victory108
victory108
 
2 years, 7 months ago
C. Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys for the
VMs
upvoted 
1 
times
Koushick
Koushick
 
2 years, 8 months ago
Answer C as per https://cloud.google.com/iam/docs/understanding-service-
accounts#migrating_data_to_google_cloud_platform
upvoted 
1 
times
Ausias18
Ausias18
 
2 years, 9 months ago
Answer is C
upvoted 
1 
times
pawel_ski
pawel_ski
 
2 years, 9 months ago
When you provision service account keys for the on-premises infrastructure you must then manage them. So if the kyes are
managed by GCP you must set up a process to rotate keys in the on-prem infrastructure. It is quite chalenging. 
Therefore I prefer option B.
upvoted 
1 
times
AGG
AGG
 
2 years, 10 months ago
I will go with A which is very similar to C but answer C suggest use Google Cloud Platform (GCP) managed keys for the VMs
(there is no word : "ONL" for the VMs) but it's suggestion (this is how I perceive it)(there is no word : "ONL" for the VMs) but it's suggestion (this is how I perceive it)
Answer A is copy/paste from link : https://cloud.google.com/iam/docs/understanding-service-
accounts#migrating_data_to_google_cloud_platform
A service account is a special type of Google account intended to represent a non-human user that needs to authenticate and
be authorized to access data in Google APIs.
Typically, service accounts are used in scenarios such as:
- Running workloads on virtual machines (VMs).(first part of answer A = and for the GCE virtual machines (VMs))
- Running workloads on on-premises workstations or data centers that call Google APIs. 
(Second part of answer A = Provision
service account keys for the on-premises infrastructure)
upvoted 
4 
times
ahmedemad3
ahmedemad3
 
2 years, 10 months ago
Ans: C 
make sense of the service account for infrastructure and managed key for VM
upvoted 
1 
times
bnlcnd
bnlcnd
 
2 years, 11 months ago
A /B / C are all playing with words. But the key points is who need service account key. no matter where the key is managed.
GCP managed or customer managed.
Only the on-prom resource need the service account key. so, only C is right.
upvoted 
1 
times
ybe_gcp_cert
ybe_gcp_cert
 
3 years ago
In C, the vm part is wrong. 
A VM doesn't use key directly from a conf point of view. It uses a service account that is linked with a
key pair. the key could be managed by google or user managed.
https://cloud.google.com/iam/docs/service-accounts#service_account_keys
C is only playing with words...
I would go with A.
upvoted 
2 
times
_CloudTech_
_CloudTech_
 
3 years, 1 month ago
C is ok
upvoted 
1 
times
JCGO
JCGO
 
3 years, 1 month ago
Accessing something from on-premise to google cloud done by using service accounts this days. Datastore for example:
https://cloud.google.com/datastore/docs/activate Service account keys can be managed by google, or can be self-generated
and public key uploaded. 
Question asks about provisioning service account keys during migration phase, when on-prem stuff needs access to datastore. C
looks good. A looks good also, but a involves provisioning service account keys for cloud VM's -> it is done another way. you
could give permissions to defsault compute service account per API, or create service account and give it appropriate
premissions and choose while creating cloud VM. I can not see any point bothering with service accout keys for cloud VM's
here. So i choose C.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 2
Question #4
JencoMart has built a version of their application on Google Cloud Platform that serves traffic to Asia. You want to measure
success against their business and technical goals. 
Which metrics should you track? 
A. 
Error rates for requests from Asia
B. 
Latency difference between US and Asia
C. 
Total visits, error rates, and latency from Asia 
Most Voted
D. 
Total visits and average latency for users from Asia
E. 
The number of character sets present in the database
Correct Answer:
 
C 
Comments
VASI
VASI
 
Highly Voted
 
4 years, 4 months ago
Business Requirements include: Guarantee service availability and support. I would choose C
upvoted 
38 
times
nitinz
nitinz
 
3 years, 4 months ago
D it meets both requirements. Error was never asked for in KPI.
upvoted 
9 
times
Smart
Smart
 
4 years, 4 months ago
I wonder how specific we have to be and how much common sense/best practices should we ignore.
upvoted 
11 
times
JohnWick2020
JohnWick2020
 
Highly Voted
 
3 years, 2 months ago
Answer is C; more complete imo. Those aligning to D should note that average latency is not the only metric available to
measure and is too specific.
"Total visits" covers the business requirements:
- Optimize for capacity during peak periods and value during off-peak periods.
Community vote distribution
C (67%)
D (33%)- Optimize for capacity during peak periods and value during off-peak periods.
- Expand services into Asia.
"Error rates" covers business requirement:
- Guarantee service availability and support. ** if service is unavailable, errors are reported!
"Latency" covers technical requirement:
- Decrease latency in Asia.
upvoted 
25 
times
ammonia_free
ammonia_free
 
Most Recent
 
5 months, 1 week ago
Selected Answer: 
C
I understand the Case Study is already deprecated by here is my 5 cents:
I chose C. Total visits, error rates, and latency from Asia.
Here's why this option is the most appropriate:
Total Visits: Monitoring total visits helps in assessing the application's capacity to handle peak and off-peak periods, which
aligns with the goal of optimizing capacity. It also provides insight into the success of their expansion efforts in Asia.
Error Rates: Tracking error rates is crucial for ensuring service availability and support. It directly relates to the goal of
guaranteeing availability by identifying and resolving issues that could lead to service disruptions.
Latency from Asia: Monitoring latency specifically for users in Asia addresses the technical goal of decreasing latency in this
region. It's a direct measure of the performance improvements and user experience enhancements expected from the cloud
migration.
upvoted 
3 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
D
D is the best answer
upvoted 
1 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months ago
Selected Answer: 
C
C.
It says Guarantee service availability, we need to check for error rates to make sure our application is working perfectly fine.
upvoted 
1 
times
H_S
H_S
 
2 years ago
CASE STUDY DEPRECATED PLEASE REMOVE
Review the case studies that may be used in the exam.
EHR Healthcare
Helicopter Racing League
Mountkirk Games
TerramEarth
upvoted 
12 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
C
error and latency will cover technical and visits buissness hence C
upvoted 
1 
times
nqthien041292
nqthien041292
 
2 years, 7 months ago
Selected Answer: 
C
Vote C
upvoted 
5 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
D
vote D
upvoted 
4 
times
joe2211
joe2211
 
2 years, 6 months ago
Keywords, their business and technical goals are aimed to Asian users onlyKeywords, their business and technical goals are aimed to Asian users only
Business Requirements:
Expand services into Asia
Technical Requirements:
- Decrease latency in Asia
upvoted 
1 
times
AMohanty
AMohanty
 
1 year, 11 months ago
A part of their Goal is : Guarantee service availability and support
Errors doesn't cater well to their Service Availability.
upvoted 
1 
times
kopper2019
kopper2019
 
2 years, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
2 
times
Urban_Life
Urban_Life
 
2 years, 11 months ago
guys trust me - it's C
upvoted 
4 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is D
upvoted 
2 
times
victory108
victory108
 
3 years, 1 month ago
C. Total visits, error rates, and latency from Asia
upvoted 
4 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is D
upvoted 
1 
times
pawel_ski
pawel_ski
 
3 years, 3 months ago
I prefer to see the latency change not only to have an average value. 
I choose C.
upvoted 
2 
times
ebinv2
ebinv2
 
3 years, 4 months ago
Success against business and and technical goals- should be C , as it check error rates also
upvoted 
1 
times
Fadhli
Fadhli
 
3 years, 4 months ago
its D, based on the case, the technical requirement is to reduce latency
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 2
Question #5
 
The migration of JencoMart's application to Google Cloud Platform (GCP) is progressing too slowly. The infrastructure is shown
in the diagram. You want to maximize throughput. 
What are three potential bottlenecks? (Choose three.) 
A. 
A single VPN tunnel, which limits throughput 
Most Voted
B. 
A tier of Google Cloud Storage that is not suited for this task
C. 
A copy command that is not suited to operate over long distances 
Most Voted
D. 
Fewer virtual machines (VMs) in GCP than on-premises machines
E. 
A separate storage layer outside the VMs, which is not suited for this task
F. 
Complicated internet connectivity between the on-premises infrastructure and GCP 
Most Voted
Correct Answer:
 
ACF 
Community vote distribution
ACF (63%)
AC (26%)
ACE (11%)Comments
_CloudTech_
_CloudTech_
 
Highly Voted
 
4 years, 1 month ago
Where are you Tartor?
upvoted 
21 
times
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
Think A,D,E. "Copy command not suited for long distances", what does it mean?
upvoted 
18 
times
kaush
kaush
 
4 years, 6 months ago
you need reliable agent software installed to copy files with retries 
,copy command not sufficient
upvoted 
1 
times
dayody
dayody
 
4 years, 4 months ago
the diagram does not show copy command.
upvoted 
13 
times
6a8c7ad
6a8c7ad
 
Most Recent
 
5 months ago
ABD single vpn, cloud storage vs previous local, and 6 machines in gcp, 9 on prem.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
I don't get why B isn't correct. Cloud Storage isn't a good replacement for SAN, especially when we have a database running on
some of the VMs.
upvoted 
3 
times
taer
taer
 
1 year, 9 months ago
Selected Answer: 
ACF
I think ACF
upvoted 
5 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
ACE
ACE are the best asnwers
upvoted 
2 
times
6721sora
6721sora
 
2 years, 3 months ago
Selected Answer: 
ACF
Single VPN tunnel limits throughput. Copying 20TB across long distances is a big bottleneck. VPN across internet cannot be
relied upon for high performance
upvoted 
2 
times
RGTest
RGTest
 
2 years, 6 months ago
Selected Answer: 
ACF
ACF ...
upvoted 
1 
times
Joanale
Joanale
 
2 years, 10 months ago
Why "A" dudes? even if you put 20 tunnels connection the bandwidth of on premisses remains the same. I'm sure its B, D, F, who
make's migrations know google storage isn't made for migrations, i think D means that make some compression process or
transformation and F is right cause all connectivity goes trough internet. If for some reason the internet bw is shared this can be
congestionated.
upvoted 
1 
times
mad314
mad314
 
2 years, 8 months agohttps://cloud.google.com/network-connectivity/docs/vpn/quotas#limits
Bandwidth per VPN tunnel: Up to 3 Gbps for the sum of ingress and egress
upvoted 
1 
times
sjmsummer
sjmsummer
 
2 years, 11 months ago
Selected Answer: 
ACF
ACF, though there are a lot of room to imagine in answer B to make it a risk. Not sure what the test designer is implying in this
case.
upvoted 
4 
times
OrangeTiger
OrangeTiger
 
2 years, 11 months ago
Looking at the figure is confusing. 
The necessary information cannot be read from this figure.
Therefore, select the factors that deteriorate the throughput from the options.
I think A,C and F are factors that deteriorate the throughput.
Anyone pls tell me why choose E?
upvoted 
2 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
AC
vote ACF
upvoted 
5 
times
sudarchary
sudarchary
 
3 years, 3 months ago
Correct answer is ACE only
upvoted 
3 
times
MikeB19
MikeB19
 
3 years, 4 months ago
There is just not enough data in this q to answer correctly. For instance it does not state they r using a copy command and we
don’t know what the internet connection is. 
I think adf but this q needs more data points
upvoted 
1 
times
victory108
victory108
 
3 years, 7 months ago
A. A single VPN tunnel, which limits throughput
C. A copy command that is not suited to operate over long distances
E. A separate storage layer outside the VMs, which is not suited for this task
upvoted 
3 
times
dlzhang
dlzhang
 
3 years, 6 months ago
"E. A separate storage layer outside the VMs, which is not suited for this task". My question here is that there is no 'storage
layer' in the diagram given in the question.
upvoted 
3 
times
Manh
Manh
 
3 years, 3 months ago
it's Cloud storage mouth to VM instance
upvoted 
1 
times
JohnWick2020
JohnWick2020
 
3 years, 8 months ago
Answer is A,C,F.
Breakdown:
A. A single VPN tunnel, which limits throughput.
Recommended practice is to have two redundant connections, usually Dedicated Interconnect and VPN. VPNs suit low volume
data connections so won't cut it for this company.
C. A copy command that is not suited to operate over long distances.
Advisable to use GCP recommended tools that support multi-part, parallel composite and resumable uploads to Cloud
Storage. 
F. Complicated internet connectivity between the on-premises infrastructure and GCP.F. Complicated internet connectivity between the on-premises infrastructure and GCP.
With some existing VMs being dual-homed, firewalls, VPC and routing configs, things could get pretty complicated.
upvoted 
18 
times
rbarrote
rbarrote
 
3 years, 7 months ago
But for C it mentions specifically the long distance, and the copy command problem would happen independent of distance,
right?
upvoted 
2 
times
Ausias18
Ausias18
 
3 years, 9 months ago
answer are A, C, F
upvoted 
10 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 2
Question #6
JencoMart wants to move their User Profiles database to Google Cloud Platform. 
Which Google Database should they use? 
A. 
Cloud Spanner
B. 
Google BigQuery
C. 
Google Cloud SQL
D. 
Google Cloud Datastore 
Most Voted
Correct Answer:
 
D 
Comments
JJu
JJu
 
Highly Voted
 
4 years, 7 months ago
answer is D. Google Cloud Datastorage
Google Cloud Datastorage use:
* User profile
* game state
* product catalogs
upvoted 
26 
times
Ishu_awsguy
Ishu_awsguy
 
1 year, 9 months ago
They have a relational oracle DB with complex table structure. I dont think Data storw will work for them.
Answer should be A ( Cloud Spanner)
upvoted 
7 
times
Ishu_awsguy
Ishu_awsguy
 
1 year, 9 months ago
But yeah
it is debatable on how much moderization we take.
With Cloudspanner ( require less modernisation compared to Datastore)
With Cloud SQL ( Require less modernisation compared to Datastore)
With datastore - Best solution eventually - but the most tricky migration and modernisation.
very subjective question
upvoted 
5 
times
Community vote distribution
D (56%)
A (31%)
C (12%)vvillar
vvillar
 
3 years, 2 months ago
datastore*
upvoted 
1 
times
dabrat
dabrat
 
Highly Voted
 
4 years, 7 months ago
oracle= Relational
+Gloabal = spanner =>A)
upvoted 
17 
times
VishalB
VishalB
 
2 years, 11 months ago
User Profile not necessary would have relational database, Datastore is the best option for User Profile
upvoted 
1 
times
nitinz
nitinz
 
3 years, 4 months ago
D, profiles go to datastore
upvoted 
1 
times
DrLu
DrLu
 
4 years, 7 months ago
Technical Requirements
• Assess key application for cloud suitability.
• Modify application for the cloud. **(Which means possible to change the code or database when it migrate to GCP
)
upvoted 
5 
times
tartar
tartar
 
3 years, 11 months ago
D is ok
https://cloud.google.com/datastore/docs/concepts/overview
Datastore is ideal for applications that rely on highly available structured data at scale. You can use Datastore to store and
query all of the following types of data:
Product catalogs that provide real-time inventory and product details for a retailer.
User profiles that deliver a customized experience based on the user’s past activities and preferences.
Transactions based on ACID properties, for example, transferring funds from one bank account to another.
upvoted 
11 
times
decw
decw
 
Most Recent
 
6 months, 1 week ago
I think A
https://cloud.google.com/solutions/migrate-oracle-workloads
upvoted 
1 
times
MahAli
MahAli
 
6 months, 3 weeks ago
Selected Answer: 
D
It was already mentioned in another question they are moving to data store :)
upvoted 
2 
times
Jannchie
Jannchie
 
7 months ago
Selected Answer: 
C
C, because this company used PostgreSQL before. No reason to use NoSQL(D), Not for analytic(B), not PB level data so no need
to use Spanner(A).
upvoted 
1 
times
thewalker
thewalker
 
7 months, 1 week ago
This case study is not listed in GCP PCA exam as on date.
upvoted 
2 
times
cchiaramelli
cchiaramelli
 
8 months, 1 week ago
Selected Answer: 
C
Since it is relational data, it should be A or C.Since it is relational data, it should be A or C.
Cloud SQL is the only that supports the Oracle SQL features, like Procedures and triggers, etc. I also think Cloud Spanner seems
to be an overkill since it is a 20TB database, which Cloud SQL easily handles
IMO, C is the cheapest and easiest, while A would require some refactorings and longer migration, which doesnt pay off for the
current database's size
upvoted 
1 
times
dman69
dman69
 
1 year ago
Selected Answer: 
D
Data store is used for user profiles
upvoted 
1 
times
dman69
dman69
 
1 year ago
Selected Answer: 
D
Datastore is used for user profiles
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
A: Spanner retains the RDBMS compatibility and helps to reduce latency for any Asia resources
B: BigQuery no good for OLTP
C: CloudSQL, the closest option we have to Oracle
D: Significant architectural change but potentially a great alternative to RDBMS, document database typically a good fit for
user data & as others have said it can eliminate the complex RDBMS structure and queries
The problem I have with Spanner is the cost, it is extremely expensive compared to the other options, so is it really needed? 
But when I review the requierments I don't see cost as being up there whilst latency to Asia it is, so whilst I intuitively picked
Datastore I'll revise to Spanner.
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
I don't get why B isn't correct. 
Cloud Storage isn't a good replacement for SAN, especially when we have a database running on
some of the VMs.
upvoted 
1 
times
TheCloudGuruu
TheCloudGuruu
 
1 year, 1 month ago
Selected Answer: 
D
Datastore is best for user profiles
upvoted 
1 
times
geekgirl007
geekgirl007
 
1 year, 2 months ago
Selected Answer: 
D
says here https://cloud.google.com/datastore/docs/concepts/overview
upvoted 
1 
times
taer
taer
 
1 year, 3 months ago
Selected Answer: 
A
Cloud Spanner is a fully managed, scalable, and globally distributed relational database service. It provides strong consistency,
high availability, and low-latency capabilities, which would be suitable for JencoMart's User Profiles database requirements.
upvoted 
1 
times
HD2023
HD2023
 
1 year, 3 months ago
Selected Answer: 
D
Option D
upvoted 
1 
times
Deb2293
Deb2293
 
1 year, 3 months agoDeb2293
Deb2293
 
1 year, 3 months ago
Selected Answer: 
A
Oracle (relational) + global => Spanner
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 4 months ago
Selected Answer: 
A
This is pretty straightforward.
Relational + Global = Spanner.
DS not meant for TB size.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 3
Question #1
For this question, refer to the Helicopter Racing League (HRL) case study. Your team is in charge of creating a payment card
data vault for card numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders. You
need to implement a custom card tokenization service that meets the following requirements: 
* 
It must provide low latency at minimal cost. 
* 
It must be able to identify duplicate credit cards and must not store plaintext card numbers. 
* 
It should support annual key rotation. 
Which storage approach should you adopt for your tokenization service? 
A. 
Store the card data in Secret Manager after running a query to identify duplicates.
B. 
Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode. 
Most Voted
C. 
Encrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.
D. 
Use column-level encryption to store the data in Cloud SQL.
Correct Answer:
 
B 
Comments
Neo_ACE
Neo_ACE
 
Highly Voted
 
2 years, 7 months ago
Answer would be B
https://cloud.google.com/community/tutorials/pci-tokenizer
Deterministic output means that a given set of inputs (card number, expiration, and userID) will always generate the same
token. This is useful if you want to rely on the token value to deduplicate your token stores. You can simply match a newly
generated token to your existing catalog of tokens to determine whether the card has been previously stored. Depending on
your application architecture, this can be a very useful feature. However, this could also be accomplished using a salted hash of
the input values.
https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss
Firestore is the next major version of Datastore. Firestore can run in Datastore mode, which uses the same API as Datastore and
scales to millions of writes per second,
upvoted 
39 
times
Community vote distribution
B (97%)
D
(3%)upvoted 
39 
times
technodev
technodev
 
Highly Voted
 
2 years, 5 months ago
Got this question in my exam, answered B
upvoted 
19 
times
OrangeTiger
OrangeTiger
 
Most Recent
 
5 months, 2 weeks ago
A's SecretManager and C's Memorystore are absolutely different because their purposes are different. D is different because it
does not mention duplication. What remains is B.
upvoted 
2 
times
TopTalk
TopTalk
 
9 months, 2 weeks ago
Why isn't it C since Firestore doesn't meet the low latency requirement as someone said before? Bard thinks the answer is C for
low latency and even cost because you're only paying for what you use. Thoughts?
upvoted 
2 
times
sampon279
sampon279
 
1 year ago
Selected Answer: 
B
Between B (firestore in datastore mode)and D (Cloud SQL) B is better solution since firestore is preferred for low latency
queries, also since firestore is in datastore mode (does not include real time capabilities supported in native mode - i.e mobile
updates) it's cost effective.
upvoted 
4 
times
mimicha1
mimicha1
 
1 year ago
Why not C ?
upvoted 
2 
times
bargou
bargou
 
4 months, 3 weeks ago
if we choose C, the card number can be duplicated, since we are using multiple memorystore
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
From what I can work out column level encryption needs to be implemented by the client in Cloud SQL.
So both B & D are identical solutions except for the database type?
Cloud SQL seems to do a better job of the avoiding duplicates requirement & seems a better fit. 
Don't see why B seems to be so popular, would have expect a bigger split on the vote. 
Am I missing something
upvoted 
2 
times
mtj2018
mtj2018
 
11 months, 3 weeks ago
I agree, both answers would fit the bill but I think B just shades it due to low latency requirements.
upvoted 
1 
times
tdotcat
tdotcat
 
1 year, 5 months ago
Selected Answer: 
B
B fits the case
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
B
B Is the Correct Answer
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
timesupvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
B
B as its clear in the example by google https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss
upvoted 
4 
times
aut0pil0t
aut0pil0t
 
1 year, 10 months ago
Selected Answer: 
B
B, but should be reworded as follows for clarify.
"B. Encrypt the card data with a deterministic algorithm and store in Firestore using Datastore mode."
https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-
dss#a_service_for_handling_sensitive_information
upvoted 
4 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
I would go with B.
upvoted 
1 
times
cpi_web
cpi_web
 
2 years, 1 month ago
Hmmm. What is about the very first point low latency? Firefstore is not the one with best latency values...
https://cloud.google.com/architecture/building-scalable-apps-with-cloud-firestore#latency
upvoted 
2 
times
kapara
kapara
 
2 years, 1 month ago
Selected Answer: 
D
ans is D
upvoted 
1 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
B
Had this question on my exam.
upvoted 
5 
times
slars2k
slars2k
 
2 years, 3 months ago
Considering low latency and minimal cost, will go with D.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 3
Question #2
For this question, refer to the Helicopter Racing League (HRL) case study. Recently HRL started a new regional racing league
in Cape Town, South Africa. In an effort to give customers in Cape Town a better user experience, HRL has partnered with the
Content Delivery Network provider, Fastly. HRL needs to allow traffic coming from all of the Fastly IP address ranges into their
Virtual Private Cloud network (VPC network). You are a member of the HRL security team and you need to configure the update
that will allow only the Fastly IP address ranges through the External HTTP(S) load balancer. Which command should you use? 
A. 
 
B. 
 
C. 
 
D. 
 
Correct Answer:
 
A 
Reference: 
https://cloud.google.com/load-balancing/docs/https
Comments
technodev
technodev
 
Highly Voted
 
2 years, 11 months agoGot this question in my exam, answered D
upvoted 
44 
times
elrizos
elrizos
 
Highly Voted
 
2 years, 9 months ago
Is D:
In the GCP doc can see the same example
https://cloud.google.com/armor/docs/configure-security-policies#gcloud_11
"gcloud compute security-policies rules create 1000 \
--security-policy my-policy \
--expression "evaluatePreconfiguredExpr('sourceiplist-fastly')" \
--action "allow"
"
upvoted 
30 
times
6b13108
6b13108
 
1 year, 1 month ago
I can not see the same example in that document and I saw "evaluatePreconfiguredExpr" is for preconfigure WAF rules
https://cloud.google.com/armor/docs/rule-tuning
upvoted 
1 
times
dfizban
dfizban
 
Most Recent
 
2 months, 2 weeks ago
It's D
upvoted 
1 
times
Begum
Begum
 
3 months, 1 week ago
The correct answer is D. The syntax for command must include --Security-policy, --expression or --src-in-ranges ( for option A
IP range is wild card) hence correct answer is D.
upvoted 
1 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 1 week ago
The most appropriate command for allowing traffic from all Fastly IP address ranges into the HRL Virtual Private Cloud (VPC)
network through the External HTTP(S) load balancer would be:
A. Create Cloud Armor Security Policy with the source IP ranges.
Explanation:
Cloud Armor is the tool designed specifically for protecting HTTP(S) load balancers and controlling access based on IP address
ranges. It allows you to create security policies to allow or deny traffic from specific IP ranges, which is what you need to do for
Fastly IPs.
This approach is specifically designed for managing traffic to HTTP(S) load balancers, providing an additional layer of security
that fits this scenario perfectly.
upvoted 
1 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 1 week ago
Why Not the Other Options?
B. Create Cloud Armor Security Policy with the source IP list: Cloud Armor requires IP ranges, not a simple list of IPs.
C. Create firewall rule to allow source IP list: Firewall rules operate at the VPC network level, and while they control network
access, they are not specifically tied to HTTP(S) load balancers and would not efficiently apply to this context.
D. Create firewall rule to allow source IP range: Firewall rules can allow traffic from IP ranges, but again, they are applied at
the VPC level. For HTTP(S) load balancer traffic, Cloud Armor is the correct tool to manage IP range access control.
upvoted 
1 
times
researched_answer_boi
researched_answer_boi
 
8 months, 1 week ago
(D), or "Create Cloud Armor Security Policy with the source ip list" (considering @hashi's comment) looks correct.
https://codelabs.developers.google.com/codelabs/cloud-cloudarmor#0
upvoted 
3 
times
dija123
dija123
 
8 months, 2 weeks ago
Totally agree with D
upvoted 
1 
times
hashi
hashi
 
9 months, 2 weeks agoI got this question in March 2024. 
As someone pointed out answers are reworked. 
Instead of asking for the command, the choices were given in wordings - something like the below. (Not the exact words)
A. Create Cloud Armor Security Policy with the source ip ranges.
B. Create Cloud Armor Security Policy with the source ip list
C. Create firewall rule to allow source ip list
D. Create firewall rule to allow source ip range
Based on the answers for this question I went with "Create Cloud Armor Security Policy with the source ip list"
upvoted 
15 
times
Chandankm
Chandankm
 
6 months, 2 weeks ago
what's the difference between options A & B, i.e. source IP "ranges" and "list" ? what's the reason for choosing one over
another ? I've been through the documentation and these terms are used intermittently.
upvoted 
1 
times
Chandankm
Chandankm
 
6 months, 1 week ago
If the question really makes a distinction between ranges and lists as specified above, I'm quite disappointed with Google. It
looks like they're more interested in throwing the examinee off-balance by confusing them with useless jargon rather than
evaluating the actual skills.
upvoted 
1 
times
ccpmad
ccpmad
 
6 months, 3 weeks ago
Thank you for the info, but for me, in your question, I would choose D. Firewall rule. Firewalls are designed to efficiently
manage network traffic. Allowing IP ranges simplifies administration and enhances performance by handling access from
multiple IP addresses effectively.
upvoted 
2 
times
VidhyaBupesh
VidhyaBupesh
 
10 months, 2 weeks ago
D is right
upvoted 
1 
times
d0094d6
d0094d6
 
11 months ago
should be D
upvoted 
1 
times
Pime13
Pime13
 
11 months, 1 week ago
D is the solution
upvoted 
1 
times
didek1986
didek1986
 
11 months, 2 weeks ago
D d d d
upvoted 
1 
times
gun123
gun123
 
12 months ago
D is the ans
upvoted 
1 
times
MahAli
MahAli
 
1 year ago
I guess D
upvoted 
1 
times
odacir
odacir
 
1 year, 1 month ago
D -> https://cloud.google.com/armor/docs/configure-security-policies#create-rules
upvoted 
2 
times
didek1986
didek1986
 
1 year, 3 months ago
D for sure
upvoted 
2 
timesBiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
A. Looks like it opens to all IPs
B. Incorrect syntax "ACTION must be one of: allow, deny, goto_next."
C. Incorrect syntax "ACTION must be one of: allow, deny, goto_next."
D. 
Assuming the preconfigured expression is good then its right.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 3
Question #3
For this question, refer to the Helicopter Racing League (HRL) case study. The HRL development team releases a new version
of their predictive capability application every Tuesday evening at 3 a.m. UTC to a repository. The security team at HRL has
developed an in-house penetration test Cloud Function called 
Airwolf. The security team wants to run Airwolf against the predictive capability application as soon as it is released every
Tuesday. You need to set up Airwolf to run at the recurring weekly cadence. What should you do? 
A. 
Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.
B. 
Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function.
C. 
Configure the deployment job to notify a Pub/Sub queue that triggers a Cloud Function. 
Most Voted
D. 
Set up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function.
Correct Answer:
 
C 
Comments
umashankar_a
umashankar_a
 
Highly Voted
 
2 years, 12 months ago
Answer C seems to be ok. Triggering Pub/Sub to invoke Cloud Functions seems to be relevant. Cloud Storage doesn't make any
sense. It would have been straight forward if Cloud Scheduler is mentioned in Option C instead of Deployment Job. But if you
make a bit of research on deployment jobs, it's pointing me to cron jobs which is making perfect sense.
https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml
https://cloud.google.com/scheduler/docs/tut-pub-sub
upvoted 
58 
times
elainexs
elainexs
 
2 years ago
Cannot understand why push CICD event to pub/sub... which is only one event, why need pub/sub
upvoted 
6 
times
stefanop
stefanop
 
2 years, 8 months ago
But the question requires a scheduled execution, not one triggered by the deployment job. Shouldn’t A be the correct answer?
upvoted 
5 
times
Nimbus2021
Nimbus2021
 
2 years, 7 months ago
Community vote distribution
C (81%)
Other (19%)Nimbus2021
Nimbus2021
 
2 years, 7 months ago
I think no because question mentions "as soon as it is released every Tuesday."
upvoted 
3 
times
Gino17m
Gino17m
 
2 months ago
Teh question requires recurring not scheduled execution
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
2 years, 12 months ago
Answer is A
upvoted 
18 
times
nandoD
nandoD
 
1 year, 2 months ago
Please elaborate.
upvoted 
3 
times
dija123
dija123
 
Most Recent
 
2 months, 3 weeks ago
Selected Answer: 
C
Totally agree with C
upvoted 
1 
times
mouthwash
mouthwash
 
7 months ago
Passed the GCP test today, answer is C
The key is to use google's native tools
upvoted 
7 
times
Jconnor
Jconnor
 
7 months ago
How is A even an option? What do you use cloud storage for? A good architecture is event driven, as it would be more resilient
to failures, change in time, error and it is easier to debung, log and scale. That is what Pub/Sub is for.
upvoted 
1 
times
thewalker
thewalker
 
7 months, 1 week ago
Selected Answer: 
A
A is simple and clean compared to the other options provided.
upvoted 
1 
times
MikeH20
MikeH20
 
6 months, 3 weeks ago
Where does a Cloud Storage bucket come into play here? Nothing in the question implies anything about storage. If they
needed a place to store the results of the Airwolf job, then sure. But that isn't mentioned anywhere.
upvoted 
2 
times
Sarin
Sarin
 
8 months, 2 weeks ago
Answer C seems to be right. 
There are 2 requirements here, 
1. Run every time it is released on Tuesday
2. Set Airwolf to run weekly
Since 
a new version of the predictive capability application is released every tuesday evening at 3.00 am, the deployment job
would run every time its released which is every week recurring. 
So both the requirements above are satisfied
upvoted 
2 
times
sampon279
sampon279
 
1 year ago
Selected Answer: 
C
Should be C. Cannot be A, to schedule cloud task you need to know when the deployment is complete, deployments usually are
unpredictable and do not meet scheduled time. With option C, CICD pipeline which deploys the code and publish a message to
pub/sub to trigger cloud function - better solution to trigger via http endpoint if that is an option. pub/sub is till okay.
upvoted 
3 
times
WinSxS
WinSxS
 
1 year, 3 months ago
Selected Answer: 
CTo run Airwolf against the predictive capability application as soon as it is released every Tuesday, you should configure the
deployment job to notify a Pub/Sub queue that triggers a Cloud Function.
upvoted 
2 
times
zerg0
zerg0
 
1 year, 5 months ago
Selected Answer: 
A
Cloud task is supports scheduling
upvoted 
1 
times
tdotcat
tdotcat
 
1 year, 5 months ago
Selected Answer: 
C
c fits scenario
upvoted 
1 
times
main_street
main_street
 
1 year, 6 months ago
Answer A seems correct since cloud tasks support scheduled delivery but pub/sub doesn't
see https://cloud.google.com/pubsub/docs/choosing-pubsub-or-cloud-tasks
upvoted 
2 
times
jlambdan
jlambdan
 
1 year ago
https://cloud.google.com/tasks/docs/comp-tasks-sched
it seems to be scheduling of task ahead of time, not scheduling at fixed time interval.
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
To set up Airwolf to run at a recurring weekly cadence, the correct option would be C: Configure the deployment job to notify a
Pub/Sub queue that triggers a Cloud Function.
To set up Airwolf to run at the desired weekly cadence, you can configure the deployment job to send a notification to a
Pub/Sub queue when a new version of the predictive capability application is released. Then, you can set up a Cloud Function
that is triggered by messages in the Pub/Sub queue and runs the Airwolf penetration test. This way, the Cloud Function will be
triggered every time a new message is published to the queue, which will occur every Tuesday evening at 3 a.m. UTC when a
new version of the application is released.
upvoted 
4 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option A, Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function, would not be the correct solution
because Cloud Tasks is a service for creating and managing asynchronous tasks that are executed later, but it does not support
recurring schedules.
Option B, Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function, would not be the correct
solution because Cloud Logging is a service for collecting, viewing, and analyzing logs, but it does not support triggering
Cloud Functions on a recurring basis.
Option D, Set up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function, would not
be the correct solution because IAM is a service for managing access to Google Cloud resources and Confidential Computing
is a service for running sensitive workloads in hardware-isolated environments, but neither of these services can be used to
trigger Cloud Functions on a recurring basis.
upvoted 
3 
times
kat1969
kat1969
 
1 year, 6 months ago
This conflicts with your earlier statements? 
Is this statement intended as a correction?
upvoted 
1 
times
nandoD
nandoD
 
1 year, 2 months ago
how I see it, the first post is the correct answer explanation, the second post is why the other 3 answers are wrong.
upvoted 
1 
times
thamaster
thamaster
 
1 year, 6 months ago
answer A does not make sense why put a cloud task and check a storage (which is never updated) for cloud function? If theanswer A does not make sense why put a cloud task and check a storage (which is never updated) for cloud function? If the
release has some late the task run for nothing.
Pub sub + cloud function is best practice
upvoted 
2 
times
amelm
amelm
 
1 year, 4 months ago
That's what I taught too. "Why do I need Cloud Storage?"
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The correct answer is A: Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.
To set up Airwolf to run at a recurring weekly cadence, you should set up Cloud Tasks and a Cloud Storage bucket that triggers
a Cloud Function.
Cloud Tasks is a fully managed service that allows you to schedule and execute background jobs in a scalable and reliable way.
You can use Cloud Tasks to create a recurring task that runs at a specified interval (e.g., every week). When the task is triggered,
it can send a message to a Cloud Storage bucket, which can then trigger a Cloud Function to run the Airwolf penetration test.
Option B: Setting up a Cloud Logging sink and a Cloud Storage bucket would not allow you to schedule the task to run at a
recurring weekly cadence.
Option C: Configuring the deployment job to notify a Pub/Sub queue would not allow you to schedule the task to run at a
recurring weekly cadence.
Option D: Setting up Identity and Access Management (IAM) and Confidential Computing would not allow you to schedule the
task to run at a recurring weekly cadence.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C Is the Correct Answer
upvoted 
1 
times
Jackalski
Jackalski
 
1 year, 7 months ago
Selected Answer: 
A
I vote on A
cloud task can trigger CF ... with limit to 30 days - here it is weekly - so far so good
however not sure why it would need any cloud storage .. potentially to store results of dony by CF
answer C - has no schedule option 
example:
https://cloud.google.com/tasks/docs/tutorial-gcf
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 3
Question #4
For this question, refer to the Helicopter Racing League (HRL) case study. HRL wants better prediction accuracy from their ML
prediction models. They want you to use Google's AI Platform so HRL can understand and interpret the predictions. What
should you do? 
A. 
Use Explainable AI. 
Most Voted
B. 
Use Vision AI.
C. 
Use Google Cloud's operations suite.
D. 
Use Jupyter Notebooks.
Correct Answer:
 
A 
Comments
umashankar_a
umashankar_a
 
Highly Voted
 
2 years, 12 months ago
Answer A 
AI Explanations helps you understand your model's outputs for classification and regression tasks. Whenever you request a
prediction on AI Platform, AI Explanations tells you how much each feature in the data contributed to the predicted result. You
can then use this information to verify that the model is behaving as expected, recognize bias in your models, and get ideas for
ways to improve your model and your training data.
https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview
upvoted 
29 
times
technodev
technodev
 
Highly Voted
 
2 years, 5 months ago
Got this question in my exam, answered A
upvoted 
11 
times
gcloud007
gcloud007
 
Most Recent
 
2 days, 13 hours ago
Selected Answer: 
A
The choices are wrong ... AI Explanations is Deprecated, its not called Vertex AI, see. .. https://cloud.google.com/ai-
platform/prediction/docs/ai-explanations/overview
upvoted 
1 
times
JaimeMS
JaimeMS
 
3 weeks, 3 days ago
Community vote distribution
A (100%)Selected Answer: 
A
Answer A,
BUT: "This legacy version of AI Platform Prediction is deprecated and will no longer be available on Google Cloud after January
31, 2025. All models, associated metadata, and deployments will be deleted after January 31, 2025. Migrate your resources to
Vertex AI to get new machine learning features that are unavailable in AI Platform."
upvoted 
3 
times
yas_cloud
yas_cloud
 
3 months, 2 weeks ago
Option A - Now its Vertex AI
upvoted 
5 
times
CyanideX
CyanideX
 
8 months, 2 weeks ago
Selected Answer: 
A
It's now Vertex AI
upvoted 
7 
times
Kamngur
Kamngur
 
9 months, 3 weeks ago
Selected Answer: 
A
Answer A is almost correct ;). Not it should be Vertex AI.
upvoted 
3 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The correct answer is A: Use Explainable AI.
To understand and interpret the predictions made by HRL's ML prediction models, you should use Explainable AI. Explainable
AI, also known as XAI, is a suite of tools and techniques that helps you understand and interpret the predictions made by
machine learning models. With Explainable AI, you can get insights into how the model made a particular prediction, which can
help you understand the underlying factors that influenced the prediction. This can help HRL improve the accuracy of their
predictions and make more informed decisions based on the output of their models.
Option B: Vision AI is a suite of tools and services that helps you build and deploy computer vision applications. It is not
relevant to understanding and interpreting the predictions made by HRL's ML prediction models.
upvoted 
5 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Option C: Google Cloud's operations suite is a set of tools and services that helps you monitor, troubleshoot, and optimize
your Google Cloud resources. It is not relevant to understanding and interpreting the predictions made by HRL's ML prediction
models.
Option D: Jupyter Notebooks is an open-source web application that allows you to create and share documents that contain
live code, equations, visualizations, and narrative text. It is not relevant to understanding and interpreting the predictions
made by HRL's ML prediction models.
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
6721sora
6721sora
 
1 year, 9 months ago
Selected Answer: 
A
Explainable AI now included with Vertex AI
https://cloud.google.com/explainable-ai
upvoted 
3 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
Answer is A. 
https://cloud.google.com/explainable-ai
upvoted 
2 
timesAzureDP900
AzureDP900
 
1 year, 12 months ago
https://cloud.google.com/aiplatform/prediction/docs/ai-explanations/overview
upvoted 
1 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
A
Had this quection on my exam.
upvoted 
2 
times
esnecho
esnecho
 
2 years, 6 months ago
Selected Answer: 
A
A is Correct
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
4 
times
kopper2019
kopper2019
 
2 years, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
1 
times
victory108
victory108
 
2 years, 11 months ago
A. Use Explainable AI.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 3
Question #5
For this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking for a cost-effective approach for
storing their race data such as telemetry. They want to keep all historical records, train models using only the previous
season's data, and plan for data growth in terms of volume and information collected. You need to propose a data solution.
Considering HRL business requirements and the goals expressed by CEO S. Hawke, what should you do? 
A. 
Use Firestore for its scalable and flexible document-based database. Use collections to aggregate race data by season
and event.
B. 
Use Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as
a primary key.
C. 
Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season. 
Most Voted
D. 
Use Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate
database instances for each season.
Correct Answer:
 
C 
Comments
MamthaSJ
MamthaSJ
 
Highly Voted
 
2 years, 12 months ago
Answer is C
upvoted 
17 
times
Wonka
Wonka
 
2 years, 5 months ago
These questions are sounding too simple, are these really coming in exam or these are mocked up?
upvoted 
3 
times
ashrafh
ashrafh
 
1 year, 7 months ago
In exam actually :)
upvoted 
6 
times
victory108
victory108
 
Highly Voted
 
2 years, 11 months ago
Community vote distribution
C (100%)C. Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.
upvoted 
6 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Vote for C
upvoted 
1 
times
Prakzz
Prakzz
 
9 months ago
How can it be C because BigQuery only support timestamp based partitions?
upvoted 
1 
times
parthkulkarni998
parthkulkarni998
 
6 months, 1 week ago
Exactly. That means it will use timestamp of season for partition
upvoted 
2 
times
Kamngur
Kamngur
 
9 months, 3 weeks ago
C looks good. 
Remember Telemetry would be stored and used for predictions later
A. would be good if it would be stored only, as documents. 
B. Spanner is over kill. It is too expensive and we have no need to serve this data outside
D. Cloud SQL is too simple solution and use multiple db's is only complicating architecture.
upvoted 
4 
times
examch
examch
 
1 year, 5 months ago
Selected Answer: 
C
C is the correct Answer,
We can use BigQuery for making Predictions with live and trained data with Cloud ML Engine, and BigQuery can handle large
amounts of data.
Refer to the Link for more details on Game Predictions,
https://cloud.google.com/blog/products/gcp/architecting-live-ncaa-predictions-from-archives-to-insights
upvoted 
5 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
For HRL's data storage needs, it is recommended to use BigQuery due to its scalability and ability to handle large amounts of
data. By partitioning the race data based on season, HRL can easily access and query specific seasons' data while also taking
into consideration future data growth. BigQuery also allows for the flexibility to add new columns to the schema as needed,
making it easier to adapt to changes in the data being collected. Additionally, BigQuery's pay-per-use pricing model allows
HRL to only pay for the data storage and querying they use, making it a cost-effective solution.
upvoted 
3 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Nirca
Nirca
 
1 year, 10 months ago
Selected Answer: 
C
C it isC it is
upvoted 
1 
times
Nirca
Nirca
 
1 year, 11 months ago
c C C c
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
C most obvious when data is growing day by day.
upvoted 
1 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
C
Had this question on my exam.
upvoted 
2 
times
SAMBIT
SAMBIT
 
2 years, 3 months ago
Telemetry data in a relational table..what?? Why they gave firestore then
upvoted 
3 
times
technodev
technodev
 
2 years, 5 months ago
Got this question in my exam, answered C
upvoted 
3 
times
Pime13
Pime13
 
2 years, 6 months ago
to me it's C -> https://cloud.google.com/architecture/mobile-gaming-analysis-telemetry
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 3
Question #6
For this question, refer to the Helicopter Racing League (HRL) case study. A recent finance audit of cloud infrastructure noted
an exceptionally high number of 
Compute Engine instances are allocated to do video encoding and transcoding. You suspect that these Virtual Machines are
zombie machines that were not deleted after their workloads completed. You need to quickly get a list of which VM instances
are idle. What should you do? 
A. 
Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis.
B. 
Use the gcloud compute instances list to list the virtual machine instances that have the idle: true label set.
C. 
Use the gcloud recommender command to list the idle virtual machine instances. 
Most Voted
D. 
From the Google Console, identify which Compute Engine instances in the managed instance groups are no longer
responding to health check probes.
Correct Answer:
 
C 
Comments
MamthaSJ
MamthaSJ
 
Highly Voted
 
2 years, 12 months ago
Answer is C
upvoted 
21 
times
khadar
khadar
 
1 year, 9 months ago
I too got this question in 10-09-22 exam with similar option and result is pass
upvoted 
9 
times
mkhaired
mkhaired
 
Highly Voted
 
3 years ago
C is the Correct answer 
C. Use the gcloud recommender command to list the idle virtual machine instances.
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations
upvoted 
19 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
C
Community vote distribution
C (100%)Selected Answer: 
C
C i scorrect
upvoted 
2 
times
tdotcat
tdotcat
 
1 year, 5 months ago
answer C
check here : 
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-
recommendations#viewing_idle_vm_instance_recommendations
upvoted 
3 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
zellck
zellck
 
1 year, 9 months ago
Selected Answer: 
C
C is the answer which is to use recommender.
https://www.youtube.com/watch?v=VBsLG4jCHJk
upvoted 
1 
times
satamex
satamex
 
1 year, 11 months ago
Selected Answer: 
C
Some one who can explain me why A is an answer? is it deliberate?
upvoted 
1 
times
randy8984
randy8984
 
11 months, 3 weeks ago
because the questions says "quickly"
upvoted 
1 
times
Mikado211
Mikado211
 
1 year, 11 months ago
Don't pay too much attention to the "correct answer", the "most voted" is much more reliable.
upvoted 
8 
times
deenee
deenee
 
1 year, 11 months ago
Selected Answer: 
C
C looks decent as identification has to be done quickly. Manually checking each machines will take lot of time. Moreover---
even option A says "CPUs" 
and not GPUs 
"Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis."
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
C is right
upvoted 
2 
times
Tillssatya12
Tillssatya12
 
2 years ago
All the quesrions had come from this site , especially couple of case studies and answer is C
upvoted 
3 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
CHad this question on my exam.
upvoted 
5 
times
technodev
technodev
 
2 years, 5 months ago
Got this question in my exam, answered C
upvoted 
4 
times
hightech
hightech
 
2 years, 5 months ago
Did all the questions in the exam come from this site?
upvoted 
1 
times
Pime13
Pime13
 
2 years, 6 months ago
Selected Answer: 
C
to me should be C -> https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-
recommendations#before-you-begin
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
SamGCP
SamGCP
 
2 years, 6 months ago
Selected Answer: 
C
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations
upvoted 
2 
times
SamGCP
SamGCP
 
2 years, 6 months ago
Selected Answer: 
C
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 4
Question #1
For this question, refer to the EHR Healthcare case study. You are responsible for ensuring that EHR's use of Google Cloud will
pass an upcoming privacy compliance audit. What should you do? (Choose two.) 
A. 
Verify EHR's product usage against the list of compliant products on the Google Cloud compliance page. 
Most Voted
B. 
Advise EHR to execute a Business Associate Agreement (BAA) with Google Cloud. 
Most Voted
C. 
Use Firebase Authentication for EHR's user facing applications.
D. 
Implement Prometheus to detect and prevent security breaches on EHR's web-based applications.
E. 
Use GKE private clusters for all Kubernetes workloads.
Correct Answer:
 
AB 
Comments
raf2121
raf2121
 
Highly Voted
 
2 years, 10 months ago
My option it's A & B
A - OK (Google Cloud compliance page will give list of products those are HIPAA compliant
https://cloud.google.com/security/compliance/offerings?
skip_cache=true#/regions=USA&industries=Healthcare_and_life_sciences&focusArea=Privacy)
B - OK (BAA means HIPAA Business Associate amendment or Business Associate Agreement entered into between Google and
Customer. With EHR being a leading provider of health record software, this agreement is required.
https://cloud.google.com/files/gcp-hipaa-overview-guide.pdf?hl=en)
C - Eliminated (Firebase authentication provides backend services, easy-to-use SDKs and ready-made libraries to users on App.
https://firebase.google.com/docs/auth)
D - Eliminated (more of an observability platform)
E - Eliminated (Running distributed services in GKE private clusters gives enterprises both secure and reliable services. Not sure
how this may help with Private Compliance Audit)
upvoted 
44 
times
SoniaJacob521
SoniaJacob521
 
Highly Voted
 
2 years, 10 months ago
A& B https://cloud.google.com/security/compliance/hipaa
upvoted 
17 
times
Dav_96
Dav_96
 
Most Recent
 
2 months, 2 weeks ago
Community vote distribution
AB (73%)
A (27%)Selected Answer: 
A
Just got out of the exam. The option B was not in the answers, so the only option left for me was A.
upvoted 
7 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
AB
A & B is the correct answer
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
AB
A & B is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
AB
AB is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
AB
A, B that's the only two things that I see related
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
A, B is straight forward, I didn’t even think too much before making my mind. You need to read all case studies understand
throughly before the exam. This whole set of case studies waste lot of time if you don’t prepare in advance and trying go
through during exam. My approach is focus on key words..
upvoted 
10 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
AB
Agree with raf 2121 A& B
upvoted 
2 
times
mbenhassine1986
mbenhassine1986
 
2 years, 4 months ago
A & B 
https://cloud.google.com/security/compliance/hipaa#customer_responsibilities
upvoted 
2 
times
muky31dec
muky31dec
 
2 years, 4 months ago
Ans is A and B
upvoted 
2 
times
Arjun1983
Arjun1983
 
2 years, 5 months ago
A and B
https://cloud.google.com/security/compliance/hipaa
Essential best practices:
1. Execute a Google Cloud BAA. You can request a BAA directly from your account manager.
2. Disable or otherwise ensure that you do not use Google Cloud Products that are not explicitly covered by the BAA (see
Covered Products) when working with PHI.
upvoted 
3 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
Selected Answer: 
AB
I chose A&B by Elimination method.
upvoted 
2 
timesupvoted 
2 
times
Pime13
Pime13
 
2 years, 6 months ago
A and B https://cloud.google.com/security/compliance/hipaa
upvoted 
1 
times
PhilipKoku
PhilipKoku
 
2 years, 6 months ago
Selected Answer: 
AB
https://cloud.google.com/security/compliance/offerings?
skip_cache=true#/regions=USA&industries=Healthcare_and_life_sciences&focusArea=Privacy
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
AB
AB is the correct answer
upvoted 
3 
times
pakilodi
pakilodi
 
2 years, 7 months ago
Selected Answer: 
AB
Vote AB
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 4
Question #2
For this question, refer to the EHR Healthcare case study. You need to define the technical architecture for securely deploying
workloads to Google Cloud. You also need to ensure that only verified containers are deployed using Google Cloud services.
What should you do? (Choose two.) 
A. 
Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline. 
Most Voted
B. 
Configure Jenkins to utilize Kritis to cryptographically sign a container as part of a CI/CD pipeline.
C. 
Configure Container Registry to only allow trusted service accounts to create and deploy containers from the registry.
D. 
Configure Container Registry to use vulnerability scanning to confirm that there are no vulnerabilities before deploying
the workload. 
Most Voted
Correct Answer:
 
AD 
Comments
raf2121
raf2121
 
Highly Voted
 
3 years, 4 months ago
A & D
Binary Authorization to ensure only verified containers are deployed 
To ensure deployment are secure and and consistent, automatically scan images for vulnerabilities with container analysis
(https://cloud.google.com/docs/ci-cd/overview?hl=en&skip_cache=true)
upvoted 
49 
times
cloudmon
cloudmon
 
2 years, 8 months ago
Also see references to the combination of using binary authorization and vulnerability scanning here:
https://cloud.google.com/binary-authorization/docs/overview
upvoted 
12 
times
KillerGoogle
KillerGoogle
 
Highly Voted
 
3 years, 4 months ago
IMHO its A&C
upvoted 
32 
times
mgm7
mgm7
 
3 years ago
Community vote distribution
AD (40%)
AC (36%)
A (12%)
Other (12%)I see a lot of people answered D but I don't see how it answers the question. 
I can securely deploy complete junk code. 
There
is no contradiction in this phrase even if one obviously should avoid doing this.
upvoted 
6 
times
BeCalm
BeCalm
 
1 year, 10 months ago
Dude the same applies to C. Trusted service accounts can deploy junk too.
upvoted 
8 
times
medi01
medi01
 
1 year, 8 months ago
But that's the goal: secure the deployment process.
upvoted 
4 
times
desertlotus1211
desertlotus1211
 
Most Recent
 
1 month ago
Selected Answer: 
AB
Answer is A&B. 
remember the questions ask abut securely deployment container images and verified containers. 
scanning for
vulnerabilities does not accomplish this. 
I know this goes against common sense, but good code or bad code - how would you securely deploy the container? Answer is
A&B.
upvoted 
1 
times
dfizban
dfizban
 
2 months, 3 weeks ago
Selected Answer: 
AD
A&D I'm sure
upvoted 
1 
times
pcamaster
pcamaster
 
3 months ago
Selected Answer: 
AC
AC
Question is about:
- Securing the deployment process
- Make sure only verified containers can run on the cluster
A: Covers the secondo point thanks to binary authorization. It also covers the signing requirement, as it is performed at CICD
level.
B: This is already covered by A. 
C: Makes sure that only required Service Accounts can pull the code from registry, so it covers the first part of the questione
D. Secure scanning is about "security vulnerability in code". So it does not cover deployment phase, nor authorization phase.
So, it's A & C
upvoted 
1 
times
ukivanlamlpi
ukivanlamlpi
 
5 months, 3 weeks ago
Selected Answer: 
AB
who deploy is not an issue, the question is 'only verified containers' ....kritis can do that.
upvoted 
2 
times
upliftinghut
upliftinghut
 
6 months, 3 weeks ago
Selected Answer: 
AD
A : use binary authorization then D check vulnerabilities before being able to deploy
upvoted 
1 
times
Dav_96
Dav_96
 
8 months, 2 weeks ago
Selected Answer: 
A
Just got out of the exam. You only need to specify one answer, hence I chose A.
upvoted 
11 
times
Pime13
Pime13
 
11 months agoSelected Answer: 
AD
ad for me
upvoted 
1 
times
Pime13
Pime13
 
11 months, 1 week ago
Selected Answer: 
AD
https://cloud.google.com/docs/ci-cd/overview?hl=en&skip_cache=true 
https://cloud.google.com/binary-authorization/docs/overview
upvoted 
1 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Selected Answer: 
AC
For surę AC
upvoted 
1 
times
JohnDohertyDoe
JohnDohertyDoe
 
11 months, 3 weeks ago
Selected Answer: 
AC
Answer should be A & C, as the ask is to ensure only verified containers to be deployed. With just Binary Authorisation and
signing images, you can't fulfil the requirement, you would need to also restrict it at the IAM level, so that no bad actor can
create an image in the registry and bypass Binary Authorization to deploy an image.
upvoted 
5 
times
sudaraka
sudaraka
 
11 months, 4 weeks ago
I think A&B
Kritis is an admission controller webhook for Kubernetes that enforces deploy-time security policies. By configuring Jenkins to
use Kritis, you can cryptographically sign containers as part of the CI/CD pipeline, ensuring only signed containers are deployed.
https://cloud.google.com/binary-authorization/docs/creating-attestations-kritis
upvoted 
5 
times
[Removed]
[Removed]
 
1 year ago
Selected Answer: 
AD
Option C is incorrect because while limiting access to trusted service accounts enhances security, it doesn't ensure that only
verified containers are deployed.
upvoted 
3 
times
Prudvi3266
Prudvi3266
 
1 year ago
Selected Answer: 
AD
Checked with standard process for this. I found the below.
Image Building and Scanning:
Developers build container images locally or using Cloud Build.
Images are scanned for vulnerabilities using integrated tools or third-party services.
Clean images are pushed to GCR.
Image Verification:
Binary Authorization enforces policies for image acceptance.
Attestations from Cloud Security Scanner or third-party tools can be used.
upvoted 
3 
times
oidajoi
oidajoi
 
1 year ago
A&D.
C is incorrect because you configuring Container Registry doesn't only allow trusted service accounts to create/deploy
containers. With IAM permissions, anyone can create non-trusted service accounts to deploy containers, or users can still deploy
containers not in Container Registry.
upvoted 
2 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
AC
A & C correctA & C correct
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 4
Question #3
You need to upgrade the EHR connection to comply with their requirements. The new connection design must support
business-critical needs and meet the same network and security policy requirements. What should you do? 
A. 
Add a new Dedicated Interconnect connection. 
Most Voted
B. 
Upgrade the bandwidth on the Dedicated Interconnect connection to 100 G.
C. 
Add three new Cloud VPN connections.
D. 
Add a new Carrier Peering connection.
Correct Answer:
 
A 
Comments
[Removed]
[Removed]
 
Highly Voted
 
2 years, 3 months ago
Selected Answer: 
A
I will go A cause note in https://cloud.google.com/network-connectivity/docs/interconnect/how-to/dedicated/modifying-
interconnects says
" It is not possible to change the link type on an Interconnect connection circuit from 10 Gbps to 100 Gbps. If you want to
migrate to 100 Gbps, you must first provision a new 100-Gbps Interconnect connection alongside your existing 10-Gbps
connection, and then migrate the traffic onto the 100-Gbps connection."
upvoted 
35 
times
LaxmanTiwari
LaxmanTiwari
 
1 year, 1 month ago
spot on
upvoted 
1 
times
cloudmon
cloudmon
 
2 years, 2 months ago
This is awesome! That answers it perfectly!
upvoted 
3 
times
rishab86
rishab86
 
Highly Voted
 
2 years, 10 months ago
Answer A ; 99.9% availability requires 2 interconnect
upvoted 
17 
times
Community vote distribution
A (100%)upvoted 
17 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
[Removed]
[Removed]
 
6 months, 1 week ago
Selected Answer: 
A
https://cloud.google.com/network-connectivity/docs/interconnect/how-to/dedicated/modifying-
interconnects#:~:text=The%20link%20type,100%E2%80%91Gbps%20connection.
upvoted 
1 
times
Kamngur
Kamngur
 
9 months, 3 weeks ago
I propose A.
B - it is not possible to change interconnect link type.
C - more problematic and probably more expensive
D - Google does not offer a service level agreement (SLA) with Carrier Peering
upvoted 
3 
times
dar10
dar10
 
10 months, 2 weeks ago
Answer is A, as explained in the note at page 339 "Implementing Hybrid Connectivity" of the newly released book:
https://a.co/d/asbIhln
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
3 
times
ashrafh
ashrafh
 
1 year, 7 months ago
Selected Answer: 
A
Answer is A
upvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is best answer
upvoted 
2 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months ago
Selected Answer: 
A
A.
In the text it mentions high connection - reliable, only dedicated interconnect could achieve that.
upvoted 
1 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
A
A is correct
upvoted 
2 
times
vaibhav15
vaibhav15
 
2 years, 7 months ago
Selected Answer: 
A
Vote A
upvoted 
2 
times
sapsant
sapsant
 
2 years, 7 months ago
Selected Answer: 
ADedicated Interconnect provides direct physical connections between your on-premises network and Google's network.
https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview
upvoted 
3 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
4 
times
[Removed]
[Removed]
 
2 years, 7 months ago
A is correct.
Marked D is wrong
upvoted 
2 
times
Ari_GCP
Ari_GCP
 
2 years, 9 months ago
Says "Upgrade keeping same network requirements". Remember infrastructure costs need to be kept low. A new dedicated
connection will involve purchasing new installation equipment. Upgrading to a higher bandwidth is more cost-friendly. I go
with B.
upvoted 
2 
times
MikeB19
MikeB19
 
2 years, 9 months ago
A is correct. Carrier peering is dedicated connection to google workspace not gcp
https://cloud.google.com/network-connectivity/docs/carrier-peering
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 4
Question #4
For this question, refer to the EHR Healthcare case study. You need to define the technical architecture for hybrid connectivity
between EHR's on-premises systems and Google Cloud. You want to follow Google's recommended practices for production-
level applications. Considering the EHR Healthcare business and technical requirements, what should you do? 
A. 
Configure two Partner Interconnect connections in one metro (City), and make sure the Interconnect connections are
placed in different metro zones.
B. 
Configure two VPN connections from on-premises to Google Cloud, and make sure the VPN devices on-premises are in
separate racks.
C. 
Configure Direct Peering between EHR Healthcare and Google Cloud, and make sure you are peering at least two Google
locations.
D. 
Configure two Dedicated Interconnect connections in one metro (City) and two connections in another metro, and make
sure the Interconnect connections are placed in different metro zones. 
Most Voted
Correct Answer:
 
D 
Comments
raf2121
raf2121
 
Highly Voted
 
2 years, 10 months ago
Answer : D (based on the requirement of secure and high-performance connection between on-premises systems to Google
Cloud)
Between A and D, picked D as with Direct Connect EHR can get the bandwidth of 10 GBS to 100GBS 
(VPN ruled out as traffic is
over internet and due to bandwidth. Direct Peering is more for Workspace rather than Google Cloud)
upvoted 
31 
times
jask
jask
 
Highly Voted
 
2 years, 9 months ago
If we notice this line in question - "Google's recommended practices for production-level applications" and then see overview
of these 2 pages- https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview and
https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/non-critical-overview. It is clear answer should be
D , which is topology for production level applications recommended by Google
upvoted 
28 
times
cloudmon
cloudmon
 
2 years, 2 months ago
Community vote distribution
D (90%)
A (10%)cloudmon
cloudmon
 
2 years, 2 months ago
^^^ This is the best explanation. 
Considering all of those factors, D looks best.
upvoted 
5 
times
moota
moota
 
1 year, 5 months ago
Specifically this page https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/dedicated-creating-9999-
availability
upvoted 
2 
times
medi01
medi01
 
1 year, 2 months ago
That's one more 9 than required.
upvoted 
1 
times
theBestStudent
theBestStudent
 
Most Recent
 
6 months, 1 week ago
Selected Answer: 
D
Answer is clearly D:
https://cloud.google.com/network-connectivity/docs/interconnect/concepts/best-practices#scenarios
upvoted 
1 
times
red_panda
red_panda
 
1 year ago
Selected Answer: 
D
For me is D.
By Technical requirement we need to establish a stable, low-latency connection between on-prem and cloud
upvoted 
1 
times
Wael216
Wael216
 
1 year, 6 months ago
Selected Answer: 
D
simple hint : in this EHR case study, whenever there is a network connection, it's a dedicated interconnect answer !
upvoted 
15 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The recommended solution for hybrid connectivity between on-premises systems and Google Cloud is to configure two
Dedicated Interconnect connections in two different metros (cities). This ensures that EHR Healthcare has a redundant
connection to Google Cloud, with each connection providing a separate physical path. Placing the Interconnect connections in
different metro zones also helps to ensure that the connection is resilient to failures in a single geographic region. This solution
meets the business requirement of providing a secure and high-performance connection between on-premises systems and
Google Cloud, as well as the technical requirement of maintaining regulatory compliance. It also helps to meet the requirement
of providing consistent logging, log retention, monitoring, and alerting capabilities, as Dedicated Interconnect connections can
be used in conjunction with Cloud Router to establish a connection between on-premises networks and Google Cloud VPC
networks.
upvoted 
4 
times
neokyle
neokyle
 
1 year, 6 months ago
Selected Answer: 
D
EHR is supposed to be massive in size. so the option of 100 GBps / Dedicated is warranted.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
exam9391
exam9391
 
1 year, 8 months ago
Selected Answer: 
DSelected Answer: 
D
Business requirements for this case:
* Provide a minimum 99.9% availability for all customer-facing systems.
* Provide a secure and high-performance connection between on-premises systems and Google Cloud.
A. - builds us a 99.9% SLA partner interconnect, covering all business requirements.
B. - VPN is not suitable for the business requirements.
C. - Direct peering is used for workspace, instead of DMZ, again - not suitable.
D. - builds us a 99.99% SLA dedicated interconnect, covering all business requirements.
The answer to choosing A or D lies in the question, stating: "You want to follow Google's recommended practices for
production-level applications."
Google recommends using the 99.99% SLA interconnect (dedicated or partner) for production-level applications as stated here:
https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview
The answer is D.
upvoted 
9 
times
deepdowndave
deepdowndave
 
1 year, 9 months ago
Selected Answer: 
A
The case study requires 99.9% availability. Only the setup with two partner interconnects in two metro zones serves 99.9%
https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/partner-creating-999-availability
D is wrong since 99.9% availability does not require 4 interconnects in two metros and case study mentions to keep costs low.
upvoted 
2 
times
jahiye3916
jahiye3916
 
1 year, 8 months ago
"Google's recommended practices for production-level applications"
https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview
Google's recommended practice is to use 4 Interconnect connections split across two regions. Answer D mentions 2
Interconnect connections in one metro/city (region) and another 2 Interconnect connections in another metro (region), which
is clearly referring to Google's recommended practice.
upvoted 
3 
times
kuboraam
kuboraam
 
1 year, 10 months ago
Selected Answer: 
A
two points to note:
- requirements says a minimum of 99.9% (not 99.99%)
- also, "Decrease infrastructure administration costs."
upvoted 
2 
times
rmahendra
rmahendra
 
1 year, 10 months ago
I think D is more suitable because it requires low latency and more spread out datacenter locations. Moreover, it is not
necessarily the location of the legacy datacenter support provider interconnect
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
D
D seems to be the correct answer
upvoted 
2 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
D
vote D
upvoted 
4 
times
rishab86
rishab86
 
2 years, 9 months ago
case study says " Provide a minimum 99.9% availability for all customer-facing systems", i think minimum is the keyword , hence
I would go with D
upvoted 
3 
timesupvoted 
3 
times
BrijMohan08
BrijMohan08
 
2 years, 9 months ago
Both A and D will work, but they want the SLA 99.9% and keep the cost low, which is possible with A (cost low)
upvoted 
1 
times
BrijMohan08
BrijMohan08
 
2 years, 9 months ago
OPEX low
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 4
Question #5
For this question, refer to the EHR Healthcare case study. You are a developer on the EHR customer portal team. Your team
recently migrated the customer portal application to Google Cloud. The load has increased on the application servers, and now
the application is logging many timeout errors. You recently incorporated Pub/Sub into the application architecture, and the
application is not logging any Pub/Sub publishing errors. You want to improve publishing latency. 
What should you do? 
A. 
Increase the Pub/Sub Total Timeout retry value.
B. 
Move from a Pub/Sub subscriber pull model to a push model.
C. 
Turn off Pub/Sub message batching. 
Most Voted
D. 
Create a backup Pub/Sub message queue.
Correct Answer:
 
C 
Comments
raf2121
raf2121
 
Highly Voted
 
3 years, 4 months ago
Answer : C 
(https://cloud.google.com/pubsub/docs/publisher?hl=en#batching)
Cost of Batching is latency for individual messages,. To minimize latency batching should be turned off
upvoted 
37 
times
A21325412
A21325412
 
1 year, 1 month ago
Updated link:
https://cloud.google.com/pubsub/docs/publish-best-practices?hl=en#configure-batch
upvoted 
4 
times
gingerbeer
gingerbeer
 
Highly Voted
 
3 years, 3 months ago
C - The cost of batching is latency for individual messages, which are queued in memory until their corresponding batch is filled
and ready to be sent over the network. To minimize latency, batching should be turned off.
https://cloud.google.com/pubsub/docs/publisher?hl=en#batching
A incorrect. Application timeout because of publisher latency, nothing to do with timeout retry with publish request.
D does not make sense at all.
B is about receiver, not publisher.
Community vote distribution
C (72%)
B (16%)
A (12%)B is about receiver, not publisher.
upvoted 
14 
times
desertlotus1211
desertlotus1211
 
Most Recent
 
1 month ago
Selected Answer: 
A
The issue at hand is increased load causing timeout errors during interactions with Pub/Sub. The lack of Pub/Sub publishing
errors indicates that the problem is likely related to retries or network performance. By increasing the Total Timeout retry value,
you allow the application more time to complete message publishing during high-load scenarios, reducing the chances of
timeouts
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 1 week ago
Selected Answer: 
C
"the application is not loggin any Pub/Sub publishing errors", so no need to increase Pub/Sub Total Timeout.
upvoted 
2 
times
a53fd2c
a53fd2c
 
8 months, 4 weeks ago
Latency in Pub/Sub can be of two types:
End-to-end latency is the time it takes for a message to be published by a publisher and delivered to the corresponding
subscribers for processing.
Publish latency is the amount of time it takes to publish a message.
When using batching, increasing both types of latencies is a trade off for improving efficiency and throughput.
upvoted 
3 
times
decw
decw
 
1 year ago
Selected Answer: 
C
C
https://cloud.google.com/pubsub/docs/publish-best-practices#configure-batch
upvoted 
2 
times
Prakzz
Prakzz
 
1 year, 2 months ago
Selected Answer: 
A
See Solution #1 below link
https://saturncloud.io/blog/how-to-fix-deadlineexceeded-when-publishing-to-a-cloud-pubsub-topic-from-compute-
engine/#:~:text=The%20timeout%20limit%20for%20publishing,receive%20the%20DEADLINE_EXCEEDED%20error%20message.
upvoted 
1 
times
TopTalk
TopTalk
 
1 year, 3 months ago
Selected Answer: 
C
See video "Cloud Pub/Sub Publishers - ep. 4" at 2m 25 sec where Priyanka says "to reduce latency, batching should be turned
off" 
https://www.youtube.com/watch?v=ML6P1ksHcqo&list=PLIivdWyY5sqKwVLe4BLJ-vlh9r9zCdOse&index=4. 
Batching
increases throughput but adds latency for individual messages as they are queued in memory until their batch is filled.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
I voted C but I think it has to be A. "Total timeout: the amount of time after a client library stops retrying publish requests."
"After each publish request, the request timeout increases by the request timeout multiplier, up to the maximum request
timeout."
So it's increasing the timeout value on the retry which seems like the best solution.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 8 months ago
Selected Answer: 
B
B. Move from a Pub/Sub subscriber pull model to a push model.
Explanation:
Moving from a pull model to a push model in Google Cloud Pub/Sub can help improve the latency in your application. In aMoving from a pull model to a push model in Google Cloud Pub/Sub can help improve the latency in your application. In a
push model, the messages are pushed from the Pub/Sub service to the subscriber application, reducing the time it takes for the
application to receive messages. This can help mitigate the timeout errors that you are experiencing due to increased load on
the application servers.
upvoted 
5 
times
JC0926
JC0926
 
1 year, 8 months ago
Option A, increasing the Pub/Sub Total Timeout retry value, would not address the latency issue directly; it would only increase
the time the publisher would wait for a response before considering it a failure.
Option C, turning off Pub/Sub message batching, might actually increase latency and decrease throughput, as batching can
improve the efficiency of message delivery.
Option D, creating a backup Pub/Sub message queue, would not solve the latency issue directly; it might provide a failover
mechanism but would not address the root cause of the problem.
upvoted 
1 
times
MaryMei
MaryMei
 
1 year, 9 months ago
Selected Answer: 
A
This is chatgpt's choice
I agree that Pub/Sub message batching can be a useful optimization for improving overall throughput and reducing the
number of API calls required to publish messages. However, in the context of addressing timeout errors during publishing,
turning off message batching may not be the most appropriate solution.
In cases where message batching is causing issues, such as network or system resource constraints, reducing the batch size or
adjusting the batch duration can help improve publishing latency. However, in the case of timeout errors, increasing the Total
Timeout retry value would be a more effective solution, as it allows more time for the message to be successfully published
and reduces the likelihood of encountering timeout errors.
upvoted 
2 
times
afxwin
afxwin
 
1 year ago
Have you read ChatGPT's disclaimer?
upvoted 
5 
times
omermahgoub
omermahgoub
 
2 years ago
To improve publishing latency in this scenario, it is recommended to turn off Pub/Sub message batching. By turning off
message batching, you can send messages individually as soon as they are published, rather than waiting for a batch of
messages to be created before sending them. This can help to reduce the risk of timeout errors and improve the overall
performance of the application. It is also a good idea to monitor the application's performance and error logs to identify any
other potential issues that may be contributing to the timeout errors.
upvoted 
5 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
Prashant2022
Prashant2022
 
2 years ago
but how is this related to turnoff batching??
upvoted 
1 
times
Prashant2022
Prashant2022
 
2 years ago
Let me ans: becuz we need to speed up the time to deliver the msgs to the app! and it waits and timesout..
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Tesla
Tesla
 
2 years, 2 months agoBut no where in the question or scenario it says they turned on Pub/Sub batching.
upvoted 
1 
times
BlankSong
BlankSong
 
2 years ago
Batch messaging is enabled by default in a client library.
https://cloud.google.com/pubsub/docs/publisher?hl=en#batching
upvoted 
3 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
C
C is better option, even though increasing total timeout would help reduce timeout errors but remember that that in this case
we are getting too many messages from the server since load increased and we need to reduce latency
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 4
Question #6
For this question, refer to the EHR Healthcare case study. In the past, configuration errors put public IP addresses on backend
servers that should not have been accessible from the Internet. You need to ensure that no one can put external IP addresses
on backend Compute Engine instances and that external IP addresses can only be configured on frontend Compute Engine
instances. What should you do? 
A. 
Create an Organizational Policy with a constraint to allow external IP addresses only on the frontend Compute Engine
instances. 
Most Voted
B. 
Revoke the compute.networkAdmin role from all users in the project with front end instances.
C. 
Create an Identity and Access Management (IAM) policy that maps the IT staff to the compute.networkAdmin role for the
organization.
D. 
Create a custom Identity and Access Management (IAM) role named GCE_FRONTEND with the compute.addresses.create
permission.
Correct Answer:
 
A 
Comments
rvopoqvmtlwdlzrqxr
rvopoqvmtlwdlzrqxr
 
Highly Voted
 
2 years, 10 months ago
A - configuration by Organization policy service
upvoted 
23 
times
Snowball998877
Snowball998877
 
Highly Voted
 
1 year, 10 months ago
It's A.
Following is from Google page:
"Using an Organization Policy, you can restrict external IP addresses to specific VMs with constraints to control use of external IP
addresses for your VM instances within an organization or a project."
upvoted 
8 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
Community vote distribution
A (100%)upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is the clear answer as per google recommendation
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
I will choose A
upvoted 
1 
times
chickennuggets
chickennuggets
 
1 year, 10 months ago
Compute Network admin role info:
https://cloud.google.com/compute/docs/access/iam#compute.networkAdmin I think it may be B
upvoted 
1 
times
chickennuggets
chickennuggets
 
1 year, 10 months ago
D cant be right - A is closet per https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-
address#disableexternalip
There are some risks to not being able to create new MiG and GKE clusters
upvoted 
1 
times
andre123
andre123
 
2 years, 1 month ago
the question say " to ensure no one can put external IP addresses on backend Compute Engine instances and that external IP
addresses can only be configured on frontend Compute Engine instances ". I think D. what do you think
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 5 months ago
Selected Answer: 
A
vote A
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
I think A is correct.
https://cloud.google.com/blog/ja/products/identity-security/limiting-public-ips-google-cloud
upvoted 
2 
times
Pime13
Pime13
 
2 years, 6 months ago
Selected Answer: 
A
A ->
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip
upvoted 
3 
times
[Removed]
[Removed]
 
2 years, 7 months ago
I'm not sure if A is correct. 
From the doc posted in the discussion (https://cloud.google.com/compute/docs/ip-
addresses/reserve-static-external-ip-address#disableexternalip); the organization policy only applies to created instances and
won't apply if they're recreated. 
So it doesn't seem like an option that *prevents* the creation of non-public instances.
upvoted 
1 
times[Removed]
[Removed]
 
2 years, 7 months ago
From that link: 
"Specifications
You can only apply this list constraint to VM instances.
You cannot apply the constraint retroactively. All VM instances that have external IP addresses before the policy is enabled
retain their external IP address.
This constraint accepts either an allowedList or a deniedList but not both in the same policy.
It is up to you or an administrator with the required permissions to manage and maintain the instance lifecycle and integrity.
The constraint only verifies the instance's URI, and it does not prevent the allowlisted VMs from being altered, deleted, or
recreated."
upvoted 
2 
times
pakilodi
pakilodi
 
2 years, 7 months ago
Selected Answer: 
A
Vote A
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
8 
times
[Removed]
[Removed]
 
2 years, 8 months ago
A is right. D does not define any rule so it is not making any sense.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 4
Question #7
For this question, refer to the EHR Healthcare case study. You are responsible for designing the Google Cloud network
architecture for Google Kubernetes 
Engine. You want to follow Google best practices. Considering the EHR Healthcare business and technical requirements, what
should you do to reduce the attack surface? 
A. 
Use a private cluster with a private endpoint with master authorized networks configured. 
Most Voted
B. 
Use a public cluster with firewall rules and Virtual Private Cloud (VPC) routes.
C. 
Use a private cluster with a public endpoint with master authorized networks configured.
D. 
Use a public cluster with master authorized networks enabled and firewall rules.
Correct Answer:
 
A 
Comments
jask
jask
 
Highly Voted
 
3 years, 3 months ago
It should be A. 
Public endpoint access disabled is the most secure option as it prevents all internet access to the control plane. This is a good
choice if you have configured your on-premises network to connect to Google Cloud using Cloud Interconnect (EHR has
enabled this) or Cloud VPN.
If you disable public endpoint access, then you must configure authorized networks for the private endpoint. If you don't do
this, you can only connect to the private endpoint from cluster nodes or VMs in the same subnet as the cluster. 
Public endpoint access enabled, authorized networks enabled: This is a good choice if you need to administer the cluster from
source networks that are not connected to your cluster's VPC network using Cloud Interconnect or Cloud VPN (but EHR is
already using interconnect) So answer C is wrong. 
Reference- 
https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
upvoted 
69 
times
turbo8p
turbo8p
 
2 years, 1 month ago
Agreed with this answer but just one thing to point out. 
I can't find any info mention that "EHR is already using interconnect".
So this should not be use as the main factor to make a decision.
upvoted 
2 
times
bogdant
bogdant
 
2 years, 11 months ago
Community vote distribution
A (66%)
C (34%)I agree with @Jask's answer above. 
According to the documentation, answer A is the most secure and in my opinion correct: "Public endpoint access disabled:
This is the most secure option as it prevents all internet access to the control plane. This is a good choice if you have
configured your on-premises network to connect to Google Cloud using Cloud Interconnect or Cloud VPN."
I just don't understand why so many people voted C. 
Ref: https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept#overview ,
upvoted 
2 
times
Begum
Begum
 
2 years, 3 months ago
Configure NAT as mater authorized networks
upvoted 
1 
times
BalaGCPArch
BalaGCPArch
 
2 years, 1 month ago
"Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes
clusters" This statement in the case study tells it needs to be Public, So i assume the answer should be A
upvoted 
3 
times
victory108
victory108
 
Highly Voted
 
3 years, 4 months ago
A. Use a private cluster with a private endpoint with master authorized networks configured.
--> Private clusters run nodes without external IP addresses, and optionally run their cluster control plane without a publicly-
reachable endpoint. Additionally, private clusters do not allow Google Cloud IP addresses to access the control plane endpoint
by default. Using private clusters with authorized networks makes your control plane reachable only by the allowed CIDRs, by
nodes within your cluster's VPC, and by Google's internal production jobs that manage your control plane.
upvoted 
8 
times
192dcc7
192dcc7
 
Most Recent
 
3 months, 3 weeks ago
Selected Answer: 
A
They do not have Interconnect today. But considering high-performance network requirements it will be interconnect. if
interconnect is there no reason to have public end point enabled for cluster management. Public end point is never enabled on
on a cloud system (for almost any service) for any medium/large scale enterprise.
upvoted 
2 
times
someone2011
someone2011
 
1 year, 4 months ago
A: https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
Public endpoint access disabled: This is the most secure option as it prevents all internet access to the control plane. This is a
good choice if you have configured your on-premises network to connect to Google Cloud using Cloud Interconnect or Cloud
VPN.
If you disable public endpoint access, then you must configure authorized networks for the private endpoint. If you don't do
this, you can only connect to the private endpoint from cluster nodes or VMs in the same subnet as the cluster.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
A seems the most secure as it's the only option that makes access to the control plane private
using authorized networks work to limit access even further.
Although the nodes are private the pods can still be accessed via an externally exposed service
"An external client with a source IP address on the internet can connect to an external Service of type LoadBalancer"
upvoted 
1 
times
JC0926
JC0926
 
1 year, 8 months ago
Selected Answer: 
A
A. Use a private cluster with a private endpoint with master authorized networks configured.
Using a private cluster with a private endpoint and master authorized networks configured is the best way to reduce the attack
surface in Google Kubernetes Engine (GKE). A private cluster ensures that the nodes have private IP addresses, which are not
accessible from the internet. The private endpoint allows access to the GKE API server only within the same VPC or through a
secure connection (e.g., VPN or VPC peering). Configuring master authorized networks restricts access to the GKE control plane
to specific CIDR blocks, further securing the environment and adhering to EHR Healthcare's business and technical
requirements.
upvoted 
4 
times
rr4444
rr4444
 
1 year, 9 months agorr4444
rr4444
 
1 year, 9 months ago
Selected Answer: 
C
C is correct. A is wrong because, despite what everyone is thinking, because you cannot have a private endpoint for the control
plane WITH authorised networks. It's a contradiction of ideas. The authorised networks are specifically to manage access to a
public endpoint to only a set RFC1918 addresses, for example. Which, ironically, is covered by the link that everyone is pasting
referring to answer A https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
upvoted 
5 
times
pigracer
pigracer
 
1 year, 8 months ago
from the link:
"If you disable public endpoint access, then you must configure authorized networks for the private endpoint. If you don't do
this, you can only connect to the private endpoint from cluster nodes or VMs in the same subnet as the cluster. With this
setting, authorized networks must be internal IP addresses."
I combed through the documentation to see what you were saying but couldn't find it and only found this. So I think it's A
upvoted 
7 
times
Jeena345
Jeena345
 
1 year, 10 months ago
Selected Answer: 
A
Public endpoint access disabled is the most secure option as it prevents all internet access to the control plane. This is a good
choice if you have configured your on-premises network to connect to Google Cloud using Cloud Interconnect (EHR has
enabled this) or Cloud VPN.
If you disable public endpoint access, then you must configure authorized networks for the private endpoint. If you don't do
this, you can only connect to the private endpoint from cluster nodes or VMs in the same subnet as the cluster.
Public endpoint access enabled, authorized networks enabled: This would be a good choice if you need to administer the
cluster from source networks that are not connected to your cluster's VPC network (using Cloud Interconnect or Cloud VPN) but
EHR is already using interconnect!
Reference:
https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
upvoted 
2 
times
RVivek
RVivek
 
1 year, 10 months ago
Selected Answer: 
C
Private cluter will have only one end Private end point and 
It is not poosible to autherize any specific Master network
upvoted 
1 
times
tdotcat
tdotcat
 
1 year, 11 months ago
Selected Answer: 
A
A is good
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
To reduce the attack surface and follow Google's best practices for network architecture in Google Kubernetes Engine, you
should use a private cluster with a private endpoint and configure master authorized networks.
Private clusters allow you to create clusters with nodes that are not reachable from the public internet. This reduces the attack
surface by making it more difficult for an attacker to target the nodes. Additionally, by using a private endpoint and
configuring master authorized networks, you can further restrict access to the cluster to only authorized users and networks. This
helps to ensure that only authorized users and systems can access the cluster and helps to prevent unauthorized access.
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
Using a public cluster with firewall rules and Virtual Private Cloud (VPC) routes may provide some level of security, but it does
not provide the same level of protection as a private cluster. Similarly, using a private cluster with a public endpoint and
master authorized networks can also provide some level of security, but it is not as secure as using a private cluster with a
private endpoint and master authorized networks. In summary, to reduce the attack surface and follow best practices, it is
recommended to use a private cluster with a private endpoint and configure master authorized networks.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is okA is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A is the best answer https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept#overview
upvoted 
1 
times
abdelilahfa
abdelilahfa
 
2 years, 2 months ago
Selected Answer: 
C
Public endpoint access enabled, authorized networks enabled (recommended): This option provides restricted access to the
control plane from source IP addresses that you define. This is a good choice if you don't have existing VPN infrastructure or
have remote users or branch offices that connect over the public internet instead of the corporate VPN and Cloud Interconnect
or Cloud VPN.
https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-
cluster#restrict_network_access_to_the_control_plane_and_nodes
upvoted 
3 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
2 years, 4 months ago
Selected Answer: 
A
I'll go with A as it is the most secure option. C would be more cost-effective for when EHR has no plans for Cloud Interconnect /
VPN (which they do!).
upvoted 
1 
times
RitwickKumar
RitwickKumar
 
2 years, 4 months ago
Selected Answer: 
A
Why would we need access to control plane from outside. It is better to keep everything private and expose the web/ui through
an external ingress.
upvoted 
3 
times
cdcollector
cdcollector
 
2 years, 6 months ago
Putting a Autn Network on a private endpoint is moot
Note: Authorized networks block untrusted IP addresses from outside Google Cloud. Addresses from inside Google Cloud (such
as traffic from Compute Engine virtual machines (VMs), Cloud Functions and Cloud Run) can reach your control plane using
HTTPS, provided that they have the necessary Kubernetes credentials.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 5
Question #1
Mountkirk Games wants you to design their new testing strategy. How should the test coverage differ from their existing
backends on the other platforms? 
A. 
Tests should scale well beyond the prior approaches 
Most Voted
B. 
Unit tests are no longer required, only end-to-end tests
C. 
Tests should be applied after the release is in the production environment
D. 
Tests should include directly testing the Google Cloud Platform (GCP) infrastructure
Correct Answer:
 
A 
Comments
Smart
Smart
 
Highly Voted
 
4 years, 10 months ago
Tests should include directly "testing the Google Cloud Platform (GCP) infrastructure". I don't what does this mean? Testing
their resources running on GCP infrastructure or testing GCP services itself? Regardless, Option D suggests SLA requirements
which is not the problem. The problem is scalability that can be resolved through stress testing. Also, GCP services are indirectly
tested through it (Option D). Hence, I choose option A.
upvoted 
35 
times
elainexs
elainexs
 
2 years, 6 months ago
Same, no sense at all to test GCP infra
upvoted 
2 
times
ShadowLord
ShadowLord
 
2 years, 4 months ago
Can GCP Infrastructure configuration can be tested like Security, Firewall, Performance, Scaling, Failure ... Istion Failure
Injections ... 
A is too generic ... without any reference to what it means
upvoted 
4 
times
HD2023
HD2023
 
1 year, 9 months ago
"How should the test coverage differ from their existing backends on the other platforms?"
Test coverage. They are already testing scale. Testing GPA isn’t currently covered.
Community vote distribution
A (67%)
D (33%)Test coverage. They are already testing scale. Testing GPA isn’t currently covered.
upvoted 
1 
times
Karthic
Karthic
 
Highly Voted
 
5 years ago
Should be D, bcaz need to test GCP products too....
upvoted 
15 
times
tartar
tartar
 
4 years, 4 months ago
A is ok
upvoted 
16 
times
tartar
tartar
 
4 years, 4 months ago
" had problems scaling their global audience, application servers MySQL databases, and analytics tools."
upvoted 
5 
times
tartar
tartar
 
4 years, 4 months ago
sorry, changing to D
upvoted 
7 
times
ShadowLord
ShadowLord
 
2 years, 4 months ago
Configuration of GCP Platform, scaling, scale down, etc ,,,, so D ... A option is to generic 
.. Testing to scale beyond previous
approaches
upvoted 
1 
times
nitinz
nitinz
 
3 years, 10 months ago
It is D. New to GCP, migrated to GCP.... time to test if it works or not.
upvoted 
8 
times
Nick89GR
Nick89GR
 
Most Recent
 
2 months ago
We dont need to test the GCP infra since we are talking about managed services. A is correct.
upvoted 
1 
times
Harshithh
Harshithh
 
6 months ago
Selected Answer: 
A
option a suits
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
A
A
All other options are funny! D the most - we are charged to increase the test coverage and/or test strategy not for GCP infra :-)
upvoted 
3 
times
ArtistS
ArtistS
 
1 year, 1 month ago
If D is correct, A is correct too. A include D. If we only choose 1 option, it must be A.
upvoted 
2 
times
sampon279
sampon279
 
1 year, 6 months ago
Selected Answer: 
A
Tests should scale well beyond the prior approaches seems correct, since they are migrating to new GCP infrastructure they
should have higher scalability in tests. D - Tests should include directly testing the Google Cloud Platform (GCP) infrastructure
seems not required, if they test the app in GCP env that makes sense, just testing GCP infrastructure without any app code in
between is not needed.
upvoted 
1 
times
joesatriani
joesatriani
 
1 year, 6 months ago
Are these Mountkirk Games case questions still appearing on the exam?
upvoted 
4 
timesupvoted 
4 
times
taer
taer
 
1 year, 9 months ago
Selected Answer: 
A
As they expect the new game to be very popular, their testing strategy should be designed to scale beyond their previous
approaches to ensure that the game can handle the increased traffic and user demands.
upvoted 
3 
times
HD2023
HD2023
 
1 year, 9 months ago
Selected Answer: 
D
How should the test coverage differ from their existing backends on the other platforms?" 
Test coverage. They are already
testing scale. Testing GPA isn’t currently covered.
upvoted 
1 
times
JC0926
JC0926
 
1 year, 9 months ago
Selected Answer: 
D
DDDDDDDDD
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 9 months ago
Wording in Answer Choice D is intended to confuse. What is being tested is the ability of GCP to handle the scale that
Mountkirk needs as the infra of the prior cloud provider could not scale.
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 9 months ago
Selected Answer: 
D
The previous "cloud" provider could not scale so the need here is to ensure that GCP scales. A is meaningless and generic.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is A: Tests should scale well beyond the prior approaches.
Mountkirk Games' new game is expected to be very popular, so it's important to have a testing strategy that can handle a high
volume of users and ensure that the game can scale well. This means that the test coverage for the new game's backend should
be significantly higher than the test coverage for their existing backends on other platforms.
Unit tests and end-to-end tests are both important for ensuring the quality and reliability of the game's backend, so both types
of tests should be included in the testing strategy. Testing the GCP infrastructure directly may not be necessary, as GCP provides
managed services that are expected to be reliable and well-maintained.
It's generally not recommended to perform testing only in the production environment, as this can potentially cause problems
for live users and result in lost revenue. Instead, it's important to test the game's backend thoroughly in a staging or testing
environment before deploying it to production.
upvoted 
6 
times
habros
habros
 
2 years, 1 month ago
Selected Answer: 
A
A is essential as customer want a scalable system
B&C does not make sense from testing perspective. 
D is remotely impossible as it might breach acceptable use policy conditions… best to check with Google before doing so.
upvoted 
2 
times
BeCalm
BeCalm
 
1 year, 9 months ago
You don't need permission to stress test your app on GCP.
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
2 
timesupvoted 
2 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A makes sense to me
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 5
Question #2
Mountkirk Games has deployed their new backend on Google Cloud Platform (GCP). You want to create a through testing
process for new versions of the backend before they are released to the public. You want the testing environment to scale in
an economical way. How should you design the process? 
A. 
Create a scalable environment in GCP for simulating production load 
Most Voted
B. 
Use the existing infrastructure to test the GCP-based backend at scale
C. 
Build stress tests into each component of your application using resources internal to GCP to simulate load
D. 
Create a set of static environments in GCP to test different levels of load 
ג
 "€for example, high, medium, and low
Correct Answer:
 
A 
Comments
shandy
shandy
 
Highly Voted
 
4 years, 7 months ago
A is correct because simulating production load in GCP can scale in an economical way.
B is not correct because one of the pain points about the existing infrastructure was precisely that the environment did not
scale well. C is not correct because it is a best practice to have a clear separation between test and production environments.
Generating test load should not be done from a production environment. D is not correct because Mountkirk Games wants the
testing environment to scale as needed. Defining several static environments for specific levels of load goes against this
requirement.
upvoted 
40 
times
nitinz
nitinz
 
3 years, 4 months ago
A is correct.
upvoted 
2 
times
mlantonis
mlantonis
 
Highly Voted
 
4 years ago
The question is taken from Google Practice Exam
A: is correct because simulating production load in GCP can scale in an economical way. 
B: is not correct because one of the pain points about the existing infrastructure was precisely that the environment did not
scale well. 
Community vote distribution
A (86%)
C (14%)C: is not correct because it is a best practice to have a clear separation between test and production environments. Generating
test load should not be done from a production environment. 
D: is not correct because Mountkirk Games wants the testing environment to scale as needed. Defining several static
environments for specific levels of load goes against this requirement.
upvoted 
17 
times
elainexs
elainexs
 
2 years ago
C option doesn't mention if that takes production load.
upvoted 
4 
times
didek1986
didek1986
 
Most Recent
 
5 months, 2 weeks ago
Selected Answer: 
A
No doubts
upvoted 
1 
times
e5019c6
e5019c6
 
6 months, 1 week ago
Selected Answer: 
C
People are saying that test and production environments must be separate, and that's correct, but I don't see where in C it's
speaking about the production environment. It's speaking about stress testing each component by itself to see if it's able to
handle the expected load. So I'm sticking with C.
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 3 months ago
Not sure why C is not correct. That is the approach for testing an app in a non-prod environment.
upvoted 
3 
times
gltestis
gltestis
 
2 weeks, 3 days ago
IMHO it is not correct because it is testing individual components.
upvoted 
1 
times
e5019c6
e5019c6
 
6 months, 1 week ago
I think the same. People are saying that test and production environments must be separate, and that's correct, but I don't see
where in C it's speaking about the production environment. It's speaking about stress testing each component by itself to see if
it's able to handle the expected load.
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The correct answer is A: Create a scalable environment in GCP for simulating production load.
To create a thorough testing process for new versions of the backend before they are released to the public, it's important to
simulate production load in the testing environment. This will help ensure that the backend can handle the expected volume of
users and scale appropriately.
One way to do this is to create a scalable environment in GCP that can simulate production load. This could involve using GCP
services such as Google Compute Engine or Google Kubernetes Engine to spin up multiple virtual machines or containers to
simulate a high volume of requests and traffic. This approach will allow you to test the backend at scale and ensure that it can
handle the expected load.
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Using the existing infrastructure to test the GCP-based backend at scale may not be feasible, as the existing infrastructure may
not be capable of simulating the expected production load. Building stress tests into each component of the application using
internal GCP resources could be time-consuming and may not provide a realistic simulation of production load. Creating a set
of static environments in GCP to test different levels of load may not be sufficient for testing the backend at scale, as the load
may not be consistent with what is expected in production.
upvoted 
1 
times
gonlafer
gonlafer
 
1 year, 7 months ago
Selected Answer: 
A
A is the right answerA is the right answer
upvoted 
1 
times
megumin
megumin
 
1 year, 8 months ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
I would like to choose A because it satisfy the given requirements
upvoted 
2 
times
Nirca
Nirca
 
1 year, 9 months ago
Selected Answer: 
A
A is correct because simulating production load in GCP can scale in an economical way.
upvoted 
1 
times
Nirca
Nirca
 
1 year, 9 months ago
The question is taken from Google Practice Exam
A: is correct because simulating production load in GCP can scale in an economical way.
upvoted 
3 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A
upvoted 
3 
times
Yogikant
Yogikant
 
3 years, 1 month ago
Answer: A
upvoted 
1 
times
victory108
victory108
 
3 years, 1 month ago
A. Create a scalable environment in GCP for simulating production load
upvoted 
2 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 5
Question #3
Mountkirk Games wants to set up a continuous delivery pipeline. Their architecture includes many small services that they
want to be able to update and roll back quickly. Mountkirk Games has the following requirements: 
  Services are deployed redundantly across multiple regions in the US and Europe 
  Only frontend services are exposed on the public internet 
  They can provide a single frontend IP for their fleet of services 
  Deployment artifacts are immutable 
Which set of products should they use? 
A. 
Google Cloud Storage, Google Cloud Dataflow, Google Compute Engine
B. 
Google Cloud Storage, Google App Engine, Google Network Load Balancer
C. 
Google Kubernetes Registry, Google Container Engine, Google HTTP(S) Load Balancer 
Most Voted
D. 
Google Cloud Functions, Google Cloud Pub/Sub, Google Cloud Deployment Manager
Correct Answer:
 
C 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
5 years, 2 months ago
Correct answer is C.
upvoted 
47 
times
nitinz
nitinz
 
3 years, 10 months ago
C is correct
upvoted 
4 
times
rishab86
rishab86
 
3 years, 4 months ago
Answer is C but options should have been Google Kubernetes Engine, Google Container Registry, Google HTTP(S) Load
Balancer
upvoted 
43 
times
squishy_fishy
squishy_fishy
 
1 year ago
Community vote distribution
C (100%)Answer is C, but Container Registry is deprecated, Google is using Artifact Registry now.
upvoted 
3 
times
kolcsarzs
kolcsarzs
 
Highly Voted
 
5 years ago
If the Question is erroneously formulated, and they mean Google Container Registry and Google Kubernetes Engine, then C is
the right answer
upvoted 
33 
times
Kysmor
Kysmor
 
3 years, 10 months ago
Maybe is not: they could have mispelled on purpose! ;)
upvoted 
2 
times
Revedeep
Revedeep
 
4 years, 6 months ago
Yeah Exactly! The choice was confusing.
upvoted 
3 
times
Toothpick
Toothpick
 
Most Recent
 
5 months, 1 week ago
How does C work? 
There is no such thing as Google Kubernetes Registry
Google Container Engine is the underlying virtualization stack, not a service you can use.
The whole terminology is completely off here, completely outdated
upvoted 
1 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Selected Answer: 
C
Easy easy
upvoted 
1 
times
don_v
don_v
 
11 months, 3 weeks ago
Refer to this: "Mountkirk Games makes online, session-based, multiplayer games for the most popular *mobile* platforms."
Who says it's web-based, so it's required to have HTTP(S) Load Balancer? The games are for mobile devices. TCP/UDP
protocols would suffice.
I bet they (MountKirk Games or EA ;0 ) just also want to collect non-proxied IP addresses from the players. Also, for the mobile
platform games, the requests most likely use UDP protocol for inter-player communication/chat. This filter out HTTP(S) proxied
Load Balancer effectively, as it has to be an external Network Load Balancer which allow UDP and pass-through requests.
Seems, like all answers are wrong.
upvoted 
2 
times
rr4444
rr4444
 
1 year, 9 months ago
Selected Answer: 
C
Oooooh. Old skool wording
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 9 months ago
How is C the answer? No such thing as Google Kubernetes Registry and Google Container Engine
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is C: Google Kubernetes Registry, Google Container Engine, Google HTTP(S) Load Balancer.
To meet the requirements listed in the question, Mountkirk Games should use Google Kubernetes Registry to store their
immutable deployment artifacts, Google Container Engine to run their services in a Kubernetes cluster, and Google HTTP(S)
Load Balancer to expose their frontend services on the public internet and provide a single frontend IP for their fleet of services.
upvoted 
6 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
Cok for C
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is correct
upvoted 
1 
times
Nirca
Nirca
 
2 years, 3 months ago
Selected Answer: 
C
Only C and we will see.
upvoted 
3 
times
Nirca
Nirca
 
2 years, 5 months ago
Selected Answer: 
C
HTTP(S) Load Balancer is GLOBAL !!!!
upvoted 
1 
times
erika_vazquez
erika_vazquez
 
3 years ago
A is not correct because Mountkirk Games wants to set up a continuous delivery pipeline, not a data processing pipeline. Cloud
Dataflow is a fully managed service for creating data processing pipelines.
B is not correct because a Cloud Load Balancer distributes traffic to Compute Engine instances. App Engine and Cloud Load
Balancer are parts of different solutions.
C is correct because:
-Google Kubernetes Engine is ideal for deploying small services that can be updated and rolled back quickly. It is a best
practice to manage services using immutable containers. -Cloud Load Balancing supports globally distributed services across
multiple regions. It provides a single global IP address that can be used in DNS records. Using URL Maps, the requests can be
routed to only the services that Mountkirk wants to expose. -Container Registry is a single place for a team to manage Docker
images for the services.
D is not correct because you cannot reserve a single frontend IP for cloud functions. When deployed, an HTTP-triggered cloud
function creates an endpoint with an automatically assigned IP.
upvoted 
14 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
mudot
mudot
 
3 years, 1 month ago
Selected Answer: 
C
"They can provide a single frontend IP for their fleet of services"
only C can do that
upvoted 
3 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
C
vote C
upvoted 
2 
times
ACE_ASPIRE
ACE_ASPIRE
 
3 years, 3 months ago
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 5
Question #4
Mountkirk Games' gaming servers are not automatically scaling properly. Last month, they rolled out a new feature, which
suddenly became very popular. A record number of users are trying to use the service, but many of them are getting 503 errors
and very slow response times. What should they investigate first? 
A. 
Verify that the database is online
B. 
Verify that the project quota hasn't been exceeded
C. 
Verify that the new feature code did not introduce any performance bugs 
Most Voted
D. 
Verify that the load-testing team is not running their tool against production
Correct Answer:
 
C 
Comments
sri007
sri007
 
Highly Voted
 
4 years, 11 months ago
Ans is B
upvoted 
25 
times
Ziegler
Ziegler
 
Highly Voted
 
4 years, 7 months ago
B is the correct answer
Error code starting like 5xx is something related to server
503 UNAVAILABLE Service unavailable. Typically the server is down.
upvoted 
11 
times
AGG
AGG
 
3 years, 10 months ago
When server is down you will get timeout (503 - service unavailable - not server)
upvoted 
2 
times
bshmndl
bshmndl
 
Most Recent
 
6 days, 13 hours ago
Selected Answer: 
B
B is correct as the statement is "but many of them are getting 503 errors and very slow response times" this clearly states that
few of the requests are getting passed and are slow which can be caused due to load on the servers and not having enough
quota to spin up additional instances.
Community vote distribution
C (45%)
B (41%)
D (14%)quota to spin up additional instances.
upvoted 
2 
times
dfizban
dfizban
 
2 months, 3 weeks ago
Selected Answer: 
B
It's B
upvoted 
2 
times
mani14287
mani14287
 
7 months ago
Selected Answer: 
B
When you encounter the 503 error, it means that the server in question is unavailable. That could be because it's too busy, for
example, or it's under maintenance. Unlike other similar error codes, 503 signifies that your website is online and running, but
can't be reached at the present moment
upvoted 
3 
times
666Amitava666
666Amitava666
 
8 months, 1 week ago
Selected Answer: 
B
Its very common that the Gateway goes down and throws 503 if the load on the server is beyond limit. Performance Bug is a
good option but it will take a lot of time. Its always better to start debugging from the "step-1"; checking the project quota will
be faster, hence, I go with B
upvoted 
3 
times
rafalmajewski
rafalmajewski
 
8 months, 2 weeks ago
Selected Answer: 
C
In my opinion C
upvoted 
2 
times
dija123
dija123
 
8 months, 3 weeks ago
Selected Answer: 
C
I vote for C
upvoted 
2 
times
5091a99
5091a99
 
9 months, 3 weeks ago
B. 
"What should the team investigate first?" Quota limits are easy to check, and considering the scaling issues this could be the
cause even though Google Typically responds with a 429 to 'over Quota' issues.
Checking for bugs can be very time consuming, especially if the bugs are related to scale or are intermittent. Go with the first
easy thing to check... B.
upvoted 
2 
times
Amrita2012
Amrita2012
 
10 months, 3 weeks ago
Selected Answer: 
D
Such symptoms occur when servers are overloaded which might have caused because of load testing. hence option D would be
first thing I would do before I go into deep details of analyzing performance bug.
upvoted 
2 
times
ratsoft
ratsoft
 
11 months, 1 week ago
It cannot be A or B for Sure.
If you exceeded a quota value with an HTTP/REST request, Google Cloud returns an HTTP 429 TOO MANY REQUESTS status
code.
Still not sure between C & D. Will prefer D. Any thoughts?
upvoted 
3 
times
Pime13
Pime13
 
11 months, 1 week ago
Selected Answer: 
C
for me it's C. 
503 is service unavailable, for quota problem in GCP we get 429 (https://cloud.google.com/docs/quota/troubleshoot : If you
exceeded a quota value with an HTTP/REST request, Google Cloud returns an HTTP 429 TOO MANY REQUESTS status code.)A. Verify that the database is online - if db is down it would be down for all users
B. Verify that the project quota hasn't been exceeded Most Voted - quota error is 429
https://cloud.google.com/docs/quota/troubleshoot
C. Verify that the new feature code did not introduce any performance bugs - performance issues would translate into latency
issue and possible 503 
D. Verify that the load-testing team is not running their tool against production -
upvoted 
7 
times
0verK0alafied
0verK0alafied
 
8 months, 1 week ago
Thanks for this explanation.
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
Selected Answer: 
D
I think D is the correct one.
A couldn't be because all players would get the error, not only a few.
B I don't think it is because the Business objetives tell us that they want to scale globally. Setting a quota would limit their
scalability to reduce costs, and the CFO didn't set any cost-reducing objectives.
C is a good one to do, but would not be the first one. Searching the code for bugs is a task that would take long hours to
complete, so I would do it only if the firsts and obvious checks didn't find anything.
That leaves D as the first thing to check before going with C. Discarding A & B.
upvoted 
3 
times
ieboaix
ieboaix
 
10 months, 1 week ago
agree is D, quota error code are 4xx. 
https://cloud.google.com/docs/quotas/troubleshoot
upvoted 
1 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Lol  Three is always default quota SO B is correct
upvoted 
1 
times
don_v
don_v
 
11 months, 3 weeks ago
Finally, the correct voice here.
https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/503
"503 Service Unavailable
The HyperText Transfer Protocol (HTTP) 503 Service Unavailable server error response code indicates that the server is not
ready to handle the request.
Common causes are a server that is down for maintenance or that is *overloaded*."
upvoted 
1 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
C
There has been new feature implemented last month, when the requests reached new level the infra didn't scale well anymore, I
will first investigate that feature didn't cause performance issues or memory leaks.
upvoted 
2 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
B
It is B. They key here is "...which suddenly became very popular." Which might have caused a quota limit issue all sudden.
upvoted 
1 
times
Prakzz
Prakzz
 
1 year, 2 months ago
Selected Answer: 
D
I think it should be D. Load testing should not be done in Production and if it is getting dome there and actual high traffic
comes from users then application will 
crash.
upvoted 
3 
timesPKookNN
PKookNN
 
1 year, 4 months ago
Selected Answer: 
B
my focus is 'quickly investigated first' - quota is the easiest and quickest to check
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 5
Question #5
Mountkirk Games needs to create a repeatable and configurable mechanism for deploying isolated application environments.
Developers and testers can access each other's environments and resources, but they cannot access staging or production
resources. The staging environment needs access to some services from production. 
What should you do to isolate development environments from staging and production? 
A. 
Create a project for development and test and another for staging and production
B. 
Create a network for development and test and another for staging and production
C. 
Create one subnetwork for development and another for staging and production
D. 
Create one project for development, a second for staging and a third for production 
Most Voted
Correct Answer:
 
D 
Comments
shandy
shandy
 
Highly Voted
 
5 years, 1 month ago
Correct Answer is D. 
https://cloud.google.com/appengine/docs/standard/php/creating-separate-dev-environments
upvoted 
49 
times
nitinz
nitinz
 
3 years, 10 months ago
it is A
upvoted 
6 
times
tartar
tartar
 
4 years, 4 months ago
D is ok
upvoted 
9 
times
tartar
tartar
 
4 years, 4 months ago
not D, A
upvoted 
15 
times
 
3 years, 3 months ago
Community vote distribution
D (45%)
A (44%)
B (11%)ACE_ASPIRE
ACE_ASPIRE
 
3 years, 3 months ago
hey man...it should be D...
upvoted 
2 
times
KOERA99
KOERA99
 
3 years, 1 month ago
It's D!!!!
upvoted 
2 
times
Wonka
Wonka
 
2 years, 11 months ago
its standard but look at requirement given here
upvoted 
1 
times
euclid
euclid
 
Highly Voted
 
5 years ago
Correct is A
upvoted 
47 
times
TiagoM
TiagoM
 
3 years, 8 months ago
"The staging environment needs access to some services from production"
Its not the best practice, but A has less effort
upvoted 
8 
times
army234
army234
 
3 years, 9 months ago
Incorrect. Not a best practice to have Staging and Prod resources in the same project. D is correct
upvoted 
9 
times
Wonka
Wonka
 
2 years, 11 months ago
by standard it is absolutely incorrect but here it is requirement. will you still separate it out?
upvoted 
2 
times
AWS56
AWS56
 
4 years, 11 months ago
Agree with A
upvoted 
5 
times
walkwolf3
walkwolf3
 
2 years, 11 months ago
D
In the requirement, the staging environment needs access to production, not the other way around. Answer A could allow
staging and production to access each other. In answer D, staging and production are in different project, you can limit the
access from either side. So D is correct.
upvoted 
21 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 3 months ago
Best approach is D.
A will also work based on question requirement
upvoted 
4 
times
hogtrough
hogtrough
 
2 years, 11 months ago
End goal is to separate dev from staging/production. Putting staging/production in same project fits the requirements.
Further effort would be required to change access between Staging and Production projects that is out of scope of question. 
It is not best practice, but fits requirements of question.
upvoted 
8 
times
Wonka
Wonka
 
2 years, 11 months ago
yes and there is no mention of test environment in option D.
upvoted 
5 
times
 
 
2 weeks, 3 days agotask_7
task_7
 
Most Recent
 
2 weeks, 3 days ago
Selected Answer: 
A
D doesn't include test
upvoted 
1 
times
[Removed]
[Removed]
 
4 months, 2 weeks ago
Selected Answer: 
A
Special case
upvoted 
2 
times
nhatne
nhatne
 
6 months ago
Selected Answer: 
D
better to separate each environment, why have to merge them?
upvoted 
2 
times
dija123
dija123
 
8 months, 3 weeks ago
Selected Answer: 
A
We have to follow the case requirements not the best practices.
upvoted 
2 
times
yas_cloud
yas_cloud
 
9 months, 2 weeks ago
I guess this is a tie up between best practices vs what this org (Mountkirk) demands. If this was a standard question on how to
setup each env, then answer D fits. But for this use case, seems like A fits the bill. 
Further more, if all environments/projects are within same org setup, then resources can still be accesses across.
upvoted 
2 
times
lanjr01
lanjr01
 
10 months ago
Answer is D:
Question asked is - "What should you do to isolate development environments from staging and production?". If the question
meant to include "Test Environment", the question would have looked like this - "What should you do to isolate development &
test environments from staging and production?" but that is not the case so the only logical answer will be to vote D.
upvoted 
1 
times
Roro_Brother
Roro_Brother
 
1 year ago
Selected Answer: 
D
Definitively D
upvoted 
2 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
D
D
repeatable and configurable mechanism - so a network each for dev/test, staging and prod is ideal. But as this option is not
there I am choosing D - each one having a project.
upvoted 
1 
times
Ahmed_Safwat
Ahmed_Safwat
 
1 year, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
BisoWafik
BisoWafik
 
1 year, 1 month ago
Selected Answer: 
D
as per conventional best practices.
upvoted 
1 
times
Sarin
Sarin
 
1 year, 2 months ago
Selected Answer: 
AAlthough D is follows industry standards its an incomplete answer, it does not say anything about test. 
A is a much complete
answer
upvoted 
4 
times
CyanideX
CyanideX
 
1 year, 2 months ago
Selected Answer: 
D
D for sure
upvoted 
1 
times
CyanideX
CyanideX
 
1 year, 2 months ago
Selected Answer: 
D
D for me.
upvoted 
1 
times
b6f53d8
b6f53d8
 
1 year, 2 months ago
I think it is B, because you can create separate project for each environment (Test, dev, stage, prod) and connect to right
network. You can use firewall rules to set communication between stage and prod.
upvoted 
1 
times
Murtuza
Murtuza
 
1 year, 3 months ago
There is another thread on this topic to iron out the choices and its mostly leaning D
https://www.reddit.com/r/googlecloud/comments/kbkoev/gcp_question_environment_separations/
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 5
Question #6
Mountkirk Games wants to set up a real-time analytics platform for their new game. The new platform must meet their
technical requirements. 
Which combination of Google technologies will meet all of their requirements? 
A. 
Kubernetes Engine, Cloud Pub/Sub, and Cloud SQL
B. 
Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery 
Most Voted
C. 
Cloud SQL, Cloud Storage, Cloud Pub/Sub, and Cloud Dataflow
D. 
Cloud Dataproc, Cloud Pub/Sub, Cloud SQL, and Cloud Dataflow
E. 
Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc
Correct Answer:
 
B 
Comments
sri007
sri007
 
Highly Voted
 
4 years, 11 months ago
Correct Answer B
Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery
A real time requires Stream / Messaging so Pub/Sub, Analytics by Big Query. Ingest millions of streaming events per second
from anywhere in the world with Cloud Pub/Sub, powered by Google's unique, high-speed private network. Process the streams
with Cloud Dataflow to ensure reliable, exactly-once, low-latency data transformation. Stream the transformed data into
BigQuery, the cloud-native data warehousing service, for immediate analysis via SQL or popular visualization tools
upvoted 
33 
times
AWS56
AWS56
 
Highly Voted
 
4 years, 11 months ago
Agree with B
upvoted 
33 
times
tartar
tartar
 
4 years, 4 months ago
B is ok
upvoted 
9 
times
Community vote distribution
B (100%)nitinz
nitinz
 
3 years, 10 months ago
it is B
upvoted 
3 
times
VijKall
VijKall
 
Most Recent
 
4 months, 2 weeks ago
Use Case description has changed, this is old use case.
Analytics and BQ goes hand in hand, and since only B is using BG, B is still the close answer.
upvoted 
2 
times
someCloudUser
someCloudUser
 
1 year, 10 months ago
Selected Answer: 
B
Agree with B
upvoted 
2 
times
habros
habros
 
2 years, 1 month ago
Selected Answer: 
B
Real-time analytics = OLAP = Bigquery
upvoted 
2 
times
ashrafh
ashrafh
 
2 years, 1 month ago
Seems old case study, dosen't match with the below
https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
I am confident is only B
upvoted 
1 
times
karmajuney
karmajuney
 
2 years, 2 months ago
b is ok
upvoted 
1 
times
Nirca
Nirca
 
2 years, 3 months ago
Selected Answer: 
B
Correct Answer B
Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery
upvoted 
2 
times
Nirca
Nirca
 
2 years, 5 months ago
Selected Answer: 
B
b it is
upvoted 
1 
times
Pime13
Pime13
 
3 years ago
Selected Answer: 
B
vote B
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
BB is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
B
vote B
upvoted 
1 
times
rjupc
rjupc
 
3 years, 2 months ago
Aggree with B
upvoted 
1 
times
JustJack21
JustJack21
 
3 years, 3 months ago
The question also says: 
"The new platform must meet their technical requirements". The case study says:
"Technical requirements:
1. Dynamically scale up or down based on game activity
2. Connect to a managed NoSQL database service
3. Run customize Linux distro"
Only E. Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc
seems to have all requirements. am I overthinking this?
upvoted 
1 
times
PleeO
PleeO
 
3 years, 3 months ago
look at the question part: Mountkirk Games wants to set up a real-time analytics platform for their new game -> BigQuery is
always a resolution
upvoted 
2 
times
kapara
kapara
 
1 year, 5 months ago
the last technical requirements is always fully manage. 
compute engine is self manage.
so its can't be E.
upvoted 
1 
times
Nik22
Nik22
 
3 years, 3 months ago
Are we seeing the older scenarios in new exam?
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 6
Question #1
For this question, refer to the Mountkirk Games case study. Mountkirk Games wants to migrate from their current analytics and
statistics reporting model to one that meets their technical requirements on Google Cloud Platform. 
Which two steps should be part of their migration plan? (Choose two.) 
A. 
Evaluate the impact of migrating their current batch ETL code to Cloud Dataflow. 
Most Voted
B. 
Write a schema migration plan to denormalize data for better performance in BigQuery. 
Most Voted
C. 
Draw an architecture diagram that shows how to move from a single MySQL database to a MySQL cluster.
D. 
Load 10 TB of analytics data from a previous game into a Cloud SQL instance, and run test queries against the full
dataset to confirm that they complete successfully.
E. 
Integrate Cloud Armor to defend against possible SQL injection attacks in analytics files uploaded to Cloud Storage.
Correct Answer:
 
AB 
Comments
sri007
sri007
 
Highly Voted
 
4 years, 5 months ago
Correct Answer A, B
Evaluate the impact of migrating their current batch ETL code to Cloud Dataflow
Write a schema migration plan to denormalize data for better performance in BigQuery.
Stream processing (ETL) Dataflow and Reference https://cloud.google.com/bigquery/docs/loading-
data#loading_denormalized_nested_and_repeated_data
upvoted 
33 
times
AWS56
AWS56
 
Highly Voted
 
4 years, 5 months ago
agree AB
upvoted 
12 
times
nitinz
nitinz
 
3 years, 4 months ago
it is AB
Community vote distribution
AB (100%)it is AB
upvoted 
1 
times
tartar
tartar
 
3 years, 10 months ago
AB is ok
upvoted 
8 
times
bigzero
bigzero
 
Most Recent
 
1 month, 3 weeks ago
A,C
B is wrong, bigquery support relational structure, so doesn't need schema denormalization, it's bigtable requirements
C is right, New MySQL cluster need new architecture for read replica and sharding...
upvoted 
1 
times
megumin
megumin
 
1 year, 8 months ago
ok for AB
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A , B right choice
upvoted 
1 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months ago
Selected Answer: 
AB
First remove non sense answers: D / E
Now A is a must, the ETL is definitely what Cloud Dataflow does. 
Now between B / C. Its talking about a lot of data. we know that Cloud SQL is not the best for huge volume of data, plus not
real time data. Big Query is the best option
upvoted 
5 
times
AMohanty
AMohanty
 
1 year, 11 months ago
DeNormalization is an essential part of BigData - Agreed.
However we don't know how the Data is. 
I would be inclined for A and C
upvoted 
1 
times
RVivek
RVivek
 
1 year, 4 months ago
CloudSQL is not recommendad for DB size greater than 1 TB
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
AB
vote AB
upvoted 
2 
times
BrijMohan08
BrijMohan08
 
2 years, 9 months ago
A and B
upvoted 
1 
times
hongha
hongha
 
2 years, 11 months ago
Refer to updated 
Mountkik Games (post 01.05.2021) 
full analysis to prepare for your exams. 
https://www.youtube.com/watch?v=1w1olPjlPZY&t=6s
upvoted 
3 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A, B
upvoted 
3 
times
victory108
victory108
 
3 years, 1 month ago
A. Evaluate the impact of migrating their current batch ETL code to Cloud Dataflow.A. Evaluate the impact of migrating their current batch ETL code to Cloud Dataflow.
B. Write a schema migration plan to denormalize data for better performance in BigQuery.
upvoted 
3 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answers are A, B
upvoted 
1 
times
lynx256
lynx256
 
3 years, 3 months ago
AB is ok
upvoted 
1 
times
bnlcnd
bnlcnd
 
3 years, 5 months ago
A
ETL --> Dataflow
B
https://cloud.google.com/solutions/bigquery-data-warehouse#managing_data
upvoted 
2 
times
noussy
noussy
 
3 years, 5 months ago
why B ?
upvoted 
3 
times
AdityaGupta
AdityaGupta
 
3 years, 8 months ago
Correct answer is AB
Cloud Dataflow -- Stream data (mobile devices)
BigQuery --------- Intensive Analytics + historic data
Both are FULLY MANAGED services.
upvoted 
5 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 6
Question #2
For this question, refer to the Mountkirk Games case study. You need to analyze and define the technical architecture for the
compute workloads for your company, Mountkirk Games. Considering the Mountkirk Games business and technical
requirements, what should you do? 
A. 
Create network load balancers. Use preemptible Compute Engine instances.
B. 
Create network load balancers. Use non-preemptible Compute Engine instances.
C. 
Create a global load balancer with managed instance groups and autoscaling policies. Use preemptible Compute Engine
instances.
D. 
Create a global load balancer with managed instance groups and autoscaling policies. Use non-preemptible Compute
Engine instances. 
Most Voted
Correct Answer:
 
D 
Comments
dabrat
dabrat
 
Highly Voted
 
3 years, 7 months ago
D) => KPI game stability = Use non-preemptible
upvoted 
49 
times
tartar
tartar
 
2 years, 10 months ago
D is ok
upvoted 
13 
times
nitinz
nitinz
 
2 years, 4 months ago
has to be C, A & B does not meet SLA. D does not meet KPI.
upvoted 
2 
times
KNG
KNG
 
Highly Voted
 
3 years, 5 months ago
Agree "D". Preemptible VM is suitable for app which is fault-tolerant. Termination of preemptive VM might affect gaming
experience, so it is not a good choice.
upvoted 
20 
times
Community vote distribution
D (88%)
C (12%)Peto12
Peto12
 
Most Recent
 
1 week ago
Selected Answer: 
C
In the business requirements in the latest case study it's says at the bottom to minimise costs. Because of this I choose C.
upvoted 
1 
times
someCloudUser
someCloudUser
 
4 months, 2 weeks ago
Selected Answer: 
D
D is correct in my opinion.
upvoted 
1 
times
megumin
megumin
 
8 months ago
Selected Answer: 
D
ok for D
upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
D is right
upvoted 
1 
times
alexandercamachop
alexandercamachop
 
9 months, 3 weeks ago
"As the system scales, ensure that data is not lost due to processing backlogs"
Answer is clearly D.
Never mentions about cost saving / optimizing.
Plus ending instances will affect current active users.
upvoted 
3 
times
Nirca
Nirca
 
9 months, 4 weeks ago
Selected Answer: 
D
"D". 
should be the right one. 
Preemptible VM is suitable for app which is fault-tolerant. 
Preemptive might lead to availability or services issues in ONLINE applications
upvoted 
1 
times
Nirca
Nirca
 
11 months, 2 weeks ago
Selected Answer: 
D
D it is!
upvoted 
1 
times
rsh3
rsh3
 
1 year, 6 months ago
I got the point that Answer should include non-preemptible option, but I am confused why not B?
upvoted 
1 
times
panqueca
panqueca
 
1 year, 1 month ago
They want to be everywhere around the globe, so u need a global load balancer to do this
upvoted 
1 
times
vincy2202
vincy2202
 
1 year, 6 months ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
D
vote D
upvoted 
2 
times
MaxNRG
MaxNRG
 
1 year, 8 months agoMaxNRG
MaxNRG
 
1 year, 8 months ago
D – create globale LB with MIG and autoscaling policies. Use non-preemptible Compute Engine instances.
Standard vs Preemtible VMs is interesting dilemma for MountKirk Games. The diff in price is 4.5 times. E.g. 1 year price of n1-
standard-8 (30 GB, 8 vCPUs) = 3329 $, and same preemptible = 740 $. If there are 10 servers running all time then saving in cost
are 25890 $ per year. Likely not super extra expense for company if they want preserve and extend users.
So, their choice should depend on user experience == their income.
Couple notes from Case Study reqs supporting UX / standard VMs:
1) Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game
2) Improve uptime - downtime is loss of players
3) Reduce latency to all customers
4) They plan to deploy the game’s backend on Google Compute Engine so they can capture streaming metrics, run intensive
analytics, and take advantage of its autoscaling server environment and integrate with a managed NoSQL database.
upvoted 
3 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
Last item about intestive analytics and streaming - is not appropriate task for preemptible VMs. And this is from GCP
preemptible page:
1) If your applications are fault-tolerant and can withstand possible instance preemptions, then preemptible instances can
reduce your Compute Engine costs significantly. For example, batch processing jobs can run on preemptible instances. 
2) Preemptible instances cannot live migrate to a regular VM instance, or be set to automatically restart when there is a
maintenance event.
3) Due to the above limitations, preemptible instances are not covered by any Service Level Agreement (and, for clarity, are
excluded from the Google Compute Engine SLA).
So, sounds as non-preemptible (standard) is a priority for Mountkirk...
upvoted 
1 
times
[Removed]
[Removed]
 
1 year, 8 months ago
D is correct.
Non-premptive CE + Load Balancer + Autoscaling. Highly available solution
upvoted 
1 
times
BrijMohan08
BrijMohan08
 
1 year, 9 months ago
I will go with D
upvoted 
1 
times
hongha
hongha
 
1 year, 11 months ago
Refer to updated 
Mountkik Games (post 01.05.2021) 
full analysis to prepare for your exams. 
https://www.youtube.com/watch?v=1w1olPjlPZY&t=6s
upvoted 
1 
times
mbrueck
mbrueck
 
1 year, 11 months ago
Answer: D
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 6
Question #3
For this question, refer to the Mountkirk Games case study. Mountkirk Games wants to design their solution for the future in
order to take advantage of cloud and technology improvements as they become available. Which two steps should they take?
(Choose two.) 
A. 
Store as much analytics and game activity data as financially feasible today so it can be used to train machine learning
models to predict user behavior in the future. 
Most Voted
B. 
Begin packaging their game backend artifacts in container images and running them on Google 
Kubernetes Engine to
improve the ability to scale up or down based on game activity. 
Most Voted
C. 
Set up a CI/CD pipeline using Jenkins and Spinnaker to automate canary deployments and improve development
velocity.
D. 
Adopt a schema versioning tool to reduce downtime when adding new game features that require storing additional
player data in the database.
E. 
Implement a weekly rolling maintenance process for the Linux virtual machines so they can apply critical kernel patches
and package updates and reduce the risk of 0-day vulnerabilities.
Correct Answer:
 
AB 
Comments
dabrat
dabrat
 
Highly Voted
 
4 years, 1 month ago
A+B)
=>as well as other metrics that provide deeper insight into usage patterns so we can adapt the game to target users. 
environment that provides autoscaling, low latency load balancing, and frees us up from managing physical servers.
upvoted 
80 
times
techalik
techalik
 
3 years, 1 month ago
Enable CI/CD integration to improve deployment velocity, agility and reaction to change. is the right answer.
Having a CI/CD pipeline means you can deploy changes to environments faster. Does this help you take advantage of cloud
and technology improvements as they become available in the future? Yes. When new features become available, you can
incorporate them into your application and deploy them to test/production environments easily/efficiently and decrease the
Community vote distribution
AB (44%)
BC (34%)
AC (10%)
Other (12%)incorporate them into your application and deploy them to test/production environments easily/efficiently and decrease the
time to go live.
Store more data and use it as training data for machine learning. is the right answer.
The more data you, the better you can train your AI model. The better the trained model, the better the prediction service can
perform. By retaining as much real data as financially feasible, you are in the best position to take advantage of AI
improvements in the future.
Ref: https://cloud.google.com/ai-platform
AC
upvoted 
17 
times
XDevX
XDevX
 
2 years, 6 months ago
Hi techalik,
I googled right now and could not finde " By retaining as much real data as financially feasible, you are in the best position
to take advantage of AI improvements in the future." - from an economic perspective that makes no sense or the wording in
the question is very "dirty". What is financially feasible? For me that means invest every cent you have into data - that cannot
be the right approach. 
I vote for B+C.
upvoted 
8 
times
kkhurana
kkhurana
 
1 year, 11 months ago
I agree .Me too for BC
upvoted 
1 
times
ArtistS
ArtistS
 
1 month, 2 weeks ago
please see the solution requirements section please
upvoted 
1 
times
tzKhalil
tzKhalil
 
2 years, 8 months ago
As they want to take advantage of cloud, it is better to choose Cloud Build which is from GCP's service to build CI/CD.
AB is the choice
upvoted 
9 
times
nitinz
nitinz
 
2 years, 10 months ago
AC makes sense in respect to this question.
upvoted 
6 
times
tartar
tartar
 
3 years, 4 months ago
AB is ok
upvoted 
21 
times
kumarp6
kumarp6
 
3 years, 2 months ago
AC make sense.
upvoted 
8 
times
AD2AD4
AD2AD4
 
Highly Voted
 
3 years, 7 months ago
Final Decision to go with Option AB
upvoted 
25 
times
ShadowLord
ShadowLord
 
1 year, 4 months ago
Say if there proposed feature are in BigQuery , DataFlow or some other GCP service... how will just doing B help ..... C can help
upvoted 
2 
times
rrope
rrope
 
Most Recent
 
1 week, 1 day ago
Selected Answer: 
BD
options B (containers and Kubernetes) and D (schema versioning) best represent a future strategy for Mountkirk Games,options B (containers and Kubernetes) and D (schema versioning) best represent a future strategy for Mountkirk Games,
focusing on scalability, agility and efficient infrastructure and database management. . Option A, while relevant in a general
data analytics context, does not focus on future-proofing infrastructure as B and D do.
upvoted 
1 
times
Jconnor
Jconnor
 
1 month ago
I do not understand B. what does it mean begin packaging? either implemented or not implemented, begin has no value today
or in the future. A and E has they use Linux today.
upvoted 
1 
times
odacir
odacir
 
1 month, 2 weeks ago
Selected Answer: 
BC
Cloud future prove mean container and CI/CD
B. Begin packaging their game backend artifacts in container images and running them on Google Kubernetes Engine to
improve the ability to scale up or down based on game activity. Most Voted
C. Set up a CI/CD pipeline using Jenkins and Spinnaker to automate canary deployments and improve development velocity.
upvoted 
5 
times
sampon279
sampon279
 
6 months, 1 week ago
Selected Answer: 
BC
B & C - C beause: "Mountkirk Games wants to design their solution for the future in order to take advantage of cloud and
technology improvements as they become available". Spinnaker makes it easy to use new features as they become available:
https://console.cloud.google.com/marketplace/product/google-cloud-platform/spinnaker?pli=1 - Pre-integrated with Google
Cloud Platform
Spinnaker for Google Cloud Platform is pre-configured to work with GCP products like Cloud Build, Compute Engine, App
Engine, Google Kubernetes Engine, and Stackdriver.
upvoted 
3 
times
BiddlyBdoyng
BiddlyBdoyng
 
6 months, 2 weeks ago
I can see why C & D are the given correct answers.
They objectively allow you to quickly incorporate tech improvements.
A. Would help with possible machine learning models in the future.
B. Makes use of an existing technology
D. Makes use of an existing technology
I chose A & E but I think C & E probably what Google are after.
upvoted 
1 
times
Atanu
Atanu
 
6 months, 3 weeks ago
Selected Answer: 
BC
B+C looks promising
upvoted 
2 
times
Murmure
Murmure
 
7 months, 2 weeks ago
Selected Answer: 
CE
Future is the key word.
upvoted 
1 
times
medi01
medi01
 
8 months, 2 weeks ago
Selected Answer: 
AC
Higher velocity (C) = adopt new features fast.
A bodes well with the statement that they are missing a lot of data/insights.
upvoted 
1 
times
rr4444
rr4444
 
9 months, 3 weeks ago
This is annoying
A, B and C seem correct.......... But only two......?
upvoted 
2 
times
BeCalm
BeCalm
 
9 months, 3 weeks agoIn a GCP exam, non GCP products(as per C) is rarely the correct answer!
upvoted 
5 
times
BeCalm
BeCalm
 
9 months, 3 weeks ago
Selected Answer: 
AB
A = ML = future stuff
B = containers = portability (since current platform runs on VM's)
upvoted 
2 
times
SandipGhosal
SandipGhosal
 
10 months, 1 week ago
In technical and business requirement of the case study it mentioned about Analytics, not machine learning. So A seems not a
correct answer here. D is very generic as OS upgrade and patches could be done outside GCP. I would go with B and C.
upvoted 
1 
times
telp
telp
 
10 months, 3 weeks ago
Selected Answer: 
AB
Answer AB
Cloud Build is the Google Solution for CI/CD 
than other third party sofware can't be used for CI/CD in a certification examen.
upvoted 
2 
times
sameer2803
sameer2803
 
1 year ago
A&B is correct
rest of the improvements can be made without being on a cloud platform as well.
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year ago
A. Store as much analytics and game activity data as financially feasible today so it can be used to train machine learning
models to predict user behavior in the future. Storing as much data as possible will allow Mountkirk Games to use machine
learning models to analyze and predict user behavior, which can help them adapt the game to target users and improve the
user experience.
B. Begin packaging their game backend artifacts in container images and running them on Google Kubernetes Engine to
improve the ability to scale up or down based on game activity. Containerization allows Mountkirk Games to package their
game backend and dependencies into a single unit that can be easily deployed and run on any platform. By using Google
Kubernetes Engine (GKE), Mountkirk Games can take advantage of GKE's autoscaling and load balancing features to ensure that
their game backend can scale up or down based on game activity. This will help Mountkirk Games improve uptime and reduce
latency to all customers, as well as increase the efficiency of their cloud resources.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 6
Question #4
For this question, refer to the Mountkirk Games case study. Mountkirk Games wants you to design a way to test the analytics
platform's resilience to changes in mobile network latency. What should you do? 
A. 
Deploy failure injection software to the game analytics platform that can inject additional latency to mobile client
analytics traffic. 
Most Voted
B. 
Build a test client that can be run from a mobile phone emulator on a Compute Engine virtual machine, and run multiple
copies in Google Cloud Platform regions all over the world to generate realistic traffic.
C. 
Add the ability to introduce a random amount of delay before beginning to process analytics files uploaded from mobile
devices.
D. 
Create an opt-in beta of the game that runs on players' mobile devices and collects response times from analytics
endpoints running in Google Cloud Platform regions all over the world.
Correct Answer:
 
A 
Comments
a66030
a66030
 
Highly Voted
 
3 years, 8 months ago
The answer is A. The question asks - test the analytics platform's resilience to changes in mobile network latency. 
Only A adds latency to mobile network. 
C - adds delay at beginning of file processing. does not add delay/latency in mobile network.
One of the lines in the requirements for analytics: Process data that arrives late because of slow mobile networks
upvoted 
38 
times
ShadowLord
ShadowLord
 
1 year, 10 months ago
C is alright as well but just too specific to file upload
upvoted 
2 
times
sri007
sri007
 
Highly Voted
 
4 years, 5 months ago
Correct Answer C
Add the ability to introduce a random amount of delay before beginning to process analytics files uploaded from mobile
devices.
Community vote distribution
A (59%)
C (23%)
B (18%)devices.
upvoted 
28 
times
turbo8p
turbo8p
 
1 year, 7 months ago
I'm not quite sure how would you do testing at scale for option C.
It's likely that you need to test with your real users to perform testing at scale. But for Option A, you can control how many test
clients you want to simulate.
upvoted 
2 
times
Ani26
Ani26
 
3 years, 10 months ago
There is nothing mentioned about uploading analytical files from mobile devices - on analytical layer we need to perform
resiliency test on the latency changes..so A
upvoted 
5 
times
tartar
tartar
 
3 years, 10 months ago
A is ok
upvoted 
17 
times
lkjhgfdsa
lkjhgfdsa
 
3 years, 3 months ago
"If you're using a service mesh like Istio to manage your app services, you can inject faults at the application layer instead of
killing pods or machines, or you can inject corrupting packets at the TCP layer. You can introduce delays to simulate network
latency or an overloaded upstream system. You can also introduce aborts, which mimic failures in upstream systems." 
-
https://cloud.google.com/solutions/scalable-and-resilient-apps
upvoted 
13 
times
nitinz
nitinz
 
3 years, 4 months ago
A is the answer
upvoted 
5 
times
MahAli
MahAli
 
Most Recent
 
6 months, 3 weeks ago
Selected Answer: 
C
Both A and C doing similar things producing delays for consumer to injest data from producer, can we agree on that? The
question doesn't mention which which environment to conduct the testing in, so we can't just assume it's monkey test to fail a
service, can we agree on that too? I would do integration tests to introduce delays to test specifically the business logic in the
app not k8s ability to provision a pod to replace failed one, I'll make my test focal to that point.
upvoted 
1 
times
theBestStudent
theBestStudent
 
7 months ago
Selected Answer: 
A
I would say A over C, as C is too specific . Latency injection can be achieved with istio.
upvoted 
1 
times
thewalker
thewalker
 
7 months, 1 week ago
Selected Answer: 
C
C is the option I feel as Resilience is like randomly switching off some instances of analytics platform instances across the
workloads, making the data available with much delay etc. Though the option C is not complete answer, Others options seems
not correct as a - injecting something is not so good in prod environment, b - does not check resilience, d - again disturbing
the existing environment which is not resilience related.
upvoted 
1 
times
marcohol
marcohol
 
8 months, 1 week ago
GPT-4 says B
I don't know but these questions are tricky and can lead to misinterpretation
upvoted 
2 
times
muh21
muh21
 
8 months, 2 weeks ago
Selected Answer: 
B
The best way to test the analytics platform's resilience to changes in mobile network latency is to build a test client that can be
run from a mobile phone emulator on a Compute Engine virtual machine, and run multiple copies in Google Cloud Platformrun from a mobile phone emulator on a Compute Engine virtual machine, and run multiple copies in Google Cloud Platform
regions all over the world to generate realistic traffic.
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
I think C
I went for A think this was a K8 Istio platform but it's not so we'd need to do something special.
Mobile clients are uploading files and the question says to check reliability when there is delay in the upload. 
C seems to meet
this requirement perfectly.
B. I don't like as it will use the internet & not slow, flakey mobile signal
D. Seems like madness. 
Just put a create timestamp in the file & compare the difference to arrive timestamp, why need an opt
in? 
Also this isn't a test approach.
upvoted 
1 
times
rr4444
rr4444
 
1 year, 3 months ago
Selected Answer: 
A
A
Use Istio
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 3 months ago
Selected Answer: 
A
A and C are both right but with no cost constraints, A is easier than C
upvoted 
1 
times
pepigeon
pepigeon
 
1 year, 4 months ago
A is most likely correct: Istio (which is now Anthos Service Mesh on GCP) is capable of injecting delays:
https://istio.io/latest/docs/tasks/traffic-management/fault-injection/#injecting-an-http-delay-fault
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
A. Deploy failure injection software to the game analytics platform that can inject additional latency to mobile client analytics
traffic.
In order to test the analytics platform's resilience to changes in mobile network latency, the best approach would be to use
failure injection software to intentionally introduce latency to the mobile client analytics traffic. This will allow Mountkirk
Games to see how the analytics platform responds to changes in network latency and identify any potential issues or
bottlenecks that may arise. This approach will also allow Mountkirk Games to test the analytics platform under realistic
conditions, as it will be simulating the type of latency that may occur in the real world.
upvoted 
3 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
gonlafer
gonlafer
 
1 year, 6 months ago
Selected Answer: 
C
I vote for C.
Random means chaos which is a good practice to test resiliency.
upvoted 
1 
times
megumin
megumin
 
1 year, 8 months ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
 
1 year, 8 months agoMahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A seems right
upvoted 
1 
times
Nirca
Nirca
 
1 year, 9 months ago
Selected Answer: 
A
A is ok to some extent.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 6
Question #5
For this question, refer to the Mountkirk Games case study. You need to analyze and define the technical architecture for the
database workloads for your company, Mountkirk Games. Considering the business and technical requirements, what should
you do? 
A. 
Use Cloud SQL for time series data, and use Cloud Bigtable for historical data queries.
B. 
Use Cloud SQL to replace MySQL, and use Cloud Spanner for historical data queries.
C. 
Use Cloud Bigtable to replace MySQL, and use BigQuery for historical data queries.
D. 
Use Cloud Bigtable for time series data, use Cloud Spanner for transactional data, and use BigQuery for historical data
queries. 
Most Voted
Correct Answer:
 
D 
Comments
misho
misho
 
Highly Voted
 
4 years, 7 months ago
For the people who say it's C in Linux Academy, did you see the Technical requirements there? The old Technical Requirements
have the line "Connect to a managed NoSQL database service" but in the Technical Requirements in Google official site and in
this question the line is replaced if the following 2 lines "Connect to a transactional database service to manage user profiles
and game state
Store game activity in a timeseries database service for future analysis". And for them definitely D is the answer!
upvoted 
66 
times
sri007
sri007
 
Highly Voted
 
4 years, 11 months ago
Correct Answer D
Use Cloud Bigtable for time series data, use Cloud Spanner for transactional data, and use BigQuery for historical data queries.
Storing time-series data in Cloud Bigtable is a natural fit, Cloud Spanner scales horizontally and serves data with low latency
while maintaining transactional consistency and industry-leading 99.999% (five 9s) availability - 10x less downtime than four
nines (<5 minutes per year). Cloud Spanner helps future-proof your database backend. After you load your data into BigQuery,
you can query the data in your tables. BigQuery supports two types of queries: Interactive queries, Batch queries
upvoted 
40 
times
AdityaGupta
AdityaGupta
 
4 years, 2 months ago
Community vote distribution
D (86%)
C (14%)AdityaGupta
AdityaGupta
 
4 years, 2 months ago
I agree with above explanation and choice
upvoted 
8 
times
ms.umes21
ms.umes21
 
Most Recent
 
6 months, 3 weeks ago
Selected Answer: 
D
DDDDDDDDDD
upvoted 
1 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
D
Thought it's C then I reviewed the requirements there is transactional database for user profiles
upvoted 
1 
times
devinss
devinss
 
1 year, 4 months ago
Selected Answer: 
C
I think it should be C. I can't justify using Spanner for transactional data. BigTable supports single-row transactions and can
easily handle transactional and time series data at lower cost.
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
D. Use Cloud Bigtable for time series data, use Cloud Spanner for transactional data, and use BigQuery for historical data
queries.
Based on the technical requirements provided, the best approach for analyzing and defining the technical architecture for the
database workloads for Mountkirk Games would be to use Cloud Bigtable for time series data, Cloud Spanner for transactional
data, and BigQuery for historical data queries.
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
ok for D
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
D is right
upvoted 
2 
times
Nirca
Nirca
 
2 years, 3 months ago
Selected Answer: 
D
D - GCP's classics best practice for tests
upvoted 
1 
times
Nirca
Nirca
 
2 years, 5 months ago
Selected Answer: 
D
DDDDD it is
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
2 years, 11 months ago
C is worong.Bigtable is NOSQL.Bigtable cant be alternative for MySQL.
I think D is correct.
Biquery most fit historical data queries.
upvoted 
1 
times
zxcv1234
zxcv1234
 
3 years ago
Selected Answer: 
D
C cannot be the answer
upvoted 
1 
timesABO_Doma
ABO_Doma
 
3 years ago
Selected Answer: 
D
D Correct Answer D
upvoted 
1 
times
ABO_Doma
ABO_Doma
 
3 years ago
Selected Answer: 
D
D is the Answer
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
pakilodi
pakilodi
 
3 years, 1 month ago
Selected Answer: 
D
Vote D
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
D
vote D
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 6
Question #6
For this question, refer to the Mountkirk Games case study. Which managed storage option meets Mountkirk's technical
requirement for storing game activity in a time series database service? 
A. 
Cloud Bigtable 
Most Voted
B. 
Cloud Spanner
C. 
BigQuery
D. 
Cloud Datastore
Correct Answer:
 
A 
Comments
Eroc
Eroc
 
Highly Voted
 
4 years, 8 months ago
@jcmoranp , that is incorrect.. https://cloud.google.com/bigtable/docs/schema-design-time-series it's A
upvoted 
27 
times
zbyszekz
zbyszekz
 
2 years, 9 months ago
It is not clear, read technical requirements: "Store game activity logs in structured files for future analysis." so I think that D is a
good option
upvoted 
1 
times
sri007
sri007
 
Highly Voted
 
4 years, 5 months ago
Correct Answer A
Cloud Bigtable
Storing time series data in Cloud Bigtable https://cloud.google.com/bigtable/docs/schema-design-time-series
upvoted 
19 
times
salvo007
salvo007
 
Most Recent
 
5 months, 1 week ago
Big Query. in the case study, interactions with users are directly with GKE. GKE push events on Pub/Sub and then Dataflow take
events and go to BQ. Activity from Stackdriver Logging to Cloud Storage to Dataflow to BQ.
upvoted 
1 
times
Community vote distribution
A (100%)upvoted 
1 
times
anirban7172
anirban7172
 
1 year ago
Selected Answer: 
A
Google Bigtable is a fully managed, scalable NoSQL database service for large analytical and operational workloads.
upvoted 
2 
times
megumin
megumin
 
1 year, 8 months ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A can capture time series data
upvoted 
1 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
1 year, 9 months ago
Selected Answer: 
A
The key word is "time series" which ultimately leads to Bigtable. It is also used for collecting data from IoT devices.
upvoted 
4 
times
burner_1984
burner_1984
 
2 years, 5 months ago
as per ACG its C. BigQuery
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
2 
times
Rzla
Rzla
 
2 years, 9 months ago
Answer is A BigTable. 
Thats the best solution to process and store the data, BiqQuery can be used to analyse the data. 
Big
Query not a suitable target for time series.
upvoted 
4 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A
upvoted 
4 
times
victory108
victory108
 
3 years, 1 month ago
A. Cloud Bigtable
upvoted 
4 
times
gosi
gosi
 
3 years, 2 months ago
C - BigQuery
BigTable is Wrong: BigTable is timeseries with "low latency" and they didnt mention anything about low-latency. They did
mention about future analysis so BQ is best.
DataStore is wrong Choice: DataStore is good for transactional data e.g. saving and updating game state as it chnages. In the
question they are talking about game activity which is a serious on insert only data based on time.
upvoted 
2 
times
Koushick
Koushick
 
3 years, 2 months agoKoushick
Koushick
 
3 years, 2 months ago
Google says we can store time series vehicle data in Bigtable which can later be used for analytical processing using BigQuery.
https://cloud.google.com/architecture/designing-connected-vehicle-platform#data_ingestion
Even though it says vehicle data and the question is for game data we eventually are storing time series type of data so I think
this link is relevant.
Based on Google explanation, I would choose A
upvoted 
2 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is A
upvoted 
1 
times
lynx256
lynx256
 
3 years, 3 months ago
A is ok
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 6
Question #7
For this question, refer to the Mountkirk Games case study. You are in charge of the new Game Backend Platform architecture.
The game communicates with the backend over a REST API. 
You want to follow Google-recommended practices. How should you design the backend? 
A. 
Create an instance template for the backend. For every region, deploy it on a multi-zone managed instance group. Use
an L4 load balancer.
B. 
Create an instance template for the backend. For every region, deploy it on a single-zone managed instance group. Use
an L4 load balancer.
C. 
Create an instance template for the backend. For every region, deploy it on a multi-zone managed instance group. Use
an L7 load balancer. 
Most Voted
D. 
Create an instance template for the backend. For every region, deploy it on a single-zone managed instance group. Use
an L7 load balancer.
Correct Answer:
 
C 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
4 years, 8 months ago
It's C. You need a L7 balancer and multi-zone
upvoted 
66 
times
nitinz
nitinz
 
3 years, 4 months ago
It is C, you need L7 & mulit-region as its the ask.
upvoted 
7 
times
tartar
tartar
 
3 years, 10 months ago
C is ok
upvoted 
15 
times
bjuneja
bjuneja
 
3 years, 7 months ago
Community vote distribution
C (89%)
Other (11%)L4 load balancing offers traffic management of transactions at the network protocol layer (TCP/UDP). ... L7 load balancing
works at the highest level of the OSI model. L7 bases its routing decisions on various characteristics of the HTTP/HTTPS
header
Mountrik requirment is global so C is ok
upvoted 
14 
times
JJu
JJu
 
Highly Voted
 
4 years, 7 months ago
I think answer is C.
This game type is mobile.
Check this link : https://cloud.google.com/solutions/gaming/cloud-game-infrastructure#dedicated_game_server
I recommend this section : ‘Request/response based servers’
explain : In particular, however, mobile game servers, without a critical demand for real-time communication, have adopted
HTTP request and response semantics like those used in web hosting.
this game use HTTP load balancer. HTTP load balancer is L7.
upvoted 
22 
times
kratosmat
kratosmat
 
Most Recent
 
1 year, 2 months ago
Selected Answer: 
A
we don't need multi-zonal if we have multi-region. the traffic is internal, we could need an L7 LB if there are specific
requirements, but I don't see them.
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C gives best option 
Key word "REST API " "L7 balancer" , "Multi Zone"
upvoted 
4 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
C perfectly make sense
upvoted 
2 
times
muneebarshad
muneebarshad
 
1 year, 9 months ago
Selected Answer: 
B
I think the answer is B and here is my reasoning.
Users never communicate with the Backend server (due to security reasons) there is always a Frontend Service that is open to
the internet and FE Service communicates with BE Server. The reason they called it Backend Serer is since it must have a Front
end server refer to the solution reference (https://reviewnprep.com/blog/gcp-how-to-work-on-mountkirk-games-case-study/)
the traffic flow should look like this...... User(mobile) -> FE Game Server (hosted in GCP) -> BE Server (hosted in GCP)
Since BE Service is not exposed to the internet therefore we would require a regional 
L4 Balancer since regional Load Balancer
is NOT internet facing and it's from VM to VM
upvoted 
1 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
1 year, 9 months ago
Multi-zone makes it high available. L7 balances load for e.g. HTTP traffic. So the answer is C. Why not A? L4 works at the
network protocol (TCP/UDP) which is not suitable for REST APIs.
upvoted 
2 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Selected Answer: 
C
L7 LB for RestAPI.
upvoted 
2 
timesupvoted 
2 
times
Nirca
Nirca
 
1 year, 11 months ago
Selected Answer: 
C
C it is
upvoted 
1 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
SAMBIT
SAMBIT
 
2 years, 3 months ago
Out dated case study. Don’t waste time 
https://cloud.google.com/certification/guides/professional-cloud-architect
https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf
upvoted 
2 
times
dija123
dija123
 
2 months, 3 weeks ago
Mountkirk is not out dated:
https://cloud.google.com/learn/certification/guides/professional-cloud-architect
upvoted 
1 
times
amanp
amanp
 
2 years, 3 months ago
It is A, it is backend API and not frontend
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
Selected Answer: 
C
Ummm.I think the case must choose L7 LB for RestAPI.
Additional,the company require world wide scale.
So i chose C.
upvoted 
2 
times
zxcv1234
zxcv1234
 
2 years, 6 months ago
Selected Answer: 
C
C is correct. HTTP/S is layer 7
upvoted 
1 
times
ABO_Doma
ABO_Doma
 
2 years, 6 months ago
C is the correct answer
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 7
Question #1
You need to optimize batch file transfers into Cloud Storage for Mountkirk Games' new Google Cloud solution. The batch files
contain game statistics that need to be staged in Cloud Storage and be processed by an extract transform load (ETL) tool.
What should you do? 
A. 
Use gsutil to batch move files in sequence.
B. 
Use gsutil to batch copy the files in parallel. 
Most Voted
C. 
Use gsutil to extract the files as the first part of ETL.
D. 
Use gsutil to load the files as the last part of ETL.
Correct Answer:
 
B 
Comments
kopper2019
kopper2019
 
Highly Voted
 
2 years, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
13 
times
victory108
victory108
 
Highly Voted
 
2 years, 11 months ago
B. Use gsutil to batch copy the files in parallel.
upvoted 
13 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
B
Vote for B
upvoted 
1 
times
e5019c6
e5019c6
 
6 months, 1 week ago
Selected Answer: 
B
I voted A.
I understand the impulse to choose B, parallel upload. But remember that this is the first step in an ETL process that, as I
understand it, when a file is uploaded an extraction process starts and then the load. If multiple files finish uploading at the
same time, the extraction and loading process get triggered in parallel too and can cause errors if they are not prepared to
Community vote distribution
B (94%)
C (6%)same time, the extraction and loading process get triggered in parallel too and can cause errors if they are not prepared to
handle this type of process.
Anyway, now that I read the question again, I don't see that it specifies any kind of automation like I was thinking, so B is
probably fine in this case...
upvoted 
1 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
B
I change my mind, is B!
upvoted 
1 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
C
Why not C? Batch copy files in parallel is fine but is for one-time case or eventually cause, it will require manual intervention..
So with these cases I will choose C. Maybe a better solution will be to have some event driven communication like Kafka or pub
sub, but in this scenario is more schedule, ETL tool looks good fit.
upvoted 
1 
times
Gino17m
Gino17m
 
2 months ago
Also chosed C first, but teh question is about "need to optimize batch file transfers into Cloud Storage" not about ETL
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
B
B is the correct answer
Batch Copying in Parallel Saves time and an efficient option to use ( -m)
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
2 
times
AHUI
AHUI
 
1 year, 8 months ago
C is incorrect, gsutil does not do extract files
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
B is good
upvoted 
2 
times
Mikado211
Mikado211
 
1 year, 10 months ago
Selected Answer: 
B
When GCP PCA certification decide to become an english comprehension test instead of a computer science test ^^'
It's B !
upvoted 
5 
times
DrishaS4
DrishaS4
 
1 year, 11 months ago
Selected Answer: 
B
Use gsutil to batch copy the files in parallel.
upvoted 
1 
times
Nirca
Nirca
 
1 year, 11 months ago
Selected Answer: 
B
B is it !
upvoted 
1 
times
muky31dec
muky31dec
 
2 years, 4 months agomuky31dec
muky31dec
 
2 years, 4 months ago
My ans 
was B in the exam
upvoted 
2 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
B
vote B
upvoted 
2 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
Correct Answer: B
https://cloud.google.com/storage/docs/gsutil/commands/cp
If you have a large number of files to transfer, you can perform a parallel multi-threaded/multi-processing copy using the top-
level gsutil -m option (see gsutil help options):
gsutil -m cp -r dir gs://my-bucket
upvoted 
5 
times
riley5
riley5
 
2 years, 12 months ago
B is the answer.
upvoted 
5 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 7
Question #2
You are implementing Firestore for Mountkirk Games. Mountkirk Games wants to give a new game programmatic access to a
legacy game's Firestore database. 
Access should be as restricted as possible. What should you do? 
A. 
Create a service account (SA) in the legacy game's Google Cloud project, add a second SA in the new game's IAM page,
and then give the Organization Admin role to both SAs.
B. 
Create a service account (SA) in the legacy game's Google Cloud project, give the SA the Organization Admin role, and
then give it the Firebase Admin role in both projects.
C. 
Create a service account (SA) in the legacy game's Google Cloud project, add this SA in the new game's IAM page, and
then give it the Firebase Admin role in both projects. 
Most Voted
D. 
Create a service account (SA) in the legacy game's Google Cloud project, give it the Firebase Admin role, and then
migrate the new game to the legacy game's project.
Correct Answer:
 
C 
Comments
MamthaSJ
MamthaSJ
 
Highly Voted
 
3 years, 6 months ago
Answer is C
upvoted 
15 
times
WFCheong
WFCheong
 
Highly Voted
 
2 years ago
I think it should not simply give out the Organization admin role so A and B is out. We should not migrate the new game to the
lagacy game's project and thus D is out. So remain C is the only choice.
upvoted 
12 
times
Toothpick
Toothpick
 
Most Recent
 
5 months, 1 week ago
All of these are stupic tbh, all you need to do is create a SA in the new game's project , grant it access to firebase in the legacy
project and you're done.
upvoted 
4 
times
Bugmenot240411
Bugmenot240411
 
7 months, 2 weeks ago
Community vote distribution
C (100%)Bugmenot240411
Bugmenot240411
 
7 months, 2 weeks ago
Selected Answer: 
C
C is the less Stupid Solution
upvoted 
3 
times
Gino17m
Gino17m
 
8 months, 1 week ago
Selected Answer: 
C
Vote for C
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
C
C is the best of the options provided.
upvoted 
1 
times
gonlafer
gonlafer
 
2 years, 1 month ago
Selected Answer: 
C
C is right
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
2 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is fine
upvoted 
2 
times
muky31dec
muky31dec
 
2 years, 10 months ago
C is correct , I chose C in real exam
upvoted 
4 
times
vincy2202
vincy2202
 
3 years ago
C is the correct answer
upvoted 
2 
times
ravisar
ravisar
 
3 years, 1 month ago
As per the best practice, we should have separate projects for every environment for each application. We should not add new
applications to an existing project. So D is out. Option C is the answer.
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
C
vote C
upvoted 
2 
times
Nik22
Nik22
 
3 years, 3 months ago
I see many of the questions have older case studies. Are we still getting those in the exam?
upvoted 
4 
times
megumin
megumin
 
2 years, 1 month ago
Dress4Win and JencoMart no more present in the exam
upvoted 
7 
times
PeppaPig
PeppaPig
 
3 years, 4 months ago
C for sure. Cross-Projects Resource sharing via SAC for sure. Cross-Projects Resource sharing via SA
upvoted 
2 
times
RamanathanPV
RamanathanPV
 
3 years, 5 months ago
How do we get two projects here? Can someone pls. explain?
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 5 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 7
Question #3
Mountkirk Games wants to limit the physical location of resources to their operating Google Cloud regions. What should you
do? 
A. 
Configure an organizational policy which constrains where resources can be deployed. 
Most Voted
B. 
Configure IAM conditions to limit what resources can be configured.
C. 
Configure the quotas for resources in the regions not being used to 0.
D. 
Configure a custom alert in Cloud Monitoring so you can disable resources as they are created in other regions.
Correct Answer:
 
A 
Comments
muhasinem
muhasinem
 
Highly Voted
 
3 years ago
A is correct .
You can limit the physical location of a new resource with the Organization Policy Service resource locations constraint. You can
use the location property of a resource to identify where it is deployed and maintained by the service. For data-containing
resources of some Google Cloud services, this property also reflects the location where data is stored. This constraint allows you
to define the allowed Google Cloud locations where the resources for supported services in your hierarchy can be created.
After you define resource locations, this limitation will apply only to newly-created resources. Resources you created before
setting the resource locations constraint will continue to exist and perform their function.
https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations
upvoted 
29 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
2 years, 12 months ago
Answer is A
upvoted 
29 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
thewalker
thewalker
 
7 months, 1 week ago
Community vote distribution
A (89%)
D (11%)thewalker
thewalker
 
7 months, 1 week ago
Selected Answer: 
A
A
Proactive step is better than reactive step. A is better than C.
upvoted 
1 
times
Majosh
Majosh
 
8 months, 1 week ago
Selected Answer: 
A
https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints
upvoted 
1 
times
Deb2293
Deb2293
 
1 year, 3 months ago
Selected Answer: 
A
Should be A.
Quotas are used to protect Google Cloud users from unforeseen spikes in usage
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The correct answer is: A. Configure an organizational policy which constrains where resources can be deployed.
Google Cloud offers the ability to use organizational policies to constrain the deployment of resources to specific regions or
zones. This allows you to control where resources can be deployed within your organization, and ensure that they are only
deployed in the regions that are appropriate for your business needs. To configure an organizational policy to constrain the
location of resources, you can use the Cloud Resource Manager to create and apply a policy that specifies the allowed regions
or zones for resource deployment.
upvoted 
5 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A Org Policy would do
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
I am going with A
upvoted 
1 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
cloudmon
cloudmon
 
2 years, 2 months ago
Selected Answer: 
D
The confusing thing here is that GCP has renamed the same solution multiple times. The concept is "Multi Cluster Ingress (MCI)",
and kubemci was the original solution for setting this up. Then GCP released "Ingress for Anthos", which replaced kubemci.
Now, they have again renamed "Ingress for Anthos" to "Multi Cluster Ingress". If you see this question in the exam, it should no
longer provide "Ingress for Anthos" as an option, but instead will say something like "Multi Cluster Ingress". The answers can be
found at these links:
https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingresshttps://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress
https://cloud.google.com/kubernetes-engine/docs/how-to/multi-cluster-ingress
upvoted 
2 
times
chickennuggets
chickennuggets
 
1 year, 10 months ago
You failed to read the case study. Region not just limited to GKE... Org policy is the ONLY way
upvoted 
1 
times
jaxclain
jaxclain
 
1 year, 7 months ago
He just got confused and wrotte the comment in the incorrect question/subject lol but I know what question is he talking
about and at least on that question he is right, the answer was D lol 
he meant to say D for the question 6 for topic 7.. :) 
But in this question for sure the answer is A
upvoted 
1 
times
muky31dec
muky31dec
 
2 years, 4 months ago
My Ans 
was A
upvoted 
1 
times
cdcollector
cdcollector
 
2 years, 6 months ago
Selected Answer: 
A
https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations#setting_the_organization_policy
upvoted 
3 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
2 
times
pakilodi
pakilodi
 
2 years, 7 months ago
Selected Answer: 
A
Select A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 7
Question #4
You need to implement a network ingress for a new game that meets the defined business and technical requirements.
Mountkirk Games wants each regional game instance to be located in multiple Google Cloud regions. What should you do? 
A. 
Configure a global load balancer connected to a managed instance group running Compute Engine instances.
B. 
Configure kubemci with a global load balancer and Google Kubernetes Engine.
C. 
Configure a global load balancer with Google Kubernetes Engine.
D. 
Configure Ingress for Anthos with a global load balancer and Google Kubernetes Engine. 
Most Voted
Correct Answer:
 
D 
Comments
XDevX
XDevX
 
Highly Voted
 
3 years, 6 months ago
IMHO d) is the correct answer, not a)
The game fulfills the business requirements as well as the technical requirements - so it is build upon an architecture that is
multi regional. 
https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress
upvoted 
31 
times
Ishu_awsguy
Ishu_awsguy
 
2 years, 3 months ago
Why is anthos needed.
In the link shared above , a multi cluster ingress ( HTTPS LB ) is sufficient.
We should go with C.
upvoted 
9 
times
Begum
Begum
 
2 years, 3 months ago
Anthos is overkill! - As per the case study, there is no mention of hybrid requirement.
upvoted 
11 
times
Titee
Titee
 
1 year, 6 months ago
Eventual migration. :-)
upvoted 
1 
times
Community vote distribution
D (46%)
C (29%)
B (14%)
Other (11%)upvoted 
1 
times
MikeB19
MikeB19
 
3 years, 4 months ago
I think anthos would work but 
i don’t think it is needed. Deploying anthos means they will maintain an on prem environment
along with gcp. Anthos will give them the ability to manage both environments from a single pane of glass. 
I think b is correc. Kubemci provides global lb for multi gke clusters 
https://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-
with-kubemci
upvoted 
13 
times
MikeB19
MikeB19
 
3 years, 3 months ago
From what 
i understand kubemci is now deprecated (although 
i have not found an official doc stating this). If this is the case
then D is correct
upvoted 
2 
times
Narinder
Narinder
 
2 years, 11 months ago
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress
upvoted 
2 
times
gingerbeer
gingerbeer
 
3 years, 3 months ago
Official doc saying kubemci deprecated in here: 
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress
“This has now been deprecated in favor of Ingress for Anthos. Ingress for Anthos is the recommended way to deploy multi-
cluster ingress.”
upvoted 
11 
times
cotam
cotam
 
3 years, 2 months ago
Interesting how they already, within this short period of time renamed 'Ingress for Anthos' to 'Multi Cluster Ingress'..
upvoted 
3 
times
taoj
taoj
 
Highly Voted
 
3 years, 5 months ago
D for me.
since it's a multiple regions game.Need multi-GKE or multi-MIG.
To configure the ingress between multi-GKE. kubemci or Ahthos
kubemci has now been deprecated in favor of Ingress for Anthos. Ingress for Anthos is the recommended way to deploy multi-
cluster ingress.
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress
So. D
upvoted 
22 
times
Tartiushenko
Tartiushenko
 
Most Recent
 
2 months, 2 weeks ago
According to the scenario they are not using GKE for now, they only plan to use it. They moved from on-premises VM almost
without modification. So answer A is also plausible
upvoted 
1 
times
Sephethus
Sephethus
 
6 months ago
Isn't kubemci the equivalent of a multi-cluster ingress?? What's the difference here between D and B?
upvoted 
1 
times
A84-64
A84-64
 
6 months, 2 weeks ago
Selected Answer: 
C
There is no need for Anthos as Mountkrik has migrated all of its games to Google Cloud. Anthos should be used in a hybrid
environment scenario.
upvoted 
3 
times
pico
pico
 
8 months ago
Selected Answer: 
C
why not the other options?
A. Managed Instance Group with Compute Engine: While this can achieve regional deployments, it wouldn't offerA. Managed Instance Group with Compute Engine: While this can achieve regional deployments, it wouldn't offer
containerization benefits like GKE. GKE provides features like autoscaling and health checks, which are well-suited for
managing game workloads.
B. Kubemci with Global Load Balancer: Kubemci is a tool for managing on-premise Kubernetes clusters, not directly applicable
to Google Cloud's GKE.
D. Ingress for Anthos: Anthos is a suite of tools for managing hybrid and multi-cloud deployments. While it can integrate with
GKE and global load balancers, it's a more complex solution compared to simply using a global load balancer with GKE for this
specific scenario.
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
11 months, 1 week ago
Selected Answer: 
D
GKE + global rb = Ingless for Anthos.
upvoted 
1 
times
mastrrrr
mastrrrr
 
1 year ago
I had a test today. kubemci and Ingress for Anthos were deprecated . Ingress for Anthos was replaced with "Multi Cluster
Ingress" in the actual test.
upvoted 
19 
times
theBestStudent
theBestStudent
 
1 year, 1 month ago
Well, here B is the deprecated right answer, but nowadays D is the current right answer.
upvoted 
2 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
D
D
As per the below documentation, the best option found is D: https://cloud.google.com/kubernetes-
engine/docs/concepts/multi-cluster-ingress
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Anthos is required to fulfill the requirement of "Mountkirk Games wants each regional game instance to be located in multiple
Google Cloud regions".
upvoted 
1 
times
msahdra
msahdra
 
1 year, 1 month ago
B is the answer: B. Configure kubemci with a global load balancer and Google Kubernetes Engine (GKE).
This approach provides a scalable, highly available, and manageable solution for routing traffic to regional game instances
deployed across multiple Google Cloud regions.
Kubemci is a multi-cluster Ingress controller that enables managing Ingress resources across multiple GKE clusters, making it
ideal for handling regional game instances.
A global load balancer distributes incoming traffic across multiple regions, ensuring that users are directed to the nearest and
most responsive game instance.
upvoted 
1 
times
theBestStudent
theBestStudent
 
1 year, 1 month ago
it used to be the right answer, now the right answer should be D https://github.com/GoogleCloudPlatform/k8s-multicluster-
ingress
upvoted 
1 
times
steghe
steghe
 
1 year, 4 months ago
Adding an article explaining why D could be the solution https://itnext.io/ingress-for-anthos-multi-cluster-ingress-and-global-
service-load-balancing-c56c57b97e82
upvoted 
1 
times
Pau123
Pau123
 
1 year, 5 months ago
Selected Answer: 
DIt is D. According to https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress ...
Multi Cluster Ingress's multi-cluster support satisfies many use cases including:
Multi-regional, multi-cluster availability through health checking and traffic failover.
upvoted 
1 
times
Vignesh_Krishnamurthi
Vignesh_Krishnamurthi
 
1 year, 5 months ago
Selected Answer: 
D
Anthos enables migration of legacy games [last business/tech requirement]. 
Since the question requires consideration of these
requirements, this is the difference maker for option D.
upvoted 
1 
times
red_panda
red_panda
 
1 year, 6 months ago
Selected Answer: 
C
For me is C.
There is no necessity of Anthos or hybrid connectivity cluster. GKE and Global LB is enought
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
Selected Answer: 
D
Needs ingress over multiple clusters which requires Multi Cluster Ingress or Antos for Ingress as it was previously known.
upvoted 
2 
times
Atanu
Atanu
 
1 year, 6 months ago
Selected Answer: 
A
A is good to go. Anthos is an overkill here
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 7
Question #5
Your development teams release new versions of games running on Google Kubernetes Engine (GKE) daily. You want to create
service level indicators (SLIs) to evaluate the quality of the new versions from the user's perspective. What should you do? 
A. 
Create CPU Utilization and Request Latency as service level indicators.
B. 
Create GKE CPU Utilization and Memory Utilization as service level indicators.
C. 
Create Request Latency and Error Rate as service level indicators. 
Most Voted
D. 
Create Server Uptime and Error Rate as service level indicators.
Correct Answer:
 
C 
Comments
XDevX
XDevX
 
Highly Voted
 
3 years, 6 months ago
IMHO c) is the correct answer, not a).
Reason is, that we have to take the users perspective (according to the given question and also to understand what the user
expects from us).
The question might be whether to choose c) or d). Considering that our requirement is to minimize the latency, we have to
choose c) - that means we are striving for no downtime of the service.
upvoted 
38 
times
medeis_jar
medeis_jar
 
3 years, 3 months ago
yeap, definition of SLI "While many numbers can function as an SLI, we generally recommend treating the SLI as the ratio of
two numbers: the number of good events divided by the total number of events" ->
https://sre.google/workbook/implementing-slos/#:~:text=or%20product%20manager).-
,What%20to%20Measure%3A%20Using%20SLIs,-Once%20you%20agree
upvoted 
4 
times
MamthaSJ
MamthaSJ
 
Highly Voted
 
3 years, 6 months ago
Answer is C
upvoted 
8 
times
Vinaykas
Vinaykas
 
Most Recent
 
4 months, 3 weeks ago
c is the right answer
Community vote distribution
C (96%)
A
(4%)c is the right answer
upvoted 
1 
times
Gino17m
Gino17m
 
8 months, 1 week ago
Selected Answer: 
C
Vote for C
upvoted 
1 
times
don_v
don_v
 
11 months, 3 weeks ago
I'm between C and D.
Can anyone explain why not D?
"Because it's deployed to K8s" is not an answer. I've been deploying to GKE for years, and the serve might be down just because
of a missing or reconfigured property or say something is wrong with a database (e.g., exclusive table lock).
upvoted 
1 
times
mustafa1p
mustafa1p
 
12 months ago
Selected Answer: 
C
Latency and response time are important.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
C
Request latency and Error rates are important SLAs
upvoted 
1 
times
Vignesh_Krishnamurthi
Vignesh_Krishnamurthi
 
1 year, 5 months ago
Selected Answer: 
A
Answer is A - because Latency based SLI provides insight on user experience, while the CPU utilization SLI provides insight for
the operations team in case the latency is high.
upvoted 
1 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
Selected Answer: 
C
CPU utilization doesn't tell us about the user experience except perhaps if it hits 100%. 
The errors & latency (driven party by
CPU ) are much better indicators
upvoted 
2 
times
alekonko
alekonko
 
1 year, 8 months ago
Selected Answer: 
C
Answer is C
upvoted 
1 
times
Deb2293
Deb2293
 
1 year, 9 months ago
Selected Answer: 
C
Definitely C
upvoted 
1 
times
somchaikin
somchaikin
 
1 year, 9 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is: C. Create Request Latency and Error Rate as service level indicators.
Request Latency measures the time it takes for a request to be processed by the game, and is an important indicator of the
responsiveness of the game from the user's perspective. A high request latency can indicate that the game is experiencing
performance issues or is under heavy load, which may negatively impact the user experience.Error Rate measures the percentage of requests that result in errors, such as HTTP errors or timeouts. A high error rate can
indicate that the game is experiencing technical issues or is not able to handle incoming requests effectively, which can also
negatively impact the user experience. By monitoring these SLIs, you can identify issues with the game's performance and take
appropriate action to improve the user experience.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
You can use tools such as Stackdriver Monitoring or Cloud Monitoring to create custom metrics and alerts based on these SLIs,
and track the performance of your games over time. You can also use these tools to set up automated responses to alerts,
such as automatically scaling the number of game instances up or down based on the current load.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
C is good choice
upvoted 
1 
times
chickennuggets
chickennuggets
 
2 years, 4 months ago
users perspective only way thats addressed is request latency and error rates..
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 7
Question #6
Mountkirk Games wants you to secure the connectivity from the new gaming application platform to Google Cloud. You want to
streamline the process and follow 
Google-recommended practices. What should you do? 
A. 
Configure Workload Identity and service accounts to be used by the application platform. 
Most Voted
B. 
Use Kubernetes Secrets, which are obfuscated by default. Configure these Secrets to be used by the application
platform.
C. 
Configure Kubernetes Secrets to store the secret, enable Application-Layer Secrets Encryption, and use Cloud Key
Management Service (Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application
platform.
D. 
Configure HashiCorp Vault on Compute Engine, and use customer managed encryption keys and Cloud Key Management
Service (Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform.
Correct Answer:
 
A 
Comments
muhasinem
muhasinem
 
Highly Voted
 
3 years, 6 months ago
A is correct .
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
Workload Identity is the recommended way to access Google Cloud services from applications running within GKE due to its
improved security properties and manageability. For information about alternative ways to access Google Cloud APIs from
GKE, refer to the alternatives section below.
upvoted 
32 
times
dhamo_555
dhamo_555
 
Highly Voted
 
3 years, 5 months ago
A) - Because Mountkrik Game is 
going to use GKE clusters for its new deployment and so work load identity is the preferred
way to connect the apps running on GKE
upvoted 
11 
times
Sephethus
Sephethus
 
Most Recent
 
6 months, 1 week ago
This question is vague and confusing, I have no idea what it wants to secure exactly. If we're talking about secrets like api keys
Community vote distribution
A (100%)This question is vague and confusing, I have no idea what it wants to secure exactly. If we're talking about secrets like api keys
and tokens another things, I'd use secrets manager, but that's not a choice, if we're talking about service account access it's
workload identity. None of this is specified and thus the question is unanswerable.
upvoted 
1 
times
thewalker
thewalker
 
1 year, 1 month ago
Selected Answer: 
A
A is the best option, as we know the work loads in Mountkirk. Read more what will be managed by GCP, if we go with
Workload Identity: https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity#what_is
upvoted 
2 
times
omermahgoub
omermahgoub
 
2 years ago
The correct answer is: A. Configure Workload Identity and service accounts to be used by the application platform.
Workload Identity is a feature of Google Cloud that allows you to map identities from your on-premises or Google Cloud
identity provider to Google Cloud service accounts. By using Workload Identity, you can secure the connectivity of your
application platform to Google Cloud by using the service accounts to authenticate and authorize access to Google Cloud
resources.
Service accounts are Google Cloud resources that represent non-human users that your application platform can use to
authenticate and authorize access to Google Cloud resources. By using service accounts, you can secure the connectivity of your
application platform to Google Cloud by controlling which resources the service accounts can access and what actions they can
perform.
upvoted 
4 
times
omermahgoub
omermahgoub
 
2 years ago
To configure Workload Identity and service accounts, you will need to create a service account and bind it to the identity of
your workload. You can then use the service account to authenticate to Google Cloud APIs and access the resources needed by
your application platform. This will help to secure the connectivity from the platform to Google Cloud and streamline the
process of managing access and permissions.
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A is right
upvoted 
1 
times
muky31dec
muky31dec
 
2 years, 10 months ago
I answered A in real exam
upvoted 
3 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
A
A is the correct answer
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
upvoted 
3 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
2 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
A. Workload Identity is the recommended way to access Google Cloud services from applications running within GKE due to its
improved security properties and manageability
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
upvoted 
1 
timesAri_GCP
Ari_GCP
 
3 years, 3 months ago
"Secure the connectivity" - gaming platform runs on GKE, and Workload Identity is the recommended way to connect to
Google Cloud services from GKE. Hence A.
upvoted 
2 
times
PeppaPig
PeppaPig
 
3 years, 4 months ago
A for sure if you are using GKE :)
upvoted 
2 
times
kopper2019
kopper2019
 
3 years, 5 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
3 
times
victory108
victory108
 
3 years, 5 months ago
A. Configure Workload Identity and service accounts to be used by the application platform.
upvoted 
2 
times
kopper2019
kopper2019
 
3 years, 5 months ago
from my view looks like C
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
3 years, 6 months ago
Answer is A
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 7
Question #7
Your development team has created a mobile game app. You want to test the new mobile app on Android and iOS devices with
a variety of configurations. You need to ensure that testing is efficient and cost-effective. What should you do? 
A. 
Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices. 
Most Voted
B. 
Create Android and iOS VMs on Google Cloud, install the mobile app on the VMs, and test the mobile app.
C. 
Create Android and iOS containers on Google Kubernetes Engine (GKE), install the mobile app on the containers, and
test the mobile app.
D. 
Upload your mobile app with different configurations to Firebase Hosting and test each configuration.
Correct Answer:
 
A 
Comments
VishalB
VishalB
 
Highly Voted
 
2 years, 11 months ago
Correct Answer: A
- Firebase Test Lab is a cloud-based app testing infrastructure that lets you test your app on a range of devices and
configurations, so you can get a better idea of how it'll perform in the hands of live users.
- Firebase Test Lab 
Run tests on a wide range of Android and iOS devices hosted by Test Lab.
upvoted 
38 
times
Enzian
Enzian
 
Highly Voted
 
3 years ago
A should be correct
B false - not really feasable
C false - cannot run Android or IOS on GKE
D false since that is what A is built to do
upvoted 
11 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
A
Firebase Test Lab is for testing mobile apps
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
Community vote distribution
A (100%)The correct answer is: A. Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS
devices.
The Firebase Test Lab is a cloud-based testing service that allows you to test your mobile app on a variety of physical devices
running Android and iOS. It provides a range of options for testing your app, including testing on different device models,
screen sizes, and operating system versions.
To use the Firebase Test Lab, you will need to upload your mobile app to the service and specify the devices and configurations
you want to test. The Test Lab will then run your tests on the specified devices and provide results, including performance
metrics and crash reports. This will allow you to test your mobile app on a variety of configurations efficiently and cost-
effectively.
upvoted 
5 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
In addition to providing performance metrics and crash reports, the Test Lab also allows you to record video of the tests being
run, so you can see how your app is behaving on each device. This can be particularly useful for identifying and debugging
issues that may not be immediately apparent from the test results.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A is correct. Firebase is mobile testing
upvoted 
2 
times
muky31dec
muky31dec
 
2 years, 4 months ago
Ans is A. Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices 
( got question
in real exam)
upvoted 
2 
times
SamGCP
SamGCP
 
2 years, 6 months ago
Selected Answer: 
A
https://firebase.google.com/docs/test-lab
upvoted 
5 
times
andeu
andeu
 
2 years, 6 months ago
Selected Answer: 
A
Correct Answer: A
upvoted 
1 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
pakilodi
pakilodi
 
2 years, 7 months ago
Selected Answer: 
A
Vote A
upvoted 
2 
times
mudot
mudot
 
2 years, 7 months agomudot
mudot
 
2 years, 7 months ago
seems the given answer was chosen at random :-D
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
2 
times
Gk21
Gk21
 
2 years, 7 months ago
I think Admin is forcing us to see the comment to find the correct answer.
upvoted 
5 
times
[Removed]
[Removed]
 
2 years, 8 months ago
A is right. Firebase Lab is platform for mobile app testing.
upvoted 
2 
times
SuperNest
SuperNest
 
2 years, 10 months ago
The given answer is ridiculous !!
upvoted 
3 
times
[Removed]
[Removed]
 
2 years, 8 months ago
Not only this Q. Many others are marked wrong answers. Admin need to take an action and consolidate the right answers
based on the experts comments and explanations.
upvoted 
4 
times
pakilodi
pakilodi
 
2 years, 6 months ago
iOS containers....a new way to debug iOS apps without a Mac :)
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #1
TerramEarth's CTO wants to use the raw data from connected vehicles to help identify approximately when a vehicle in the
field will have a catastrophic failure. 
You want to allow analysts to centrally query the vehicle data. 
Which architecture should you recommend? 
A. 
 
B. 
  
C. 
 
D. 
  
Correct Answer:
 
A 
The push endpoint can be a load balancer. 
A container cluster can be used. 
Cloud Pub/Sub for Stream Analytics 
Reference: 
https://cloud.google.com/pubsub/ 
https://cloud.google.com/solutions/iot/ 
https://cloud.google.com/solutions/designing-connected-vehicle-platform https://cloud.google.com/solutions/designing-
connected-vehicle-platform#data_ingestion http://www.eweek.com/big-data-and-analytics/google-touts-value-of-cloud-iot-connected-vehicle-platform#data_ingestion http://www.eweek.com/big-data-and-analytics/google-touts-value-of-cloud-iot-
core-for-analyzing-connected-car-data https://cloud.google.com/solutions/iot/
Comments
hems4all
hems4all
 
Highly Voted
 
3 years, 7 months ago
A is correct
As described in the Designing a Connected Vehicle Platform on Cloud IoT Core case study,
1. Google Cloud Dataflow is essential to transform, enrich and then store telemetry data by using distributed data pipelines
2. Cloud Pub/Sub is essential to handle the streams of vehicle data while at the same time decoupling the specifics of the
backend processing implementation
It now comes down to a choice between
1. Cloud SQL vs BigQuery for analytics.
2. GKE (with or without Anthos) + Cloud Load balancing vs App Engine.
For the first point, there is no doubt that BigQuery is the preferred choice for analytics. Cloud SQL does not scale to this sort of
data volume (9TB/day + data coming through when vehicles are serviced).
For the second point, GKE with Cloud Load Balancing is a better fit than App Engine. App Engine is a regional service whereas,
with the other option, you can have multiple GKE clusters in different regions. And Cloud Load Balancing can send requests to
the cluster in the region that is closest to the vehicle. This option minimizes the latency and makes the feedback loop more
real-time.
upvoted 
59 
times
MJK
MJK
 
Highly Voted
 
4 years, 6 months ago
Ans should be A
upvoted 
13 
times
nitinz
nitinz
 
3 years, 4 months ago
It is A
upvoted 
1 
times
Jphix
Jphix
 
3 years, 6 months ago
Agreed - A. For those with load balancing concerns re: FTP; transport protocol for ftp is TCP, albeit over two TCP ports. GCP
Load Balancing supports TCP.
upvoted 
3 
times
GCP_Azure
GCP_Azure
 
4 years, 1 month ago
You will need to use (install) an FTP server that supports load balancing. Google Cloud does not offer an FTP server service or
software product
upvoted 
3 
times
alii
alii
 
3 years, 5 months ago
App engine is regional https://cloud.google.com/appengine/docs/locations. Case study says it's a global business. which
rules out app engine. so we are left with A.
"App Engine is regional, which means the infrastructure that runs your apps is located in a specific region, and Google
manages it so that it is available redundantly across all of the zones within that region."
upvoted 
4 
times
alii
alii
 
3 years, 5 months ago
As per case study: "They currently have over 500 dealers and service centers in 100 countries."As per case study: "They currently have over 500 dealers and service centers in 100 countries."
upvoted 
1 
times
tartar
tartar
 
3 years, 10 months ago
B is ok
upvoted 
9 
times
bnlcnd
bnlcnd
 
3 years, 5 months ago
B missed the global LB. App Engine is regional only. You have to create App Engine in each region and use LB as the front
door.
upvoted 
1 
times
bjuneja
bjuneja
 
3 years, 7 months ago
How can you do analysis on SQL? A is ok
upvoted 
5 
times
salvo007
salvo007
 
Most Recent
 
5 months, 1 week ago
Load Balancer doesn't work on ftp. You need a server to close the connectione and put the file. than the server ftp, can put the
file on pub sub etc. so the answer is B
upvoted 
2 
times
Jannchie
Jannchie
 
6 months, 4 weeks ago
App Engine is not for analytic task, so GKE is better. A.
upvoted 
1 
times
Jconnor
Jconnor
 
7 months ago
yup. It's A. Maybe FTP in passive mode would help.
upvoted 
1 
times
rr4444
rr4444
 
1 year, 3 months ago
This question is a mess
FTP all over the place
And even into things that can't talk FTP in that way
Needs fixing
upvoted 
4 
times
BeCalm
BeCalm
 
1 year, 4 months ago
What is a Google Container Engine?
upvoted 
1 
times
n_nana
n_nana
 
1 year, 3 months ago
It is GKE in old google naming
upvoted 
3 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
A is correct
upvoted 
1 
times
JoeThach
JoeThach
 
2 years ago
I vote for A - The company has customers in 100+ countries, while App Engine is regional (rule out B).
upvoted 
1 
times
amxexam
amxexam
 
2 years, 1 month ago
B is correct.
We dont have LB that support SFTP traffic.
Hence we eleminate A & CHence we eleminate A & C
We are talking arbore analyzing IoT data = Big Quavery = B.
upvoted 
2 
times
moota
moota
 
1 year, 5 months ago
Google Cloud Load Balancing can also do TCP/SSL load balancing. Check out key features in https://cloud.google.com/load-
balancing
upvoted 
1 
times
HD2023
HD2023
 
1 year, 3 months ago
"FTP" doesn’t appear once on that entire page. Answer: B
upvoted 
1 
times
jpco
jpco
 
2 years, 4 months ago
Google Load Balancer doesn't support FTP protocol
upvoted 
3 
times
joe2211
joe2211
 
2 years, 7 months ago
vote A
upvoted 
2 
times
medeis_jar
medeis_jar
 
2 years, 9 months ago
Answer B, because of Bigquery (Analytics) and AppEngine Flex (custom runtime environments) and the fact that there is no such
a thing as Google Container Engine, there is Google Kubernetes Engine.
upvoted 
3 
times
Nik22
Nik22
 
2 years, 9 months ago
These are from old use case. Do we still need to do these?
upvoted 
5 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A
upvoted 
2 
times
XDevX
XDevX
 
3 years ago
IMHO it is b).
Background: We have in 22 hours 9 TB of ingest data. That makes 113 MB per second. 
The question is: Why do we have to use for a "simple" transformation of FTP files into Cloud Pub/Sub GKE with multiple clusters
around the world? For me it seems to be sufficient to have App Engine Flexible in one region to handle that.
We have no highly interactive game where every ms counts concerning the customer experience - we have "only" some vehicles
sending data that has to be tranformed and then analysed.
upvoted 
4 
times
gatul28
gatul28
 
3 years, 1 month ago
how B? I do not see any LB there and App Engine doesn't support LBs beyond HTTP(s). Answer Is A but B's existence is
confusing me much
upvoted 
1 
times
HD2023
HD2023
 
1 year, 3 months ago
ultra low latency isn’t required. Whereas A isn’t possible because you can’t use FTP with LB.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #2
The TerramEarth development team wants to create an API to meet the company's business requirements. You want the
development team to focus their development effort on business value versus creating a custom framework. 
Which method should they use? 
A. 
Use Google App Engine with Google Cloud Endpoints. Focus on an API for dealers and partners 
Most Voted
B. 
Use Google App Engine with a JAX-RS Jersey Java-based framework. Focus on an API for the public
C. 
Use Google App Engine with the Swagger (Open API Specification) framework. Focus on an API for the public
D. 
Use Google Container Engine with a Django Python container. Focus on an API for the public
E. 
Use Google Container Engine with a Tomcat container with the Swagger (Open API Specification) framework. Focus on an
API for dealers and partners
Correct Answer:
 
A 
Comments
kvokka
kvokka
 
Highly Voted
 
4 years, 11 months ago
agree with A
upvoted 
32 
times
Vika
Vika
 
Highly Voted
 
3 years, 10 months ago
Google offers Cloud Endpoint to develop, deploy and manage APIs on any google cloud backend. 
https://cloud.google.com/endpoints
With Endpoints Frameworks, you don't have to deploy a third-party web server (such as Apache Tomcat or Gunicorn) with your
application. You annotate or decorate the code and deploy your application as you normally would to the App Engine
standard environment.
Cloud Endpoints Frameworks for the App Engine standard environment :
https://cloud.google.com/endpoints/docs/frameworks/about-cloud-endpoints-frameworks
upvoted 
13 
times
Sephethus
Sephethus
 
Most Recent
 
5 months, 4 weeks ago
In any question where it says developer efforts need priority over operations costs or the cost of anything other effort, the
Community vote distribution
A (90%)
C (10%)In any question where it says developer efforts need priority over operations costs or the cost of anything other effort, the
answer is always app engine, if it is one of the options.
upvoted 
1 
times
VSMu
VSMu
 
1 year, 11 months ago
Not sure why these questions are using the term Google Container Engine instead of Google Kubernetes Engine. That is so
confusing
upvoted 
3 
times
n_nana
n_nana
 
1 year, 9 months ago
These are old questions where GKE was named Google Container Engine
upvoted 
1 
times
KyubiBlaze
KyubiBlaze
 
2 years, 1 month ago
Why you don't go for C, is cuz for partners, global speed will not be relevant. You can still serve globally via app engine
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
A
A correct answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
Before even reading discussions I am fixed with my answer as A and everyone said same.
upvoted 
2 
times
amxexam
amxexam
 
2 years, 7 months ago
Selected Answer: 
A
We should choose google components only. Here GCP Endpoint does the job so A.
upvoted 
3 
times
Nick89GR
Nick89GR
 
2 years, 8 months ago
Selected Answer: 
A
Definetely A
upvoted 
1 
times
cdcollector
cdcollector
 
3 years ago
Selected Answer: 
C
OAS adoption rules out framework development. Endpoints is only for indirection of the API and configuring other services
which was not part of the question
upvoted 
1 
times
cyqgz_36
cyqgz_36
 
2 years, 12 months ago
API should be partner and dealer facing, not public as per BR
upvoted 
1 
times
Knerd
Knerd
 
3 years ago
If this is A then how come the previous question answer is B (Container Engine) ?
upvoted 
2 
times
Knerd
Knerd
 
3 years ago
Sorry typo there.. previous question answer is Container Engine while here we talk about App Engine. how come ?Sorry typo there.. previous question answer is Container Engine while here we talk about App Engine. how come ?
upvoted 
2 
times
vartiklis
vartiklis
 
3 years ago
The previous question deals with analysis. This question focuses on creating an API for dealers and partners
upvoted 
3 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
2 
times
kopper2019
kopper2019
 
3 years, 5 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
2 
times
victory108
victory108
 
3 years, 5 months ago
A. Use Google App Engine with Google Cloud Endpoints. Focus on an API for dealers and partners
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
3 years, 6 months ago
Answer is A
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #3
Your development team has created a structured API to retrieve vehicle data. They want to allow third parties to develop tools
for dealerships that use this vehicle event data. You want to support delegated authorization against this data. 
What should you do? 
A. 
Build or leverage an OAuth-compatible access control system 
Most Voted
B. 
Build SAML 2.0 SSO compatibility into your authentication system
C. 
Restrict data access based on the source IP address of the partner systems
D. 
Create secondary credentials for each dealer that can be given to the trusted third party
Correct Answer:
 
A 
Comments
ravisar
ravisar
 
Highly Voted
 
1 year, 1 month ago
SAML is an authentication system. 
OAuth is an authorization system. 
Both can be used with SSO (Single sign on). SAML is for users and OAuth is more for applications.
Answer A
upvoted 
44 
times
huyhoang8344
huyhoang8344
 
4 months, 1 week ago
SAML can do both authentication and authorization If I am not mistaken
But agree 
A should be the answer
upvoted 
2 
times
AD2AD4
AD2AD4
 
Highly Voted
 
2 years, 7 months ago
Final Decision to go with Option A.
Refer - https://cloud.google.com/docs/authentication
Good Read - https://cloud.google.com/blog/products/identity-security/identity-and-authentication-the-google-cloud-way
upvoted 
25 
times
megumin
megumin
 
Most Recent
 
1 month, 4 weeks ago
Community vote distribution
A (100%)Selected Answer: 
A
ok for A
upvoted 
1 
times
Nirca
Nirca
 
3 months, 2 weeks ago
Selected Answer: 
A
Delegate application authorization with OAuth2
upvoted 
1 
times
AzureDP900
AzureDP900
 
6 months ago
OAuth Authorization is right. A is right!
upvoted 
2 
times
pakilodi
pakilodi
 
1 year, 1 month ago
Selected Answer: 
A
A is correct
upvoted 
2 
times
joe2211
joe2211
 
1 year, 1 month ago
Selected Answer: 
A
vote A
upvoted 
2 
times
MaxNRG
MaxNRG
 
1 year, 2 months ago
A – O-Auth 2 access to system (clients would use APIs) https://cloud.google.com/docs/authentication/end-user
B – SAML 2.0 is redundant, not in requirements.
upvoted 
1 
times
kopper2019
kopper2019
 
1 year, 5 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
3 
times
victory108
victory108
 
1 year, 5 months ago
A. Build or leverage an OAuth-compatible access control system
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
1 year, 6 months ago
Answer is A
upvoted 
2 
times
wzh5831
wzh5831
 
1 year, 8 months ago
just query why there is not option for service account...
upvoted 
1 
times
poseidon24
poseidon24
 
1 year, 5 months ago
Because OAuth 2.0 already take in count such flows (client credentials, that is service-to-service communication, meaning
service accounts).
upvoted 
1 
times
Ausias18
Ausias18
 
1 year, 9 months ago
Answer is A
upvoted 
1 
times
nitinz
nitinz
 
1 year, 10 months ago
A is good, they need auth not aunthentication.
upvoted 
2 
timesahmedemad3
ahmedemad3
 
1 year, 10 months ago
ANS: A
CHECK THIS LINK : https://developers.google.com/identity/protocols/oauth2/service-account
upvoted 
4 
times
bnlcnd
bnlcnd
 
1 year, 11 months ago
SAML is mostly for Single Sign On. O-Auth is better for delegation.
upvoted 
4 
times
AshokC
AshokC
 
2 years, 3 months ago
A is correct
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #4
TerramEarth plans to connect all 20 million vehicles in the field to the cloud. This increases the volume to 20 million 600 byte
records a second for 40 TB an hour. 
How should you design the data ingestion? 
A. 
Vehicles write data directly to GCS
B. 
Vehicles write data directly to Google Cloud Pub/Sub 
Most Voted
C. 
Vehicles stream data directly to Google BigQuery
D. 
Vehicles continue to write data using the existing system (FTP)
Correct Answer:
 
B 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
It's Pub/Sub, too much data streaming for Bigquery...
upvoted 
41 
times
alexspam88
alexspam88
 
3 years, 6 months ago
Too much for pubsub either https://cloud.google.com/pubsub/quotas
upvoted 
4 
times
Bill831231
Bill831231
 
3 years, 2 months ago
thanks for sharing the link, but seems pub/sub can handle more streaming data than bigquery. pub/sub 120,000,000 kB per
minute (2 GB/s) in large regions, bigquery is 1GB/s
upvoted 
7 
times
JoeShmoe
JoeShmoe
 
Highly Voted
 
5 years, 1 month ago
Its B, it exceeds the streaming limit for BQ
upvoted 
20 
times
VegasDegenerate
VegasDegenerate
 
Most Recent
 
6 months ago
Community vote distribution
B (68%)
A (26%)
C (5%)Has to be pub-sub, you have remote vehicles and need to guarantee message delivery.
upvoted 
1 
times
the1dv
the1dv
 
8 months, 4 weeks ago
Wow its almost like GCP shouldnt have offloaded their IoT Core product - you cant "Write direct to PubSub".
Its the correct answer but its overly simplified
Writing directly to GCS will cost a fortune to retrieve in GET requests etc
upvoted 
2 
times
Vesta1807
Vesta1807
 
1 year ago
Selected Answer: 
C
Streamed data is available for real-time analysis within a few seconds of the first streaming insertion into a table.
Instead of using a job to load data into BigQuery, you can choose to stream your data into BigQuery one record at a time by
using the tabledata().insertAll() method. This approach enables querying data without the delay of running a load job.
References: https://cloud.google.com/bigquery/streaming-data-into-bigquery
upvoted 
1 
times
MahAli
MahAli
 
1 year ago
Selected Answer: 
A
They are sending files through FTP why everyone is missing this point? The max message size in pub sub is 10MB as I
remember, I would keep the files solution and try to roll out updates to direct the upload to GCS
upvoted 
5 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
So many people pointing out this breaks the BigQuery quota limit but very few pointing out it also breaks the Pub/Sub quote
limit.......... 
So the answer is either not bound by the quota limit (in which case why not BigQuery) both are wrong and we stick
with FTP
upvoted 
1 
times
kapara
kapara
 
1 year, 7 months ago
Selected Answer: 
B
it's B
upvoted 
1 
times
nunopires2001
nunopires2001
 
1 year, 11 months ago
I know it's B, however the sensors are probably legacy systems, that can not communicate to a pub/sub queue. 
Ignoring how huge is to change or adapta 20 million devices is a mistake.
upvoted 
3 
times
omermahgoub
omermahgoub
 
2 years ago
To handle the volume of data that TerramEarth plans to ingest, it is recommended to use a scalable and reliable data ingestion
solution such as Google Cloud Pub/Sub. With Cloud Pub/Sub, the vehicles can stream data directly to the service, which can
handle the high volume of data and provide a buffer to absorb sudden spikes in traffic. The data can then be processed and
stored in a data warehouse such as BigQuery for analysis.
Option A (writing data directly to GCS) may not be suitable for handling high volumes of data in real-time and may result in
data loss if the volume exceeds the capacity of GCS.
Option C (streaming data directly to BigQuery) may not be suitable for handling high volumes of data in real-time as it may
result in data loss or ingestion delays.
Option D (continuing to write data using the existing system) may not be suitable as the current system may not be able to
handle the increased volume of data and may result in data loss or ingestion delays.
upvoted 
11 
times
sank8
sank8
 
2 years ago
correct. thanks for the explanation
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
B
B is the correct answerB is the correct answer
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
ok for B
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
B
B is the correct answer, this similar question was in google simple questions
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
B is right!
upvoted 
2 
times
cdcollector
cdcollector
 
2 years, 6 months ago
Should be A - see next question on 80% cellular connectivity and Avro format files streamed directly to GCS
upvoted 
2 
times
amxexam
amxexam
 
2 years, 7 months ago
Selected Answer: 
B
We need to buffer, the default limit of BigQuery is 100 API calls per second, till now this cannot be changed. Hence we should
ease using Pub/Sub so B.
upvoted 
2 
times
[Removed]
[Removed]
 
2 years, 8 months ago
Selected Answer: 
B
You can request limit increases to use BQ streaming for this load, but why pay to store data before ETL?
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #5
You analyzed TerramEarth's business requirement to reduce downtime, and found that they can achieve a majority of time
saving by reducing customer's wait time for parts. You decided to focus on reduction of the 3 weeks aggregate reporting time. 
Which modifications to the company's processes should you recommend? 
A. 
Migrate from CSV to binary format, migrate from FTP to SFTP transport, and develop machine learning analysis of
metrics
B. 
Migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of
metrics
C. 
Increase fleet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning
analysis of metrics 
Most Voted
D. 
Migrate from FTP to SFTP transport, develop machine learning analysis of metrics, and increase dealer local inventory
by a fixed factor
Correct Answer:
 
C 
Comments
shandy
shandy
 
Highly Voted
 
5 years, 1 month ago
C is right choice because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is
now, collected when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback
loop even more. Machine learning is ideal for predictive maintenance workloads.
A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling
formats and transport doesn't directly help at all. B is not correct because machine learning analysis is a good means toward
the end of reducing downtime, and moving to streaming can improve the freshness of the information in that analysis, but
changing the format doesn't directly help at all. D is not correct because machine learning analysis is a good means toward the
end of reducing downtime, but the rest of these changes don't directly help at all.
upvoted 
36 
times
nick_name_1
nick_name_1
 
1 year, 10 months ago
There are 20 million TerramEarth vehicles in operation ... Approximately 200,000 have cellular connectivity. So, you're saying
for them to keep cost low, increase cell phone bill from 0.01% connected to 80% connected? Statistical Analysis does not
Community vote distribution
C (57%)
B (43%)for them to keep cost low, increase cell phone bill from 0.01% connected to 80% connected? Statistical Analysis does not
require such a large sample size. C CANNOT BE RIGHT.
upvoted 
4 
times
nick_name_1
nick_name_1
 
1 year, 10 months ago
It's B.
upvoted 
4 
times
MrBog1
MrBog1
 
Highly Voted
 
5 years ago
A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling
formats and transport doesn't directly help at all.
B is not correct because machine learning analysis is a good means toward the end of reducing downtime, and moving to
streaming can improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.
C is correct because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now,
collected when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop
even more. Machine learning is ideal for predictive maintenance workloads.
D is not correct because machine learning analysis is a good means toward the end of reducing downtime, but the rest of these
changes don't directly help at all.
upvoted 
22 
times
ccpmad
ccpmad
 
6 months, 3 weeks ago
from PCA samples
upvoted 
2 
times
JohnJamesB1212
JohnJamesB1212
 
Most Recent
 
3 months, 1 week ago
Selected Answer: 
B
B. Migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of
metrics.
Here's why:
Migrating from FTP to streaming transport (e.g., using Google Cloud Pub/Sub) allows near real-time data transfer, significantly
reducing the 3-week delay in reporting by enabling faster data collection and processing.
Migrating from CSV to binary format improves data efficiency by reducing the size of the data payload, speeding up transfer
and processing times.
Developing machine learning analysis of metrics can help predict parts failures and optimize inventory management, further
reducing downtime by ensuring that parts are available when needed.
upvoted 
1 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 1 week ago
The other options are less optimal:
A. Migrating to SFTP wouldn't significantly reduce the reporting time because it's still a batch process.
C. Increasing fleet cellular connectivity may help collect more data, but it doesn't directly address the root issue of reducing
reporting time.
**D. Increasing dealer inventory without addressing the data collection and reporting delays won't optimize the process
effectively.
upvoted 
1 
times
e3e79d9
e3e79d9
 
3 months, 3 weeks ago
b is slightly better than c because compreessed data will allow the pipe to be expanded. 
To Increase cell connectivity could
overload the streaming process without the needed compression.
upvoted 
1 
times
Rehamss
Rehamss
 
3 months, 3 weeks ago
choosing B because it's gonna use Pub/Sub which is what Google wants in this case.
upvoted 
2 
times
46f094c
46f094c
 
6 months, 2 weeks ago
Selected Answer: 
BI don't C as a valid option, cause this might not depend on the company itself, but more on the client side, it will require a big
investing and even maybe not possible because of signal reach to remote locations like fields outside of the cities.
Option B focus on solving what the internal proceses first
upvoted 
2 
times
ccpmad
ccpmad
 
6 months, 3 weeks ago
Selected Answer: 
C
PCA Samples
A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling
formats and transport doesn't directly help at all.
B is not correct because machine learning analysis is a good means toward the end of reducing downtime, and moving to
streaming can improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.
C is correct because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now,
collected when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop
even more. Machine learning is ideal for predictive maintenance workloads.
D is not correct because machine learning analysis is a good means toward the end of reducing downtime, but the rest of these
changes don't directly help at all.
upvoted 
1 
times
Sephethus
Sephethus
 
6 months, 3 weeks ago
C makes no sense, how are you going to improve cellular connectivity with anything Google has to offer? That's a local carrier
thing. B is the answer.
upvoted 
1 
times
erin24330
erin24330
 
9 months, 2 weeks ago
Selected Answer: 
C
this question is from Goolge official PCA samples
upvoted 
4 
times
madcloud32
madcloud32
 
10 months, 1 week ago
Selected Answer: 
B
Answer is B. 
C is wrong suggestion, think of cost and time for 80% cellular connection
upvoted 
1 
times
35cd41b
35cd41b
 
11 months, 2 weeks ago
Selected Answer: 
B
answer is B, binary is faster
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
Selected Answer: 
B
I'm voting B in this one.
My take on it is that increasing the cellular connectivity will generate high costs, and is not the main culprit of the 3 weeks
delay, that is the problem we are trying to solve.
There are two parts of the introductory info that are key
We can say that the info we receive is quite fresh, 9TB a day. That makes increasing connectivity not so useful.
And we also see that the main culprit here is the ETL process. Which would be solved migrating to streaming and handling
binary format instead of FTP with CSVs.
upvoted 
5 
times
e5019c6
e5019c6
 
1 year ago
The two points of the introductory info referred:
1. Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate
of 120 fields of data per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from
these connected vehicles.
2. TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV
files from the field and upload via FTP, transform and aggregate them, and place the data in their data warehouse. Becausefiles from the field and upload via FTP, transform and aggregate them, and place the data in their data warehouse. Because
this process takes time, aggregated reports are based on data that is 3 weeks old.
upvoted 
2 
times
WinSxS
WinSxS
 
1 year, 9 months ago
Selected Answer: 
B
The most effective way to reduce the 3 weeks aggregate reporting time and achieve the business requirement of reducing
downtime would be to migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine
learning analysis of metrics. This would significantly reduce the time it takes to collect and analyze data
upvoted 
4 
times
tdotcat
tdotcat
 
1 year, 11 months ago
Selected Answer: 
B
binary format makes faster bigquery write
https://cloud.google.com/bigquery/docs/write-api#advantages
upvoted 
4 
times
foward
foward
 
1 year, 11 months ago
Selected Answer: 
C
A is not correct because machine learning analysis is a good means toward the end of
reducing downtime, but shuffling formats and transport doesn't directly help at all.
B is not correct because machine learning analysis is a good means toward the end of
reducing downtime, and moving to streaming can improve the freshness of the
information in that analysis, but changing the format doesn't directly help at all.
C is correct because using cellular connectivity will greatly improve the freshness of data
used for analysis from where it is now, collected when the machines are in for
maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop
even more. Machine learning is ideal for predictive maintenance workloads.
D is not correct because machine learning analysis is a good means toward the end of
reducing downtime, but the rest of these changes don't directly help at all.
upvoted 
4 
times
thamaster
thamaster
 
2 years ago
Selected Answer: 
C
This question is in the sample questions from google
A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling
formats and transport doesn't directly help at all.
B is not correct because machine learning analysis is a good means toward the end of reducing downtime, and moving to
streaming can improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.
C is correct because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now,
collected when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop
even more. Machine learning is ideal for predictive maintenance workloads.
D is not correct because machine learning analysis is a good means toward the end of reducing downtime, but the rest of these
changes don't directly help at all.
upvoted 
6 
times
Jackalski
Jackalski
 
2 years, 1 month ago
Selected Answer: 
B
go for B
must go for streaming and faster processing (scalability on binary format)
option C makes no sense as there is no vehicle connectivity problem mentioned (no need to change cellular network)- delay is
after data is already received .
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #6
Which of TerramEarth's legacy enterprise processes will experience significant change as a result of increased Google Cloud
Platform adoption? 
A. 
Opex/capex allocation, LAN changes, capacity planning
B. 
Capacity planning, TCO calculations, opex/capex allocation 
Most Voted
C. 
Capacity planning, utilization measurement, data center expansion
D. 
Data Center expansion, TCO calculations, utilization measurement
Correct Answer:
 
B 
Comments
sri007
sri007
 
Highly Voted
 
3 years, 5 months ago
Correct Answer B
Capacity planning, TCO calculations, opex/capex allocation
From the case study, it can conclude that Management (CXO) all concern rapid provision of resources (infrastructure) for
growing as well as cost management, such as Cost optimization in Infrastructure, trade up front capital expenditures (Capex)
for ongoing operating expenditures (Opex), and Total cost of ownership (TCO)
upvoted 
30 
times
tartar
tartar
 
2 years, 10 months ago
B is ok
upvoted 
7 
times
nitinz
nitinz
 
2 years, 4 months ago
B is correct.
upvoted 
1 
times
nick_name_1
nick_name_1
 
4 months, 2 weeks ago
Only Issue I have w/ B is that they may currently be leasing owned compute, meaning that CapEx/OpEx considerations don't
change.
Community vote distribution
B (92%)
A (8%)change.
upvoted 
2 
times
mudot
mudot
 
Highly Voted
 
1 year, 7 months ago
Selected Answer: 
B
A is not correct because LAN change management processes don't need to change significantly. TerramEarth can easily peer
their on-premises LAN with their Google Cloud Platform VPCs, and as devices and subnets move to the cloud, the LAN team's
implementation will change, but the change management process doesn't have to.
B is correct because all of these tasks are big changes when moving to the cloud. Capacity planning for cloud is different than
for on-premises data centers; TCO calculations are adjusted because TerramEarth is using services, not leasing/buying servers;
OpEx/CapEx allocation is adjusted as services are consumed vs. using capital expenditures.
C is not correct because measuring utilization can be done in the same way, often with the same tools (along with some new
ones). Data center expansion is not a concern for cloud customers; it is part of the undifferentiated heavy lifting that is taken
care of by the cloud provider.
D is not correct because data center expansion is not a concern for cloud customers; it is part of the undifferentiated heavy
lifting that is taken care of by the cloud provider. Measuring utilization can be done in the same way, often with the same tools
(along with some new ones).
upvoted 
6 
times
tdotcat
tdotcat
 
Most Recent
 
5 months, 3 weeks ago
Selected Answer: 
A
sorry not B, 
I think A is right
upvoted 
1 
times
tdotcat
tdotcat
 
5 months, 3 weeks ago
Selected Answer: 
B
TCO does not change as much as ownership of machinary is not changing
upvoted 
1 
times
megumin
megumin
 
8 months ago
Selected Answer: 
B
ok for B
upvoted 
1 
times
Nirca
Nirca
 
11 months, 1 week ago
Selected Answer: 
B
B it is !!!!
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year ago
B is perfect!
upvoted 
1 
times
vincy2202
vincy2202
 
1 year, 6 months ago
Selected Answer: 
B
B is correct answer
upvoted 
1 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
B
vote B
upvoted 
2 
times
kopper2019
kopper2019
 
1 year, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
4 
times
victory108
victory108
 
1 year, 11 months agovictory108
victory108
 
1 year, 11 months ago
B. Capacity planning, TCO calculations, opex/capex allocation
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
1 year, 12 months ago
Answer is B
upvoted 
3 
times
go5
go5
 
2 years, 2 months ago
answer is B
upvoted 
2 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is B
upvoted 
1 
times
wiqi
wiqi
 
2 years, 10 months ago
B is correct.
upvoted 
1 
times
syu31svc
syu31svc
 
3 years ago
GCP practice question confirms that B is correct
upvoted 
4 
times
gfhbox0083
gfhbox0083
 
3 years ago
B, for sure
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #7
To speed up data retrieval, more vehicles will be upgraded to cellular connections and be able to transmit data to the ETL
process. The current FTP process is error-prone and restarts the data transfer from the start of the file when connections fail,
which happens often. You want to improve the reliability of the solution and minimize data transfer time on the cellular
connections. 
What should you do? 
A. 
Use one Google Container Engine cluster of FTP servers. Save the data to a Multi-Regional bucket. Run the ETL process
using data in the bucket
B. 
Use multiple Google Container Engine clusters running FTP servers located in different regions. Save the data to Multi-
Regional buckets in US, EU, and Asia. Run the ETL process using the data in the bucket
C. 
Directly transfer the files to different Google Cloud Multi-Regional Storage bucket locations in US, EU, and Asia using
Google APIs over HTTP(S). Run the ETL process using the data in the bucket
D. 
Directly transfer the files to a different Google Cloud Regional Storage bucket location in US, EU, and Asia using Google
APIs over HTTP(S). Run the ETL process to retrieve the data from each Regional bucket 
Most Voted
Correct Answer:
 
D 
Comments
dabrat
dabrat
 
Highly Voted
 
5 years, 1 month ago
c)
Multi-Region Name Multi-Region Description
asia Data centers in Asia
eu Data centers in the European Union1
us Data centers in the United States
multi-region is a large geographic area, such as the United States, that contains two or more geographic places.
upvoted 
39 
times
JJu
JJu
 
Highly Voted
 
5 years ago
I think answer is C.
Use a multi-region when you want to serve content to data consumers that are outside of the Google network and distributed
across large geographic areas, or when you want the higher availability that comes with being geo-redundant.
Community vote distribution
D (68%)
C (32%)across large geographic areas, or when you want the higher availability that comes with being geo-redundant.
upvoted 
15 
times
valgorodetsky
valgorodetsky
 
Most Recent
 
3 weeks ago
Selected Answer: 
C
They currently have over 500 dealers and service centers in 100 countries
upvoted 
1 
times
Anj_li
Anj_li
 
1 month, 1 week ago
Selected Answer: 
C
Option C is the best choice. It leverages Google Cloud Multi-Regional Storage buckets, which ensures faster global access,
improves reliability, and minimizes data transfer time, especially for cellular connections. Using Google APIs over HTTP(S) for
transfer further improves the overall transfer speed and reliability, as HTTP(S) supports automatic retries and resumable
uploads, which significantly reduces the impact of intermittent connectivity issues on data transfers.
upvoted 
1 
times
Toothpick
Toothpick
 
5 months, 1 week ago
Everyone voting C are missing the point, you are not serving the data, merely ingesting it.
For that , regional buckets provide better latency and bandwidth with lower costs.
Multi-regions can be considered for redundancy in the case of regional failures , but it's costly and extremely unlikely for an
entire region to go down , zonal replication is good enough in this case
upvoted 
1 
times
46f094c
46f094c
 
6 months, 1 week ago
Selected Answer: 
C
in option D imagine you have 1 regional bucket in US-west and the client is in US-east... latency problems right?
with multiregional you don't have the issue... I go for C
upvoted 
2 
times
xaqanik
xaqanik
 
10 months ago
Selected Answer: 
D
Go for D. 
Telemetry data stored in regional (US, Europe and Asia) bucket.
upvoted 
2 
times
edoo
edoo
 
10 months, 3 weeks ago
Selected Answer: 
C
At the beginning I was for B, since it was not clear for me that resumable uploads were available in cloud storage, but they are:
https://cloud.google.com/storage/docs/resumable-uploads
upvoted 
2 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Selected Answer: 
C
I would say it is C
upvoted 
1 
times
35cd41b
35cd41b
 
11 months, 2 weeks ago
C is correct,
upvoted 
1 
times
[Removed]
[Removed]
 
1 year ago
Selected Answer: 
D
The real debate is C vs D. Lets look at some docs:
Low cost is one of the business requirements. Also, regional has higher Performance than multi-regional 
https://cloud.google.com/storage/docs/locations#considerations
Multi regional will be an overkill for analytics as mentioned by Google. 
https://cloud.google.com/storage/docs/locations#location_recommendations
upvoted 
5 
timesupvoted 
5 
times
parthkulkarni998
parthkulkarni998
 
1 year ago
Selected Answer: 
C
Option C seems right here. Directly transferring the files to a different Google Cloud Regional Storage bucket location in US,
EU, and Asia using Google APIs over HTTP(S), would not be an effective solution as it would not improve the reliability of the
solution. Using Google Cloud Multi-Regional Storage, which stores the data in multiple locations, would be a more reliable
solution.
upvoted 
3 
times
jits1984
jits1984
 
1 year, 4 months ago
Selected Answer: 
C
Multi-region
upvoted 
2 
times
rusll
rusll
 
1 year, 4 months ago
Selected Answer: 
D
just look at this: https://cloud.google.com/storage/docs/locations#location_recommendations
upvoted 
2 
times
gotcertified
gotcertified
 
1 year, 6 months ago
Can someone explain why A and B are incorrect ? All the responses focus on C and D.
upvoted 
1 
times
nideesh
nideesh
 
1 year, 2 months ago
Container engine which has recyclable pods as FTP server is far fetched. Compute engine and containers should not be used
for storage. GCS bucket is better as files can be uploaded from anywhere and is reliable and fast.
upvoted 
3 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year, 6 months ago
Selected Answer: 
D
D, multi-region increases latency
upvoted 
4 
times
taer
taer
 
1 year, 9 months ago
Selected Answer: 
C
By directly transferring the files to Google Cloud Multi-Regional Storage buckets using Google APIs over HTTP(S), you will
improve the reliability of the solution and minimize data transfer time on the cellular connections.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #8
TerramEarth's 20 million vehicles are scattered around the world. Based on the vehicle's location, its telemetry data is stored
in a Google Cloud Storage (GCS) regional bucket (US, Europe, or Asia). The CTO has asked you to run a report on the raw
telemetry data to determine why vehicles are breaking down after 100 K miles. You want to run this job on all the data. 
What is the most cost-effective way to run this job? 
A. 
Move all the data into 1 zone, then launch a Cloud Dataproc cluster to run the job
B. 
Move all the data into 1 region, then launch a Google Cloud Dataproc cluster to run the job
C. 
Launch a cluster in each region to preprocess and compress the raw data, then move the data into a multi-region bucket
and use a Dataproc cluster to finish the job
D. 
Launch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and
use a Cloud Dataproc cluster to finish the job 
Most Voted
Correct Answer:
 
D 
Comments
cetanx
cetanx
 
Highly Voted
 
3 years, 5 months ago
I will look at it from a different perspective;
A, B says "move all data" but analysis will try to reveal breaking down after 100K miles so there is no point of transferring data
of the vehicles with less than 100K milage.
Therefore, transferring all data is just waste of time and money.
There is one thing for sure here. If we move/copy data between continents it will cost us money therefore compressing the data
before copying to another region/continent makes sense.
Preprocessing also makes sense because we probably want to process smaller chunks of data first (remember 100K milage).
So now type of target bucket; multi-region or standard? multi-region is good for high-availability and low latency with a little
more cost however question doesn't require any of these features.
Therefore I think standard storage option is good to go given lower costs are always better.
So my answer would be D
upvoted 
65 
times
DiegoQ
DiegoQ
 
3 years, 3 months ago
I totally agree with you, and I think that what confuse people here is the "run a raw data", but preprocess doesn´t mean to
Community vote distribution
D (78%)
C (22%)I totally agree with you, and I think that what confuse people here is the "run a raw data", but preprocess doesn´t mean to
mandatory transform raw data, it could be to only select the data that you need (as you said: vehicles with less than 100K
milage)
upvoted 
2 
times
mrhege
mrhege
 
2 years, 9 months ago
You will need data from non-broken machines too for labelling.
upvoted 
1 
times
stfnz
stfnz
 
7 months, 2 weeks ago
yes, still you will be interested in 100K+ mileage, whether broken or not
upvoted 
1 
times
JoeShmoe
JoeShmoe
 
Highly Voted
 
4 years, 1 month ago
D is the most cost effective and DataProc is regional
upvoted 
32 
times
nitinz
nitinz
 
2 years, 10 months ago
It is D.
upvoted 
1 
times
Rafaa
Rafaa
 
3 years, 7 months ago
Hold on guys, you do not need to 'preprocess' the data. This rules out C,D.
upvoted 
2 
times
guid1984
guid1984
 
2 years, 10 months ago
why not it's a RAW data, so can be pre-processed for optimization
upvoted 
2 
times
passnow
passnow
 
4 years ago
Dataproc can be use global end points too.
upvoted 
1 
times
tartar
tartar
 
3 years, 4 months ago
D is ok
upvoted 
11 
times
passnow
passnow
 
4 years ago
Honestly, if we read the question well and factor in cost, D would be a better option
upvoted 
2 
times
vindahake
vindahake
 
3 years, 9 months ago
I think running additional compute regionally will be more expensive than data transfer charges and centrally processing
them
upvoted 
4 
times
msahdra
msahdra
 
Most Recent
 
1 month ago
Selected Answer: 
C
While regional preprocessing can be efficient, moving the data back to regional buckets after compression defeats the purpose
of a multi-region bucket. It adds unnecessary data transfer costs and reduces the availability of the preprocessed data for
global analysis.
upvoted 
2 
times
thewalker
thewalker
 
1 month, 3 weeks ago
D
Considering https://cloud.google.com/storage/docs/locations#considerations
upvoted 
2 
timesupvoted 
2 
times
Jeena345
Jeena345
 
11 months ago
Selected Answer: 
D
D should be fine
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year ago
Answer is C
To run the report on all of the raw telemetry data for TerramEarth's vehicles in the most cost-effective way, it would be best to
launch a cluster in each region to preprocess and compress the raw data. This will allow you to process the data in place, which
will minimize the amount of data that needs to be transferred between regions. After the data has been preprocessed and
compressed, you can then move it into a multi-region bucket and use a Dataproc cluster to finish the job.
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year ago
D, moving the data into a region bucket and using a Cloud Dataproc cluster to finish the job, would also not be as cost-
effective as moving the data into a multi-region bucket, as it would not take advantage of the lower costs of storing data in a
multi-region bucket.
upvoted 
1 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
D
ok for D
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 2 months ago
Selected Answer: 
D
D seems better
upvoted 
1 
times
AMohanty
AMohanty
 
1 year, 4 months ago
What is the use of Multi-Regional DataProc if ur Storage Data is Regional
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 6 months ago
D is fine, There is no need of multi-region as mentioned in C. D is right in my opinion.
upvoted 
2 
times
vincy2202
vincy2202
 
2 years ago
Selected Answer: 
D
D is the correct answer. Regional bucket is required, since multi regional bucket will incur additional cost to transfer the data to
a centralized location.
upvoted 
2 
times
vincy2202
vincy2202
 
2 years ago
D seems to be the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 1 month ago
Selected Answer: 
D
vote D
upvoted 
2 
times
MaxNRG
MaxNRG
 
2 years, 2 months ago
D – Launch a cluster in each region to pre-process and compress the raw data, then move the data into a regional bucket and
use Cloud Dataproc cluster.
Egress rates are most important. It is free inside of region - so make sense to move all data into one region for
processing/performance (from all continents). Cross-region cost is 0.01$ per GB, and inter-continent 0.12$ per GB. processing/performance (from all continents). Cross-region cost is 0.01$ per GB, and inter-continent 0.12$ per GB. 
If to consider just option B (moving all raw data into one region) then just monthly volume would cost:
900 TB (all 20M units daily) 30 days 0.12 $ = 3.24 M $ (just for data transfer). So, it definitely makes sense to
preprocess/compress data per region, and then move all that data into one region for final analysis. That would save up to 10-
100 times on egress costs. Also, important aspect is processing time - running it in parallel on all regions accelerates overall
analysis effort. Faster result - faster in-field improvements.
Look this interesting video about price optimization in GCP (first 11.5 mins are about Storage/Network)
https://cloud.google.com/storage/docs/locations#considerations
upvoted 
6 
times
victory108
victory108
 
2 years, 5 months ago
D. Launch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and use a
Cloud Dataproc cluster to finish the job
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
2 years, 6 months ago
Answer is D
upvoted 
3 
times
Yogikant
Yogikant
 
2 years, 7 months ago
Answer D:
moving data from one region to another region will incur network egress cost. By compressing data and then moving would
reduce this cost. Though running Dataproc for preprocessing in each region will incur additional cost but it will also reduce cost
of running Dataproc job on all pre-processed data will also reduce cost offsetting additional cost of Dataproc cluster at
regional level.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #9
TerramEarth has equipped all connected trucks with servers and sensors to collect telemetry data. Next year they want to use
the data to train machine learning models. They want to store this data in the cloud while reducing costs. 
What should they do? 
A. 
Have the vehicle's computer compress the data in hourly snapshots, and store it in a Google Cloud Storage (GCS)
Nearline bucket
B. 
Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Google
BigQuery
C. 
Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Cloud Bigtable
D. 
Have the vehicle's computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket 
Most Voted
Correct Answer:
 
D 
Comments
JoeShmoe
JoeShmoe
 
Highly Voted
 
5 years, 1 month ago
D is most cost effective as don't want to use until 'next year'
upvoted 
33 
times
tartar
tartar
 
4 years, 4 months ago
D is ok
upvoted 
8 
times
nitinz
nitinz
 
3 years, 10 months ago
D is most cost effective
upvoted 
2 
times
HCL
HCL
 
3 years, 10 months ago
Hourly snapshots in answer D does not make any sense. 
The answer is B.
upvoted 
1 
times
Community vote distribution
D (82%)
A (18%)rrope
rrope
 
Most Recent
 
1 week, 1 day ago
Selected Answer: 
A
A next year...
upvoted 
1 
times
desertlotus1211
desertlotus1211
 
1 month ago
Selected Answer: 
A
The data is going to be used next year. Which mean infrequent. Coldline is long term storage that doesn't need to be accessed
maybe once every 5 year. retrieval cost is high
upvoted 
1 
times
Sephethus
Sephethus
 
5 months, 4 weeks ago
I hope this question isn't on the test. This quesiton is the most obnoxiously thoughtless and ill considered question I've come
across and there are a lot of really bad ones on this test. 
I would probably pick D. Dataflow can be really expensive.
upvoted 
1 
times
Deb2293
Deb2293
 
1 year, 9 months ago
Selected Answer: 
D
If the words 'next year' wouldn't have been there then Big Table   . But as it will be required next year so Coldline bucket
would be the most cost effective solution.
upvoted 
1 
times
omermahgoub
omermahgoub
 
2 years ago
One option that TerramEarth could consider is storing the telemetry data in a Google Cloud Storage (GCS) Nearline bucket.
This would allow them to store the data in the cloud at a lower cost than other storage options, while still providing quick
access to the data when needed. By having the vehicle's computer compress the data in hourly snapshots, they can reduce the
amount of storage needed and further reduce costs.
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
D
D is the correct answer
Clue is "next year they want to use the data" 
Therefore moving the data to coldline storage makes more sense
upvoted 
4 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
ok for D
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
2 years, 2 months ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
I miss the point about cost optimization and I thought C is right. After reading the discussions I realized D is right answer. I am
going with D
upvoted 
1 
times
Aiffone
Aiffone
 
2 years, 11 months ago
Big query does it. B...when it's long term storage, it costs same as coldline
https://cloud.google.com/bigquery/docs/best-practices-storage
upvoted 
1 
times
Wonka
Wonka
 
2 years, 11 months agoIt cost same as nearline when not accessed, but coldline is cheaper than BQ
upvoted 
4 
times
Aiffone
Aiffone
 
2 years, 12 months ago
the highlight here is machine learning and not disaster recovery or data arhiving which is what coldline storages are for. You
also dont pay for datawarehousing in bigquery until you read from it for machine learning. So its cheap and good for ML. i go
with B
upvoted 
1 
times
vincy2202
vincy2202
 
3 years ago
D is the correct answer
upvoted 
1 
times
mgm7
mgm7
 
3 years ago
D makes sense IF the "computer" on the vehicle can compress data and can take snapshots. 
Are we supposed to to assume that
these "computers" have snapshot capability though it is no stated anywhere in the question? 
Yet, if magically this was possible,
this is the correct answer. 
If this indeed is the correct answer then the only logical deduction is that the questions is stated
horribly. 
I only can hope the real exam isn't like this.
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
D
vote D
upvoted 
2 
times
MaxNRG
MaxNRG
 
3 years, 2 months ago
D – Have vehicle’s computer compress data in hourly snapshots, and store in GCS Coldline bucket.
A – doesn’t work, since Nearline is more expensive than Coldline in D (0.01$ vs 0.007$ GB/month).
B / C – stores compressed data in relational DB, which may not be possible. Even it is implemented, then B (BigQuery) is more
expensive than Cloud Storage Coldline (0.01$ vs 0.007$ GB/month)
C – Bigtable is most expensive option (0.026$ GB/month) and also it is not integrated with Cloud ML (Dataflow, BiqQuery and
Cloud Storage are integrated)
D – Coldline fits perfectly – blob storage, cheapest price, integration with ML
upvoted 
3 
times
victory108
victory108
 
3 years, 5 months ago
D. Have the vehicle
ג
™€s computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #10
Your agricultural division is experimenting with fully autonomous vehicles. You want your architecture to promote strong
security during vehicle operation. 
Which two architectures should you consider? (Choose two.) 
A. 
Treat every micro service call between modules on the vehicle as untrusted. 
Most Voted
B. 
Require IPv6 for connectivity to ensure a secure address space.
C. 
Use a trusted platform module (TPM) and verify firmware and binaries on boot. 
Most Voted
D. 
Use a functional programming language to isolate code execution cycles.
E. 
Use multiple connectivity subsystems for redundancy.
F. 
Enclose the vehicle's drive electronics in a Faraday cage to isolate chips.
Correct Answer:
 
AC 
Comments
kvokka
kvokka
 
Highly Voted
 
4 years, 5 months ago
AC is correct
upvoted 
26 
times
nitinz
nitinz
 
3 years, 4 months ago
Agree with AC
upvoted 
1 
times
tartar
tartar
 
3 years, 10 months ago
AC is ok
upvoted 
10 
times
tluu
tluu
 
Highly Voted
 
2 years, 3 months ago
Correct answer: A & C 
B is not correct because IPv6 doesn't have any impact on the security during vehicle operation, although it improves system
scalability and simplicity.
Community vote distribution
AC (100%)scalability and simplicity.
D is not correct because merely using a functional programming language doesn't guarantee a more secure level of execution
isolation. Any impact on security from this decision would be incidental at best.
E is not correct because this improves system durability, but it doesn't have any impact on the security during vehicle operation.
F is not correct because it doesn't have any impact on the security during vehicle operation, although it improves system
durability.
upvoted 
9 
times
AlizCert
AlizCert
 
Most Recent
 
4 months, 2 weeks ago
CE. Rest is joke.
upvoted 
1 
times
odacir
odacir
 
7 months, 2 weeks ago
Selected Answer: 
AC
A→ Zero Trust → Best Practice.
TPM → More secure.
upvoted 
3 
times
PKookNN
PKookNN
 
10 months ago
Selected Answer: 
AC
A and C is correct
upvoted 
1 
times
nick_name_1
nick_name_1
 
1 year, 4 months ago
A.E. TPM relates to Cloud VM's and has nothing to do with the vehicle's self driving operation.
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
AC
AC is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
AC
A & C are correct
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
Agree with A , C
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years ago
Agreed with all discussions with great detailed explanation as A, C as right ....
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
AC
vote AC
upvoted 
2 
times
civilizador
civilizador
 
2 years, 9 months ago
I don't think it's AC. How hardware solution as TMP is related to the GCP exam. Also if you read the question it says : You want
your architecture to promote strong security during vehicle operation. During operation is the key word. So AE is the correct
answer
upvoted 
4 
times
passtest100
passtest100
 
1 year, 10 months ago
agree. Option E improvies HA, rather than 
the duability as others explainedupvoted 
2 
times
nick_name_1
nick_name_1
 
1 year, 4 months ago
A.E. "Self driving vehicle" most CERTAINLY REQUIRES CONNECTIVITY OR YOU WILL WRECK.
upvoted 
1 
times
victory108
victory108
 
2 years, 11 months ago
A. Treat every micro service call between modules on the vehicle as untrusted.
C. Use a trusted platform module (TPM) and verify firmware and binaries on boot.
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A,C
upvoted 
4 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answers are A, C
upvoted 
1 
times
bnlcnd
bnlcnd
 
3 years, 5 months ago
only A & C seemingly related. no other choices.
upvoted 
2 
times
wiqi
wiqi
 
3 years, 10 months ago
AC makes sense here.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 8
Question #11
Operational parameters such as oil pressure are adjustable on each of TerramEarth's vehicles to increase their efficiency,
depending on their environmental conditions. Your primary goal is to increase the operating efficiency of all 20 million cellular
and unconnected vehicles in the field. 
How can you accomplish this goal? 
A. 
Have you engineers inspect the data for patterns, and then create an algorithm with rules that make operational
adjustments automatically
B. 
Capture all operating data, train machine learning models that identify ideal operations, and run locally to make
operational adjustments automatically 
Most Voted
C. 
Implement a Google Cloud Dataflow streaming job with a sliding window, and use Google Cloud Messaging (GCM) to
make operational adjustments automatically
D. 
Capture all operating data, train machine learning models that identify ideal operations, and host in Google Cloud
Machine Learning (ML) Platform to make operational adjustments automatically
Correct Answer:
 
B 
Comments
JoeShmoe
JoeShmoe
 
Highly Voted
 
5 years, 1 month ago
B is correct. only 200k vehicle's are connected so need to run updates locally
upvoted 
35 
times
exampanic
exampanic
 
5 years ago
In my view, option B says "run locally" referring to the machine learning models. "Machine learning models" is the subject of
the sentence. Nowhere in the sentence says run "updates" locally. 
So running machine learning models would only make sense
in Google's ML platform, not locally. Because of this reason, I believe the correct answer should be "D".
upvoted 
33 
times
cetanx
cetanx
 
4 years, 5 months ago
Both B and D starts with "Capture all operating data, train machine learning models that identify ideal operations, ..." so they
are offering the same method for training the data.
Community vote distribution
B (60%)
D (40%)are offering the same method for training the data.
The keypoint here is "make operational adjustments" such as adjusting the oil pressure so if we host in GCP-ML, how are we
going to instruct vehicles on field to adjust their oil pressure if they have no internet connection? There is no way to use GCP-
ML model generated parameters to command the "not connected" field vehicles to make operational adjustments
automatically.
Therefore, I believe running it locally on the servers sitting in the vehicles is the only option.
My answer: B
upvoted 
33 
times
Vika
Vika
 
3 years, 10 months ago
Making operational adjustments is an operational problem after recommendations are made by ML. In my mind, new data
will keep feeding and total operational data changes every day for model and which would impact model performance
over time. Monitoring model performance to achieve required efficiency levels would need some sort of centralization of
efforts, as every machine environment condition might be different and there might be a need to create multiple models
and test and operate them. (one shoe doesn't fit all).
upvoted 
2 
times
tartar
tartar
 
4 years, 4 months ago
B is ok
upvoted 
13 
times
techalik
techalik
 
4 years, 1 month ago
What you think about D? there is a hint there :D
and host in "Google Cloud Machine Learning" (ML)
upvoted 
4 
times
nandoD
nandoD
 
1 year, 8 months ago
how will the automatic opeartional adjustments be done to the unconnected/offline vehicles?
upvoted 
3 
times
kapa900
kapa900
 
1 year, 6 months ago
end of day
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
B is correct.
upvoted 
2 
times
nick_name_1
nick_name_1
 
1 year, 10 months ago
B says "Capture all operating data". This is not right. You don't need ALL operating data to create an efficiency algorithm. The
Answer is A.
upvoted 
1 
times
dabrat
dabrat
 
Highly Voted
 
5 years, 1 month ago
B)=> unconnected vehicles in the field.
upvoted 
9 
times
Rafaa
Rafaa
 
4 years, 7 months ago
Unconnected vehicles does not mean their data is not on GCP. you would still do ML on GCP and can use that to improve
operational performance via maintenance port.
upvoted 
4 
times
JohnJamesB1212
JohnJamesB1212
 
Most Recent
 
3 months, 1 week ago
Selected Answer: 
B
B. Capture all operating data, train machine learning models that identify ideal operations, and run locally to make operational
adjustments automatically.Here's why:
Capturing all operating data allows you to comprehensively understand vehicle performance under various conditions.
Training machine learning models enables you to identify the ideal operational parameters for each vehicle based on
environmental and operational conditions.
Running these models locally on the vehicles allows real-time adjustments to be made, even for unconnected vehicles,
enhancing operational efficiency without requiring constant communication with the cloud. This is especially important for
vehicles that may not have a constant cellular connection.
upvoted 
2 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 1 week ago
The other options are less optimal because:
A relies on manual rule creation, which is less flexible and less scalable than machine learning.
C involves a streaming job and Google Cloud Messaging, which is not suitable for unconnected vehicles.
D hosting models on the Google Cloud ML Platform requires constant connectivity, which isn't viable for unconnected vehicles.
upvoted 
1 
times
ukivanlamlpi
ukivanlamlpi
 
5 months, 3 weeks ago
Selected Answer: 
D
i think the issue here is that whether or not you want the model learn itself by continue provide the environment condition data
. 
if you build and train the model in old data and deploy it local , will expect no continue learning by fitting new data.
upvoted 
1 
times
A84-64
A84-64
 
6 months, 2 weeks ago
Selected Answer: 
D
I choose answer D. 
For answer B, running ML models locally on each vehicle is resource-intensive and might not be feasible for all vehicle types. It
also makes updating and managing the models more difficult.
upvoted 
1 
times
nanasenishino
nanasenishino
 
8 months ago
Option B seems to be the most comprehensive approach. By capturing all operating data and training machine learning
models to identify ideal operations, you can adaptively adjust operational parameters for each vehicle, thereby increasing
overall efficiency. Hosting these models locally ensures real-time adjustments can be made autonomously without relying on
external services or cloud infrastructure.
The issue with option D is that it relies on hosting machine learning models in the Google Cloud Machine Learning Platform.
While this could still potentially improve efficiency, it introduces additional latency and dependency on external cloud services,
which might not be ideal for real-time operational adjustments required in the field. Additionally, hosting the models in the
cloud could incur ongoing costs and potential connectivity issues, which might not be suitable for all operational environments.
upvoted 
1 
times
didek1986
didek1986
 
11 months, 2 weeks ago
Selected Answer: 
D
It is D
upvoted 
1 
times
e5019c6
e5019c6
 
1 year ago
Selected Answer: 
D
I don't understand why everyone votes for B.
In the replies I see, it seems to me that people are expecting the ML model to run on the vehicles and make changes offline. For
this to be true, the vehicles would need a powerful computer, at least if this model is anywhere close to the AI models around...
So, in this case, the ML model would have to be uploaded to all 20 million vehicles for them to change this parameters offline?
That seems kind of crazy, but maybe I'm lacking some kind of info?
Maybe the ML models are very portable and don't require much processing power?
upvoted 
3 
times
parthkulkarni998
parthkulkarni998
 
1 year ago
Exactly. And considering the newly added data, the model would be updated, which would result in multiple versions of the
model. Better alternative would be to centrally host the model and access it via API/offline via port
upvoted 
1 
timesupvoted 
1 
times
red_panda
red_panda
 
1 year, 6 months ago
Selected Answer: 
B
Also for me it's B.
Maybe it might not seem so straightforward when reading the answer, but personally I understood it as 'running the result of
ML training on devices'. Understanding it this way it certainly can only be B as from the scenario description we have most of
the machines not connected to a cellular network and therefore it is impossible to pass data to them.
upvoted 
2 
times
Arlima
Arlima
 
1 year, 11 months ago
B is correct. only 200k vehicle's are connected so need to run updates locally, means ML Edge
upvoted 
3 
times
dija123
dija123
 
8 months, 3 weeks ago
Yes, Exactly it is ML Edge.
upvoted 
1 
times
ale_brd_111
ale_brd_111
 
2 years ago
Selected Answer: 
B
Answer is B
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
B
ok for B
upvoted 
1 
times
Nirca
Nirca
 
2 years, 3 months ago
Selected Answer: 
B
B is correct. only 200k vehicle's are connected so need to run updates locally
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
B is right , Agreed with detailed discussions !
upvoted 
1 
times
Danny2021
Danny2021
 
3 years, 1 month ago
B. Train model in the cloud and deploy model to the edge for local prediction. This is typical in IoT.
upvoted 
7 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
B
vote B
upvoted 
2 
times
Nik22
Nik22
 
3 years, 3 months ago
from exam point of view, I doubt these questions would be part of new exam. Terram earth case study has changed.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 9
Question #1
For this question, refer to the TerramEarth case study. To be compliant with European GDPR regulation, TerramEarth is required
to delete data generated from its 
European customers after a period of 36 months when it contains personal data. In the new architecture, this data will be
stored in both Cloud Storage and 
BigQuery. What should you do? 
A. 
Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use
gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.
B. 
Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use
gsutil to create a SetStorageClass to NONE action when with an Age condition of 36 months.
C. 
Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For
Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.
Most Voted
D. 
Create a BigQuery time-partitioned table for the European data, and set the partition expiration 
period to 36 months. For
Cloud Storage, use gsutil to create a SetStorageClass to NONE action with an Age condition of 36 months.
Correct Answer:
 
C 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
3 years, 8 months ago
I thought C was correct.
SetStorageClass could not be set to NONE. After data expired, data should be deleted not table.
any comment?
upvoted 
41 
times
techalik
techalik
 
2 years, 7 months ago
C
Enable a bucket lifecycle management rule to delete objects older than 36 months. Use partitioned tables in BigQuery and set the
partition expiration period to 36 months. is the right answer.
When you create a table partitioned by ingestion time, BigQuery automatically loads data into daily, date-based partitions that
Community vote distribution
C (86%)
D (14%)When you create a table partitioned by ingestion time, BigQuery automatically loads data into daily, date-based partitions that
reflect the data's ingestion or arrival time.
Ref: https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time
And Google recommends you configure the default table expiration for your datasets, configure the expiration time for your tables,
and configure the partition expiration for partitioned tables.
Ref: https://cloud.google.com/bigquery/docs/best-practices-
storage#use_the_expiration_settings_to_remove_unneeded_tables_and_partitions
If the partitioned table has a table expiration configured, all the partitions in it are deleted according to the table expiration settings.
For our specific requirement, we could set the partition expiration to 36 months so that partitions older than 36 months (and the
data within) are automatically deleted.
Ref: https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration
upvoted 
24 
times
nitinz
nitinz
 
2 years, 4 months ago
C, partition the data and expire it in big query and use life cycle on GS bucket.
upvoted 
3 
times
AMohanty
AMohanty
 
11 months ago
There is Nothing as Storage Class as NONE
upvoted 
3 
times
mister
mister
 
3 years, 8 months ago
why not A
upvoted 
2 
times
tartar
tartar
 
2 years, 10 months ago
C is ok
upvoted 
11 
times
VishalB
VishalB
 
1 year, 11 months ago
bcoz you would land up creating a table for each day which is not a good practice
upvoted 
2 
times
Wonka
Wonka
 
1 year, 6 months ago
or rather it will delete the entire table and all the data in it i.e. records less than 36 months old
upvoted 
2 
times
Pankonics
Pankonics
 
2 years, 6 months ago
C is the correct ans.
upvoted 
2 
times
MeasService
MeasService
 
Highly Voted
 
3 years, 8 months ago
answer C is the right choice here. Table expiration in BigQuery and life cycle management in GSC
upvoted 
24 
times
passnow
passnow
 
3 years, 6 months ago
i vote C
upvoted 
4 
times
thamaster
thamaster
 
Most Recent
 
6 months, 1 week ago
Selected Answer: 
C
it's C, I'm sure at 100% the other answer are incorrect there is no None as storage class and you need to actually delete data
upvoted 
1 
times
megumin
megumin
 
8 months agoSelected Answer: 
C
ok for C
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
8 months, 2 weeks ago
Selected Answer: 
C
C is correct
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year ago
C is right
upvoted 
2 
times
amxexam
amxexam
 
1 year, 1 month ago
Selected Answer: 
C
partion is way to go with big query hence B & C. 
for block storage C isvtye waybto go.
hence C.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
1 year, 5 months ago
Selected Answer: 
D
'Next year they want to use the data to train machine learning models.'
I agree with D.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
1 year, 5 months ago
I made a mistake in the question to post a comment
upvoted 
1 
times
kimharsh
kimharsh
 
1 year ago
you also screwed up the percentage of the correct answers now :P
upvoted 
3 
times
vincy2202
vincy2202
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
C
vote C
upvoted 
1 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
Correct Answer: C
A – doesn’t work since there is no “retention period” for table, there is only “expiration time” after which it is removed completely.
B/D – doesn’t work, since no such storage class like NONE.
upvoted 
1 
times
[Removed]
[Removed]
 
1 year, 8 months ago
C is correct.
Time-partioned tables AND DELETE data after 36 months using GCS life cycle management.
upvoted 
2 
times
victory108
victory108
 
1 year, 11 months agoC. Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud
Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
1 year, 12 months ago
Answer is C
upvoted 
1 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is C
upvoted 
1 
times
lynx256
lynx256
 
2 years, 3 months ago
IMO - C is ok (assuming DAY or lower level time-partitioning). 
We want to delete only partitions older than 36 month not THE WHOLE tables when aged 36 months.
upvoted 
1 
times
okixavi
okixavi
 
2 years, 6 months ago
C is the right answer
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 9
Question #2
For this question, refer to the TerramEarth case study. TerramEarth has decided to store data files in Cloud Storage. You need
to configure Cloud Storage lifecycle rule to store 1 year of data and minimize file storage cost. 
Which two actions should you take? 
A. 
Create a Cloud Storage lifecycle rule with Age: 
ג
€30
ג
 ,€Storage Class: 
ג
€Standard
ג
 ,€and Action: 
ג
€Set to Coldline
ג
,€
and create a second GCS life-cycle rule with Age: 
ג
€365
ג
 ,€Storage Class: 
ג
€Coldline
ג
 ,€and Action: 
ג
€Delete
ג
 .€
Most Voted
B. 
Create a Cloud Storage lifecycle rule with Age: 
ג
€30
ג
 ,€Storage Class: 
ג
€Coldline
ג
 ,€and Action: 
ג
€Set to Nearline
ג
,€
and create a second GCS life-cycle rule with Age: 
ג
€91
ג
 ,€Storage Class: 
ג
€Coldline
ג
 ,€and Action: 
ג
€Set to Nearline
ג
.€
C. 
Create a Cloud Storage lifecycle rule with Age: 
ג
€90
ג
 ,€Storage Class: 
ג
€Standard
ג
 ,€and Action: 
ג
€Set to Nearline
ג
,€
and create a second GCS life-cycle rule with Age: 
ג
€91
ג
 ,€Storage Class: 
ג
€Nearline
ג
 ,€and Action: 
ג
€Set to Coldline
ג
.€
D. 
Create a Cloud Storage lifecycle rule with Age: 
ג
€30
ג
 ,€Storage Class: 
ג
€Standard
ג
 ,€and Action: 
ג
€Set to Coldline
ג
,€
and create a second GCS life-cycle rule with Age: 
ג
€365
ג
 ,€Storage Class: 
ג
€Nearline
ג
 ,€and Action: 
ג
€Delete
ג
.€
Correct Answer:
 
A 
Comments
VishalB
VishalB
 
Highly Voted
 
1 year, 11 months ago
Answer A
o When Only Option A & D talks about deleting the file after 1 Year. In Option D at Age 30 the storage Class is set to Coldline
and while deleting they have used the condition Storage Class: "Nearline" which is incorrect.
upvoted 
23 
times
H_S
H_S
 
1 year ago
thank you man
upvoted 
2 
times
ale_brd_111
ale_brd_111
 
Most Recent
 
6 months, 1 week ago
Selected Answer: 
A
A) is the correct answer
upvoted 
1 
times
Community vote distribution
A (100%)upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
8 months, 2 weeks ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
Nirca
Nirca
 
9 months, 3 weeks ago
Selected Answer: 
A
A – Create Cloud Storage lifecycle rule with Age: “30”, Storage Class: “Standard” and Action: “Set to Coldline”;
and create a 2nd GCS life-cycle rule with age “365”, Storage Class: “Coldline” and action “Delete”.
upvoted 
4 
times
AzureDP900
AzureDP900
 
1 year ago
A is right!
upvoted 
2 
times
vincy2202
vincy2202
 
1 year, 6 months ago
Selected Answer: 
A
A is thee correct answer
upvoted 
1 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
A
vote A
upvoted 
1 
times
[Removed]
[Removed]
 
1 year, 8 months ago
A is correct.
upvoted 
1 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
A – Create Cloud Storage lifecycle rule with Age: “30”, Storage Class: “Standard” and Action: “Set to Coldline”;
and create a 2nd GCS life-cycle rule with age “365”, Storage Class: “Coldline” and action “Delete”.
D – doesn’t work since 2nd life-cyle rule requires “Nearline” storage, while now data is in “Coldline”.
upvoted 
2 
times
amxexam
amxexam
 
1 year, 9 months ago
The optimal answer is A, but is it Archival for 365 as per docs
https://cloud.google.com/storage/docs/storage-classes#available_storage_classes
upvoted 
1 
times
victory108
victory108
 
1 year, 11 months ago
A. Create a Cloud Storage lifecycle rule with Age: 
ג
€30
ג
 ,€Storage Class: 
ג
€Standard
ג
 ,€and Action: 
ג
€Set to Coldline
ג
 ,€and
create a second GCS life-cycle rule with Age: 
ג
€365
ג
 ,€Storage Class: 
ג
€Coldline
ג
 ,€and Action: 
ג
€Delete
ג
.€
upvoted 
2 
times
JeffClarke111
JeffClarke111
 
1 year, 12 months ago
A is ok
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
1 year, 12 months ago
Answer is A
upvoted 
4 
times
umashankar_a
umashankar_a
 
1 year, 12 months ago
Answer A 
is the correct answer
upvoted 
2 
timesupvoted 
2 
times
shaw2021
shaw2021
 
1 year, 12 months ago
The correct answer is A
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 9
Question #3
For this question, refer to the TerramEarth case study. You need to implement a reliable, scalable GCP solution for the data
warehouse for your company, 
TerramEarth. 
Considering the TerramEarth business and technical requirements, what should you do? 
A. 
Replace the existing data warehouse with BigQuery. Use table partitioning. 
Most Voted
B. 
Replace the existing data warehouse with a Compute Engine instance with 96 CPUs.
C. 
Replace the existing data warehouse with BigQuery. Use federated data sources.
D. 
Replace the existing data warehouse with a Compute Engine instance with 96 CPUs. Add an additional Compute Engine
preemptible instance with 32 CPUs.
Correct Answer:
 
A 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
3 years, 8 months ago
Bigquery partitioning, A. Federated makes no sense...
upvoted 
41 
times
tartar
tartar
 
2 years, 10 months ago
A is ok
upvoted 
9 
times
nitinz
nitinz
 
2 years, 4 months ago
A is correct
upvoted 
2 
times
Ziegler
Ziegler
 
Highly Voted
 
3 years, 1 month ago
A is the correct answer because the question was asking for a reliable way of improving the data warehouse. The reliable way is
to have a table partitioned and that can be well managed. 
https://cloud.google.com/solutions/bigquery-data-warehouse
BigQuery supports partitioning tables by date. You enable partitioning during the table-creation process. BigQuery creates new
Community vote distribution
A (88%)
C (12%)BigQuery supports partitioning tables by date. You enable partitioning during the table-creation process. BigQuery creates new
date-based partitions automatically, with no need for additional maintenance. In addition, you can specify an expiration time
for data in the partitions.
https://cloud.google.com/solutions/bigquery-data-warehouse#partitioning_tables
Federated is an option but not a reliable option.
You can run queries on data that exists outside of BigQuery by using federated data sources, but this approach has
performance implications. Use federated data sources only if the data must be maintained externally. You can also use query
federation to perform ETL from an external source to BigQuery. This approach allows you to define ETL using familiar SQL
syntax.
https://cloud.google.com/solutions/bigquery-data-warehouse#external_sources
upvoted 
23 
times
szagarella
szagarella
 
Most Recent
 
4 months, 1 week ago
Selected Answer: 
A
A is the only correct answer
upvoted 
1 
times
megumin
megumin
 
8 months ago
Selected Answer: 
A
ok for A
upvoted 
1 
times
AzureDP900
AzureDP900
 
8 months, 3 weeks ago
A is fine
upvoted 
2 
times
Nirca
Nirca
 
9 months, 3 weeks ago
Selected Answer: 
A
Bigquery partitioning, A. Federated makes no sense...
upvoted 
1 
times
DrishaS4
DrishaS4
 
11 months ago
Selected Answer: 
A
Bigquery partitioning, A. Federated makes no sense...
upvoted 
1 
times
Nirca
Nirca
 
11 months, 1 week ago
Selected Answer: 
C
C! Expand beyond a single datacenter to decrease latency to the
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year ago
A is right.
upvoted 
1 
times
H_S
H_S
 
1 year ago
Selected Answer: 
A
Bigquery partitioning, A. Federated makes no sense...
upvoted 
1 
times
vincy2202
vincy2202
 
1 year, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
A
vote A
upvoted 
1 
timesupvoted 
1 
times
MaxNRG
MaxNRG
 
1 year, 8 months ago
A – BigQuery in time-partitioned mode.
C – 
federated data source won’t be effective. It assumes that time-series data is stored in BigTable and BigQuery federates this
table for analytics. But, that’s expensive.
- BigTable charges for egress 0.08 $ GB/read (that adds charges in analytics mode)
- BigTable (HDD) – 0.026 $ GB/mo vs BigQuery 0.010 $ GB/mo (and first 10 GB are free monthly).
So, no point for BigTable at all. Stream everything to BiqQuery for storage and analytics. Also, BiqQuery can setup partitions
expiration period.
upvoted 
3 
times
VishalB
VishalB
 
1 year, 11 months ago
Answer A
o Existing Datawarehouse was hosted on single PostgreSQL server on with below configuration, replacing it with serverless
Bigquery using table partition is best recommended soltuion
  RedHat Linux
  64 CPUs 
  128 GB of RAM
  4x 6TB HDD in RAID 0
upvoted 
2 
times
victory108
victory108
 
1 year, 11 months ago
A. Replace the existing data warehouse with BigQuery. Use table partitioning.
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
1 year, 12 months ago
Answer is A
upvoted 
4 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 9
Question #4
For this question, refer to the TerramEarth case study. A new architecture that writes all incoming data to BigQuery has been
introduced. You notice that the data is dirty, and want to ensure data quality on an automated daily basis while managing cost.
What should you do? 
A. 
Set up a streaming Cloud Dataflow job, receiving data by the ingestion process. Clean the data in a Cloud Dataflow
pipeline.
B. 
Create a Cloud Function that reads data from BigQuery and cleans it. Trigger the Cloud Function from a Compute Engine
instance.
C. 
Create a SQL statement on the data in BigQuery, and save it as a view. Run the view daily, and save the result to a new
table.
D. 
Use Cloud Dataprep and configure the BigQuery tables as the source. Schedule a daily job to clean the data. 
Most Voted
Correct Answer:
 
D 
Comments
Sj10
Sj10
 
Highly Voted
 
4 years, 10 months ago
Option D, as data needs to be cleaned ..
Dataprep has the capabilities to clean dirty data
upvoted 
34 
times
tartar
tartar
 
4 years, 4 months ago
D is ok
upvoted 
13 
times
melono
melono
 
2 years, 2 months ago
looks like D
https://cloud.google.com/dataprep
upvoted 
2 
times
motty
motty
 
4 years, 6 months ago
Community vote distribution
D (77%)
A (23%)dataprep is GUI driven process to analyse adhoc data dumped on GCS, it has not place in this use case
upvoted 
5 
times
vindahake
vindahake
 
Highly Voted
 
4 years, 9 months ago
automated daily ... answer is D
upvoted 
12 
times
nbneeraj
nbneeraj
 
Most Recent
 
2 weeks, 5 days ago
Selected Answer: 
A
Question says data is dirty. So we need to clean the dirty data before loading it into big query. Option D says using Dataprep
once dirty data is in big query. 
Option A says use ETL: Dataflow to clean it first. 
A datawarehouse developer will always go with Option A. Use ETL tool to clean the data first and then load in big query
warehouse. 
So the option is A
upvoted 
1 
times
odacir
odacir
 
1 year, 1 month ago
Selected Answer: 
D
Cloud Dataprep is not cheap. Today i will recommend to used a schedule DataForm or dbt for cleaning...
upvoted 
1 
times
red_panda
red_panda
 
1 year, 6 months ago
Selected Answer: 
D
D without any doubt.
Dataflow is for data elaboration. Dataprep is for data preparation (and cleaning).
upvoted 
2 
times
RVivek
RVivek
 
1 year, 10 months ago
Selected Answer: 
D
B & C does not make sense.
A is costly and in realtime
The question says on daily basis and cost effective hence D
upvoted 
4 
times
surajkrishnamurthy
surajkrishnamurthy
 
2 years ago
Selected Answer: 
D
D is the correct answer
upvoted 
2 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
D
D is ok
upvoted 
1 
times
cbarg
cbarg
 
2 years, 3 months ago
Selected Answer: 
D
Ans is D. Please refer to this example: https://medium.com/google-cloud/how-to-schedule-a-bigquery-etl-job-with-dataprep-
b1c314883ab9
upvoted 
4 
times
ShadowLord
ShadowLord
 
2 years, 3 months ago
Selected Answer: 
A
Options should be A.
1. Cost in D would be higher. e.g. First load dirty data into DB and then run Data Prep Jobs to clean the data and load into
some different target Data . Overall cost of scanning the data and the loading is like double the cost. Then identifying already
clean data and dirty data is again a challenge on daily basis after the data growth is significant
2. Data Stream can be utilized to cleanse the data while loading
upvoted 
5 
timesupvoted 
5 
times
dayody
dayody
 
2 years, 3 months ago
you cannot clean data with Dataflow only with Dataprep
upvoted 
2 
times
Begum
Begum
 
3 months, 1 week ago
Why not ?? we have done it using both....
upvoted 
1 
times
DrishaS4
DrishaS4
 
2 years, 5 months ago
Selected Answer: 
D
automated daily ... answer is D
upvoted 
3 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
D is perfect to cleanup the data daily!
upvoted 
2 
times
vincy2202
vincy2202
 
3 years ago
Selected Answer: 
D
D is the correct answer
upvoted 
1 
times
pakilodi
pakilodi
 
3 years, 1 month ago
Selected Answer: 
D
Vote D
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
D
vote D
upvoted 
1 
times
gonzalopf94
gonzalopf94
 
3 years, 1 month ago
Option is A, Dataprep uses a UI to perform the cleaning process and under the hood it is using Dataflow to perform the
process, so I will go with A.
upvoted 
4 
times
[Removed]
[Removed]
 
3 years, 2 months ago
A and D are both will solve the purpose. A is more expensive and ask is daily basis clean-up of data. D is right choice.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 9
Question #5
For this question, refer to the TerramEarth case study. Considering the technical requirements, how should you reduce the
unplanned vehicle downtime in GCP? 
A. 
Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud
Pub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting. 
Most Voted
B. 
Use BigQuery as the data warehouse. Connect all vehicles to the network and upload gzip files to a Multi-Regional Cloud
Storage bucket using gcloud. Use Google Data Studio for analysis and reporting.
C. 
Use Cloud Dataproc Hive as the data warehouse. Upload gzip files to a Multi-Regional Cloud Storage bucket. Upload this
data into BigQuery using gcloud. Use Google Data Studio for analysis and reporting.
D. 
Use Cloud Dataproc Hive as the data warehouse. Directly stream data into partitioned Hive tables. Use Pig scripts to
analyze data.
Correct Answer:
 
A 
Comments
balajee14
balajee14
 
Highly Voted
 
4 years, 8 months ago
Definitely A
upvoted 
40 
times
AdityaGupta
AdityaGupta
 
4 years, 2 months ago
Once all the vehicle are connected to network, there is no need to use FTP; data can be ingested directly to BQ using Pub/Sub
and DataFlow.
upvoted 
7 
times
PRC
PRC
 
Highly Voted
 
4 years, 8 months ago
A is good...simple streaming of data with managed services approach
upvoted 
10 
times
CID2024
CID2024
 
Most Recent
 
6 months, 2 weeks ago
Selected Answer: 
A
Community vote distribution
A (100%)A. Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud
Pub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting.
This approach leverages the real-time data streaming capabilities of Cloud Pub/Sub and Cloud Dataflow, the scalability and
efficiency of BigQuery for data analysis, and the powerful visualization and reporting features of Google Data Studio. This
combination ensures timely insights and quick response to issues, thereby reducing unplanned vehicle downtime.
upvoted 
1 
times
Aninina
Aninina
 
2 years ago
Selected Answer: 
A
A looks like the correct one
upvoted 
1 
times
megumin
megumin
 
2 years, 1 month ago
Selected Answer: 
A
A is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 2 months ago
A is good
upvoted 
1 
times
cbarg
cbarg
 
2 years, 3 months ago
Selected Answer: 
A
Ans is A.
upvoted 
1 
times
AzureDP900
AzureDP900
 
2 years, 6 months ago
A is right, all other options doesn't make sense.
upvoted 
1 
times
H_S
H_S
 
2 years, 6 months ago
Selected Answer: 
A
Definitely A
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 9 months ago
A should be better.
https://cloud.google.com/architecture/designing-connected-vehicle-platform#data_ingestion
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
1 
times
victory108
victory108
 
3 years, 5 months ago
A. Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud
Pub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting.
upvoted 
3 
times
MamthaSJ
MamthaSJ
 
3 years, 6 months ago
Answer is A
upvoted 
4 
times
Ausias18
Ausias18
 
3 years, 9 months ago
answer is Aanswer is A
upvoted 
1 
times
lynx256
lynx256
 
3 years, 9 months ago
A is ok
upvoted 
1 
times
sekhrivijay
sekhrivijay
 
3 years, 10 months ago
Technical requirement : Create a backup strategy
Is bigquery a suitable system for data backup . Wouldn't a better system for backup be cloud storage.
Only B has that option
upvoted 
1 
times
OSNG
OSNG
 
4 years ago
A is correct, using dataflow to clean and/or convert the data for analysis makes more sense.
B does not show any sign of how data will be loaded to bigquery (as gzip) or after conversion, it seems broken process to me.
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 9
Question #6
For this question, refer to the TerramEarth case study. You are asked to design a new architecture for the ingestion of the data
of the 200,000 vehicles that are connected to a cellular network. You want to follow Google-recommended practices. 
Considering the technical requirements, which components should you use for the ingestion of the data? 
A. 
Google Kubernetes Engine with an SSL Ingress
B. 
Cloud IoT Core with public/private key pairs 
Most Voted
C. 
Compute Engine with project-wide SSH keys
D. 
Compute Engine with specific SSH keys
Correct Answer:
 
B 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
4 years, 2 months ago
Why not B?
upvoted 
31 
times
tartar
tartar
 
3 years, 4 months ago
B is ok
upvoted 
10 
times
nitinz
nitinz
 
2 years, 10 months ago
It is B
upvoted 
4 
times
dataqueen_3110
dataqueen_3110
 
Highly Voted
 
11 months ago
Google Cloud IoT Core is being retired on August 16, 2023
upvoted 
21 
times
desertlotus1211
desertlotus1211
 
Most Recent
 
3 weeks, 4 days ago
Selected Answer: 
B
Community vote distribution
B (100%)FYI: 
ClearBlade IoT Core is a replacement product that provides the same functionality as GCP IoT Core. ClearBlade IoT Core offers
1:1 product parity including a Device Table, Security, MQTT+ Messaging, Service Integrations, Edges, and Monitoring.
ClearBlade also provides a free "1-click" migration tool that fully automates the move to ClearBlade IoT Core.
upvoted 
1 
times
megumin
megumin
 
1 year, 1 month ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 6 months ago
IoT core is fine.. B is right!
upvoted 
3 
times
H_S
H_S
 
1 year, 6 months ago
Selected Answer: 
B
It's B
upvoted 
2 
times
Bobch
Bobch
 
2 years ago
Selected Answer: 
B
B is correct
upvoted 
1 
times
vincy2202
vincy2202
 
2 years ago
Selected Answer: 
B
B is the correct answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 1 month ago
Selected Answer: 
B
vote B
upvoted 
1 
times
MaxNRG
MaxNRG
 
2 years, 2 months ago
B – Cloud IoT Core with public / private key pairs.
https://cloud.google.com/iot-core/
IoT Core was developed for connecting existing devices spread around the world to GCP. Also, it supports end-to-end security
using asymmetric key authentication over TLS 1.2. So, this is exact match for Q.
upvoted 
4 
times
victory108
victory108
 
2 years, 5 months ago
B. Cloud IoT Core with public/private key pairs
upvoted 
4 
times
MamthaSJ
MamthaSJ
 
2 years, 6 months ago
Answer is B
upvoted 
3 
times
Ausias18
Ausias18
 
2 years, 9 months ago
Answer is B
upvoted 
1 
times
lynx256
lynx256
 
2 years, 9 months ago
IMO B is ok
upvoted 
1 
times
 
2 years, 9 months agoashish9_a
ashish9_a
 
2 years, 9 months ago
All options look okay but B tops them all.
upvoted 
1 
times
AD3
AD3
 
2 years, 9 months ago
'B' is more correct even though the technical requirement doesn't clearly say about the new technology, the executive summary
does say it "transformation of technology".
upvoted 
1 
times
guid1984
guid1984
 
2 years, 10 months ago
Keywords "You are asked to design a new Architecture" Cloud IOT core is the way for this requirement
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 10
Question #1
For this question, refer to the TerramEarth case study. You start to build a new application that uses a few Cloud Functions for
the backend. One use case requires a Cloud Function func_display to invoke another Cloud Function func_query. You want
func_query only to accept invocations from func_display. You also want to follow Google's recommended best practices. What
should you do? 
A. 
Create a token and pass it in as an environment variable to func_display. When invoking func_query, include the token in
the request. Pass the same token to func_query and reject the invocation if the tokens are different.
B. 
Make func_query 'Require authentication.' Create a unique service account and associate it to func_display. Grant the
service account invoker role for func_query. Create an id token in func_display and include the token to the request when
invoking func_query. 
Most Voted
C. 
Make func_query 'Require authentication' and only accept internal traffic. Create those two functions in the same VPC.
Create an ingress firewall rule for func_query to only allow traffic from func_display.
D. 
Create those two functions in the same project and VPC. Make func_query only accept internal traffic. Create an ingress
firewall for func_query to only allow traffic from func_display. Also, make sure both functions use the same service account.
Correct Answer:
 
B 
Comments
raf2121
raf2121
 
Highly Voted
 
2 years, 10 months ago
B
Authentication function to function calls. Add calling function service account as a member on the receiving function and grant
that member the cloud functions invoker
https://cloud.google.com/functions/docs/securing/authenticating
upvoted 
18 
times
MaxNRG
MaxNRG
 
Highly Voted
 
2 years, 8 months ago
B is correct. You need both service account (Authorization) and id token (Authentication)
When building services that connect multiple functions, it's a good idea to ensure that each function can only send requests to a
specific subset of your other functions. For instance, if you have a login function, it should be able to access the user profiles
function, but it probably shouldn't be able to access the search function.
To configure the receiving function to accept requests from a specific calling function, you need to grant the Cloud Functions
Community vote distribution
B (89%)
D (11%)To configure the receiving function to accept requests from a specific calling function, you need to grant the Cloud Functions
Invoker (roles/cloudfunctions.invoker) role to the calling function's service account on the receiving function.
upvoted 
14 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
Because it will be invoking the receiving function, the calling function must also provide a Google-signed ID token to
authenticate. This is a two step process:
1. Create a Google-signed ID token with the audience field (aud) set to the URL of the receiving function.
2. Include the ID token in an Authorization: Bearer ID_TOKEN header in the request to the function.
https://cloud.google.com/functions/docs/securing/authenticating#authenticating_function_to_function_calls
Authentication function to function calls. Add calling function service account as a member on the receiving function and
grant that member the cloud functions invoker.
upvoted 
8 
times
MaxNRG
MaxNRG
 
2 years, 8 months ago
Have the account you are using to access Cloud Functions assigned a role that contains the cloudfunctions.functions.invoke
permission. By default, the Cloud Functions Admin and Cloud Functions Developer roles have this permission.
https://cloud.google.com/functions/docs/securing/authenticating
Depending on who or what is invoking your cloud function the process for setting up authentication will vary, however there
are two requirements common to all types of authentication:
1. The person or service authorized to invoke the cloud function must be assigned the cloudfunctions.invoker role or some
other role with the cloudfunctions.invoke permission.
2. The person or service authorized to invoke the cloud function must send a token along with the HTTP request to prove that
they are authorized to invoke the cloud function.
https://dev.to/jakewitcher/setting-up-authorization-for-http-cloud-functions-in-gcp-45bc
upvoted 
2 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
B
Vote for B
upvoted 
1 
times
mesodan
mesodan
 
4 months ago
Selected Answer: 
D
B is overkill.Option D provides a more secure, efficient, and manageable solution that adheres to Google's best practices. 
Google Cloud Functions already have built-in mechanisms for:
- Authorization: Each Cloud Function has an associated service account assigned by default. This service account controls who
can invoke the function based on its IAM roles.
- Authentication: Cloud Functions automatically handle authentication for authorized invocations using a secure token
exchange process. You don't need to manually manage ID tokens in this context.
upvoted 
1 
times
mesodan
mesodan
 
4 months ago
B is overkill.Option D provides a more secure, efficient, and manageable solution that adheres to Google's best practices. 
Google Cloud Functions already have built-in mechanisms for:
- Authorization: Each Cloud Function has an associated service account assigned by default. This service account controls who
can invoke the function based on its IAM roles.
- Authentication: Cloud Functions automatically handle authentication for authorized invocations using a secure token
exchange process. You don't need to manually manage ID tokens in this context.
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Nirca
Nirca
 
1 year, 9 months ago
Selected Answer: 
B
service account 
- B!
upvoted 
1 
times
satamex
satamex
 
1 year, 11 months agoJust checking why not A?
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
B makes sense without too much thinking.. This is strict enforcement..
upvoted 
2 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
B
Had this question on my exam.
upvoted 
3 
times
Pime13
Pime13
 
2 years, 6 months ago
Selected Answer: 
B
vote B
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
B
vote B
upvoted 
1 
times
victory108
victory108
 
2 years, 10 months ago
B. Make func_query 'Require authentication.' Create a unique service account and associate it to func_display. Grant the service
account invoker role for func_query. Create an id token in func_display and include the token to the request when invoking
func_query.
upvoted 
8 
times
SweetieS
SweetieS
 
2 years, 10 months ago
B is OK
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 10
Question #2
For this question, refer to the TerramEarth case study. You have broken down a legacy monolithic application into a few
containerized RESTful microservices. 
You want to run those microservices on Cloud Run. You also want to make sure the services are highly available with low
latency to your customers. What should you do? 
A. 
Deploy Cloud Run services to multiple availability zones. Create Cloud Endpoints that point to the services. Create a
global HTTP(S) Load Balancing instance and attach the Cloud Endpoints to its backend.
B. 
Deploy Cloud Run services to multiple regions. Create serverless network endpoint groups pointing to the services. Add
the serverless NEGs to a backend service that is used by a global HTTP(S) Load Balancing instance. 
Most Voted
C. 
Deploy Cloud Run services to multiple regions. In Cloud DNS, create a latency-based DNS name that points to the
services.
D. 
Deploy Cloud Run services to multiple availability zones. Create a TCP/IP global load balancer. Add the Cloud Run
Endpoints to its backend service.
Correct Answer:
 
B 
Comments
MaxNRG
MaxNRG
 
Highly Voted
 
2 years, 8 months ago
B is correct. 
Cloud Run is a regional service. 
To serve global users you need to configure a Global HTTP LB and NEG as the backend.
Cloud Run services are deployed into individual regions and to route your users to different regions of your service, you need to
configure external HTTP(S) Load Balancing.
https://cloud.google.com/run/docs/multiple-regions
A network endpoint group (NEG) specifies a group of backend endpoints for a load balancer. 
A serverless NEG is a backend that points to a Cloud Run, App Engine, or Cloud Functions service.
https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts
upvoted 
31 
times
fahad01hbti
fahad01hbti
 
Highly Voted
 
2 years, 10 months ago
B
https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts
Community vote distribution
B (96%)
C
(4%)https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts
upvoted 
25 
times
rishab86
rishab86
 
2 years, 9 months ago
B is correct
upvoted 
4 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
B
I Vote for B
upvoted 
1 
times
Shin9412
Shin9412
 
1 year, 1 month ago
Selected Answer: 
B
My guess is B
upvoted 
1 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
The correct answer is B. Deploying the microservices to multiple regions will ensure high availability, as it will allow the services
to continue running even if one region experiences an outage. Creating serverless network endpoint groups pointing to the
services and adding them to a backend service used by a global HTTP(S) Load Balancer will allow the load balancer to route
traffic to the closest region, reducing latency for customers.
upvoted 
2 
times
greyhats13
greyhats13
 
1 year, 6 months ago
Why the answer is not A refer to this diagram
https://www.techgeeknext.com/google-cloud-architect/terramearth-case-study
upvoted 
3 
times
Gino17m
Gino17m
 
2 months ago
Cloud Endpoins are rather for building API Gateway and I think can't be a backed for global HTTP(s) Load Balancer
upvoted 
2 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
B
B is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
B
B is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
B
correct answer is B https://cloud.google.com/run/docs/multiple-regions
upvoted 
1 
times
zellck
zellck
 
1 year, 9 months ago
Selected Answer: 
B
B is the answer.
https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts
A serverless NEG is a backend that points to a Cloud Run, App Engine, Cloud Functions, or API Gateway service.
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
B is fine.. I agree with others ..B is fine.. I agree with others ..
upvoted 
1 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
B
Had this question on my exam.
upvoted 
6 
times
zxcv1234
zxcv1234
 
2 years, 6 months ago
Selected Answer: 
B
B is correct. https://cloud.google.com/run/docs/multiple-regions
upvoted 
2 
times
Bobch
Bobch
 
2 years, 6 months ago
Selected Answer: 
B
B is OK
upvoted 
2 
times
SamGCP
SamGCP
 
2 years, 6 months ago
Selected Answer: 
B
https://cloud.google.com/run/docs/multiple-regions
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
C
C is the 
correct answer
upvoted 
1 
times
Arad
Arad
 
2 years, 6 months ago
Selected Answer: 
B
B is correct.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 10
Question #3
For this question, refer to the TerramEarth case study. You are migrating a Linux-based application from your private data
center to Google Cloud. The 
TerramEarth security team sent you several recent Linux vulnerabilities published by Common Vulnerabilities and Exposures
(CVE). You need assistance in understanding how these vulnerabilities could impact your migration. What should you do?
(Choose two.) 
A. 
Open a support case regarding the CVE and chat with the support engineer. 
Most Voted
B. 
Read the CVEs from the Google Cloud Status Dashboard to understand the impact.
C. 
Read the CVEs from the Google Cloud Platform Security Bulletins to understand the impact. 
Most Voted
D. 
Post a question regarding the CVE in Stack Overflow to get an explanation.
E. 
Post a question regarding the CVE in a Google Cloud discussion group to get an explanation.
Correct Answer:
 
AC 
Comments
pakilodi
pakilodi
 
Highly Voted
 
2 years, 7 months ago
Selected Answer: 
AC
A&C. Though if in real life we will do D :-)
upvoted 
37 
times
victory108
victory108
 
Highly Voted
 
2 years, 10 months ago
A. Open a support case regarding the CVE and chat with the support engineer.
C. Read the CVEs from the Google Cloud Platform Security Bulletins to understand the impact.
upvoted 
29 
times
Arhamazhar
Arhamazhar
 
Most Recent
 
1 month ago
AC are right options.
upvoted 
1 
times
steghe
steghe
 
8 months, 3 weeks ago
Community vote distribution
AC (94%)
AD (6%)Selected Answer: 
AC
A & C definitely
upvoted 
3 
times
Risaa
Risaa
 
1 year ago
I think C&E
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
AC
A , C is the correct answer
upvoted 
2 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
AC
AC are ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
AC
best answers are A & C
upvoted 
1 
times
Nirca
Nirca
 
1 year, 11 months ago
Selected Answer: 
AC
we are OK with AC!
upvoted 
2 
times
binpan
binpan
 
1 year, 11 months ago
Selected Answer: 
AD
This is about vulnerabilities in the Linux based application of Terram Earth. You will not find anything in Google dashboards
and bulletins for these. Option A and D are correct as that would be the most logical steps.
upvoted 
4 
times
RitwickKumar
RitwickKumar
 
1 year, 10 months ago
CVEs are known vulnerabilities for open source and are not specific to Terram Earth. The details are available in Google Cloud
Platform Security Bulletins:
https://cloud.google.com/support/bulletins
upvoted 
7 
times
medi01
medi01
 
1 year, 2 months ago
Crazy that nonsensical post like that got 4 upvotes...
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
A,C is right
upvoted 
1 
times
JoeyCASD
JoeyCASD
 
2 years, 1 month ago
I laugh so hard when they reveal the answer including D
Vote A and C
upvoted 
5 
times
salvo007
salvo007
 
5 months, 1 week ago
Google said stackoverflow community is a ok "For Q&A around programming and development. Given the size of the
community, this is a reasonable default for questions about many products"
https://cloud.google.com/support/docs/stackexchangehttps://cloud.google.com/support/docs/stackexchange
upvoted 
1 
times
satamex
satamex
 
1 year, 11 months ago
:P Stackoverflow in GCP PCA exam !! really?? !!
upvoted 
4 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
AC
Had this question on my exam.
upvoted 
5 
times
cloudmon
cloudmon
 
2 years, 2 months ago
The question does not provide sufficient details. Option A assumes that the customer has a support package that is more than
just basic support (because with basic support, engineers would only respond to billing requests). Customers with basic support
are advised to consult StackExchange or Google Discussion groups.
https://cloud.google.com/support
https://cloud.google.com/support/docs/community
upvoted 
2 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
Selected Answer: 
AC
I vote A&C.
upvoted 
2 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
AC
Correct answer is A & C
upvoted 
2 
times
[Removed]
[Removed]
 
2 years, 7 months ago
Selected Answer: 
AC
D marked is wrong
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 10
Question #4
For this question, refer to the TerramEarth case study. TerramEarth has a legacy web application that you cannot migrate to
cloud. However, you still want to build a cloud-native way to monitor the application. If the application goes down, you want the
URL to point to a "Site is unavailable" page as soon as possible. You also want your Ops team to receive a notification for the
issue. You need to build a reliable solution for minimum cost. What should you do? 
A. 
Create a scheduled job in Cloud Run to invoke a container every minute. The container will check the application URL. If
the application is down, switch the URL to the "Site is unavailable" page, and notify the Ops team.
B. 
Create a cron job on a Compute Engine VM that runs every minute. The cron job invokes a Python program to check the
application URL. If the application is down, switch the URL to the "Site is unavailable" page, and notify the Ops team.
C. 
Create a Cloud Monitoring uptime check to validate the application URL. If it fails, put a message in a Pub/Sub queue
that triggers a Cloud Function to switch the URL to the "Site is unavailable" page, and notify the Ops team. 
Most Voted
D. 
Use Cloud Error Reporting to check the application URL. If the application is down, switch the URL to the "Site is
unavailable" page, and notify the Ops team.
Correct Answer:
 
C 
Comments
raf2121
raf2121
 
Highly Voted
 
2 years, 10 months ago
C
Cloud monitoring for Uptime check to validate the application URL and leverage pub/sub to trigger Cloud Function to switch
URL
https://cloud.google.com/monitoring/uptime-checks?hl=en
upvoted 
30 
times
ashish_t
ashish_t
 
2 years, 8 months ago
@raf2121,
You had submitted this question, so you should update the Suggested Answer accordingly. 
Such wrong answer configurations creates confusion.
Anyways thanks for submitting the questions.
upvoted 
2 
times
fahad01hbti
fahad01hbti
 
2 years, 10 months ago
Community vote distribution
C (100%)fahad01hbti
fahad01hbti
 
2 years, 10 months ago
will cloud monitoring work for on prem app without installing stackdriver agent ?
upvoted 
3 
times
MikeB19
MikeB19
 
2 years, 10 months ago
Looks like cloud monitor will work on prem .. c looks correct
https://cloud.google.com/architecture/monitoring-on-premises-resources-with-blue-medora
upvoted 
4 
times
[Removed]
[Removed]
 
Most Recent
 
6 months, 1 week ago
Selected Answer: 
C
https://cloud.google.com/monitoring/api/resources?hl=en#tag_uptime_url
Cloud monitoring works with on-prem?
Yes.
https://cloud.google.com/architecture/monitoring-on-premises-resources-with-bindplane
upvoted 
2 
times
BiddlyBdoyng
BiddlyBdoyng
 
1 year ago
If A is the correct answer what;s the point in the uptime checks in cloud monitoring? 
Why wouldn't we always use Clour Run for
this purpose?
upvoted 
1 
times
stfnz
stfnz
 
1 year, 1 month ago
I understand now... the key is in the question "you still want to build a cloud-native way "... hence only option C, even if A is the
cheapest, it is not cloud-native way.
upvoted 
2 
times
stfnz
stfnz
 
1 year, 1 month ago
though if you think about it really hard - Option A *does* the job and does it at *MINIMAL* cost... so it should be A?
upvoted 
1 
times
stfnz
stfnz
 
1 year, 1 month ago
Selected Answer: 
C
should be C...
upvoted 
1 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
1 
times
megumin
megumin
 
1 year, 7 months ago
Selected Answer: 
C
C is ok
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
C
C is correct https://cloud.google.com/blog/products/management-tools/how-to-use-pubsub-as-a-cloud-monitoring-
notification-channel
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
C is right
upvoted 
1 
times
mad314
mad314
 
2 years, 2 months agomad314
mad314
 
2 years, 2 months ago
Selected Answer: 
C
Had this question on my exam.
upvoted 
4 
times
brushek
brushek
 
2 years, 6 months ago
Selected Answer: 
C
use cloud monitoring
upvoted 
2 
times
ttosl
ttosl
 
2 years, 6 months ago
Selected Answer: 
C
Cloud monitor uptime check can check http(s) outside of GCP. 
This is different from getting other matric like CPU, mem etc.
upvoted 
3 
times
vincy2202
vincy2202
 
2 years, 6 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
[Removed]
[Removed]
 
2 years, 7 months ago
Selected Answer: 
C
Marked A is wrong.
Cloud Monitoring is right for uptime checks.
upvoted 
2 
times
pakilodi
pakilodi
 
2 years, 7 months ago
Selected Answer: 
C
Select C
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
C
vote C
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 10
Question #5
For this question, refer to the TerramEarth case study. You are building a microservice-based application for TerramEarth. The
application is based on Docker containers. You want to follow Google-recommended practices to build the application
continuously and store the build artifacts. What should you do? 
A. 
Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each
microservice, and tag them using the code commit hash. Push the images to the Container Registry. 
Most Voted
B. 
Configure a trigger in Cloud Build for new source changes. The trigger invokes build jobs and build container images for
the microservices. Tag the images with a version number, and push them to Cloud Storage.
C. 
Create a Scheduler job to check the repo every minute. For any new change, invoke Cloud Build to build container
images for the microservices. Tag the images using the current timestamp, and push them to the Container Registry.
D. 
Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build one container image, and tag the
image with the label 'latest.' Push the image to the Container Registry.
Correct Answer:
 
A 
Comments
meh_33
meh_33
 
Highly Voted
 
2 years, 10 months ago
https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash
A is ok
upvoted 
30 
times
ashish_t
ashish_t
 
2 years, 8 months ago
Just above that section, there is a section for the version number.
https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_semantic_versioning
The difference between A and B is how it gets triggered. 
A has "Invoke Cloud Build to build container images"
same with C and D.
B has "The trigger invokes build jobs"
Your pipeline should not have manual steps.
That's why I would choose B.
Community vote distribution
A (92%)
B (8%)That's why I would choose B.
B is correct.
upvoted 
8 
times
cloudmon
cloudmon
 
2 years, 2 months ago
B talks about pushing the images to Cloud Storage, which is not a best practice. A is correct
upvoted 
17 
times
KillerGoogle
KillerGoogle
 
Highly Voted
 
2 years, 10 months ago
A, commit hash is required
upvoted 
9 
times
Gino17m
Gino17m
 
Most Recent
 
2 months ago
Selected Answer: 
A
I vote for A
upvoted 
1 
times
mesodan
mesodan
 
4 months ago
Selected Answer: 
A
Commit Hash Tagging: Tagging images with the code commit hash improves version control and allows identifying the specific
code used in each image.
Container Registry Storage: Pushing images to the Container Registry is a Google-managed service specifically designed for
storing container images, providing security, access control, and integration with other Google Cloud services.
upvoted 
2 
times
thewalker
thewalker
 
7 months, 1 week ago
Selected Answer: 
A
A
Container Registry and not Cloud Storage.
upvoted 
1 
times
Prakzz
Prakzz
 
9 months ago
Selected Answer: 
B
Always give a version number for all images in container registry. No latest hash or anything
upvoted 
2 
times
omermahgoub
omermahgoub
 
1 year, 6 months ago
A. Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each
microservice, and tag them using the code commit hash. Push the images to the Container Registry.
This option follows Google-recommended practices for building and storing the build artifacts for a microservice-based
application. By configuring a trigger in Cloud Build, you can automate the build process and ensure that the build artifacts are
created whenever there are new source changes. By tagging the images with the code commit hash, you can track the changes
and have a record of the build history. Finally, by storing the images in the Container Registry, you can manage and deploy the
artifacts easily.
upvoted 
5 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A is the correct answer
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is correct
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A is fineA is fine
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
A is perfect
upvoted 
2 
times
omodara
omodara
 
2 years ago
A is the correct answer. The question referred to Docker containers not cloud storage.
https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash
upvoted 
2 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
A
I will go with commit hash to tag inwages or time stamp A is better than C so A.
upvoted 
2 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
A
Had this question on my exam.
upvoted 
3 
times
sergaebi
sergaebi
 
2 years, 2 months ago
Selected Answer: 
A
Vote for A
upvoted 
2 
times
Aiffone
Aiffone
 
2 years, 6 months ago
I go with B rather than A because the trigger should invoke the build and versioning is a better way to tag rather than commit
coments
upvoted 
1 
times
Bobch
Bobch
 
2 years, 6 months ago
Selected Answer: 
A
A is correct answer.
Google Cloud has two services for storing and managing container images such as Artifact Registry and Container Registry.
https://cloud.google.com/container-registry/docs/overview
upvoted 
4 
times
mesodan
mesodan
 
2 years, 4 months ago
Agree. B could be an option only if it wasn't for Cloud Storage which can't be used to store container images.
upvoted 
3 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 10
Question #6
For this question, refer to the TerramEarth case study. TerramEarth has about 1 petabyte (PB) of vehicle testing data in a
private data center. You want to move the data to Cloud Storage for your machine learning team. Currently, a 1-Gbps
interconnect link is available for you. The machine learning team wants to start using the data in a month. What should you
do? 
A. 
Request Transfer Appliances from Google Cloud, export the data to appliances, and return the appliances to Google
Cloud. 
Most Voted
B. 
Configure the Storage Transfer service from Google Cloud to send the data from your data center to Cloud Storage.
C. 
Make sure there are no other users consuming the 1Gbps link, and use multi-thread transfer to upload the data to Cloud
Storage.
D. 
Export files to an encrypted USB device, send the device to Google Cloud, and request an import of the data to Cloud
Storage.
Correct Answer:
 
A 
Comments
SuperNest
SuperNest
 
Highly Voted
 
2 years, 10 months ago
USB...where can I buy a 1 PB USB ...?
upvoted 
51 
times
MikeB19
MikeB19
 
2 years, 10 months ago
Lol :)
upvoted 
7 
times
satamex
satamex
 
1 year, 11 months ago
After this question I am sure why all answers are wrong, so the we come to the discussion and increase our knowledge levels
by reading from the high value notes shared by everyone.
upvoted 
7 
times
Timothy_Burton
Timothy_Burton
 
4 months, 1 week ago
Community vote distribution
A (83%)
B (17%)Have you tried B&H? 
jk, that thing would be the size of a small bus and weight severalk tons at least
upvoted 
2 
times
ACE_ASPIRE
ACE_ASPIRE
 
Highly Voted
 
2 years, 9 months ago
Who puts the answer here? 
how can you say that put the data in a USB....It should be A
upvoted 
8 
times
PleeO
PleeO
 
2 years, 9 months ago
definitely A is answer, why has someone placed a trap here?
upvoted 
3 
times
rottzy
rottzy
 
2 years, 8 months ago
;) ;) ;)
upvoted 
1 
times
Pime13
Pime13
 
Most Recent
 
5 months, 1 week ago
Selected Answer: 
A
A. transfer appliance take up 20 days and we have a month: https://cloud.google.com/transfer-
appliance/docs/4.0/overview#transfer-speeds
upvoted 
1 
times
prakata
prakata
 
1 year, 6 months ago
Why can't we use storage transfer service ?
upvoted 
3 
times
sampon279
sampon279
 
1 year ago
Storage Transfer Service is usually for inter cloud, if you don't see any other cloud or cloud storage compliant service as the
source you can ignore that option, as it is in this question - Storage Transfer Service
Transfer data quickly and securely between object and file storage across Google Cloud, Amazon, Azure, on-premises, and
more.
upvoted 
1 
times
thamaster
thamaster
 
1 year, 6 months ago
Selected Answer: 
A
1 pb to move in one month with 1 gbps you need an appliance it will take 3 weeks.
Ans A
upvoted 
2 
times
pawan7869
pawan7869
 
1 year, 6 months ago
jus purchased 1pb usb
upvoted 
6 
times
surajkrishnamurthy
surajkrishnamurthy
 
1 year, 6 months ago
Selected Answer: 
A
A Is the best answer 
1 Pb Data Transfer with 1 Gbps speed takes 124 days to transfer data
upvoted 
1 
times
medi01
medi01
 
1 year, 2 months ago
92, but doesn't matter.
upvoted 
1 
times
AzureDP900
AzureDP900
 
1 year, 8 months ago
A is perfect for this use case
upvoted 
2 
timesupvoted 
2 
times
abdelilahfa
abdelilahfa
 
1 year, 9 months ago
Selected Answer: 
A
It will take 123 days to transfer 
The right answer is to use Transfer Appliance
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time
upvoted 
3 
times
desertlotus1211
desertlotus1211
 
1 year, 10 months ago
Answer is a A..
123 days to transfer 1PB of data using a 1GB link... this is from DC edge to GCP... this may be longer due to internal DC Fabric
design
upvoted 
2 
times
AzureDP900
AzureDP900
 
1 year, 12 months ago
USB is big joke whoever wrote this question they should have make it much better, 
Transfer Appliance is perfect for 1PB. A is
right.
upvoted 
1 
times
Superr
Superr
 
2 years, 1 month ago
Selected Answer: 
A
Transfer appliance can be used to transfer data more than 10 TB from on-prem
upvoted 
3 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
A
There is no special use care to call for B like third party cloud.or any special massage of data required. A is the always way to
go for private sturge and peta byto of data .
A
upvoted 
2 
times
mad314
mad314
 
2 years, 2 months ago
Selected Answer: 
A
Had this question on my exam.
upvoted 
3 
times
kimharsh
kimharsh
 
2 years, 3 months ago
Selected Answer: 
A
IT's A, 
Whoever answered B did you calculate how long it will take for the data to be transferred ? it have to be less than 1 month
from the question requirement 
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time
upvoted 
2 
times
Skr6266
Skr6266
 
2 years, 3 months ago
Selected Answer: 
A
Answer is A. 
https://cloud.google.com/transfer-appliance/docs/4.0/overview#location-availability
With a typical network bandwidth of 100 Mbps, one petabyte of data takes about 3 years to upload. However, with Transfer
Appliance, you can receive the appliance and capture a petabyte of data in under 25 days. Your data can be accessed in Cloud
Storage within another 25 days, all without consuming any outbound network bandwidth.
with 1GBps - online STS will take 124 days ..
upvoted 
3 
times
GauravLahoti
GauravLahoti
 
2 years, 6 months ago
Correct Answer is A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #1
The Dress4Win security team has disabled external SSH access into production virtual machines (VMs) on Google Cloud
Platform (GCP). 
The operations team needs to remotely manage the VMs, build and push Docker containers, and manage Google Cloud Storage
objects. 
What can they do? 
A. 
Grant the operations engineer access to use Google Cloud Shell. 
Most Voted
B. 
Configure a VPN connection to GCP to allow SSH access to the cloud VMs.
C. 
Develop a new access request process that grants temporary SSH access to cloud VMs when an operations engineer
needs to perform a task.
D. 
Have the development team build an API service that allows the operations team to execute specific remote procedure
calls to accomplish their tasks.
Correct Answer:
 
A 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
4 years, 8 months ago
I thought operations team doesn't need SSH access to manage VMs. All it needs is Cloud Shell with the Cloud SDK and gcloud
tools.
Maybe A is correct answer.
upvoted 
33 
times
nitinz
nitinz
 
3 years, 4 months ago
A, you can do pretty much everything from cloud shell.
upvoted 
3 
times
tartar
tartar
 
3 years, 10 months ago
A is ok
upvoted 
8 
times
Crick76
Crick76
 
Highly Voted
 
1 year, 10 months ago
Community vote distribution
A (93%)
D (7%)Crick76
Crick76
 
Highly Voted
 
1 year, 10 months ago
Selected Answer: 
A
Old Case Study 
- Should be removed
upvoted 
11 
times
mesodan
mesodan
 
Most Recent
 
4 months ago
Selected Answer: 
D
While Google Cloud Shell provides access to the GCP console and some tools, it might not offer the full functionality needed for
managing VMs, containers, and Cloud Storage objects. Additionally, granting shell access to the entire console carries a higher
security risk.
upvoted 
1 
times
Mahmoud_E
Mahmoud_E
 
1 year, 8 months ago
Selected Answer: 
A
A is OK
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
1 
times
chorizama
chorizama
 
2 years, 7 months ago
We could misunderstand the question. It's not talking about SSH into the instance to deploy the images. The team only needs an
environment to build and publish to the repositories.
upvoted 
1 
times
victory108
victory108
 
2 years, 11 months ago
A. Grant the operations engineer access to use Google Cloud Shell.
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A
upvoted 
1 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is A
upvoted 
1 
times
VenV
VenV
 
3 years, 3 months ago
how cloudshell works to login to vms if we block port 22 in the firewall rules for external access? try this in your environment and
see if it works.....not A. if we dont block external access, then cloudshell will be good option in this case.
upvoted 
2 
times
lynx256
lynx256
 
3 years, 3 months ago
I think we don't have to login to VMs; we only have to MANAGE them - which is quite different. The same for Docker containers
and GCS objects.
IMO A is the best...
upvoted 
1 
times
lynx256
lynx256
 
3 years, 3 months ago
Of course - you can still SSH to VMS but from Cloud Shell (NOT externally, as task states "security team has disabled external
SSH access).
upvoted 
1 
times
cert2020
cert2020
 
3 years, 4 months ago
Answer A - With Cloud Shell can manage your resources with its online terminal preloaded with utilities.upvoted 
1 
times
aaabbbc1
aaabbbc1
 
3 years, 4 months ago
A will be considered as the final decision, I promise
upvoted 
1 
times
CloudGenious
CloudGenious
 
3 years, 4 months ago
ans is A ..When the ops team login through cloud shell, the credential acc is there.
the ops team engineer typically has all the necessary permission required to manage system such as - build, push docker and
manage. when the team execute command from cloud shell the command will excecute through there credential acc and succed
as log as they have permission ehich they should as ops team. may not able to ssh but i am role let them to carry out action like
start ,stop ,terminate all don't need ssh .
upvoted 
2 
times
bnlcnd
bnlcnd
 
3 years, 5 months ago
A is wrong. how do you push a docker image from your on-prem server to GCP with cloud shell?
B is the only option.
upvoted 
2 
times
BobBui
BobBui
 
3 years, 5 months ago
My choice is A
upvoted 
3 
times
iamoct
iamoct
 
3 years, 5 months ago
This is the official answer. No more argue.
A. Grant operations team access to use Cloud Shell.  
A - The operations team doesn't actually need SSH access to manage VMs. All it
needs is Cloud Shell with the Cloud SDK and gcloud tools.
Cloud Shell provides all the tools for managing Compute Engine instances. In this
case the assumption that SSH access is needed is incorrect.
Business requirement:
"Improve security by defining and adhering to a set of security and Identity and
Access Management (IAM) best practices for cloud."
B - A VPN is a way to connect from remote to the internal IP of an instance. If SSH is
blocked everywhere, this work-around won't help.
C - Developing an application that would use the Cloud API would be redundant with
the gcloud command line tool.
D - An application the provides temporary access to SSH is basically just violating the
security practices.
upvoted 
10 
times
okixavi
okixavi
 
3 years, 6 months ago
I'll go with A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #2
At Dress4Win, an operations engineer wants to create a tow-cost solution to remotely archive copies of database backup files. 
The database files are compressed tar files stored in their current data center. 
How should he proceed? 
A. 
Create a cron script using gsutil to copy the files to a Coldline Storage bucket.
B. 
Create a cron script using gsutil to copy the files to a Regional Storage bucket.
C. 
Create a Cloud Storage Transfer Service Job to copy the files to a Coldline Storage bucket. 
Most Voted
D. 
Create a Cloud Storage Transfer Service job to copy the files to a Regional Storage bucket.
Correct Answer:
 
C 
Comments
Ayzen
Ayzen
 
Highly Voted
 
3 years, 8 months ago
Should be C: https://cloud.google.com/storage-transfer/docs/on-prem-overview
Especially, when Google docs explicitly states, that custom scripts are unreliable, slow, insecure, difficult to maintain and
troubleshoot.
upvoted 
37 
times
cetanx
cetanx
 
3 years, 5 months ago
I would go with A
Storage Transfer Service has many valuable features but it comes with some dependencies such as;
- min 300-Mbps internet connection
- A docker engine on-prem (app runs inside a container)
https://cloud.google.com/storage-transfer/docs/on-prem-overview#what_requirements_does_have
... and these may not be available at Dress4Win (we have no data if D4W satisfies these requirements)
Based on the recommendations here: https://cloud.google.com/storage-transfer/docs/overview#gsutil
[# gsutil rsync] command seems to be a better option in a cron job with regular intervals as it will be much easier to implement
compared to setting up Storage Transfer Service.
upvoted 
8 
times
Jphix
Jphix
 
2 years, 12 months ago
Community vote distribution
C (100%)Jphix
Jphix
 
2 years, 12 months ago
We[re talking about potentially 100s of TBs of data based on the case study (at least 65TBs as that's how much they are using
in their NAS storage for backups/logs). I certainly hope they have the minimum 300-Mbps connection and a computer in their
data center that they can install docker on....
upvoted 
5 
times
[Removed]
[Removed]
 
3 months, 3 weeks ago
https://cloud.google.com/storage-transfer/docs/transfer-options
upvoted 
2 
times
SamirJ
SamirJ
 
Highly Voted
 
3 years, 2 months ago
Answer should be C. As per the latest case study on google cloud website , they have DB storage of 1 PB out of which 600 TB is
used. So you get the size of the data.
These are the thumb rules as per GCP documentation - 
Transfer scenario Recommendation
Transferring from another cloud storage provider 
Use Storage Transfer Service
Transferring less than 1 TB from on-premises 
Use gsutil
Transferring more than 1 TB from on-premises 
Use Transfer service for on-premises data
https://cloud.google.com/storage-transfer/docs/overview
upvoted 
21 
times
AdityaGupta
AdityaGupta
 
3 years, 2 months ago
I agree with Samir, when there is nothing mentioned about data size, refer the case study again. Storage appliance section
mentioned total size and available size. Which means we should be using storage transfer service. I will go with option C.
upvoted 
2 
times
massacare
massacare
 
Most Recent
 
4 months, 3 weeks ago
Selected Answer: 
C
Although Dress4Win already removed from PCA case studies list, the answer should be C.
upvoted 
2 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
1 year, 2 months ago
IMPORTANT: Dress4Win is not anymore part of the officially listed case studies:
https://cloud.google.com/certification/guides/professional-cloud-architect
upvoted 
9 
times
alexandercamachop
alexandercamachop
 
1 year, 3 months ago
Selected Answer: 
C
Answer is C.
upvoted 
1 
times
ramzez4815
ramzez4815
 
1 year, 4 months ago
Selected Answer: 
C
C is the correct answer
upvoted 
2 
times
Aiffone
Aiffone
 
1 year, 11 months ago
I'd go with C, transfer service. gsutil is best used for transfer within GCS
upvoted 
1 
times
burner_1984
burner_1984
 
1 year, 11 months ago
Storage Transfer Service is to be used when data is available online, not in physical datacenter
upvoted 
1 
times
GCPCloudArchitectUser
GCPCloudArchitectUser
 
1 year, 11 months ago
Dress4Win case is not listed as exam case study 
https://cloud.google.com/certification/guides/professional-cloud-architecthttps://cloud.google.com/certification/guides/professional-cloud-architect
upvoted 
4 
times
ravisar
ravisar
 
2 years, 1 month ago
Here are the guidelines from Google: 
From Azure/AWS Transfer: Storage Transfer Service 
Between two different bucket: Storage Transfer service 
For less than 1 TB From Private datacenter to Google: gsutil 
For more than 1 TB with enough bandwidth for Private datacenter to Google 
- Use Storage Transfer Service for on-premises data 
Not enough bandwidth to meet project deadline for private data center to Google for more than 1 TB - Transfer Appliance.
(Transfer Appliance is recommended for data that exceeds 20 TB or would take more than a week to upload)
I assume the DB size will be more than 1 TB. 
(2 million TerramEarth vehicles each generate generates 200 to 500 megabytes of
data per day)
Since it is more than 1 TB, based on google guidelines, I will go with Storage Transfer Service Answer C 
https://cloud.google.com/storage-transfer/docs/overview
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets
Case: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdf
upvoted 
1 
times
GCPCloudArchitectUser
GCPCloudArchitectUser
 
1 year, 11 months ago
This question is for Dress4Win case study and you are referring different one
upvoted 
1 
times
joe2211
joe2211
 
2 years, 1 month ago
Selected Answer: 
C
vote C
upvoted 
1 
times
Amirso
Amirso
 
2 years, 3 months ago
IMO option A is correct.
According to the technical requirement;
- Support multiple VPN connections between the production data center and cloud 
environment.
Cloud VPN tunnel can support up to 3 gigabits per second (Gbps).
There is no deadline for this usecase, And also by considering the industry I can say the database size wouldn’t be bigger than
1TB; hence gsutil is suitable for this case.
upvoted 
1 
times
victory108
victory108
 
2 years, 5 months ago
C. Create a Cloud Storage Transfer Service Job to copy the files to a Coldline Storage bucket.
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
2 years, 6 months ago
Answer is C
upvoted 
2 
times
Pb55
Pb55
 
2 years, 8 months ago
Follow these rules of thumb when deciding whether to use gsutil or Storage Transfer Service:
Transfer scenario Recommendation
Transferring from another cloud storage provider Use Storage Transfer Service.
Transferring less than 1 TB from on-premises Use gsutil.
Transferring more than 1 TB from on-premises Use Transfer service for on-premises data.
Transferring less than 1 TB from another Cloud Storage region Use gsutil.
Transferring more than 1 TB from another Cloud Storage region Use Storage Transfer Service.
https://cloud.google.com/storage-transfer/docs/overview
upvoted 
1 
times
jasim21
jasim21
 
2 years, 8 months ago
Answer is C
Current DB disk size is 5 TB & backup size is 600 TB.
If size is more than 1 TB google recommend transfer service. regardless from other cloud/on-premise
https://cloud.google.com/storage-transfer/docs/overview#gsutil
upvoted 
2 
timesupvoted 
2 
times
mrhege
mrhege
 
2 years, 9 months ago
"Fibre channel SAN - MySQL databases
- 1 PB total storage; 400 TB available"
Definitely a use-case for Storage Transfer Service. (C)
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #3
Dress4Win has asked you to recommend machine types they should deploy their application servers to. 
How should you proceed? 
A. 
Perform a mapping of the on-premises physical hardware cores and RAM to the nearest machine types in the cloud.
B. 
Recommend that Dress4Win deploy application servers to machine types that offer the highest RAM to CPU ratio
available.
C. 
Recommend that Dress4Win deploy into production with the smallest instances available, monitor them over time, and
scale the machine type up until the desired performance is reached.
D. 
Identify the number of virtual cores and RAM associated with the application server virtual machines align them to a
custom machine type in the cloud, monitor performance, and scale the machine types up until the desired performance is
reached. 
Most Voted
Correct Answer:
 
D 
Comments
MyPractice
MyPractice
 
Highly Voted
 
4 years, 6 months ago
A - not correct. as its talking about Physical server size
B - not correct. as we its talking about Max spec
C - not correct. as its talking about the Smallest spec
D - is CORRECT. as its recommending to map with on premises app VM Size
upvoted 
40 
times
Smart
Smart
 
4 years, 4 months ago
Agree. Starting with a minimal size at which your application can run efficiently/optimally is the best practice. This would be
best estimate from past usage so monitor closely and apply vertical and horizontal scaling.
upvoted 
3 
times
Ziegler
Ziegler
 
4 years, 1 month ago
What if the on-premises workloads are oversized? C is the correct answer instead
upvoted 
4 
times
Community vote distribution
D (53%)
A (47%)JMSTP
JMSTP
 
2 years, 8 months ago
Agreed, but confused as question states they're moving Test/Dev first and C says "into Production". 
If they're looking to save
money, start small and grow until performance needs are met. 
Test/Dev is typically tolerant to these incremental changes.
upvoted 
1 
times
Rafaa
Rafaa
 
4 years ago
They have scaling problem, hence decided to move to GCP.
upvoted 
3 
times
rottzy
rottzy
 
2 years, 8 months ago
Also, cost-cutting was a primary concern = while moving to cloud
upvoted 
2 
times
jcmoranp
jcmoranp
 
Highly Voted
 
4 years, 8 months ago
It's D. You can monitor machines to scale until necessary
upvoted 
25 
times
techalik
techalik
 
3 years, 7 months ago
D i think:
Start with the smallest instances and scale up to a larger machine type until the performance is of the desired standard. is the
right answer.
In continuation of the above explanation, although you could use a predefined machine type like e2-highmem-4/n2-
highmem-4/n2d-highmem-4 etc. if you need 4 VCPUs and 32GB memory, there's no guarantee that it performs similar to the
existing VM in the data centre. The networking fabric is different, the disk I/O is different, and the CPUs are different too. We
don't know the exact specifications of the data centre CPUs to draw a parallel to the processors offered by GCP. As you can
see, the performance can vary a lot depending on the frequency. (remember shelling out additional 500$ for upgrading CPU
from 2.6GHz to 2.8GHz when buying your a laptop?). You may realize that you need more vCPUs after migrating to Google
Cloud or maybe less, but until you migrate and test it out, there is no way to say which is the best machine type. So the
recommendation should be to start small, increase the instance size as needed until the performance is of an acceptable
standard, and that is your machine type.
upvoted 
3 
times
techalik
techalik
 
3 years, 7 months ago
I meant C :) explanation is for C
upvoted 
3 
times
nitinz
nitinz
 
3 years, 4 months ago
C is the best answer. Thats the beauty of cloud. You can change machine type by just shutting it down. Also Google
discourages using custom VM sizing.
upvoted 
2 
times
Pankonics
Pankonics
 
3 years, 6 months ago
C is correct. In option D, it says No.of virtual cores and RAM associated... Which is not mentioned in Case study as well. This
option just trying to confuse that set. So will go with C.
upvoted 
1 
times
chiar
chiar
 
4 years, 7 months ago
Remember that the VM's CPU RAM and Disk 
are designed in GCP for optimized performance, I think C is the best
upvoted 
3 
times
MyPractice
MyPractice
 
4 years, 6 months ago
the smallest instance avail is as low as 1CPU with 128MB RAM - Is that sufficient for prod run (assume currently its running
with 24CPU with 128GB of ram in on premise)
upvoted 
5 
times
Rathish
Rathish
 
4 years, 1 month ago
I thought, only dev and testing environment migrated first and in that case, I prefer to start with smallest instance available.I thought, only dev and testing environment migrated first and in that case, I prefer to start with smallest instance available.
- Dynamic scalability is the reason, we are moving to cloud.
upvoted 
2 
times
kk4gcp
kk4gcp
 
3 years, 4 months ago
but the option c says - C. Recommend that Dress4Win deploy into production - they are talking about production - so D
would be correct
upvoted 
2 
times
bigzero
bigzero
 
Most Recent
 
1 month, 3 weeks ago
Option A
D can do after migration finished in production environment
before migration, you can only do the option A
upvoted 
1 
times
hynglly
hynglly
 
6 months ago
Selected Answer: 
D
it is D,
upvoted 
1 
times
MahAli
MahAli
 
6 months, 3 weeks ago
Selected Answer: 
A
Application servers as mentioned in the study, doesn't mean VMs, nothing is mentioning VMs on prem, besides custom types on
GCP cost more than predefined types, I'll map their physical servers to the nearest machine type.
upvoted 
2 
times
luke19962023
luke19962023
 
1 year, 2 months ago
Selected Answer: 
D
since they have VM's in current environment, I'll choose D. This lets you like for like replace the on prem footprint and scale
from there.
I originally thought A, however, using custom machine image will prevent you from paying for running a larger machine image
than necessary.
upvoted 
1 
times
taer
taer
 
1 year, 3 months ago
Selected Answer: 
D
By identifying the number of virtual cores and RAM associated with the application server virtual machines and aligning them
to custom machine types in the cloud, you can create an environment tailored to Dress4Win's specific needs. Monitoring
performance and adjusting the machine types accordingly ensures that the infrastructure is optimized for both performance
and cost.
upvoted 
1 
times
BeCalm
BeCalm
 
1 year, 3 months ago
Selected Answer: 
A
D is an iterative approach to finding the right specs which is definitely not the way it works IRL.
upvoted 
1 
times
stevehlw
stevehlw
 
1 year, 7 months ago
I'll be truly surprised if the correct answer is C.
Deploying the smallest instances into production will likely trigger a ton of error messages in real life.
If you're oversizing the VMs, just simply scale it down.
upvoted 
2 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
1 year, 8 months ago
IMPORTANT: Dress4Win is not anymore part of the officially listed case studies:
https://cloud.google.com/certification/guides/professional-cloud-architect
upvoted 
5 
times
AhmedH7793
AhmedH7793
 
1 year, 9 months ago
Selected Answer: 
DSelected Answer: 
D
It is D
upvoted 
1 
times
ShadowLord
ShadowLord
 
1 year, 9 months ago
Selected Answer: 
A
https://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing
1. Performance-based recommendations: Recommends Compute Engine instances based on the CPU and RAM currently
allocated to the on-premises VM. This recommendation is the default.
upvoted 
5 
times
AlizCert
AlizCert
 
1 year, 1 month ago
I agree with the thought process, but A is about the _physical_ HW that hosts the on-prem VMs, not about the VM
requirements, so it should be D.
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 3 months ago
A should be better.
https://cloud.google.com/architecture/resource-mappings-from-on-premises-hardware-to-gcp
upvoted 
3 
times
Matalf
Matalf
 
1 year, 11 months ago
Resource Mapping: "In a scenario where your applications are running on bare-metal "
Rightsizing: "migrating a virtual machine to Compute Engine"
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
Selected Answer: 
D
I vote D.
https://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing
upvoted 
2 
times
MF2C
MF2C
 
2 years, 5 months ago
Selected Answer: 
D
vote D
upvoted 
1 
times
atlasga
atlasga
 
2 years, 6 months ago
Wow, these answers are so dumb I'm compelled to comment so new people don't mistakenly think this is how it works in the
real world. You're not supposed to map sizes one-to-one OR start with the smallest instance sizes available. The industry best
practice is to right-size based on actual utilization. I guess I'll pick C if this question comes up, because at least it will be
cheaper than starting with big instances (but it could cause production operational issues if the instances are undersized!).
upvoted 
2 
times
theBestStudent
theBestStudent
 
6 months, 4 weeks ago
but then, you don't want to cause production issues for wrong sizing. If so, it ca not be C in my opinion. The main idea is
always avoid prod issues
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
D
vote D
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #4
As part of Dress4Win's plans to migrate to the cloud, they want to be able to set up a managed logging and monitoring system
so they can handle spikes in their traffic load. 
They want to ensure that: 
* The infrastructure can be notified when it needs to scale up and down to handle the ebb and flow of usage throughout the
day 
* Their administrators are notified automatically when their application reports errors. 
* They can filter their aggregated logs down in order to debug one piece of the application across many hosts 
Which Google StackDriver features should they use? 
A. 
Logging, Alerts, Insights, Debug
B. 
Monitoring, Trace, Debug, Logging
C. 
Monitoring, Logging, Alerts, Error Reporting 
Most Voted
D. 
Monitoring, Logging, Debug, Error Report
Correct Answer:
 
C 
Comments
AWS56
AWS56
 
Highly Voted
 
4 years, 5 months ago
It is C. 
With D you cannot achieve "Their administrators are notified automatically when their application reports errors." However
with Error Reporting(Log insights) - Notifies you when new errors are detected.
So I will go with C
upvoted 
43 
times
Jos
Jos
 
4 years, 5 months ago
You're wrong, with Reporting (https://cloud.google.com/error-reporting) you can: 
"...Opt in to receive email and mobile alerts
on new errors."
upvoted 
4 
times
amxexam
amxexam
 
2 years, 9 months ago
There is no requirement for debugging
Community vote distribution
C (59%)
D (41%)There is no requirement for debugging
upvoted 
7 
times
JoeShmoe
JoeShmoe
 
Highly Voted
 
4 years, 7 months ago
D is correct
upvoted 
21 
times
TiagoM
TiagoM
 
3 years, 2 months ago
Assuming this is an old question, Alert feature was a different section in Stackdriver and Debug is not mentioned in the
requirements I would pick C.
C in the current GCP setup doesnt make sence because Alerts are inside Monitoring. The only problem with D is that the
Debug is not requested like I said.
upvoted 
6 
times
mesodan
mesodan
 
Most Recent
 
4 months ago
Selected Answer: 
C
Debug is not needed.
upvoted 
1 
times
Gall
Gall
 
4 months, 3 weeks ago
Selected Answer: 
C
C. There's no " Error Report" StackDrive feature.
upvoted 
1 
times
hynglly
hynglly
 
6 months ago
Selected Answer: 
C
It is C
upvoted 
1 
times
MahAli
MahAli
 
6 months, 3 weeks ago
Selected Answer: 
C
Think it's c
upvoted 
1 
times
Shinobi75
Shinobi75
 
6 months, 3 weeks ago
It is C. Debug is not needed.
upvoted 
1 
times
theBestStudent
theBestStudent
 
6 months, 4 weeks ago
Selected Answer: 
D
I changed my mind after re reading the question, they certainly mentioned "debug application..." Correct answer is D, but Hey,
Cloud debugger is deprecated now.
upvoted 
1 
times
theBestStudent
theBestStudent
 
6 months, 4 weeks ago
Selected Answer: 
C
As of today, cloud debugger is deprecated. Either way there is no need of it based on the question's context. Correct answer is
C.
upvoted 
1 
times
Ahmed_Safwat
Ahmed_Safwat
 
7 months, 1 week ago
Selected Answer: 
D
Google Cloud's operations suite (formerly Stackdriver) has alerting service under monitoring
https://cloud.google.com/products/operations#all-features
upvoted 
1 
times
salim_
salim_
 
1 year, 1 month ago
Selected Answer: 
DSelected Answer: 
D
Cloud Logging: 
• Logging allows you to store, 
search, analyze, monitor, and alert on log data and events from GCP an AWS
• 30 Days retention
Error Reporting:
• Error Reporting counts, analyses, and aggregates the errors in your running Cloud services
Cloud Debugger:
• To inspect the state of a running application in real time without stopping or slowing it.
• Specifically, the debugger adds less than 10 milliseconds to the request latency when the application state is captured.
• To understand the behavior of your code in production and analyze its state to locate those hard to find bugs.
Cloud Trace :
• It's a distributed tracing system that collects latency data from your applications and displays it in the GCP console
upvoted 
1 
times
kc4
kc4
 
11 months ago
D doesn't provide alerts feature which is one of the requirements "Their administrators are notified automatically when their
application reports errors."
upvoted 
1 
times
taer
taer
 
1 year, 3 months ago
Selected Answer: 
C
Stackdriver Alerts can notify the infrastructure when it needs to scale up or down based on the traffic load.
upvoted 
1 
times
[Removed]
[Removed]
 
1 year, 3 months ago
Selected Answer: 
D
no alerts component on Stackdriver
upvoted 
1 
times
RVivek
RVivek
 
1 year, 4 months ago
Selected Answer: 
D
StackDriver's features:
1.Stackdriver Monitoring
2.Stackdriver Logging and Error Reporting
3.Stackdriver Debugger
4.Stackdriver Trace
5.Stackdriver Profiler
https://versprite.com/blog/security-operations/google-
stackdriver/#:~:text=Stackdriver%20Logging%20allows%20you%20to,API%20to%20manage%20logs%20programmatically.
upvoted 
1 
times
gonlafer
gonlafer
 
1 year, 6 months ago
Selected Answer: 
D
Initially I was to say C, but Stackdriver services are:
Monitoring, Logging, Debug, Error Report ,Trace and Profiler
So D
upvoted 
1 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
C
As answered below
upvoted 
1 
times
nkit
nkit
 
2 years, 2 months ago
Selected Answer: 
C
Debugging is not part of the request, hence I would not select D.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #5
Dress4Win would like to become familiar with deploying applications to the cloud by successfully deploying some applications
quickly, as is. They have asked for your recommendation. 
What should you advise? 
A. 
Identify self-contained applications with external dependencies as a first move to the cloud. 
Most Voted
B. 
Identify enterprise applications with internal dependencies and recommend these as a first move to the cloud.
C. 
Suggest moving their in-house databases to the cloud and continue serving requests to on-premise applications.
D. 
Recommend moving their message queuing servers to the cloud and continue handling requests to on-premise
applications.
Correct Answer:
 
A 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
4 years, 8 months ago
A looks better for me.
upvoted 
38 
times
PRC
PRC
 
Highly Voted
 
4 years, 2 months ago
A for me..Self contained application with no internal dependencies which means it does not need to integrate with any on-
premise systems. External dependencies are easier to manage through API based integration in Cloud..Other options have
either dependencies, or multiple hops between on-premise/cloud thereby causing latency issues,
upvoted 
29 
times
mesodan
mesodan
 
Most Recent
 
4 months ago
Selected Answer: 
A
Moving databases to the cloud while keeping applications on-premises approach creates a hybrid environment with increased
complexity, introducing challenges in managing network connectivity, latency, and security between on-premises and cloud
components.
upvoted 
1 
times
Jconnor
Jconnor
 
7 months ago
Community vote distribution
A (83%)
C (17%)Whoever says C was never involved in a real life migration of a production system. 
A. They are looking to gain experience, not
to shutdown the whole company.
upvoted 
3 
times
WinSxS
WinSxS
 
1 year, 3 months ago
Selected Answer: 
A
To become familiar with deploying applications to the cloud, it is recommended to start with simple, self-contained
applications with external dependencies that can be easily moved to the cloud. These applications are likely to have fewer
dependencies on other components in the infrastructure and can be migrated with minimal effort, helping the team to get
comfortable with the cloud deployment process. Once the team has gained experience with the cloud deployment process,
they can gradually move more complex applications with internal dependencies to the cloud.
upvoted 
4 
times
akhilesh_pundir
akhilesh_pundir
 
1 year, 5 months ago
Case study nowhere mentioned that Dress4Win has some self contained Application with no dependencies on Internal
resources. And databases are available out of the box and would be easy to migrate in my view.
upvoted 
2 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months ago
Selected Answer: 
A
A. Identify self-contained applications with external dependencies as a first move to the cloud.
upvoted 
1 
times
shekarcfc
shekarcfc
 
1 year, 10 months ago
Selected Answer: 
A
A for me
upvoted 
1 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
C
In std practice to move db first.
upvoted 
2 
times
szefco
szefco
 
1 year, 11 months ago
Moving database to cloud and letting that database serve apps hosted on prem? It doesn't make sense. A seems better option
- move self-contained application. External dependencies can be handled by APIs
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 3 months ago
Selected Answer: 
A
A for me
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
2 years, 5 months ago
I don't understand the meaning of 'self-contained applications with external dependencies'.
if just 'self-contained applications' ,then i understand....
upvoted 
2 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
2 
times
victory108
victory108
 
2 years, 11 months ago
A. Identify self-contained applications with external dependencies as a first move to the cloud.
upvoted 
4 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months agoAnswer is A
upvoted 
3 
times
gosi
gosi
 
3 years, 2 months ago
A - Every experienced IT resource will give a solution - pick something which is "low hanging fruit". Now which one is the low
hanging fruit ? 
Something which is independent of any internal dependencies. i.e. A.
upvoted 
3 
times
Pb55
Pb55
 
3 years, 2 months ago
A - self contained / external dependancies = no latency issues.
upvoted 
1 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is A
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #6
Dress4Win has asked you for advice on how to migrate their on-premises MySQL deployment to the cloud. 
They want to minimize downtime and performance impact to their on-premises solution during the migration. 
Which approach should you recommend? 
A. 
Create a dump of the on-premises MySQL master server, and then shut it down, upload it to the cloud environment, and
load into a new MySQL cluster.
B. 
Setup a MySQL replica server/slave in the cloud environment, and configure it for asynchronous replication from the
MySQL master server on-premises until cutover. 
Most Voted
C. 
Create a new MySQL cluster in the cloud, configure applications to begin writing to both on premises and cloud MySQL
masters, and destroy the original cluster at cutover.
D. 
Create a dump of the MySQL replica server into the cloud environment, load it into: Google Cloud Datastore, and
configure applications to read/write to Cloud Datastore at cutover.
Correct Answer:
 
B 
Comments
chiar
chiar
 
Highly Voted
 
4 years, 7 months ago
I think D it can't be, because you want to load a dump in a Cloud Datastore. If it were a Cloud Storage, it could be, but a Cloud
datastore is a nosql.
It's true that you hace to use a dump, but to create a replica server/slave to promote to Cloud SLQ. So I think it is B.
upvoted 
32 
times
AD2AD4
AD2AD4
 
Highly Voted
 
4 years, 1 month ago
Final Decision to go with Option B
upvoted 
24 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
Most Recent
 
1 year, 8 months ago
IMPORTANT: Dress4Win is not anymore part of the officially listed case studies:
https://cloud.google.com/certification/guides/professional-cloud-architect
upvoted 
6 
times
Community vote distribution
B (100%)Pime13
Pime13
 
5 months, 1 week ago
it's cool for training,
upvoted 
3 
times
alexandercamachop
alexandercamachop
 
1 year, 9 months ago
Selected Answer: 
B
Cloud datastore is a nosql.
It's true that you hace to use a dump, but to create a replica server/slave to promote to Cloud SLQ. So I think it is B.
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
B
vote B
upvoted 
1 
times
victory108
victory108
 
2 years, 11 months ago
B. Setup a MySQL replica server/slave in the cloud environment, and configure it for asynchronous replication from the MySQL
master server on-premises until cutover.
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is B
upvoted 
4 
times
Ausias18
Ausias18
 
3 years, 2 months ago
Answer is B
upvoted 
2 
times
lynx256
lynx256
 
3 years, 3 months ago
B is ok
upvoted 
1 
times
bnlcnd
bnlcnd
 
3 years, 5 months ago
https://cloud.google.com/sql/docs/mysql/replication/external-server
B is good.
upvoted 
3 
times
okixavi
okixavi
 
3 years, 6 months ago
B it is
upvoted 
1 
times
Chulbul_Pandey
Chulbul_Pandey
 
3 years, 7 months ago
B is correct
upvoted 
1 
times
Hjameel
Hjameel
 
3 years, 7 months ago
If a database SLA or other requirements do not allow for an export-based migration, you should consider creating a replica of
the database in which the replica database is in the Google cloud. This configuration is referred to as primary/replica or
leader/follower , and in general it is the preferred migration method. Whenever there is a change to the primary or leader, the
same change is made to the replica or follower instance. Once the database has synchronized the data, database applications
can be configured to point to the cloud database.
Answer B
upvoted 
1 
times
AdityaGupta
AdityaGupta
 
3 years, 8 months ago
I will go with answer B, because this will avoid any downtime and performance impact. And post cutover this database can be
used a master.used a master.
A -> Will cause downtime.
B -> Right choice
C -> Business impact, incosistency in data.
D -> Cloud DataStore is NoSQL DB
upvoted 
3 
times
homer_simpson
homer_simpson
 
3 years, 8 months ago
B is correct answer. 
Datastore is no SQL database. And when we create a dupm and upload it we might lose some data during this process time
that was served in the promiss site
upvoted 
1 
times
bidibidiiii
bidibidiiii
 
3 years, 9 months ago
It's B.
There's a similar question in the Linux Academy practice exam: 
"Dress4Win is ready to migrate their on-premises MySQL deployment to the cloud. They want to reduce downtime and
performance impact to their on-premises solution during the migration. What should they do?" 
Answer: "Set up a MySQL replica/slave in Google Cloud using Cloud SQL and configure it for asynchronous replication from the
MySQL master server on-premises until cutover."
upvoted 
3 
times
ESP_SAP
ESP_SAP
 
3 years, 9 months ago
Correct Answer is (B):
Please, stop to confuse the people with crazy ideas.
Every migration we should try to smooth or near zero downtime in DB cases.
The question clearly mention move MySQL to the cloud, minimize downtime and performance impact. 
How you 
can recommend "D" to accomplish the previous premises? 
Then if you setup a MySQL replica server/slave will be easy the cutover, just shut down the replica to on-premise and change
the role for replica server from slave to primary. That is all! No impact, no service disruption, almost near zero downtime.
upvoted 
8 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #7
Dress4Win has configured a new uptime check with Google Stackdriver for several of their legacy services. The Stackdriver
dashboard is not reporting the services as healthy. 
What should they do? 
A. 
Install the Stackdriver agent on all of the legacy web servers.
B. 
In the Cloud Platform Console download the list of the uptime servers' IP addresses and create an inbound firewall rule
C. 
Configure their load balancer to pass through the User-Agent HTTP header when the value matches
GoogleStackdriverMonitoring-UptimeChecks (https:// cloud.google.com/monitoring)
D. 
Configure their legacy web servers to allow requests that contain user-Agent HTTP header when the value matches
GoogleStackdriverMonitoring- UptimeChecks (https://cloud.google.com/monitoring) 
Most Voted
Correct Answer:
 
D 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
5 years, 2 months ago
It's B. B must be done. For a health check on http (port 80) you don't need to configure nothing in the server (it makes a get o
something similar).
upvoted 
34 
times
tartar
tartar
 
4 years, 4 months ago
B is ok
upvoted 
6 
times
GopiSivanathan
GopiSivanathan
 
4 years, 2 months ago
If the resource you are checking isn't publicly available, you must configure the resource's firewall to permit incoming traffic
from the uptime-check servers. See Getting IP addresses to download a list of the IP addresses
If the resource you are checking doesn't have an external IP address, uptime checks are unable to reach it.
upvoted 
1 
times
nitinz
nitinz
 
3 years, 10 months ago
Community vote distribution
D (47%)
B (41%)
C (12%)ans is B
upvoted 
3 
times
gcp2019
gcp2019
 
Highly Voted
 
5 years, 1 month ago
Correct answer is B 
https://cloud.google.com/monitoring/uptime-checks/using-uptime-checks#monitoring_uptime_check_list_ips-console
upvoted 
15 
times
JohnJamesB1212
JohnJamesB1212
 
Most Recent
 
3 months, 1 week ago
Selected Answer: 
D
D. Configure their legacy web servers to allow requests that contain the User-Agent HTTP header when the value matches
GoogleStackdriverMonitoring-UptimeChecks (https://cloud.google.com/monitoring).
Here's why:
Google Cloud Monitoring uptime checks use a specific User-Agent (GoogleStackdriverMonitoring-UptimeChecks) to make
health checks on services. If the legacy web servers are not configured to accept these requests or block certain User-Agent
headers, they will reject the checks, causing them to be reported as unhealthy.
By configuring the legacy web servers to allow traffic from uptime checks that include the proper User-Agent header,
Dress4Win ensures that the uptime check traffic can reach the services, allowing Google Monitoring to report accurate health
status.
upvoted 
2 
times
JohnJamesB1212
JohnJamesB1212
 
3 months, 1 week ago
Why other options are less ideal:
Option A (install the Stackdriver agent) is not relevant to this issue. The Stackdriver agent is used for logging and monitoring
system metrics but does not directly relate to uptime checks.
Option B (creating inbound firewall rules) is unnecessary unless there's evidence that the firewall is blocking traffic from
Google's uptime check servers. Typically, Google uptime check servers should not require special rules.
Option C (configuring the load balancer to pass through the User-Agent header) is relevant if there’s a load balancer in place,
but it only solves part of the problem. The legacy web servers still need to accept requests with the appropriate User-Agent
header.
upvoted 
2 
times
6a8c7ad
6a8c7ad
 
4 months, 4 weeks ago
D. Not B. “legacy”. So not in console.
upvoted 
3 
times
mesodan
mesodan
 
10 months ago
Selected Answer: 
C
Option D suggests configuring legacy web servers to allow requests with the specific User-Agent header. While this might seem
appropriate, it's unnecessary and potentially risky. Modifying the servers' configuration introduces additional steps and
potential security implications. Since the uptime checks are already reaching the web servers, the key is ensuring the load
balancer doesn't interfere with them, not modifying the servers themselves.
upvoted 
2 
times
jabrrJ68w02ond1
jabrrJ68w02ond1
 
2 years, 2 months ago
IMPORTANT: Dress4Win is not anymore part of the officially listed case studies:
https://cloud.google.com/certification/guides/professional-cloud-architect
upvoted 
15 
times
amxexam
amxexam
 
2 years, 7 months ago
Selected Answer: 
B
It would be missing firewall rule that would be causing problem
upvoted 
1 
times
DivAl272829
DivAl272829
 
2 years, 9 months ago
B: If the resource you are checking isn't publicly available, you must configure the resource's firewall to permit incoming traffic
from the uptime-check servers. See List uptime-check server IP addresses to download a list of the IP addresses.
upvoted 
1 
times
mesodan
mesodan
 
2 years, 10 months ago
Selected Answer: 
BSelected Answer: 
B
It's B. Read: https://cloud.google.com/monitoring/uptime-checks
upvoted 
3 
times
nagibator163
nagibator163
 
2 years, 11 months ago
Selected Answer: 
D
I don't understand why everyone's saying it's B. The question talks about "legacy services". They are not on GCP, are they? So
setting up inbound rules on a firewall in GCP will have no effect.
upvoted 
6 
times
Andrea67
Andrea67
 
3 years ago
I think B is ok, "Your use of uptime checks is affected by any firewalls protecting your service." 
from
https://cloud.google.com/monitoring/uptime-checks
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
B
vote B
upvoted 
3 
times
kopper2019
kopper2019
 
3 years, 5 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
1 
times
[Removed]
[Removed]
 
3 years, 5 months ago
B) (if the answer talk about firewall of legacy services ant not firewall on GCP) and D) are valid solutions.
I prefer D) because with B) you have to modify firewall rules once per quarter as Ips can change
(https://cloud.google.com/monitoring/uptime-checks/using-uptime-checks#get-ips).
upvoted 
2 
times
victory108
victory108
 
3 years, 5 months ago
B. In the Cloud Platform Console download the list of the uptime servers' IP addresses and create an inbound firewall rule
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is B
upvoted 
2 
times
Ausias18
Ausias18
 
3 years, 9 months ago
Answer is B
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #8
As part of their new application experience, Dress4Wm allows customers to upload images of themselves. 
The customer has exclusive control over who may view these images. 
Customers should be able to upload images with minimal latency and also be shown their images quickly on the main
application page when they log in. 
Which configuration should Dress4Win use? 
A. 
Store image files in a Google Cloud Storage bucket. Use Google Cloud Datastore to maintain metadata that maps each
customer's ID and their image files. 
Most Voted
B. 
Store image files in a Google Cloud Storage bucket. Add custom metadata to the uploaded images in Cloud Storage that
contains the customer's unique ID.
C. 
Use a distributed file system to store customers' images. As storage needs increase, add more persistent disks and/or
nodes. Assign each customer a unique ID, which sets each file's owner attribute, ensuring privacy of images.
D. 
Use a distributed file system to store customers' images. As storage needs increase, add more persistent disks and/or
nodes. Use a Google Cloud SQL database to maintain metadata that maps each customer's ID to their image files.
Correct Answer:
 
A 
Comments
chiar
chiar
 
Highly Voted
 
4 years, 7 months ago
I think it's A, because in the question says "The customer has exclusive control over who may view these images"
And I think it is easier to develop this feature having in cloud datastore a NOSQL database where you can manage the control
of file's viewer
upvoted 
33 
times
DrCoola
DrCoola
 
Highly Voted
 
4 years, 4 months ago
A - using gsutil for this purpose makes querries on such metadata painful for application logic.
upvoted 
9 
times
svkds
svkds
 
Most Recent
 
1 month, 3 weeks ago
Selected Answer: 
B
Community vote distribution
A (67%)
B (33%)This approach leverages Google Cloud Storage for storing the image files, which provides scalability, durability, and low-
latency access. By adding custom metadata containing the customer's unique ID to the uploaded images, Dress4Win can
maintain control over access to the images while enabling efficient retrieval based on customer identity. This configuration
aligns well with the requirement for minimal latency and quick access to images while ensuring customer privacy and security.
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
2 
times
PeppaPig
PeppaPig
 
2 years, 11 months ago
A is correct. The whole idea is simply build and maintain an external metadata service using NoSQL database to associate the
GS object key with its metadata, in order to facilitate object findings based on attributes you pre defined in metatdata
This AWS blog provides a solution in the context of AWS S3, but the idea behind is applicable to Google Storage as well
https://aws.amazon.com/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers/
upvoted 
4 
times
kopper2019
kopper2019
 
2 years, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 11 months ago
B) is not correct because we cannot search on bucket using metada (maybe in the future ...). 
So for now we have to get all files
from the bucket and fiter on metadata ( very bad performance).
The answer is A).
upvoted 
2 
times
victory108
victory108
 
2 years, 11 months ago
A. Store image files in a Google Cloud Storage bucket. Use Google Cloud Datastore to maintain metadata that maps each
customer's ID and their image files.
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A
upvoted 
3 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is A
upvoted 
1 
times
Rightsaidfred
Rightsaidfred
 
3 years, 4 months ago
Between A & B. A will take quickest time. A is the answer.
upvoted 
2 
times
okixavi
okixavi
 
3 years, 6 months ago
A is the man
upvoted 
2 
times
Bijesh
Bijesh
 
3 years, 7 months ago
The customer has exclusive control over who can view their image. 
Is this possible by option B, by merely adding metadata on the object. I don't think so. 
A is better suited.
upvoted 
2 
times
hems4all
hems4all
 
3 years, 7 months ago
A is correct
upvoted 
2 
timesAdityaGupta
AdityaGupta
 
3 years, 8 months ago
I will go with A, Store the images in GCS is cost-optimized way of storing images/ objects. DataStore is best option to store
user profiles, which gives control to user.
upvoted 
2 
times
zzaric
zzaric
 
3 years, 11 months ago
A is correct
upvoted 
2 
times
mlantonis
mlantonis
 
4 years ago
I agree with A
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #9
Dress4Win has end-to-end tests covering 100% of their endpoints. 
They want to ensure that the move to the cloud does not introduce any new bugs. 
Which additional testing methods should the developers employ to prevent an outage? 
A. 
They should enable Google Stackdriver Debugger on the application code to show errors in the code.
B. 
They should add additional unit tests and production scale load tests on their cloud staging environment. 
Most Voted
C. 
They should run the end-to-end tests in the cloud staging environment to determine if the code is working as intended.
D. 
They should add canary tests so developers can measure how much of an impact the new release causes to latency.
Correct Answer:
 
B 
Comments
examtaker11
examtaker11
 
Highly Voted
 
4 years, 4 months ago
C- I would run the same test suite in cloud) to see what breaks
upvoted 
27 
times
Jphix
Jphix
 
3 years, 6 months ago
Going with B. "Final answer" lol. C is a good answer except that they're asking for an additional testing method. Since they're
already testing endpoints specifically, you'd literally be running the exact same test after migration. That said, for B, I'm still at
a loss of why we'd need to do additional unit testing--best explanation is that some of the applications will have needed to
be retooled for PaaS offerings if they're doing more than a lift-and-shift, thereby actually changing the underlying code; but
the production-level load testing is like the most GCP thing you can do here
upvoted 
9 
times
mesodan
mesodan
 
2 years, 4 months ago
I would go with B. "Additional" seems to be the keyword here so adding unit tests and production scale load tests to the ones
they already have makes more sense.
upvoted 
2 
times
Smart
Smart
 
4 years, 4 months ago
Agree, however, I think running at production-scale would not only show what breaks but also when it breaks? I go with B
Community vote distribution
B (100%)upvoted 
6 
times
FAB1010
FAB1010
 
Highly Voted
 
3 years, 11 months ago
Question mention that "end-to-end tests covering 100% of their endpoints", "ensure that the move to the cloud does not
introduce any new bugs", and "additional testing methods should the *developers* employ to *prevent an outage*"
A - Not Correct. Developer can debug the problem, but cannot *prevent* the outage. 
B - Correct. Developers are responsible for writing unit tests. They already have end-to-end tests for *endpoints* but nothing
mentioned about the unit tests. Cloud will auto-scale but you need to define your auto-scaling configuration (desired count,
max count etc) and production scale load test will help you to configure the auto-scaling policies
C - Not Correct. They already have end-to-end test. Running it on staging environment will not prevent an outage
D - Not Correct. Answers says "an impact the new release causes to latency" but question ask for preventing an outage and so
this one is ruled out
upvoted 
20 
times
bigzero
bigzero
 
Most Recent
 
1 month, 3 weeks ago
D
they already has 100% covering end-to-end testing, that means unit tests was finished before this
so, additional test remains canary in production environment
upvoted 
1 
times
Jconnor
Jconnor
 
7 months ago
B is testing scale, C is testing the code is working as intended in the cloud. the question is for bugs, not for scale. It should be C,
but I understand why everyone goes for B. Neurodivergents will go for C.
upvoted 
1 
times
RitwickKumar
RitwickKumar
 
1 year, 10 months ago
Selected Answer: 
B
Note the 
ask "prevent an outage". 
One of the way to test outage 
scenarios is through load testing. Option B covers this where as option C only covers checking the
intended behaviour.
upvoted 
2 
times
[Removed]
[Removed]
 
2 years, 3 months ago
Testing pipeline: 
Unit ---- Integration --- end-to-end
If choose B, add additional unit tests. It should be continue to do Integrationt test and end-to-end test.
But it just has a unit test, then go to produciton scale load test.
C should be better, becasue it has finished end-to-end tests before, so for move to the cloud, it should be test the end-to-end
in cloud preproduction enviroment to check whethere it's also working fine on cloud.
https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-
testing#end-to-end_tests
upvoted 
1 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
B
vote B
upvoted 
2 
times
rottzy
rottzy
 
2 years, 8 months ago
end-to-end is already present, go for additional tests
upvoted 
1 
times
amxexam
amxexam
 
2 years, 9 months ago
It should be B as for all those going with C if you do all staging you will still leave out the performance test that scales the
application which is covered in B that means even if the application works well but will not scale properly will lead to an
outage, which we are asked to prevent.
upvoted 
1 
times
victory108
victory108
 
2 years, 11 months ago
B. They should add additional unit tests and production scale load tests on their cloud staging environment.B. They should add additional unit tests and production scale load tests on their cloud staging environment.
upvoted 
3 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is B
upvoted 
4 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is B (but C... uff... is also possible, but as the question says end-to-end is already done)
upvoted 
3 
times
lynx256
lynx256
 
3 years, 3 months ago
B is ok
upvoted 
1 
times
ybe_gcp_cert
ybe_gcp_cert
 
3 years, 3 months ago
Question asks about "ADDITIONAL testing methods".
B adds production scale load tests.
C should also be executed in a new cloud env but this question dosn't ask for this.
In real life serious projects, B and C are mandatory (end to end and perf tests).
Should be B.
upvoted 
2 
times
guid1984
guid1984
 
3 years, 4 months ago
Should be B 
Reasoning: They already had end-to-end contract tests coverage for all their service(s) endpoints. So, additionally they should
add unit test coverage and perform prod load tests in staging environment which will help find out performance related issues
before deploying it to production.
upvoted 
1 
times
Rightsaidfred
Rightsaidfred
 
3 years, 5 months ago
Obviously C. Yes they have end-to-end tests on prem with 100% coverage, however this hasn't been tested in the cloud yet.
upvoted 
1 
times
bnlcnd
bnlcnd
 
3 years, 5 months ago
B vs C
B sounds right other than additional "unit" test. C is nothing wrong but not mentioning load. I will throw a dime to decide
which to choose.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #10
You want to ensure Dress4Win's sales and tax records remain available for infrequent viewing by auditors for at least 10 years. 
Cost optimization is your top priority. 
Which cloud services should you choose? 
A. 
Google Cloud Storage Coldline to store the data, and gsutil to access the data. 
Most Voted
B. 
Google Cloud Storage Nearline to store the data, and gsutil to access the data.
C. 
Google Bigtabte with US or EU as location to store the data, and gcloud to access the data.
D. 
BigQuery to store the data, and a web server cluster in a managed instance group to access the data. Google Cloud SQL
mirrored across two distinct regions to store the data, and a Redis cluster in a managed instance group to access the data.
Correct Answer:
 
A 
Comments
chiar
chiar
 
Highly Voted
 
3 years, 7 months ago
I think it's A, because when you read documentation both of them (nearline and coldline) you can see the expresion infrecuent
access. And in this case, your priority is the cost, and you are going to sabe 10 years
upvoted 
32 
times
MyPractice
MyPractice
 
Highly Voted
 
3 years, 6 months ago
its A 
- 
"Cold data storage - Infrequently accessed data, such as data stored for legal or regulatory reasons, can be stored at low
cost as Coldline Storage and be available when you need it"
https://cloud.google.com/storage/docs/storage-classes
upvoted 
10 
times
RVivek
RVivek
 
Most Recent
 
4 months, 3 weeks ago
Selected Answer: 
B
Coldline Data retrival takes hours 
If the data is infrequently 
accessed then Nearline is better.
upvoted 
1 
times
RVivek
RVivek
 
4 months, 3 weeks ago
Community vote distribution
A (80%)
B (20%)I change it to A. Just noticed the phrase " Cost optimization is top priority" in question
upvoted 
1 
times
NodummyIQ
NodummyIQ
 
6 months ago
Option A is not a correct answer because Google Cloud Storage Coldline is not suitable for infrequent access to data. Coldline
storage is optimized for archival storage with a 90-day minimum storage duration and a retrieval period measured in hours. It
is not suitable for data that needs to be accessed frequently or within a short period of time.
Option B is a better choice for infrequent access to data. Google Cloud Storage Nearline is optimized for infrequent access
with a 30-day minimum storage duration and a retrieval period measured in seconds. It is more suitable for data that needs to
be accessed infrequently or within a short period of time, such as data that needs to be accessed by auditors.
upvoted 
2 
times
alexandercamachop
alexandercamachop
 
9 months, 3 weeks ago
Selected Answer: 
A
Cost + 10 years, should be Archival, but since is not here. Lets go with ColdLine.
upvoted 
2 
times
gcpAMa
gcpAMa
 
11 months ago
Answer: A
upvoted 
1 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
A
vote A
upvoted 
2 
times
victory108
victory108
 
1 year, 11 months ago
A. Google Cloud Storage Coldline to store the data, and gsutil to access the data.
upvoted 
1 
times
kopper2019
kopper2019
 
1 year, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
2 
times
mbrueck
mbrueck
 
1 year, 11 months ago
Answer: A
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
1 year, 12 months ago
Answer is A
upvoted 
3 
times
Ausias18
Ausias18
 
2 years, 3 months ago
Answer is A
upvoted 
1 
times
lynx256
lynx256
 
2 years, 3 months ago
A is ok
upvoted 
1 
times
gu9singg
gu9singg
 
2 years, 3 months ago
A - Coldline is cheapest one
upvoted 
1 
times
Joyrex
Joyrex
 
2 years, 4 months ago
Today the answer would be Archive Storage instead of coldline, so keep in mind the options on the test may be updated.
upvoted 
4 
timesupvoted 
4 
times
LoganIsh
LoganIsh
 
2 years, 8 months ago
A is the answer... The catch here is that 10 years archives to store thus coldline storage is the right pick.
upvoted 
1 
times
mlantonis
mlantonis
 
3 years ago
I hate when we have to guess what infrequent actually means. I believe because "Cost optimization is your top priority" we
should choose Coldline.
A provides a more cost-effective solution.
upvoted 
1 
times
definepi314
definepi314
 
3 years ago
Infrequent means not frequently, that is "less". You don't have to guess.
upvoted 
1 
times
lynx256
lynx256
 
2 years, 3 months ago
"Infrequent" is unclear same as "not frequently" :)
I think @mlantonis wants to say "I'd like more precision: twice a year, once a year or so on"
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 11
Question #11
The current Dress4Win system architecture has high latency to some customers because it is located in one data center. 
As of a future evaluation and optimizing for performance in the cloud, Dresss4Win wants to distribute its system architecture to
multiple locations when Google cloud platform. 
Which approach should they use? 
A. 
Use regional managed instance groups and a global load balancer to increase performance because the regional
managed instance group can grow instances in each region separately based on traffic. 
Most Voted
B. 
Use a global load balancer with a set of virtual machines that forward the requests to a closer group of virtual machines
managed by your operations team.
C. 
Use regional managed instance groups and a global load balancer to increase reliability by providing automatic failover
between zones in different regions.
D. 
Use a global load balancer with a set of virtual machines that forward the requests to a closer group of virtual machines
as part of a separate managed instance groups.
Correct Answer:
 
A 
Comments
KouShikyou
KouShikyou
 
Highly Voted
 
4 years, 8 months ago
Agree. A looks correct for me.
upvoted 
30 
times
kimharsh
kimharsh
 
2 years, 1 month ago
I thought A is talking about MIG , but if your read the question carefully you will see MIG's , which changed my answer from D
to A
upvoted 
1 
times
MeasService
MeasService
 
Highly Voted
 
4 years, 8 months ago
I am not convinced with D. A sounds correct answer. Creating regional MIGs and connecting it to GLB. Anyone ?
upvoted 
19 
times
Community vote distribution
A (100%)nitinz
nitinz
 
3 years, 4 months ago
A is correct
upvoted 
3 
times
tartar
tartar
 
3 years, 10 months ago
A is ok
upvoted 
5 
times
thewalker
thewalker
 
Most Recent
 
7 months, 4 weeks ago
A.
Creating MIGs across regions behind a GLB, gives HA across Zones and Regions.
upvoted 
1 
times
NodummyIQ
NodummyIQ
 
1 year, 6 months ago
Answer D is correct. Answer A is not correct because it does not mention the aspect of distributing the system architecture to
multiple locations. A regional managed instance group can increase performance by allowing the group to grow instances in
each region separately based on traffic, but it does not address the issue of distributing the system architecture to multiple
locations.
upvoted 
2 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
A
vote A
upvoted 
3 
times
Ari_GCP
Ari_GCP
 
2 years, 9 months ago
Agree with A. It says optimize for performance, and multiple regional MIG's can definitely help you do that.
upvoted 
1 
times
PeppaPig
PeppaPig
 
2 years, 10 months ago
A is correct for sure
D is wrong. GLB is already capable of forwarding traffic to MIG in the closer region so why would you implement that again
upvoted 
1 
times
parthkulkarni998
parthkulkarni998
 
6 months, 1 week ago
Exactly. D states about implementing GLB in a set of VMs rather than using a managed service of GLBs
upvoted 
1 
times
kopper2019
kopper2019
 
2 years, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
1 
times
victory108
victory108
 
2 years, 11 months ago
A. Use regional managed instance groups and a global load balancer to increase performance because the regional managed
instance group can grow instances in each region separately based on traffic.
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
2 years, 12 months ago
Answer is A
upvoted 
2 
times
Pb55
Pb55
 
3 years, 2 months ago
It’s A. Each region can have an instance group linked to a global load balancer. Instance groups do not need to be multi
regional for this to work.
upvoted 
1 
times
tzKhalil
tzKhalil
 
3 years, 2 months agoA is not good, because a regional MIG, which deploys instances to multiple zones across the same region. This will not deploy
instances in multi regions.
D is good
upvoted 
2 
times
taoj
taoj
 
2 years, 11 months ago
your statement is right. But A was MIGs.
upvoted 
2 
times
tzKhalil
tzKhalil
 
3 years, 2 months ago
Doc: https://cloud.google.com/compute/docs/instance-groups#types_of_managed_instance_groups
upvoted 
1 
times
jaguarrr
jaguarrr
 
3 years, 3 months ago
D is the correct answer.
With A it says "because the regional managed instance group can grow instances in each region separately based on traffic." A
regional instance group cannot grow instances in Multiple Regions, only in one. 
With D, you have multiple separate Regional Instance Groups, which is what is missing in answer A.
upvoted 
3 
times
Ausias18
Ausias18
 
3 years, 3 months ago
Answer is A
upvoted 
1 
times
lynx256
lynx256
 
3 years, 3 months ago
IMO - A.
We are gointg to use a few MIGs - one per region; each of them can scale independetly from others.
upvoted 
1 
times
gu9singg
gu9singg
 
3 years, 3 months ago
A- because with Regional resources and global load balancer we can route traffic to nearest VM machine
upvoted 
1 
times
bnlcnd
bnlcnd
 
3 years, 5 months ago
https://cloud.google.com/load-balancing/docs/https/setting-up-https
Seems D is correct
upvoted 
2 
times
bnlcnd
bnlcnd
 
3 years, 5 months ago
"closer" group of virtual machines as part of a separate managed instance groups.
The key word closer in the answer D means routing the client request to a closer regional MIG. Anything wrong?
A seems not mentioning routing request to the MIG that is closer to the client.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 12
Question #1
For this question, refer to the Dress4Win case study. Dress4Win is expected to grow to 10 times its size in 1 year with a
corresponding growth in data and traffic that mirrors the existing patterns of usage. The CIO has set the target of migrating
production infrastructure to the cloud within the next 6 months. How will you configure the solution to scale for this growth
without making major application changes and still maximize the ROI? 
A. 
Migrate the web application layer to App Engine, and MySQL to Cloud Datastore, and NAS to Cloud Storage. Deploy
RabbitMQ, and deploy Hadoop servers using Deployment Manager.
B. 
Migrate RabbitMQ to Cloud Pub/Sub, Hadoop to BigQuery, and NAS to Compute Engine with Persistent Disk storage.
Deploy Tomcat, and deploy Nginx using Deployment Manager.
C. 
Implement managed instance groups for Tomcat and Nginx. Migrate MySQL to Cloud SQL, RabbitMQ to Cloud Pub/Sub,
Hadoop to Cloud Dataproc, and NAS to Compute Engine with Persistent Disk storage.
D. 
Implement managed instance groups for the Tomcat and Nginx. Migrate MySQL to Cloud SQL, RabbitMQ to Cloud
Pub/Sub, Hadoop to Cloud Dataproc, and NAS to Cloud Storage. 
Most Voted
Correct Answer:
 
D 
Comments
MeasService
MeasService
 
Highly Voted
 
4 years, 8 months ago
Why do we need to put NAS data on persistant disk and not on GCS ? 
I would go with D!
upvoted 
43 
times
techalik
techalik
 
3 years, 7 months ago
1. Use Cloud Marketplace to provision Tomcat and Nginx on Google Compute Engine.
2. Replace MySQL with Cloud SQL for MySQL.
3. Use the Deployment Manager to provision Jenkins on Google Compute Engine. is the right answer.
As explained above, you would use Cloud SQL to replace MySQL. For the other requirements, i.e. Nginx/Tomcat and Jenkins,
you can deploy these through Cloud Deployment Manager by using custom images.
Ref: https://cloud.google.com/compute/docs/images
Community vote distribution
D (53%)
C (47%)Ref: https://cloud.google.com/compute/docs/images
Using the same custom images every time ensures that your environments are "reliable and reproducible" and you achieve
"rapid provisioning".
D
upvoted 
11 
times
nitinz
nitinz
 
3 years, 4 months ago
ans is D
upvoted 
4 
times
tartar
tartar
 
3 years, 10 months ago
D is ok
upvoted 
11 
times
Jphix
Jphix
 
3 years, 5 months ago
Agreed. Looking to maximize ROI as well according to the question, and even the most expensive cloud storage is still going
to be half the price of cheapest Persistent Disk storage, and that's without even including your compute costs. D all the way.
upvoted 
3 
times
KouShikyou
KouShikyou
 
Highly Voted
 
4 years, 8 months ago
I prefer D.
Original NAS is for image, log, backup. GCS fits it perfectly.
upvoted 
21 
times
exampanic
exampanic
 
4 years, 6 months ago
I agree that GCS fits perfectly for storing images, log, backup. However, the question asks to avoid major application changes.
GCS is not NAS, meaning it does not provide SMB or NFS shares. Therefore moving the NAS files to Google Cloud Storage
would require a major application change in the way they access these files. I believe the correct answer would be C.
upvoted 
10 
times
poseidon24
poseidon24
 
2 years, 11 months ago
It can, check on Cloud Storage FUSE. Buckets can be mounted as file systems.
upvoted 
4 
times
mesodan
mesodan
 
Most Recent
 
4 months ago
Selected Answer: 
D
Use case suitability:
Cloud Storage: Ideally suited for storing large, unstructured data like images, videos, and backups, which is likely the case for
Dress4Win's NAS data.
Persistent Disk: More appropriate for frequently accessed data that requires block-level access, such as databases or operating
systems for virtual machines.
upvoted 
1 
times
kampatra
kampatra
 
4 months, 3 weeks ago
Selected Answer: 
D
Correct Ans: A
NAS `" image storage, logs, backups 
: for storing images, logs and backups Cloud Storage is best practice and cost effective
also.
upvoted 
1 
times
kampatra
kampatra
 
4 months, 3 weeks ago
Wrongly typed A, it must be D
upvoted 
1 
times
mbacelar
mbacelar
 
6 months, 3 weeks ago
Selected Answer: 
D
Should be DShould be D
upvoted 
1 
times
MahAli
MahAli
 
6 months, 3 weeks ago
Selected Answer: 
C
Voting c NAS could have been replaced with file store to minimize any change, moving to GCS is not that easy change in
overall architecture
upvoted 
1 
times
Jannchie
Jannchie
 
6 months, 4 weeks ago
Selected Answer: 
C
C, because we can run some script on NAS. It can
act like a normal server. But GCS cannot.
upvoted 
2 
times
techtitan
techtitan
 
7 months ago
Selected Answer: 
C
without making major application changes and still maximize the ROI --> compute engine with persistent disk. without
knowing access patterns, GCS may not be an easy change.
upvoted 
1 
times
thamaster
thamaster
 
1 year, 6 months ago
Selected Answer: 
D
you don't need NAS to store archive and Image disk
upvoted 
1 
times
amxexam
amxexam
 
2 years, 1 month ago
Selected Answer: 
D
D is the correct chand equivalent mapping
upvoted 
1 
times
[Removed]
[Removed]
 
2 years, 3 months ago
D is OK
https://cloud.google.com/architecture/filers-on-compute-engine?hl=en#managed_file_storage_solutions
upvoted 
2 
times
MF2C
MF2C
 
2 years, 5 months ago
SAN -> persistent disk, NAS -> Cloud Storage
upvoted 
2 
times
edilramos
edilramos
 
2 years, 6 months ago
Managed Instances With Tomcat and Nginx would bring the minimum necessary tweaking to the new environment.
Migrating from MySql to Cloud SQL does not require any syntax changes.
Moving from Rabbit MQ to Pub/Sub is relatively straightforward and has very complete documentation.
DataProc has Libraries and tools to ensure Apache Hadoop interoperability.
Without many changes in the environment, mainly keeping the original architecture, Datastorage will keep the presentation
characteristics of a shared area, mapped to the instances.
upvoted 
2 
times
phantomsg
phantomsg
 
2 years, 7 months ago
Selected Answer: 
C
The answer should be C. 'A' and 'B' are ruled out as they introduce significant architecture changes. or irrelevant. 'D' is fine
except proposes to replace NAS with Cloud Storage. This will introduce major architectural changes. Instead, if the choice was
to move 'NAS' to 'Cloud Filestore' then it would have made sense. Answer 'C' is the closest with the least amount of architectural
changes involved in migration.
upvoted 
3 
times
joe2211
joe2211
 
2 years, 7 months ago
Selected Answer: 
Dvote D
upvoted 
3 
times
kopper2019
kopper2019
 
2 years, 11 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
1 
times
anku15
anku15
 
2 years, 9 months ago
I dont see the questions now. Did you remove it?
upvoted 
1 
times
victory108
victory108
 
2 years, 11 months ago
D. Implement managed instance groups for the Tomcat and Nginx. Migrate MySQL to Cloud SQL, RabbitMQ to Cloud Pub/Sub,
Hadoop to Cloud Dataproc, and NAS to Cloud Storage.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 12
Question #2
For this question, refer to the Dress4Win case study. Considering the given business requirements, how would you automate
the deployment of web and transactional data layers? 
A. 
Deploy Nginx and Tomcat using Cloud Deployment Manager to Compute Engine. Deploy a Cloud SQL server to replace
MySQL. Deploy Jenkins using Cloud Deployment Manager. 
Most Voted
B. 
Deploy Nginx and Tomcat using Cloud Launcher. Deploy a MySQL server using Cloud Launcher. Deploy Jenkins to
Compute Engine using Cloud Deployment Manager scripts.
C. 
Migrate Nginx and Tomcat to App Engine. Deploy a Cloud Datastore server to replace the MySQL server in a high-
availability configuration. Deploy Jenkins to Compute Engine using Cloud Launcher.
D. 
Migrate Nginx and Tomcat to App Engine. Deploy a MySQL server using Cloud Launcher. Deploy Jenkins to Compute
Engine using Cloud Launcher.
Correct Answer:
 
A 
Comments
jcmoranp
jcmoranp
 
Highly Voted
 
4 years, 2 months ago
It's A, "Cloud Datastore server" doesn't exist. A fits OK.
upvoted 
26 
times
nitinz
nitinz
 
2 years, 10 months ago
A is the answer
upvoted 
2 
times
cetanx
cetanx
 
3 years, 5 months ago
Also, GAE uses Jetty for http and servlet engine. Therefore Tomcat cannot be run on GAE (unless on flexible env.) - this rules
out "C and D"
upvoted 
2 
times
tartar
tartar
 
3 years, 4 months ago
A is ok
Community vote distribution
A (75%)
D (25%)A is ok
upvoted 
5 
times
Jphix
Jphix
 
2 years, 12 months ago
agreed, A. For those saying C, the question is about "automating the deployment" in line with the business requirements.
Going from MySQL to datastore might be a good idea long term, but it won't make automating the deployment to the
cloud any easier or smoother. Automate the deployment to Cloud SQL because it's a natural fit, and once that's working, re-
assess the requirements to decide if it's worth the hefty lift of shifting from MySQL to a NoSQL Document DB.
upvoted 
1 
times
Eroc
Eroc
 
Highly Voted
 
4 years, 2 months ago
The requriements also specify:
"Easily create non-production environment in the cloud.
Implement an automation framework for provisioning resources in cloud.
Implement a continuous deployment process for deploying applications to the on-premises datacenter or cloud."
So A is better.
upvoted 
11 
times
SSQX
SSQX
 
3 years, 9 months ago
You can only deploy Jenkins with Cloud Launcher, not with Deployment manager
upvoted 
2 
times
Ayzen
Ayzen
 
3 years, 8 months ago
Jenkins is just an app that should be run on a VM. You definitely can use Deployment Manager to set up a VM with needed
image.
upvoted 
3 
times
rrope
rrope
 
Most Recent
 
1 week, 1 day ago
Selected Answer: 
A
A. Deploy Nginx and Tomcat using Cloud Deployment Manager to Compute Engine
upvoted 
1 
times
theBestStudent
theBestStudent
 
3 weeks, 4 days ago
Selected Answer: 
D
For me is D:
- Deploy NGINX and and Tomcat to App Engine, so both can scale up and down automatically
- Deploy MySQL server using Cloud Launcer (nowadays called Marketplace)
- Deploy Jenkins to Compute Engine using Cloud Launcher (nowadays called MarketPlace): 
Here literally they are choosing an
instance (a compute instance to do so through MarketPlace) https://cloud.google.com/architecture/using-jenkins-for-
distributed-builds-on-compute-engine.
Answer A can not be. it talks about SQL Server, why to bring that? Plus the way they want ton tackle Jenkins installation makes
no sense if you already have MarketPlace. Also I'm ok that compute instances for NGINX and Tomcat could fit, BUT it doesn't
talk about MIG or not MIG. It is not ensuring right declaration to have MIG and scale them up down through it will be in place.
Answer is D.
upvoted 
1 
times
tuan072090
tuan072090
 
3 months, 3 weeks ago
Selected Answer: 
A
A is the most sense answer
upvoted 
1 
times
joe2211
joe2211
 
2 years, 1 month ago
Selected Answer: 
A
vote A
upvoted 
1 
times
victory108
victory108
 
2 years, 5 months agoA. Deploy Nginx and Tomcat using Cloud Deployment Manager to Compute Engine. Deploy a Cloud SQL server to replace
MySQL. Deploy Jenkins using Cloud Deployment Manager.
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
2 years, 5 months ago
Answer is A
upvoted 
2 
times
gosi
gosi
 
2 years, 8 months ago
D. With produciton parity, you cant replace MySQL with 128 GB of memory with Cloud SQL as there is no such image available.
I have checked it. MySQL has to go on GCE with PD
I would go for either go for B or D.
D is better because it is scalable better than B as B has no details if it is going to use MIG or just fleet of tomcat servers for web
apps.
upvoted 
1 
times
Ausias18
Ausias18
 
2 years, 9 months ago
Answer is A
upvoted 
1 
times
vruizm
vruizm
 
2 years, 10 months ago
I think B is a valid response, please check:
https://cloud.google.com/blog/products/it-ops/google-cloud-launcher-simplifies-running-third-party-apps-in-the-cloud
and
https://medium.com/@PeetDenny/automated-provisioning-of-jenkins-on-google-cloud-c297b2e0be2
upvoted 
2 
times
bnlcnd
bnlcnd
 
2 years, 11 months ago
the question and the answers are so confusing. what is "Cloud Launcher"? Never heard of it.
Only A does not mention that launcher thingy. I can only choose A.
upvoted 
2 
times
Wira
Wira
 
2 years, 9 months ago
its an old question - its cloud marketplace now
given size of mysql and type of data, the only valid choice is C for me
upvoted 
2 
times
pawel_ski
pawel_ski
 
2 years, 9 months ago
It's the previous name of GCP Marketplace.
upvoted 
1 
times
ybe_gcp_cert
ybe_gcp_cert
 
2 years, 11 months ago
A or B;
B doesn't tell which automation tool is used to deploy Cloud SQL. Cloud launcher generates Cloud Deployment Manager
scripts. I would go with B
upvoted 
1 
times
ybe_gcp_cert
ybe_gcp_cert
 
2 years, 11 months ago
Sorry A doesn't tell which tool is used to deploy Cloud SQL. 
I would go with B.
upvoted 
1 
times
Mndwsk
Mndwsk
 
3 years ago
B. 
Only option that automates the deployment of all the tools mentioned.
Cloud Launcher creates a Deployment in Deployment Manager.
upvoted 
1 
times
SKSKSK
SKSKSK
 
3 years, 1 month agoAfter reading the question more and kind of linking back to question one, i think it's asking how to "automate the deployment"
of web and transactional data layers". 
In that case, I think focus on deployment automation of existing technology might be a
better than mapping new cloud technology in this case? so, A might be a better fit?
upvoted 
1 
times
homer_simpson
homer_simpson
 
3 years, 2 months ago
the answer is A because datastore is nosql db and in business requirements it is clarly sais that improve bussiness agility and
speed innovation through rapid provisoning of new ressources
upvoted 
1 
times
brati_sankar
brati_sankar
 
3 years, 3 months ago
I believe this is D. Here is my logic.
In D we are using a MySQL from the Marketplace. Presently, on-prem the amount of data is 600 TB (1 PB SAN for MySQL of
which 400 TB is free) . This would not go in Cloud SQL which has a limit of 30 TB. Hence, we must go for MySQL on compute
using Launcher/Marketplace.
upvoted 
4 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 12
Question #3
For this question, refer to the Dress4Win case study. Which of the compute services should be migrated as-is and would still be
an optimized architecture for performance in the cloud? 
A. 
Web applications deployed using App Engine standard environment
B. 
RabbitMQ deployed using an unmanaged instance group
C. 
Hadoop/Spark deployed using Cloud Dataproc Regional in High Availability mode 
Most Voted
D. 
Jenkins, monitoring, bastion hosts, security scanners services deployed on custom machine types
Correct Answer:
 
C 
Comments
Hemant_C
Hemant_C
 
Highly Voted
 
3 years ago
Question is about compute services to be migrated as ""is and would still be an optimized architecture for performance -
Apache Hadoop/Spark servers underline is compute and Hadoop/Spark deployed using Cloud Dataproc 
seems to be the
correct answer.. Hence 
C seems correct answer to me
upvoted 
29 
times
SAMBIT
SAMBIT
 
1 year, 3 months ago
They are not sure which components of their architecture they can migrate as is and which components they need to change
before migrating them.
upvoted 
1 
times
jcmoranp
jcmoranp
 
Highly Voted
 
3 years, 8 months ago
It's D. You cannot migrate to APP Engine "as-is"
upvoted 
19 
times
tartar
tartar
 
2 years, 10 months ago
C is ok
upvoted 
11 
times
army234
army234
 
2 years, 3 months ago
Community vote distribution
C (100%)C is correct
upvoted 
7 
times
akhilesh_pundir
akhilesh_pundir
 
Most Recent
 
5 months ago
Read the previous questions ... they are going to use Managed instance groups with Tomcat &nginx installed on that so app
engine is not in picture. Hadoop workloads goes to dataproc as it is.
upvoted 
1 
times
OrangeTiger
OrangeTiger
 
1 year, 5 months ago
Selected Answer: 
C
I agree with C.
'as-is'
upvoted 
4 
times
ABO_Doma
ABO_Doma
 
1 year, 6 months ago
Google Cloud includes Dataproc, which is a managed Hadoop and Spark environment. You can use Dataproc to run most of
your existing jobs with minimal alteration, so you don't need to move away from all of the Hadoop tools you already know.
upvoted 
2 
times
ABO_Doma
ABO_Doma
 
1 year, 6 months ago
Selected Answer: 
C
Answer is C
upvoted 
2 
times
joe2211
joe2211
 
1 year, 7 months ago
Selected Answer: 
C
vote C
upvoted 
2 
times
victory108
victory108
 
1 year, 11 months ago
C. Hadoop/Spark deployed using Cloud Dataproc Regional in High Availability mode
upvoted 
6 
times
MamthaSJ
MamthaSJ
 
1 year, 12 months ago
Answer is C
upvoted 
4 
times
Ausias18
Ausias18
 
2 years, 2 months ago
Answer is C
upvoted 
2 
times
hkmsn
hkmsn
 
2 years, 4 months ago
A. Web applications deployed using App Engine standard environment - there are multiple web apps, seems project limit of 1 -
and 
not clear on the implications of Standard Env, with Nginx (there seems to be discussions) -- no not clear on this.
B. RabbitMQ - is always replaced by Pub/Sub - So No.
C. Hadoop/Spark 
- This is a well know Use Case
D. Jenkins, 
Etc, these duplicate GCP products so it can't be the answer.
My bet is C
upvoted 
2 
times
ahmedemad3
ahmedemad3
 
2 years, 4 months ago
ans: C
compute services should be migrated as is and would still be an optimized architecture for performance in the cloud?
upvoted 
1 
times
bnlcnd
bnlcnd
 
2 years, 5 months ago
It's C. hardoop == dataproc. pretty much a cloud version.
D is "Jenkins, monitoring, bastion hosts, security scanners". How can you make them as-is to run in cloud? Monitoring? on-premD is "Jenkins, monitoring, bastion hosts, security scanners". How can you make them as-is to run in cloud? Monitoring? on-prem
to cloud no change? security scanner? no change?
upvoted 
3 
times
BobBui
BobBui
 
2 years, 5 months ago
I choose C
upvoted 
1 
times
okixavi
okixavi
 
2 years, 6 months ago
C is the correct answer. The question says: "...as is"
upvoted 
1 
times
practicioner
practicioner
 
2 years, 8 months ago
C and D make sense. However, "would still be an optimized architecture". In this case, I chose C because we can move our
services as is and we can get significant benefits from GCP
upvoted 
1 
times
gcparchitect007
gcparchitect007
 
2 years, 8 months ago
C is correct answer.
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 12
Question #4
For this question, refer to the Dress4Win case study. To be legally compliant during an audit, Dress4Win must be able to give
insights in all administrative actions that modify the configuration or metadata of resources on Google Cloud. 
What should you do? 
A. 
Use Stackdriver Trace to create a Trace list analysis.
B. 
Use Stackdriver Monitoring to create a dashboard on the project's activity.
C. 
Enable Cloud Identity-Aware Proxy in all projects, and add the group of Administrators as a member.
D. 
Use the Activity page in the GCP Console and Stackdriver Logging to provide the required insight. 
Most Voted
Correct Answer:
 
D 
Comments
MeasService
MeasService
 
Highly Voted
 
2 years, 2 months ago
D is the correct answer !
https://cloud.google.com/logging/docs/audit/
upvoted 
49 
times
nitinz
nitinz
 
10 months ago
ans is D
upvoted 
3 
times
mikey007
mikey007
 
1 year, 6 months ago
Agree,...
upvoted 
3 
times
newbie2020
newbie2020
 
1 year, 11 months ago
Agree, Answer is D
upvoted 
4 
times
Eroc
Eroc
 
2 years, 2 months ago
I agree
Community vote distribution
D (100%)I agree
upvoted 
3 
times
joe2211
joe2211
 
Most Recent
 
1 month, 1 week ago
Selected Answer: 
D
vote D
upvoted 
2 
times
Shahariargcppca
Shahariargcppca
 
2 months, 1 week ago
answer is d
upvoted 
1 
times
victory108
victory108
 
5 months, 2 weeks ago
D. Use the Activity page in the GCP Console and Stackdriver Logging to provide the required insight.
upvoted 
1 
times
MamthaSJ
MamthaSJ
 
5 months, 4 weeks ago
Answer is D..
upvoted 
3 
times
Ausias18
Ausias18
 
9 months, 1 week ago
Answer is D
upvoted 
1 
times
lynx256
lynx256
 
9 months, 1 week ago
IMO - D is ok
upvoted 
2 
times
pihuanshu
pihuanshu
 
11 months ago
D should be
upvoted 
2 
times
bnlcnd
bnlcnd
 
11 months ago
D for sure
upvoted 
2 
times
Chulbul_Pandey
Chulbul_Pandey
 
1 year, 1 month ago
D is the choice
upvoted 
1 
times
gcparchitect007
gcparchitect007
 
1 year, 2 months ago
D is the right answer.
upvoted 
1 
times
homer_simpson
homer_simpson
 
1 year, 2 months ago
the answer is D 
Admin Activity audit logs
Admin Activity audit logs contain log entries for API calls or other administrative actions that modify the configuration or
metadata of resources. For example, these logs record when users create VM instances or change Identity and Access
Management permissions.
To view these logs, you must have the IAM role Logging/Logs Viewer or Project/Viewer.
upvoted 
1 
times
Kabiliravi
Kabiliravi
 
1 year, 4 months ago
D is correct
upvoted 
1 
times
 
1 year, 4 months agowiqi
wiqi
 
1 year, 4 months ago
D is correct.
upvoted 
1 
times
mbiy
mbiy
 
1 year, 4 months ago
D is the correct option
upvoted 
1 
times
ry9280087
ry9280087
 
1 year, 5 months ago
Seriously GCP must have written these answers as poison pills.
upvoted 
3 
times
mlantonis
mlantonis
 
1 year, 6 months ago
Yeah D is the correct
upvoted 
2 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 12
Question #5
For this question, refer to the Dress4Win case study. You are responsible for the security of data stored in Cloud Storage for
your company, Dress4Win. You have already created a set of Google Groups and assigned the appropriate users to those
groups. You should use Google best practices and implement the simplest design to meet the requirements. 
Considering Dress4Win's business and technical requirements, what should you do? 
A. 
Assign custom IAM roles to the Google Groups you created in order to enforce security requirements. Encrypt data with a
customer-supplied encryption key when storing files in Cloud Storage.
B. 
Assign custom IAM roles to the Google Groups you created in order to enforce security requirements. Enable default
storage encryption before storing files in Cloud Storage.
C. 
Assign predefined IAM roles to the Google Groups you created in order to enforce security requirements. Utilize Google's
default encryption at rest when storing files in Cloud Storage. 
Most Voted
D. 
Assign predefined IAM roles to the Google Groups you created in order to enforce security requirements. Ensure that the
default Cloud KMS key is set before storing files in Cloud Storage.
Correct Answer:
 
C 
Comments
JoeShmoe
JoeShmoe
 
Highly Voted
 
5 years, 1 month ago
C is the simplest
upvoted 
34 
times
AWS56
AWS56
 
4 years, 11 months ago
I am a bit confused "You should use Google best practices and implement the simplest design to meet the requirements." 
--->
Simplest -- agree with D, but for googles best practice I will go with A
upvoted 
3 
times
AWS56
AWS56
 
4 years, 11 months ago
Ignore my comment, Agree C is the simple -- https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
upvoted 
4 
times
Community vote distribution
C (75%)
D (25%)tartar
tartar
 
4 years, 4 months ago
C is ok
upvoted 
5 
times
rockstar9622
rockstar9622
 
4 years, 11 months ago
c is correct - going by simplest design whereas google manages the encrytion though by default and thats sufficient
upvoted 
2 
times
nitinz
nitinz
 
3 years, 10 months ago
ans is C
upvoted 
3 
times
kimharsh
kimharsh
 
2 years, 9 months ago
how come it's C , and for best practice we need to use Custom Roles
upvoted 
1 
times
newbie2020
newbie2020
 
Highly Voted
 
4 years, 11 months ago
There 2 requirements
1) best practices = least privilege = custom role
2) simplest = default encryption as 
: If you use customer-supplied encryption keys or client-side encryption, you must securely manage your keys and ensure that
they are not lost. If you lose your keys, you are no longer able to read your data, and you continue to be charged for storage of
your objects until you delete them.
upvoted 
12 
times
Dannyygcp
Dannyygcp
 
4 years, 10 months ago
What about option B..default encryption[which is simple to manage] + Custom role[which is secure compared to predefined
and not difficult to create]
upvoted 
3 
times
sivass
sivass
 
4 years, 7 months ago
I agrre. I will go with B.
upvoted 
5 
times
GCP_Azure
GCP_Azure
 
4 years, 7 months ago
It has to be B
upvoted 
4 
times
Rafaa
Rafaa
 
4 years, 7 months ago
there is no option to 'enable default encyption' as such! It is provided by default if you dont do anything.
upvoted 
2 
times
Vika
Vika
 
3 years, 8 months ago
Check out this link - https://cloud.google.com/iam/docs/using-iam-securely 
Basic roles include thousands of permissions across all Google Cloud services. In production environments, do not grant basic
roles unless there is no alternative. Instead, grant the most limited predefined roles or custom roles that meet your needs.
upvoted 
1 
times
tlopsm
tlopsm
 
Most Recent
 
6 months, 3 weeks ago
Selected Answer: 
C
C is answer
upvoted 
1 
times
Ahmed_Safwat
Ahmed_Safwat
 
1 year, 1 month ago
Selected Answer: 
D
Encrypt Cloud Storage data with Cloud KMSEncrypt Cloud Storage data with Cloud KMS
upvoted 
1 
times
SAMBIT
SAMBIT
 
2 years, 9 months ago
B custom IAM & out of box encryption
upvoted 
1 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
C
vote C
upvoted 
2 
times
kopper2019
kopper2019
 
3 years, 5 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 5 months ago
hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
1 
times
victory108
victory108
 
3 years, 5 months ago
C. Assign predefined IAM roles to the Google Groups you created in order to enforce security requirements. Utilize Google
ג
™€s
default encryption at rest when storing files in Cloud Storage.
upvoted 
2 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is B
upvoted 
1 
times
wilwong
wilwong
 
3 years, 6 months ago
C is correct
upvoted 
1 
times
Pb55
Pb55
 
3 years, 8 months ago
C. Best practice is predefined not custom. Only use custom when predefined to broard.
upvoted 
1 
times
ansh0692
ansh0692
 
3 years, 8 months ago
From "Google's best practices and simplest design" Answer should be C
upvoted 
1 
times
Skeeter
Skeeter
 
3 years, 9 months ago
Cloud storage encryption is enabled by default. Why would you need to enable it as stated in B? Answer is A, use CSEK and
specify a .boto file during upload with gsutil, simple!
upvoted 
2 
times
Ausias18
Ausias18
 
3 years, 9 months ago
it says simple, what you say is not as easy as possible... default encryption is easier
upvoted 
1 
times
Ausias18
Ausias18
 
3 years, 9 months ago
Answer is B
upvoted 
1 
times
lynx256
lynx256
 
3 years, 9 months ago
IMO - C is ok.
Simplest --> predefined roles + default encryption
upvoted 
2 
timesupvoted 
2 
times
Rightsaidfred
Rightsaidfred
 
3 years, 11 months ago
C is the 'Google' answer here 
:)
upvoted 
1 
times 
Exam Professional Cloud Architect 
All Actual Questions
        
Topic 12
Question #6
For this question, refer to the Dress4Win case study. You want to ensure that your on-premises architecture meets business
requirements before you migrate your solution. 
What change in the on-premises architecture should you make? 
A. 
Replace RabbitMQ with Google Pub/Sub.
B. 
Downgrade MySQL to v5.7, which is supported by Cloud SQL for MySQL.
C. 
Resize compute resources to match predefined Compute Engine machine types.
D. 
Containerize the micro-services and host them in Google Kubernetes Engine. 
Most Voted
Correct Answer:
 
D 
Comments
chiar
chiar
 
Highly Voted
 
5 years, 1 month ago
Be careful, because in the case study that you can find in google website MySQL version is 5.7
https://cloud.google.com/certification/guides/cloud-architect/casestudy-dress4win-rev2
upvoted 
20 
times
jasim21
jasim21
 
3 years, 8 months ago
cloud SQL support MySQL 5.7 
https://cloud.google.com/sql/docs/mysql/db-versions
Answer is D
upvoted 
8 
times
crypt0
crypt0
 
Highly Voted
 
5 years, 2 months ago
I would tend to answer B "Second Generation instances support MySQL 5.6 or 5.7,"
https://cloud.google.com/sql/docs/mysql/features
upvoted 
14 
times
chiar
chiar
 
5 years, 2 months ago
I agree, the answer is B
Community vote distribution
D (94%)
B (6%)I agree, the answer is B
upvoted 
7 
times
addy007
addy007
 
5 years ago
Downgrading is not supported. Seems C is the right choice.
https://dev.mysql.com/doc/refman/8.0/en/downgrading.html
upvoted 
4 
times
xps
xps
 
4 years, 8 months ago
Downgrade is possible from 5.7 to 5.6, which is in the same major version. So downgrade from 5.8 to 5.7 makes sense.
Containerize the java applications require to build kubernetes infrastructures in the on-premise environment, it's not in the
plan. So the answer goes to B.
upvoted 
3 
times
6a8c7ad
6a8c7ad
 
Most Recent
 
4 months, 4 weeks ago
It’s not D. Says on prem change prior to migrate. Definitely not D.
upvoted 
1 
times
jcataluna
jcataluna
 
1 year ago
Selected Answer: 
B
MySQL 5.8 not supported on Cloud SQL
upvoted 
1 
times
SAMBIT
SAMBIT
 
2 years, 9 months ago
Replace RabbitMQ with pub sub …A
https://docs.devicewise.com/Content/Products/GatewayDevelopersGuide/CloudConnectors/GoogleCloud/GoogleCloudPlatfor
m.htm
upvoted 
3 
times
ABO_Doma
ABO_Doma
 
3 years ago
Selected Answer: 
D
Containerizing the existing applications ensures efficient use of resources. This activity the business requirement “optimize
architecture for performance in the cloud”. As a precursor to Cloud migration, you could convert the microservices to containers
and host them on GKE on-prem: https://cloud.google.com/anthos/gke/docs/on-prem/overview which also makes it very easy
for you to migrate to Cloud. GKE on-prem is hybrid cloud software that brings Google Kubernetes Engine (GKE) to on-premises
data centres. With GKE on-prem, you can create, manage, and upgrade Kubernetes clusters in your on-premises environment.
upvoted 
11 
times
didek1986
didek1986
 
3 years, 1 month ago
D read business req.
upvoted 
2 
times
kvenkatasudhakar
kvenkatasudhakar
 
3 years, 1 month ago
Cloud SQL supports MySQL 5.6, 5.7 and 8.0 and the current onprem version is MySQL 5.8. 
So downgrade from 5.8 to 5.7 is the
right answer (D).
upvoted 
1 
times
phantomsg
phantomsg
 
3 years, 1 month ago
Selected Answer: 
D
'D' as it matches the requirement - Improve business agility and speed of innovation through rapid provisioning of new
resources. Among the choices, containerizing microservices will allow the company to deploy and scale services independantly.
upvoted 
4 
times
joe2211
joe2211
 
3 years, 1 month ago
Selected Answer: 
D
vote D
upvoted 
1 
times
kopper2019
kopper2019
 
3 years, 5 months agohey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152
upvoted 
3 
times
vishwassahu
vishwassahu
 
3 years, 3 months ago
it seems 21 new questions are deleted
upvoted 
2 
times
victory108
victory108
 
3 years, 5 months ago
D. Containerize the micro-services and host them in Google Kubernetes Engine.
upvoted 
6 
times
MamthaSJ
MamthaSJ
 
3 years, 5 months ago
Answer is D
upvoted 
4 
times
wilwong
wilwong
 
3 years, 6 months ago
Answer is D
upvoted 
2 
times
getzsagar
getzsagar
 
3 years, 8 months ago
One important Update related to case study - Date - 27-04-2021
Right answer is option D --- 
Case Study for Dress4win is updated, MYSQL version 5.7 
is mentioned in the case study and not 5.8 as given in here. This makes
option B invalid. Right answer is option D. In the exam I saw option B was still given to confuse people, but in the case study
they mentioned the MYSQL version 5.7 and not 5.8. So despite of being confident about the case studies, ensure that you read it
thoroughly even during the exam. Time is given sufficient enough.
upvoted 
7 
times
ansh0692
ansh0692
 
3 years, 8 months ago
Answer is D
A: If you are expecting to "install" pub/sub locally and not connect to the hosted GCP service, you cannot use pub/sub, if it is
okay for you to use hosted pub/sub and then do the rest of the processing on prem but you won't get a readily available
system or you'll have to integrate a lot of things.
B: 5.8 is basically 8.0 and it is supported
C: Best practice is to find the predefined types on GCP as close to the physical server spec and not the other way around.
upvoted 
4 
times
Ausias18
Ausias18
 
3 years, 9 months ago
Answer is D
upvoted 
5 
times